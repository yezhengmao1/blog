# Paper Read

* [Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning](./91-ASPLOS%2024/Centauri%20Enabling%20Efficient%20Scheduling%20for%20%20Communication-Computation%20Overlap%20in%20Large%20Model%20%20Training%20via%20Communication%20Partitioning.md)
* [Slapo: A schedule language for progressive optimization of large deep learning model training](./91-ASPLOS%2024/Slapo%20A%20Schedule%20Language%20for%20Progressive%20Optimization%20of%20Large%20Deep%20Learning%20Model%20Training.md)
* [Optimus Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation](./92-arXiv%2024/Optimus%20Accelerating%20Large-Scale%20Multi-Modal%20LLM%20Training%20by%20Bubble%20Exploitation.md)
* [ASPEN Breaking Operator Barriers for Efficient Parallel Execution of Deep Neural Networks](./93-NIPS%2023/ASPEN%20Breaking%20Operator%20Barriers%20for%20Efficient%20Parallel%20Execution%20of%20Deep%20Neural%20Networks.md)
* [Breaking the computation and communication abstraction barrier in distributed machine learning workloads](./94-ASPLOS%2022/Breaking%20the%20computation%20and%20communication%20abstraction%20barrier%20in%20distributed%20machine%20learning%20workloads.md)
* [Alpa: Automating inter-and Intra-Operator parallelism for distributed deep learning](./95-OSDI%2022/Alpa:%20Automating%20inter-and%20{Intra-Operator}%20parallelism%20for%20distributed%20deep%20learning.md)
* [Nimble: Lightweight and parallel gpu task scheduling for deep learning](./96-NIPS%2020/Nimble:%20Lightweight%20and%20parallel%20gpu%20task%20scheduling%20for%20deep%20learning.md)