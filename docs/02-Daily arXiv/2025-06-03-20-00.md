# 2025-06-03-20-00

## [SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization](https://arxiv.org/abs/2411.10958)

### Abstract
arXiv:2411.10958v5 Announce Type: replace-cross 
Abstract: Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrices $(\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK^\top$. Third, we propose a two-level accumulation strategy for $\widetilde PV$ to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 4.5x on RTX4090, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation. The code is available at https://github.com/thu-ml/SageAttention.

### 摘要
尽管线性层的量化技术已得到广泛应用，但其在注意力计算加速领域的应用仍存在局限。为在保持精度的前提下进一步提升注意力计算效率（相较于SageAttention），我们提出SageAttention2方案，该方案采用速度显著提升的4位矩阵乘法（Matmul）并集成多项精度增强技术。首先，我们提出以硬件友好的线程级粒度将矩阵$(Q, K)$量化为INT4格式，同时将矩阵$(\widetilde P, V)$量化为FP8格式。其次，我们提出$Q$矩阵平滑处理方法以提升INT4格式$QK^\top$的计算精度。第三，针对$\widetilde PV$计算提出两级累加策略以增强FP8格式$\widetilde PV$的精度。在RTX4090显卡上，SageAttention2的每秒运算次数（OPS）分别达到FlashAttention2和xformers的约3倍和4.5倍。此外，在Hopper架构GPU上，SageAttention2在保持与FlashAttention3(fp8)相当速度的同时实现了更高精度。综合实验表明，我们的方法在语言、图像及视频生成等多种模型上仅产生可忽略的端到端指标损失。项目代码已开源：https://github.com/thu-ml/SageAttention。

---

