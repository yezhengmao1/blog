# 2025-06-03-12-13

## [Literature Review Of Multi-Agent Debate For Problem-Solving](https://arxiv.org/abs/2506.00066)

### Abstract
arXiv:2506.00066v1 Announce Type: new 
Abstract: Multi-agent large language models (MA-LLMs) are a rapidly growing research area that leverages multiple interacting language agents to tackle complex tasks, outperforming single-agent large language models. This literature review synthesizes the latest research on agent profiles, communication structures, and decision-making processes, drawing insights from both traditional multi-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims to address the lack of direct comparisons in the field, illustrating how factors like scalability, communication structure, and decision-making processes influence MA-LLM performance. By examining frequent practices and outlining current challenges, the review reveals that multi-agent approaches can yield superior results but also face elevated computational costs and under-explored challenges unique to MA-LLM. Overall, these findings provide researchers and practitioners with a roadmap for developing robust and efficient multi-agent AI solutions.

### 摘要
多智能体大语言模型（MA-LLMs）是一个快速发展的研究领域，其通过多个交互的语言智能体处理复杂任务，性能优于单智能体大语言模型。本文献综述综合了关于智能体配置、通信结构和决策过程的最新研究，从传统多智能体系统和前沿MA-LLM研究中汲取见解。通过这一工作，旨在解决该领域缺乏直接比较的问题，阐明可扩展性、通信结构和决策过程等因素如何影响MA-LLM的性能。通过分析常见实践并概述当前挑战，本综述表明多智能体方法能够产生更优结果，但也面临更高的计算成本和MA-LLM特有的未充分探索的挑战。总体而言，这些发现为研究人员和实践者开发鲁棒高效的多智能体人工智能解决方案提供了路线图。

---

## [Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise](https://arxiv.org/abs/2506.00242)

### Abstract
arXiv:2506.00242v1 Announce Type: new 
Abstract: The integration of large language models (LLMs) into global applications necessitates effective cultural alignment for meaningful and culturally-sensitive interactions. Current LLMs often lack the nuanced understanding required for diverse cultural contexts, and adapting them typically involves costly full fine-tuning. To address this, we introduce a novel soft prompt fine-tuning framework that enables efficient and modular cultural alignment. Our method utilizes vectorized prompt tuning to dynamically route queries to a committee of culturally specialized 'expert' LLM configurations, created by optimizing soft prompt embeddings without altering the base model's parameters. Extensive experiments demonstrate that our framework significantly enhances cultural sensitivity and adaptability, improving alignment scores from 0.208 to 0.820, offering a robust solution for culturally-aware LLM deployment. This research paves the way for subsequent investigations into enhanced cultural coverage and dynamic expert adaptation, crucial for realizing autonomous AI with deeply nuanced understanding in a globally interconnected world.

### 摘要
将大型语言模型（LLMs）整合到全球应用中，需要有效的文化对齐以实现有意义且具有文化敏感性的交互。当前LLMs往往缺乏对不同文化背景的细微理解，而适应这些背景通常涉及成本高昂的完整微调。为此，我们提出了一种新颖的软提示微调框架，能够实现高效且模块化的文化对齐。该方法利用向量化提示调优，动态将查询路由至由文化专用“专家”LLM配置组成的委员会，这些配置通过优化软提示嵌入创建，无需修改基础模型的参数。大量实验表明，我们的框架显著提升了文化敏感性和适应性，将对齐分数从0.208提高到0.820，为文化感知的LLM部署提供了稳健解决方案。本研究为后续研究铺平了道路，包括增强文化覆盖范围和动态专家适应，这对于在全球互联世界中实现具有深度细微理解的自主AI至关重要。

---

## [Toward Knowledge-Guided AI for Inverse Design in Manufacturing: A Perspective on Domain, Physics, and Human-AI Synergy](https://arxiv.org/abs/2506.00056)

### Abstract
arXiv:2506.00056v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is reshaping inverse design across manufacturing domain, enabling high-performance discovery in materials, products, and processes. However, purely data-driven approaches often struggle in realistic settings characterized by sparse data, high-dimensional design spaces, and nontrivial physical constraints. This perspective argues for a new generation of design systems that transcend black-box modeling by integrating domain knowledge, physics-informed learning, and intuitive human-AI interfaces. We first demonstrate how expert-guided sampling strategies enhance data efficiency and model generalization. Next, we discuss how physics-informed machine learning enables physically consistent modeling in data-scarce regimes. Finally, we explore how large language models emerge as interactive design agents connecting user intent with simulation tools, optimization pipelines, and collaborative workflows. Through illustrative examples and conceptual frameworks, we advocate that inverse design in manufacturing should evolve into a unified ecosystem, where domain knowledge, physical priors, and adaptive reasoning collectively enable scalable, interpretable, and accessible AI-driven design systems.

### 摘要
人工智能（AI）正在重塑制造领域的逆向设计，推动材料、产品和工艺的高性能发现。然而，纯数据驱动方法在数据稀疏、高维设计空间和复杂物理约束的现实场景中往往表现不佳。本文提出新一代设计系统应超越黑箱建模，通过整合领域知识、物理信息学习和直观的人机交互界面来实现突破。我们首先论证专家引导的采样策略如何提升数据效率和模型泛化能力；其次探讨物理信息机器学习如何在数据稀缺条件下实现物理一致性建模；最后分析大型语言模型如何作为交互式设计代理，将用户意图与仿真工具、优化流程及协作工作流相连接。通过案例分析和概念框架，我们主张制造业逆向设计应发展为融合领域知识、物理先验和自适应推理的统一生态系统，从而构建可扩展、可解释且易用的AI驱动设计系统。

---

## [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)

### Abstract
arXiv:2506.00073v1 Announce Type: new 
Abstract: AI agents are increasingly used in consumer-facing applications to assist with tasks such as product search, negotiation, and transaction execution. In this paper, we explore a future scenario where both consumers and merchants authorize AI agents to fully automate negotiations and transactions. We aim to answer two key questions: (1) Do different LLM agents vary in their ability to secure favorable deals for users? (2) What risks arise from fully automating deal-making with AI agents in consumer markets? To address these questions, we develop an experimental framework that evaluates the performance of various LLM agents in real-world negotiation and transaction settings. Our findings reveal that AI-mediated deal-making is an inherently imbalanced game -- different agents achieve significantly different outcomes for their users. Moreover, behavioral anomalies in LLMs can result in financial losses for both consumers and merchants, such as overspending or accepting unreasonable deals. These results underscore that while automation can improve efficiency, it also introduces substantial risks. Users should exercise caution when delegating business decisions to AI agents.

### 摘要
人工智能代理在面向消费者的应用中日益普及，用于协助完成产品搜索、谈判和交易执行等任务。本文探讨了一种未来场景，即消费者和商家均授权AI代理全自动进行谈判与交易。我们致力于回答两个关键问题：(1) 不同大型语言模型代理在为用户争取有利交易方面的能力是否存在差异？(2) 在消费市场中完全自动化AI代理交易会引发哪些风险？为解决这些问题，我们开发了一个实验框架，用于评估各类LLM代理在真实谈判与交易场景中的表现。研究发现：AI中介的交易本质上是一场不平衡的博弈——不同代理为其用户达成的交易结果存在显著差异。此外，大型语言模型的行为异常可能导致消费者和商家遭受经济损失，例如超额支出或接受不合理交易。这些结果表明，虽然自动化能提升效率，但也会带来重大风险。用户将商业决策委托给AI代理时需保持谨慎。

---

## [Evaluation of LLMs for mathematical problem solving](https://arxiv.org/abs/2506.00309)

### Abstract
arXiv:2506.00309v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive performance on a range of educational tasks, but are still understudied for their potential to solve mathematical problems. In this study, we compare three prominent LLMs, including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of varying complexities (GSM8K, MATH500, and UNSW datasets). We take a five-dimensional approach based on the Structured Chain-of-Thought (SCoT) framework to assess final answer correctness, step completeness, step validity, intermediate calculation accuracy, and problem comprehension. The results show that GPT-4o is the most stable and consistent in performance across all the datasets, but particularly it performs outstandingly in high-level questions of the UNSW dataset. DeepSeek-V3 is competitively strong in well-structured domains such as optimisation, but suffers from fluctuations in accuracy in statistical inference tasks. Gemini-2.0 shows strong linguistic understanding and clarity in well-structured problems but performs poorly in multi-step reasoning and symbolic logic. Our error analysis reveals particular deficits in each model: GPT-4o is at times lacking in sufficient explanation or precision; DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in mathematical reasoning in higher dimensions.

### 摘要
大语言模型（LLMs）在一系列教育任务中展现出卓越性能，但其解决数学问题的潜力仍未得到充分研究。本研究基于结构化思维链（SCoT）框架，从五个维度（最终答案正确性、步骤完整性、步骤有效性、中间计算准确性和问题理解度）比较了GPT-4o、DeepSeek-V3和Gemini-2.0三个主流大语言模型在GSM8K、MATH500和UNSW三个不同复杂度数学数据集上的表现。结果表明：GPT-4o在所有数据集上表现最为稳定一致，尤其在UNSW数据集的高阶问题上表现突出；DeepSeek-V3在优化等结构化领域竞争力强，但在统计推断任务中存在准确率波动；Gemini-2.0在结构化问题中展现出优秀的语言理解与表述清晰度，但在多步推理和符号逻辑方面表现欠佳。错误分析揭示了各模型的特定缺陷：GPT-4o偶尔缺乏充分解释或精确性；DeepSeek-V3会遗漏中间步骤；Gemini-2.0在高维数学推理中灵活性不足。

---

## [MIR: Methodology Inspiration Retrieval for Scientific Research Problems](https://arxiv.org/abs/2506.00249)

### Abstract
arXiv:2506.00249v1 Announce Type: new 
Abstract: There has been a surge of interest in harnessing the reasoning capabilities of Large Language Models (LLMs) to accelerate scientific discovery. While existing approaches rely on grounding the discovery process within the relevant literature, effectiveness varies significantly with the quality and nature of the retrieved literature. We address the challenge of retrieving prior work whose concepts can inspire solutions for a given research problem, a task we define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset tailored for training and evaluating retrievers on MIR, and establish baselines. To address MIR, we build the Methodology Adjacency Graph (MAG); capturing methodological lineage through citation relationships. We leverage MAG to embed an "intuitive prior" into dense retrievers for identifying patterns of methodological inspiration beyond superficial semantic similarity. This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and +4.8 in mAP. Through extensive ablation studies and qualitative analyses, we exhibit the promise of MIR in enhancing automated scientific discovery and outline avenues for advancing inspiration-driven retrieval.

### 摘要
近年来，利用大语言模型(LLMs)的推理能力加速科学发现的研究兴趣激增。现有方法主要基于相关文献进行发现过程，但其效果因检索文献的质量和性质差异显著。我们致力于解决"检索能够为给定研究问题提供解决方案启发的先前工作"这一挑战，将其定义为方法启发检索(MIR)。为此，我们构建了一个专门用于MIR任务检索模型训练与评估的新型数据集，并建立了基线标准。针对MIR问题，我们建立了方法邻近图(MAG)，通过引用关系捕捉方法学谱系。利用MAG，我们将"直觉先验"嵌入密集检索器，以识别超越表面语义相似性的方法启发模式。该方法在Recall@3指标上较基线提升5.4，平均精度均值(mAP)提升7.8。进一步地，我们为MIR任务适配了基于LLM的重排序策略，使Recall@3再提升4.5，mAP提升4.8。通过大量消融实验和定性分析，我们展示了MIR在增强自动化科学发现方面的潜力，并提出了推进启发驱动检索的发展路径。

---

## [Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings](https://arxiv.org/abs/2506.00178)

### Abstract
arXiv:2506.00178v1 Announce Type: new 
Abstract: Prompt engineering represents a critical bottleneck to harness the full potential of Large Language Models (LLMs) for solving complex tasks, as it requires specialized expertise, significant trial-and-error, and manual intervention. This challenge is particularly pronounced for tasks involving subjective quality assessment, where defining explicit optimization objectives becomes fundamentally problematic. Existing automated prompt optimization methods falter in these scenarios, as they typically require well-defined task-specific numerical fitness functions or rely on generic templates that cannot capture the nuanced requirements of complex use cases. We introduce DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that guides prompt evolution through a debate-driven evaluation with an Elo-based selection. Contrary to prior work, DEEVOs approach enables exploration of the discrete prompt space while preserving semantic coherence through intelligent crossover and strategic mutation operations that incorporate debate-based feedback, combining elements from both successful and unsuccessful prompts based on identified strengths rather than arbitrary splicing. Using Elo ratings as a fitness proxy, DEEVO simultaneously drives improvement and preserves valuable diversity in the prompt population. Experimental results demonstrate that DEEVO significantly outperforms both manual prompt engineering and alternative state-of-the-art optimization approaches on open-ended tasks and close-ended tasks despite using no ground truth feedback. By connecting LLMs reasoning capabilities with adaptive optimization, DEEVO represents a significant advancement in prompt optimization research by eliminating the need of predetermined metrics to continuously improve AI systems.

### 摘要
提示工程是充分发挥大语言模型(LLM)解决复杂任务潜力的关键瓶颈，因其需要专业知识、大量试错和人工干预。这一挑战在涉及主观质量评估的任务中尤为突出，因为定义明确的优化目标存在根本性困难。现有自动化提示优化方法在这些场景中表现不佳，它们通常需要明确定义的特定任务数值适应度函数，或依赖无法捕捉复杂用例细微需求的通用模板。我们提出DEEVO(基于辩论的进化提示优化框架)，该创新框架通过基于Elo评分的辩论驱动评估来引导提示进化。与先前工作不同，DEEVO方法能在保持语义连贯性的同时探索离散提示空间，这得益于融合辩论反馈的智能交叉和策略性变异操作——根据已识别的优势(而非随机拼接)结合成功与失败提示的要素。采用Elo评分作为适应度代理，DEEVO在推动改进的同时保持了提示种群的宝贵多样性。实验结果表明，在无需真实反馈的情况下，DEEVO在开放式和封闭式任务上的表现均显著优于人工提示工程和其他最先进的优化方法。通过将LLM的推理能力与自适应优化相结合，DEEVO消除了对预设指标的依赖，实现了AI系统的持续改进，代表了提示优化研究的重大进展。

---

## [Control-R: Towards controllable test-time scaling](https://arxiv.org/abs/2506.00189)

### Abstract
arXiv:2506.00189v1 Announce Type: new 
Abstract: This paper target in addressing the challenges of underthinking and overthinking in long chain-of-thought (CoT) reasoning for Large Reasoning Models (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time approach that injects structured control signals to guide reasoning from a tree search perspective. RCF enables models to adjust reasoning effort according to given control conditions when solving complex tasks. Additionally, we present the Control-R-4K dataset, which consists of challenging problems annotated with detailed reasoning processes and corresponding control fields. To further enhance reasoning control, we propose a Conditional Distillation Finetuning (CDF) method, which trains model--particularly Control-R-32B--to effectively adjust reasoning effort during test time. Experimental results on benchmarks such as AIME2024 and MATH500 demonstrate that our approach achieves state-of-the-art performance at the 32B scale while enabling a controllable Long CoT reasoning process (L-CoT). Overall, this work introduces an effective paradigm for controllable test-time scaling reasoning.

### 摘要
本文旨在通过引入推理控制场（RCF）——一种从树搜索视角注入结构化控制信号以引导推理的新型测试时方法，解决大型推理模型（LRM）在长思维链（CoT）推理中存在的欠思考与过思考挑战。RCF使模型能够在解决复杂任务时根据给定控制条件动态调整推理强度。此外，我们提出Control-R-4K数据集，该数据集包含标注详细推理过程及对应控制场的复杂问题。为进一步增强推理控制，我们提出条件蒸馏微调（CDF）方法，专门训练Control-R-32B等模型在测试时有效调节推理强度。在AIME2024和MATH500等基准测试上的实验结果表明，我们的方法在320亿参数规模下实现了最先进性能，同时实现了可控的长思维链推理过程（L-CoT）。本研究整体提出了一个有效的测试时可控扩展推理范式。

---

## [Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents](https://arxiv.org/abs/2506.00320)

### Abstract
arXiv:2506.00320v1 Announce Type: new 
Abstract: Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent's world modeling ability via objectives such as state prediction or critique generation, and then improve the agent's action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent's in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.

### 摘要
近期在大型语言模型（如DeepSeek-R1）推理方面的进展，通过展现验证、目标分解和自我反思等复杂认知行为，在数学和编程等领域展示了令人印象深刻的能力。然而，对于长视野AI代理任务而言，哪些行为有效、哪些行为缺失尚不明确。本研究提出Dyna-Think思维框架，该框架通过将规划与内部世界模型相结合，整合推理与行动以提升AI代理性能。为实现Dyna-Think，我们提出Dyna-Think模仿学习（DIT）和Dyna-Think动态训练（DDT）。DIT通过重构R1的思维过程来初始化策略，聚焦于执行与拟议（及规划）行动相关的世界模型模拟，并利用重构数据进行策略训练。DDT采用两阶段训练流程，先通过状态预测或批评生成等目标提升代理的世界建模能力，再通过策略训练优化代理行动。我们在OSWorld上评估所提方法，结果表明Dyna-Think提升了代理的领域内及跨领域性能，在达到与R1相当的最佳n次性能的同时，平均生成标记数减少2倍。大量实证研究表明：1）利用批评生成进行世界模型训练能有效提升策略性能；2）性能更优的AI代理与更强的世界建模能力存在相关性。我们相信这些发现为将世界模型模拟整合至AI代理以增强其推理、规划与行动能力指明了有前景的研究方向。

---

## [A "Wenlu" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge](https://arxiv.org/abs/2506.00570)

### Abstract
arXiv:2506.00570v1 Announce Type: new 
Abstract: With the rapid penetration of artificial intelligence across industries and scenarios, a key challenge in building the next-generation intelligent core lies in effectively integrating the language understanding capabilities of foundation models with domain-specific knowledge bases in complex real-world applications. This paper proposes a multimodal cognition and embodied decision-making brain system, ``Wenlu", designed to enable secure fusion of private knowledge and public models, unified processing of multimodal data such as images and speech, and closed-loop decision-making from cognition to automatic generation of hardware-level code. The system introduces a brain-inspired memory tagging and replay mechanism, seamlessly integrating user-private data, industry-specific knowledge, and general-purpose language models. It provides precise and efficient multimodal services for enterprise decision support, medical analysis, autonomous driving, robotic control, and more. Compared with existing solutions, ``Wenlu" demonstrates significant advantages in multimodal processing, privacy security, end-to-end hardware control code generation, self-learning, and sustainable updates, thus laying a solid foundation for constructing the next-generation intelligent core.

### 摘要
随着人工智能在各行业和场景的快速渗透，构建下一代智能核心的关键挑战在于如何将基础模型的语言理解能力与复杂现实应用中的领域专用知识库有效融合。本文提出一种多模态认知与具身决策大脑系统"问辂"，旨在实现私有知识与公共模型的安全融合、图像与语音等多模态数据的统一处理，以及从认知到硬件级代码自动生成的闭环决策。该系统引入类脑记忆标记与回放机制，无缝整合用户私有数据、行业特定知识与通用语言模型，为企业决策支持、医疗分析、自动驾驶、机器人控制等领域提供精准高效的多模态服务。与现有方案相比，"问辂"在多模态处理、隐私安全、端到端硬件控制代码生成、自主学习和可持续更新等方面展现出显著优势，为构建下一代智能核心奠定了坚实基础。

---

## [Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language Models](https://arxiv.org/abs/2506.00258)

### Abstract
arXiv:2506.00258v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly deployed in open-ended, real-world environments where inputs are messy, underspecified, and not always trustworthy. Unlike curated benchmarks, these settings frequently involve instructions that refer to missing objects or contradictory facts, rely on ambiguous references, or request infeasible actions. In such cases, success hinges not on task execution alone, but on a model's ability to detect when something is silently wrong. This paper presents a systematic analysis of how current MLLMs handle such implicit reasoning scenarios: cases where the flaw is not explicitly stated but must be inferred from context. Using a curated diagnostic suite spanning four categories of real-world failure modes, we evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently fail to surface hidden issues, even when they possess the necessary perceptual and reasoning skills. Explicit prompting reveals that the underlying capabilities exist but are often suppressed in favor of user compliance. We further show that simple inference-time interventions, such as cautious persona prompting and, in particular, requiring a clarifying question, can dramatically recover performance. Our findings highlight a persistent gap between reasoning competence and behavioral compliance in current MLLMs and suggest practical strategies for making these models more trustworthy in underconstrained environments.

### 摘要
多模态大语言模型（MLLMs）正越来越多地被部署在开放的真实环境中，这些环境的输入信息混乱、不完整且并不总是可信。与经过筛选的基准测试不同，这些场景中的指令常常涉及引用缺失对象或矛盾事实、依赖模糊指代，或要求不可行的操作。在此类情况下，成功的关键不仅在于任务执行，更在于模型能否检测到潜在问题。本文系统分析了当前MLLMs如何处理此类隐含推理场景——即缺陷未被明确陈述而需通过上下文推断的情况。通过构建涵盖四类现实世界故障模式的诊断测试集，我们对包括o3和GPT-4o在内的六种MLLMs进行评估，发现即使模型具备必要的感知和推理能力，仍经常无法识别隐藏问题。显式提示表明，模型底层能力实际存在，但往往因服从用户需求而被抑制。我们进一步证明，简单的推理时干预措施（如谨慎的角色提示，特别是要求澄清问题）能显著提升性能。研究结果揭示了当前MLLMs在推理能力与行为服从性之间的持续差距，并为在约束不足环境中增强模型可信度提出了实用策略。

---

## [Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs](https://arxiv.org/abs/2506.00577)

### Abstract
arXiv:2506.00577v1 Announce Type: new 
Abstract: Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively $\textit&#123;generalize&#125;$ to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce $\textbf&#123;Recon&#125;$ ($\textbf&#123;R&#125;$easoning like an $\textbf&#123;ECON&#125;$omist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .

### 摘要
直接针对多智能体系统（MAS）训练大型语言模型（LLMs）仍面临挑战，原因包括复杂的奖励建模、动态的智能体交互以及严苛的泛化需求。本文探讨后训练技术——特别是监督微调（SFT）和可验证奖励的强化学习（RLVR）——能否有效$	extit&#123;泛化&#125;$到多智能体场景。我们以经济推理为测试平台，依托其坚实的数学与博弈论基础、对结构化分析推理的要求，以及在市场设计、资源分配和政策分析等现实应用中的相关性。我们提出$	extbf&#123;Recon&#125;$（像$	extbf&#123;经济学&#125;$家一样$	extbf&#123;推理&#125;$），这是一个基于2,100道精选高质量经济推理问题数据集进行后训练的70亿参数开源LLM。在经济推理基准测试和多智能体游戏中的综合评估表明，该模型在结构化推理和经济合理性方面均有显著提升。这些结果印证了领域对齐的后训练对增强推理与智能体对齐的潜力，并揭示了SFT和RL在塑造模型行为中的作用。代码发布于https://github.com/MasterZhou1/Recon。

---

## [Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs](https://arxiv.org/abs/2506.00582)

### Abstract
arXiv:2506.00582v1 Announce Type: new 
Abstract: Psychology research has shown that humans are poor at estimating their performance on tasks, tending towards underconfidence on easy tasks and overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct, Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and show that models exhibit subtle differences from human patterns of overconfidence: less sensitive to task difficulty, and when prompted to answer based on different personas -- e.g., expert vs layman, or different race, gender, and ages -- the models will respond with stereotypically biased confidence estimations even though their underlying answer accuracy remains the same. Based on these observations, we propose Answer-Free Confidence Estimation (AFCE) to improve confidence calibration and LLM interpretability in these settings. AFCE is a self-assessment method that employs two stages of prompting, first eliciting only confidence scores on questions, then asking separately for the answer. Experiments on the MMLU and GPQA datasets spanning subjects and difficulty show that this separation of tasks significantly reduces overconfidence and delivers more human-like sensitivity to task difficulty.

### 摘要
心理学研究表明，人类在评估自身任务表现时存在明显偏差，倾向于在简单任务中低估自己、在困难任务中高估自己。本研究考察了Llama-3-70B-instruct、Claude-3-Sonnet和GPT-4o三种大语言模型在不同难度问答任务中的表现，发现模型展现出与人类过度自信模式的微妙差异：对任务难度敏感性较低，且当被要求基于不同角色（如专家与普通人，或不同种族、性别和年龄）作答时，模型会表现出带有刻板偏见的置信度评估，尽管其底层答案准确率保持不变。基于这些发现，我们提出无答案置信度估计（AFCE）方法来改进这些场景下的置信度校准和模型可解释性。AFCE是一种两阶段提示的自评估方法：首阶段仅获取问题置信度评分，次阶段单独获取答案。在涵盖多学科和难度梯度的MMLU与GPQA数据集上的实验表明，这种任务分离方法能显著降低过度自信，并产生更接近人类的任务难度敏感性。

---

## [OntoRAG: Enhancing Question-Answering through Automated Ontology Derivation from Unstructured Knowledge Bases](https://arxiv.org/abs/2506.00664)

### Abstract
arXiv:2506.00664v1 Announce Type: new 
Abstract: Ontologies are pivotal for structuring knowledge bases to enhance question answering (QA) systems powered by Large Language Models (LLMs). However, traditional ontology creation relies on manual efforts by domain experts, a process that is time intensive, error prone, and impractical for large, dynamic knowledge domains. This paper introduces OntoRAG, an automated pipeline designed to derive ontologies from unstructured knowledge bases, with a focus on electrical relay documents. OntoRAG integrates advanced techniques, including web scraping, PDF parsing, hybrid chunking, information extraction, knowledge graph construction, and ontology creation, to transform unstructured data into a queryable ontology. By leveraging LLMs and graph based methods, OntoRAG enhances global sensemaking capabilities, outperforming conventional Retrieval Augmented Generation (RAG) and GraphRAG approaches in comprehensiveness and diversity. Experimental results demonstrate OntoRAGs effectiveness, achieving a comprehensiveness win rate of 85% against vector RAG and 75% against GraphRAGs best configuration. This work addresses the critical challenge of automating ontology creation, advancing the vision of the semantic web.

### 摘要
本体论在构建知识库以增强基于大语言模型(LLM)的问答系统(QA)中具有关键作用。然而传统本体创建依赖领域专家的手动工作，这一过程耗时费力、容易出错，且难以应对大规模动态知识领域。本文提出OntoRAG，一种从非结构化知识库自动生成本体的处理流程，重点关注电气继电器文档领域。该框架整合了网络爬取、PDF解析、混合分块、信息抽取、知识图谱构建和本体创建等先进技术，将非结构化数据转化为可查询的本体。通过结合LLM和图计算方法，OntoRAG显著提升了全局语义理解能力，在全面性和多样性方面优于传统的检索增强生成(RAG)和图增强生成(GraphRAG)方法。实验结果表明，OntoRAG的综合性能达到85%的向量RAG胜率和75%的GraphRAG最佳配置胜率。本研究解决了本体自动创建的核心挑战，推动了语义网愿景的实现。

---

## [MIRROR: Cognitive Inner Monologue Between Conversational Turns for Persistent Reflection and Reasoning in Conversational LLMs](https://arxiv.org/abs/2506.00430)

### Abstract
arXiv:2506.00430v1 Announce Type: new 
Abstract: Human intelligence relies on inner monologue to process complex information through simultaneous reflection, memory retrieval, and response formulation. We introduce MIRROR (Modular Internal Reasoning, Reflection, Orchestration, and Response), a cognitive architecture that systematically implements these parallel reasoning capabilities in large language models. MIRROR operates as a unified system with two distinct functional layers: the Thinker and the Talker. The Thinker encompasses: (1) the Inner Monologue Manager, coordinating reasoning threads across cognitive dimensions (Goals, Reasoning, and Memory); and (2) the Cognitive Controller, synthesizing these threads into a coherent internal narrative maintained across conversation turns. The Talker component then leverages this integrated narrative for context-aware responses. Evaluated on the CuRaTe benchmark--testing personalized dialogue with safety-critical constraints, conflicting preferences, and multi-turn consistency--LLMs utilizing the MIRROR architecture achieve up to 156% relative improvement in critical safety scenarios involving three persons with conflicting preferences, maintaining an average accuracy of ~&gt;80% on all scenarios. Across scenario-specific comparisons, GPT-4o, Gemini 1.5 Pro, Claude 3.7 Sonnet, Llama 4 variants, and Mistral 3 variants with the MIRROR architecture outperformed baseline models by 21% on average (15.5 percentage points absolute). MIRROR directly addresses three critical LLM failure modes: sycophancy, attentional deficits to critical information, and inconsistent prioritization of conflicting constraints. This work bridges cognitive science and AI by implementing modular internal reasoning inspired by human cognition, creating a persistent internal model that significantly enhances multi-turn conversation capabilities.

### 摘要
人类智能依赖内部独白通过同步反思、记忆检索和应答构建来处理复杂信息。我们提出MIRROR（模块化内部推理、反思、协调与应答）认知架构，该系统性地在大型语言模型中实现了这些并行推理能力。MIRROR作为统一系统包含两个功能层：思考者与表达者。思考者包含：（1）内部独白管理器，跨认知维度（目标、推理与记忆）协调推理线程；（2）认知控制器，将这些线程合成为跨对话轮次保持连贯的内部叙事。表达者组件则利用这一整合叙事生成情境感知的应答。在CuRaTe基准测试（评估具有安全关键约束、冲突偏好和多轮一致性的个性化对话）中，采用MIRROR架构的LLM在涉及三人冲突偏好的关键安全场景中取得最高156%的相对性能提升，所有场景平均准确率保持~&gt;80%。在特定场景比较中，配备MIRROR架构的GPT-4o、Gemini 1.5 Pro、Claude 3.7 Sonnet、Llama 4变体和Mistral 3变体模型平均比基线模型提升21%（绝对百分比15.5个点）。MIRROR直接解决了LLM三种典型失效模式：谄媚行为、对关键信息的注意力缺失以及冲突约束的优先级不一致问题。这项工作通过实现受人类认知启发的模块化内部推理，构建了持续存在的内部模型，显著增强了多轮对话能力，从而架起了认知科学与人工智能之间的桥梁。

---

## [RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents](https://arxiv.org/abs/2506.00618)

### Abstract
arXiv:2506.00618v1 Announce Type: new 
Abstract: With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce \textbf&#123;RiOSWorld&#125;, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on \textbf&#123;RiOSWorld&#125; demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.

### 摘要
随着多模态大语言模型（MLLMs）的快速发展，它们越来越多地被部署为能够完成复杂计算机任务的自主计算机使用代理。然而，一个紧迫问题随之产生：针对对话场景设计的通用MLLM安全风险原则能否有效迁移至真实世界计算机使用场景？现有基于MLLM的计算机使用代理安全风险评估研究存在若干局限：要么缺乏真实的交互环境，要么仅狭隘地关注一种或少数特定风险类型。这些局限忽视了现实环境的复杂性、多变性和多样性，从而限制了对计算机使用代理的全面风险评估。为此，我们提出RIOSWorld基准，旨在评估基于MLLM的代理在真实世界计算机操作中的潜在风险。我们的基准包含492个跨计算机应用的风险任务，涉及网络、社交媒体、多媒体、操作系统、电子邮件和办公软件。根据风险来源，我们将这些风险分为两大类：（i）用户端风险；（ii）环境风险。在评估方面，我们从两个角度衡量安全风险：（i）风险目标意图；（ii）风险目标完成度。通过在RIOSWorld上对多模态代理进行大量实验，我们发现当前计算机使用代理在真实场景中面临重大安全风险。研究结果揭示了现实计算机操作中对计算机使用代理进行安全对齐的必要性和紧迫性，为开发可信赖的计算机使用代理提供了重要启示。本基准已公开于https://yjyddq.github.io/RiOSWorld.github.io/。

---

## [AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents](https://arxiv.org/abs/2506.00641)

### Abstract
arXiv:2506.00641v1 Announce Type: new 
Abstract: Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents' step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce \sys, a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators. \sys constructs an experiential memory by having an LLM adaptively extract structured semantic features (e.g., scenario, risk, behavior) and generate associated chain-of-thought reasoning traces for past interactions. A multi-stage, context-aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM evaluator's assessment of new cases. Moreover, we developed \data, the first benchmark designed to check how well LLM-based evaluators can spot both safety risks and security threats. \data comprises \textbf&#123;2293&#125; meticulously annotated interaction records, covering \textbf&#123;15&#125; risk types across \textbf&#123;29&#125; application scenarios. A key feature of \data is its nuanced approach to ambiguous risk situations, employing ``Strict'' and ``Lenient'' judgment standards. Experiments demonstrate that \sys not only consistently improves the evaluation performance of LLMs across all benchmarks but also sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, achieving human-level accuracy. Our work is openly openly accessible.

### 摘要
尽管基于大语言模型（LLM）的智能体发展迅速，但其安全性与可靠性的评估仍面临重大挑战。现有基于规则或LLM的评估方法常存在以下缺陷：忽视智能体分步动作中的潜在危险、遗漏细微语义关联、未能识别小问题的叠加效应，以及因安全规则模糊而产生误判。为解决这一评估危机，我们提出\sys框架——一种无需训练、具备记忆增强推理能力的通用评估框架，使LLM评估器能够模拟人类专家评估模式。该框架通过LLM自适应提取结构化语义特征（如场景、风险、行为）并为历史交互生成关联思维链推理轨迹，构建经验记忆库。随后采用多阶段上下文感知的检索增强生成机制，动态检索最相关推理经验来指导新案例评估。此外，我们开发了首个专注于LLM评估器安全风险与安全威胁检测能力的基准数据集\data，包含\textbf&#123;2293条&#125;精细标注的交互记录，涵盖\textbf&#123;29种&#125;应用场景下的\textbf&#123;15类&#125;风险类型。\data的核心特色在于采用"严格"与"宽松"双标准对模糊风险情境进行细粒度判定。实验表明，\sys不仅在所有基准测试中持续提升LLM评估性能，更在智能体安全评估领域达到人类水平准确率，创下LLM-as-a-judge的新标杆。本研究成果已全面开源。

---

## [DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains](https://arxiv.org/abs/2506.00708)

### Abstract
arXiv:2506.00708v1 Announce Type: new 
Abstract: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.

### 摘要
知识图谱补全（KGC）旨在利用现有三元组和文本信息预测知识图谱（KG）中缺失的三元组。近年来，生成式大语言模型（LLM）越来越多地被应用于图任务。然而，现有方法通常以文本形式编码图上下文，未能充分发挥LLM感知和推理图结构的潜力。为解决这一局限，我们提出DrKGC（基于动态子图检索增强LLM的知识图谱补全方法）。该方法采用灵活的轻量级模型训练策略学习KG内部的结构嵌入与逻辑规则，随后通过新颖的自底向上图检索方法，在习得规则的指导下为每个查询提取子图。最后，图卷积网络（GCN）适配器利用检索到的子图增强结构嵌入，并将其整合至提示模板中以实现有效的LLM微调。在两个通用领域基准数据集和两个生物医学数据集上的实验结果表明，DrKGC具有卓越性能。此外，生物医学领域的实际案例研究进一步验证了其可解释性与实用价值。

---

## [Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.00782)

### Abstract
arXiv:2506.00782v1 Announce Type: new 
Abstract: As large language models (LLMs) grow in power and influence, ensuring their safety and preventing harmful output becomes critical. Automated red teaming serves as a tool to detect security vulnerabilities in LLMs without manual labor. However, most existing methods struggle to balance the effectiveness and diversity of red-team generated attack prompts. To address this challenge, we propose \ourapproach, a novel automated red teaming training framework that utilizes reinforcement learning to explore and generate more effective attack prompts while balancing their diversity. Specifically, it consists of three training stages: (1) Cold Start: The red team model is supervised and fine-tuned on a jailbreak dataset obtained through imitation learning. (2) Warm-up Exploration: The model is trained in jailbreak instruction following and exploration, using diversity and consistency as reward signals. (3) Enhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually enhance the jailbreak performance of the red-team model. Extensive experiments on a variety of LLMs show that \ourapproach effectively balances the diversity and effectiveness of jailbreak prompts compared to existing methods. Our work significantly improves the efficiency of red team exploration and provides a new perspective on automated red teaming.

### 摘要
随着大型语言模型（LLM）能力与影响力的提升，确保其安全性并防止有害输出变得至关重要。自动化红队测试作为一种无需人工干预的漏洞检测工具，可用于识别LLM的安全缺陷。然而，现有方法大多难以平衡红队生成攻击提示的有效性与多样性。为解决这一问题，我们提出\ourapproach——一种基于强化学习的新型自动化红队训练框架，该框架在探索生成更有效攻击提示的同时保持其多样性。具体包含三个训练阶段：（1）冷启动阶段：通过模仿学习获取越狱数据集，对红队模型进行监督式微调；（2）预热探索阶段：以多样性和一致性作为奖励信号，训练模型遵循越狱指令并进行探索；（3）强化越狱阶段：引入渐进式越狱奖励机制，逐步提升红队模型的越狱性能。在多类LLM上的实验表明，相较于现有方法，\ourapproach能有效平衡越狱提示的多样性与有效性。本研究显著提升了红队探索效率，为自动化红队测试提供了新思路。

---

## [Do not Abstain! Identify and Solve the Uncertainty](https://arxiv.org/abs/2506.00780)

### Abstract
arXiv:2506.00780v1 Announce Type: new 
Abstract: Despite the widespread application of Large Language Models (LLMs) across various domains, they frequently exhibit overconfidence when encountering uncertain scenarios, yet existing solutions primarily rely on evasive responses (e.g., "I don't know") overlooks the opportunity of identifying and addressing the uncertainty to generate more satisfactory responses. To systematically investigate and improve LLMs' ability of recognizing and addressing the source of uncertainty, we introduce \textbf&#123;ConfuseBench&#125;, a benchmark mainly focus on three types of uncertainty: document scarcity, limited capability, and query ambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to accurately identify the root cause of uncertainty and solve it. They prefer to attribute uncertainty to query ambiguity while overlooking capability limitations, especially for those weaker models. To tackle this challenge, we first generate context-aware inquiries that highlight the confusing aspect of the original query. Then we judge the source of uncertainty based on the uniqueness of the inquiry's answer. Further we use an on-policy training method, InteractDPO to generate better inquiries. Experimental results demonstrate the efficacy of our approach.

### 摘要
尽管大型语言模型（LLMs）在各领域得到广泛应用，但其在面对不确定场景时经常表现出过度自信。现有解决方案主要依赖回避性回应（如“我不知道”），却忽视了识别并解决不确定性以生成更满意回答的机会。为系统研究并提升LLMs识别与解决不确定性根源的能力，我们提出\textbf&#123;ConfuseBench&#125;基准，重点关注三类不确定性：文档稀缺性、能力局限性和查询模糊性。通过ConfuseBench实验发现，当前LLMs难以准确识别不确定性根源并解决问题，其更倾向于将不确定性归因于查询模糊性而忽视能力局限性——这一现象在较弱模型中尤为明显。针对该挑战，我们首先生成能突出原始查询混淆点的上下文感知询问，随后基于询问答案的唯一性判断不确定性来源。进一步采用策略内训练方法InteractDPO来生成更优质的询问。实验结果验证了本方法的有效性。

---

## [Alignment Revisited: Are Large Language Models Consistent in Stated and Revealed Preferences?](https://arxiv.org/abs/2506.00751)

### Abstract
arXiv:2506.00751v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) highlight the need to align their behaviors with human values. A critical, yet understudied, issue is the potential divergence between an LLM's stated preferences (its reported alignment with general principles) and its revealed preferences (inferred from decisions in contextualized scenarios). Such deviations raise fundamental concerns for the interpretability, trustworthiness, reasoning transparency, and ethical deployment of LLMs, particularly in high-stakes applications. This work formally defines and proposes a method to measure this preference deviation. We investigate how LLMs may activate different guiding principles in specific contexts, leading to choices that diverge from previously stated general principles. Our approach involves crafting a rich dataset of well-designed prompts as a series of forced binary choices and presenting them to LLMs. We compare LLM responses to general principle prompts stated preference with LLM responses to contextualized prompts revealed preference, using metrics like KL divergence to quantify the deviation. We repeat the analysis across different categories of preferences and on four mainstream LLMs and find that a minor change in prompt format can often pivot the preferred choice regardless of the preference categories and LLMs in the test. This prevalent phenomenon highlights the lack of understanding and control of the LLM decision-making competence. Our study will be crucial for integrating LLMs into services, especially those that interact directly with humans, where morality, fairness, and social responsibilities are crucial dimensions. Furthermore, identifying or being aware of such deviation will be critically important as LLMs are increasingly envisioned for autonomous agentic tasks where continuous human evaluation of all LLMs' intermediary decision-making steps is impossible.

### 摘要
大语言模型（LLMs）的最新进展凸显了将其行为与人类价值观对齐的必要性。一个关键但尚未充分研究的问题在于，LLMs的陈述偏好（其报告的对通用原则的遵循）与揭示偏好（从具体情境决策中推断出的倾向）之间可能存在偏差。这种偏离对LLMs的可解释性、可信度、推理透明度及伦理部署（尤其是在高风险应用中）提出了根本性质疑。本研究正式定义了这种偏好偏差并提出了量化方法。我们探究了LLMs如何在特定情境中激活不同指导原则，从而导致其选择与先前陈述的通用原则产生分歧。我们的方法包括构建一个精心设计的提示词数据集作为系列强制二选一问题，并将其提交给LLMs。通过比较LLMs对通用原则提示（陈述偏好）与情境化提示（揭示偏好）的响应，并采用KL散度等指标量化偏差程度。我们在不同偏好类别和四种主流LLMs上重复分析，发现提示格式的微小变化常能扭转模型选择，且该现象与偏好类别及测试模型无关。这种普遍现象揭示了当前对LLM决策能力认知与控制的不足。本研究对于LLMs融入服务（尤其是直接与人交互的领域，其中道德、公平和社会责任是关键维度）至关重要。此外，随着LLMs日益被设想用于自主代理任务（人类无法持续评估其所有中间决策步骤），识别或意识到此类偏差将具有决定性意义。

---

## [GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning](https://arxiv.org/abs/2506.00785)

### Abstract
arXiv:2506.00785v1 Announce Type: new 
Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating step-by-step geographic reasoning in multimodal large language models (MLLMs). Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each image with a 21-step chain-of-thought (CoT) question sequence (over 30 million Q&amp;A pairs). These sequences guide models from coarse attributes to fine-grained localization across four reasoning categories - visual, spatial, cultural, and precise geolocation - annotated by difficulty. Images are also enriched with semantic segmentation (150 classes) and a visual locatability score. Our benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5 variants) on a diverse 2,088-image subset reveals consistent challenges: models frequently exhibit weaknesses in visual grounding, display erratic reasoning, and struggle to achieve accurate localization, especially as the reasoning complexity escalates. GeoChain offers a robust diagnostic methodology, critical for fostering significant advancements in complex geographic reasoning within MLLMs.

### 摘要
本文介绍GeoChain——一个用于评估多模态大语言模型（MLLMs）渐进式地理推理能力的大规模基准。该基准利用146万张Mapillary街景图像，为每张图像配设包含21步思维链（CoT）的问题序列（总计超3000万问答对）。这些序列引导模型从粗粒度属性到细粒度定位，涵盖视觉、空间、文化及精确地理定位四大标注难度等级的推理类别。图像数据还附加了语义分割（150类别）和视觉可定位性评分。通过对当代主流MLLMs（GPT-4.1变体、Claude 3.7、Gemini 2.5变体）在2088张多样化图像子集上的测试，我们发现模型普遍存在三大挑战：视觉基础能力薄弱、推理过程不稳定，以及随着推理复杂度提升时精准定位能力显著下降。GeoChain提供了一套强有力的诊断方法，对推动MLLMs复杂地理推理能力的实质性突破具有重要意义。

---

## [SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning](https://arxiv.org/abs/2506.00835)

### Abstract
arXiv:2506.00835v1 Announce Type: new 
Abstract: Fine-grained video captioning aims to generate detailed, temporally coherent descriptions of video content. However, existing methods struggle to capture subtle video dynamics and rich detailed information. In this paper, we leverage preference learning to enhance the performance of vision-language models in fine-grained video captioning, while mitigating several limitations inherent to direct preference optimization (DPO). First, we propose a pipeline for constructing preference pairs that leverages the intrinsic properties of VLMs along with partial assistance from large language models, achieving an optimal balance between cost and data quality. Second, we propose Synergistic Preference Optimization (SynPO), a novel optimization method offering significant advantages over DPO and its variants. SynPO prevents negative preferences from dominating the optimization, explicitly preserves the model's language capability to avoid deviation of the optimization objective, and improves training efficiency by eliminating the need for the reference model. We extensively evaluate SynPO not only on video captioning benchmarks (e.g., VDC, VDD, VATEX) but also across well-established NLP tasks, including general language understanding and preference evaluation, using diverse pretrained models. Results demonstrate that SynPO consistently outperforms DPO variants while achieving 20\% improvement in training efficiency. Code is available at https://github.com/longmalongma/SynPO

### 摘要
细粒度视频描述生成旨在对视频内容产生细致且时序连贯的文本描述。然而，现有方法难以捕捉视频中的细微动态变化与丰富细节信息。本文通过偏好学习提升视觉语言模型在细粒度视频描述任务中的性能，同时克服直接偏好优化（DPO）的若干固有缺陷。首先，我们提出一种构建偏好对的流程，利用视觉语言模型的内在特性并辅以大型语言模型的局部协助，实现成本与数据质量的最佳平衡。其次，我们提出协同偏好优化（SynPO），这种新型优化方法相较DPO及其变体具有显著优势：通过阻止负面偏好主导优化过程、显式保持模型语言能力以避免优化目标偏离，以及消除参考模型需求来提升训练效率。我们在视频描述基准（如VDC、VDD、VATEX）和多种预训练模型上对SynPO进行了全面评估，包括通用语言理解和偏好评估等成熟NLP任务。实验结果表明SynPO始终优于DPO变体，同时实现20%的训练效率提升。代码发布于https://github.com/longmalongma/SynPO

---

## [Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision](https://arxiv.org/abs/2506.00807)

### Abstract
arXiv:2506.00807v1 Announce Type: new 
Abstract: The reasoning capabilities of large language models (LLMs) have significantly advanced their performance by enabling in-depth understanding of diverse tasks. With growing interest in applying LLMs to the time series domain, this has proven nontrivial, as evidenced by the limited efficacy of straightforwardly adapting text-domain reasoning techniques. Although recent work has shown promise in several time series tasks, further leveraging advancements in LLM reasoning remains under-explored for time series classification (TSC) tasks, despite their prevalence and significance in many real-world applications. In this paper, we propose ReasonTSC, a novel framework designed to effectively leverage LLM reasoning for time series classification through both a multi-turn reasoning and a fused decision-making strategy tailored to TSC. Rather than straightforwardly applying existing reasoning techniques or relying solely on LLMs' built-in reasoning capabilities, ReasonTSC first steers the model to think over the essential characteristics of time series data. Next, it integrates predictions and confidence scores from plug-in classifiers, e.g., domain-specific time series models, as in-context examples. Finally, ReasonTSC guides the LLM through a structured reasoning process: it evaluates the initial assessment, backtracks to consider alternative hypotheses, and compares their merits before arriving at a final classification. Extensive experiments and systematic ablation studies demonstrate that ReasonTSC consistently outperforms both existing time series reasoning baselines and plug-in models, and is even capable of identifying and correcting plug-in models' false predictions.

### 摘要
大语言模型（LLMs）的推理能力通过深入理解多样化任务显著提升了其性能。随着将LLMs应用于时间序列领域的兴趣日益增长，这一尝试被证明并非易事，直接迁移文本领域推理技术的有限效果即为明证。尽管近期研究在多项时间序列任务中展现出潜力，但针对时间序列分类（TSC）任务——尽管其在众多实际应用中具有普遍性和重要性——如何进一步利用LLM推理进展仍未被充分探索。本文提出ReasonTSC框架，该框架通过专为TSC设计的多轮推理与融合决策策略，有效利用LLM推理进行时间序列分类。ReasonTSC并非简单应用现有推理技术或仅依赖LLM内置推理能力，而是首先引导模型思考时间序列数据的基本特征，随后整合插件分类器（如领域专用时间序列模型）的预测结果及置信度分数作为上下文示例。最终，该框架引导LLM执行结构化推理流程：评估初始判断、回溯考量替代假设、比较优劣后得出最终分类。大量实验与系统性消融研究表明，ReasonTSC始终优于现有时间序列推理基线及插件模型，甚至能识别并修正插件模型的错误预测。

---

## [Predicting Empirical AI Research Outcomes with Language Models](https://arxiv.org/abs/2506.00794)

### Abstract
arXiv:2506.00794v1 Announce Type: new 
Abstract: Many promising-looking ideas in AI research fail to deliver, but their validation takes substantial human labor and compute. Predicting an idea's chance of success is thus crucial for accelerating empirical AI research, a skill that even expert researchers can only acquire through substantial experience. We build the first benchmark for this task and compare LMs with human experts. Concretely, given two research ideas (e.g., two jailbreaking methods), we aim to predict which will perform better on a set of benchmarks. We scrape ideas and experimental results from conference papers, yielding 1,585 human-verified idea pairs published after our base model's cut-off date for testing, and 6,000 pairs for training. We then develop a system that combines a fine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human experts to compare with. In the NLP domain, our system beats human experts by a large margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77% accuracy, while off-the-shelf frontier LMs like o3 perform no better than random guessing, even with the same retrieval augmentation. We verify that our system does not exploit superficial features like idea complexity through extensive human-written and LM-designed robustness tests. Finally, we evaluate our system on unpublished novel ideas, including ideas generated by an AI ideation agent. Our system achieves 63.6% accuracy, demonstrating its potential as a reward model for improving idea generation models. Altogether, our results outline a promising new direction for LMs to accelerate empirical AI research.

### 摘要
人工智能研究中许多看似前景广阔的构想最终未能实现，但其验证过程需要耗费大量人力与算力。因此，预测构想的成功概率对加速实证性AI研究至关重要——这种技能即使专家研究者也需通过大量实践才能掌握。我们构建了该任务的首个基准测试，并比较语言模型与人类专家的表现。具体而言，给定两个研究构想（例如两种越狱方法），我们旨在预测哪个在基准测试中表现更优。我们从会议论文中爬取构想及实验结果，构建了1,585对截止日期后发表且经人工验证的测试集（基础模型训练时未接触），以及6,000对训练集。随后开发了结合微调GPT-4.1与文献检索代理的系统，并招募25位人类专家进行对比。在NLP领域，我们的系统以显著优势超越人类专家（64.4%对48.9%）。在全测试集上系统准确率达77%，而如o3等现成前沿语言模型即使采用相同检索增强策略，表现也不优于随机猜测。通过大量人工编写和语言模型设计的鲁棒性测试，我们验证系统未利用构想复杂度等表面特征。最后，我们在未发表的新构想（包括AI构思代理生成的构想）上评估系统，其准确率达63.6%，表明其作为奖励模型改进构想生成模型的潜力。总体而言，我们的研究为语言模型加速实证性AI研究指明了一个新方向。

---

## [Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models](https://arxiv.org/abs/2506.00911)

### Abstract
arXiv:2506.00911v1 Announce Type: new 
Abstract: Modern language model deployments must often balance competing objectives, for example, helpfulness versus harmlessness, cost versus accuracy, and reward versus safety. We introduce Conformal Arbitrage, a post hoc framework that learns a data driven threshold to mediate between a Primary model optimized for a primary objective and a more conservative Guardian which could be another model or a human domain expert aligned with a guardrail objective. The threshold is calibrated with conformal risk control, yielding finite sample, distribution free guarantees that the long run frequency of undesirable events, such as factual errors or safety violations, does not exceed a user specified quota. Because Conformal Arbitrage operates wholly at the API level, without requiring access to model logits or updating model weights, it complements weight based alignment techniques and integrates seamlessly with existing cost aware cascades. Empirically, Conformal Arbitrage traces an efficient frontier, allowing users to define an acceptable performance level for one objective while maximizing utility in another. We observe that our method outperforms, in terms of accuracy, cost matched random routing between models. These properties make Conformal Arbitrage a practical, theoretically grounded tool for trustworthy and economical deployment of large language models across a broad range of potentially competing objectives.

### 摘要
现代语言模型部署常需平衡相互竞争的目标，例如帮助性与无害性、成本与准确性、奖励与安全性。本文提出'保形套利'框架，该后处理方法通过学习数据驱动的阈值，在优化主要目标的'主模型'与遵循护栏目标的保守型'守护者'（可以是另一模型或人类领域专家）之间进行仲裁。该阈值通过保形风险控制进行校准，可提供有限样本且与分布无关的保证，确保不良事件（如事实错误或安全违规）的长期发生频率不超过用户设定的配额。由于保形套利完全在API层面运行，无需访问模型逻辑或更新权重，它可与基于权重的对齐技术互补，并无缝集成现有成本感知级联系统。实证研究表明，保形套利能描绘有效边界，允许用户在确保某一目标达到可接受水平的同时，最大化另一目标的效用。实验观察到，在模型间随机路由的成本匹配条件下，本方法在准确性方面表现更优。这些特性使保形套利成为在广泛潜在竞争目标下，实现大型语言模型可信且经济部署的实用化、理论完备的工具。

---

## [MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book](https://arxiv.org/abs/2506.00855)

### Abstract
arXiv:2506.00855v1 Announce Type: new 
Abstract: The accelerating development of general medical artificial intelligence (GMAI), powered by multimodal large language models (MLLMs), offers transformative potential for addressing persistent healthcare challenges, including workforce deficits and escalating costs. The parallel development of systematic evaluation benchmarks emerges as a critical imperative to enable performance assessment and provide technological guidance. Meanwhile, as an invaluable knowledge source, the potential of medical textbooks for benchmark development remains underexploited. Here, we present MedBookVQA, a systematic and comprehensive multimodal benchmark derived from open-access medical textbooks. To curate this benchmark, we propose a standardized pipeline for automated extraction of medical figures while contextually aligning them with corresponding medical narratives. Based on this curated data, we generate 5,000 clinically relevant questions spanning modality recognition, disease classification, anatomical identification, symptom diagnosis, and surgical procedures. A multi-tier annotation system categorizes queries through hierarchical taxonomies encompassing medical imaging modalities (42 categories), body anatomies (125 structures), and clinical specialties (31 departments), enabling nuanced analysis across medical subdomains. We evaluate a wide array of MLLMs, including proprietary, open-sourced, medical, and reasoning models, revealing significant performance disparities across task types and model categories. Our findings highlight critical capability gaps in current GMAI systems while establishing textbook-derived multimodal benchmarks as essential evaluation tools. MedBookVQA establishes textbook-derived benchmarking as a critical paradigm for advancing clinical AI, exposing limitations in GMAI systems while providing anatomically structured performance metrics across specialties.

### 摘要
由多模态大语言模型（MLLMs）驱动的通用医疗人工智能（GMAI）加速发展，为解决医疗领域长期存在的挑战（如劳动力短缺和成本上升）带来了变革性潜力。与之并行发展的系统性评估基准成为实现性能评估和提供技术指导的关键需求。然而，作为宝贵的知识来源，医学教科书在基准开发中的潜力尚未得到充分挖掘。本文提出MedBookVQA——一个基于开放获取医学教科书构建的系统性、综合性多模态基准。为构建该基准，我们提出了一种标准化流程，用于自动提取医学图像并使其与对应的医学叙述内容上下文对齐。基于此整理的数据，我们生成了5,000个涵盖模态识别、疾病分类、解剖结构识别、症状诊断和手术流程等临床相关问题的数据集。通过包含医学影像模态（42类）、人体解剖结构（125种）和临床专科（31个科室）的多层级分类体系，我们建立了分层标注系统以实现跨医学子领域的精细化分析。我们评估了包括专有模型、开源模型、医疗专用模型和推理模型在内的多种MLLMs，发现不同任务类型和模型类别间存在显著性能差异。研究结果既揭示了当前GMAI系统的关键能力缺陷，也确立了教科书衍生的多模态基准作为重要评估工具的价值。MedBookVQA将教科书衍生的基准测试确立为推进临床AI发展的关键范式，在暴露GMAI系统局限性的同时，提供了跨专科的解剖结构化性能度量标准。

---

## [Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](https://arxiv.org/abs/2506.00958)

### Abstract
arXiv:2506.00958v1 Announce Type: new 
Abstract: Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input.

### 摘要
非语言交流是人类互动的重要组成部分，手势、面部表情和肢体语言传递了意图与情感的关键维度。然而，现有大语言模型（LLMs）未能有效整合这些非语言要素，限制了其构建完全沉浸式对话体验的能力。我们提出MARS——一种多模态语言模型，旨在理解并生成文本与非语言线索，以弥合对话型AI的这一缺陷。核心创新是VENUS数据集，该大规模标注视频库包含时间对齐的文本、面部表情及肢体语言数据。基于VENUS，我们采用下一标记预测目标训练MARS，通过将文本与矢量量化的非语言表征相结合，在统一框架内实现多模态理解与生成。通过对VENUS数据集的多种分析，我们验证了其显著规模与高效性。定量与定性结果表明，MARS能成功生成与对话输入相对应的文本及非语言内容。

---

## [Toward a Theory of Agents as Tool-Use Decision-Makers](https://arxiv.org/abs/2506.00886)

### Abstract
arXiv:2506.00886v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) evolve into increasingly autonomous agents, fundamental questions about their epistemic foundations remain unresolved: What defines an agent? How should it make decisions? And what objectives should guide its behavior? In this position paper, we argue that true autonomy requires agents to be grounded in a coherent epistemic framework that governs what they know, what they need to know, and how to acquire that knowledge efficiently. We propose a unified theory that treats internal reasoning and external actions as equivalent epistemic tools, enabling agents to systematically coordinate introspection and interaction. Building on this framework, we advocate for aligning an agent's tool use decision-making boundary with its knowledge boundary, thereby minimizing unnecessary tool use and maximizing epistemic efficiency. This perspective shifts the design of agents from mere action executors to knowledge-driven intelligence systems, offering a principled path toward building foundation agents capable of adaptive, efficient, and goal-directed behavior.

### 摘要
随着大型语言模型（LLMs）逐渐发展为高度自主的智能体，其认知基础的根本问题仍未解决：智能体的本质是什么？它应如何做出决策？又该以何种目标指导其行为？在本立场论文中，我们主张真正的自主性要求智能体必须建立在统一的认知框架之上，该框架需规范其已知内容、需知内容以及如何高效获取知识。我们提出一种将内部推理与外部行动视为同等认知工具的统一理论，使智能体能够系统协调内省与交互。基于此框架，我们建议将智能体的工具使用决策边界与其知识边界对齐，从而最大限度减少不必要的工具使用并提升认知效率。这一视角将智能体设计从单纯的行为执行者转变为知识驱动的智能系统，为构建具有适应性、高效性和目标导向行为的基础智能体提供了原则性路径。

---

## [CoP: Agentic Red-teaming for Large Language Models using Composition of Principles](https://arxiv.org/abs/2506.00781)

### Abstract
arXiv:2506.00781v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have spurred transformative applications in various domains, ranging from open-source to proprietary LLMs. However, jailbreak attacks, which aim to break safety alignment and user compliance by tricking the target LLMs into answering harmful and risky responses, are becoming an urgent concern. The practice of red-teaming for LLMs is to proactively explore potential risks and error-prone instances before the release of frontier AI technology. This paper proposes an agentic workflow to automate and scale the red-teaming process of LLMs through the Composition-of-Principles (CoP) framework, where human users provide a set of red-teaming principles as instructions to an AI agent to automatically orchestrate effective red-teaming strategies and generate jailbreak prompts. Distinct from existing red-teaming methods, our CoP framework provides a unified and extensible framework to encompass and orchestrate human-provided red-teaming principles to enable the automated discovery of new red-teaming strategies. When tested against leading LLMs, CoP reveals unprecedented safety risks by finding novel jailbreak prompts and improving the best-known single-turn attack success rate by up to 19.0 times.

### 摘要
大型语言模型（LLMs）的最新进展推动了从开源到专有模型在各领域的变革性应用。然而，越狱攻击——通过诱使目标LLMs生成有害和危险响应以破坏安全对齐和用户合规性的行为——正成为亟待解决的问题。针对LLMs的红队测试实践旨在前沿AI技术发布前主动探索潜在风险和易错实例。本文提出一种基于原则组合（CoP）框架的自主工作流，通过将人类提供的红队测试原则作为指令输入AI代理，自动编排有效红队策略并生成越狱提示，从而实现LLM红队测试过程的自动化与规模化。区别于现有红队方法，CoP框架提供统一且可扩展的架构，能够整合并协调人工提供的红队原则，实现新红队策略的自动化发现。在对主流LLMs的测试中，CoP通过发现新型越狱提示，将已知最佳单轮攻击成功率提升高达19.0倍，揭示了前所未有的安全风险。

---

## [Unlocking Personalized Knowledge in Federated Large Language Model: The Power of Mixture of Experts](https://arxiv.org/abs/2506.00965)

### Abstract
arXiv:2506.00965v1 Announce Type: new 
Abstract: The Mixture of Experts (MoE) architecture has emerged as a prominent strategy for scaling large language models (LLMs), effectively leveraging sparse activation and facilitating task-specific personalization. However, current federated learning (FL) approaches are primarily designed for dense models, making them unable to directly exploit the sparsity inherent in MoE architectures. Treating MoE models as dense networks in federated scenarios results in excessive communication overhead and computational costs, undermining the potential for personalized knowledge sharing. To address these challenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel federated learning framework explicitly tailored for MoE-based LLMs. FLEx efficiently personalizes by pruning the global MoE model to keep only one expert per client, and employs an adaptive gating mechanism to reintegrate these personalized experts into the pre-trained MoE layers, ensuring the original backbone architecture remains unchanged. These personalized experts are trained with local data and stored locally on each client, while the shared modules are aggregated globally. Extensive evaluations on diverse instruction-based datasets under non-IID conditions consistently demonstrate that FLEx outperforms existing federated baselines. Our code is available at https://anonymous.4open.science/r/FLEx-8F12.

### 摘要
混合专家（MoE）架构已成为扩展大语言模型（LLM）的重要策略，它能有效利用稀疏激活并促进任务个性化。然而，当前联邦学习（FL）方法主要针对密集模型设计，无法直接利用MoE架构固有的稀疏性。在联邦场景中将MoE模型视为密集网络会导致过高的通信开销和计算成本，从而破坏个性化知识共享的潜力。为解决这些问题，我们提出FLEx（基于个性化专家的联邦大语言模型），这是专为基于MoE的大语言模型设计的新型联邦学习框架。FLEx通过剪枝全局MoE模型（每个客户端仅保留一个专家）实现高效个性化，并采用自适应门控机制将这些个性化专家重新整合到预训练的MoE层中，确保原始主干架构保持不变。这些个性化专家使用本地数据进行训练并存储在客户端本地，而共享模块则进行全局聚合。在非独立同分布条件下对多种指令数据集的广泛评估表明，FLEx始终优于现有联邦基线方法。代码已发布于https://anonymous.4open.science/r/FLEx-8F12。

---

## [FedQuad: Adaptive Layer-wise LoRA Deployment and Activation Quantization for Federated Fine-Tuning](https://arxiv.org/abs/2506.01001)

### Abstract
arXiv:2506.01001v1 Announce Type: new 
Abstract: Federated fine-tuning (FedFT) provides an effective paradigm for fine-tuning large language models (LLMs) in privacy-sensitive scenarios. However, practical deployment remains challenging due to the limited resources on end devices. Existing methods typically utilize parameter-efficient fine-tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), to substantially reduce communication overhead. Nevertheless, significant memory usage for activation storage and computational demands from full backpropagation remain major barriers to efficient deployment on resource-constrained end devices. Moreover, substantial resource heterogeneity across devices results in severe synchronization bottlenecks, diminishing the overall fine-tuning efficiency. To address these issues, we propose FedQuad, a novel LoRA-based FedFT framework that adaptively adjusts the LoRA depth (the number of consecutive tunable LoRA layers from the output) according to device computational capabilities, while employing activation quantization to reduce memory overhead, thereby enabling efficient deployment on resource-constrained devices. Specifically, FedQuad first identifies the feasible and efficient combinations of LoRA depth and the number of activation quantization layers based on device-specific resource constraints. Subsequently, FedQuad employs a greedy strategy to select the optimal configurations for each device, effectively accommodating system heterogeneity. Extensive experiments demonstrate that FedQuad achieves a 1.4-5.3x convergence acceleration compared to state-of-the-art baselines when reaching target accuracy, highlighting its efficiency and deployability in resource-constrained and heterogeneous end-device environments.

### 摘要
联邦微调(FedFT)为隐私敏感场景下的大型语言模型(LLMs)微调提供了有效范式。然而，由于终端设备资源有限，实际部署仍面临挑战。现有方法通常采用参数高效微调(PEFT)技术（如低秩自适应(LoRA)）来大幅降低通信开销。但激活存储的高内存消耗和完整反向传播的计算需求，仍是资源受限终端设备高效部署的主要障碍。此外，设备间显著的资源异构性会导致严重的同步瓶颈，降低整体微调效率。针对这些问题，我们提出FedQuad——一种基于LoRA的新型FedFT框架，该框架根据设备计算能力自适应调整LoRA深度（从输出端开始连续可调LoRA层数），同时采用激活量化减少内存开销，从而实现资源受限设备的高效部署。具体而言，FedQuad首先基于设备特定资源约束，确定LoRA深度与激活量化层数的可行高效组合；随后采用贪心策略为各设备选择最优配置，有效适应系统异构性。大量实验表明，在达到目标精度时，FedQuad相比最先进基线可实现1.4-5.3倍的收敛加速，凸显了其在资源受限和异构终端设备环境中的高效性与可部署性。

---

## [IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory](https://arxiv.org/abs/2506.01048)

### Abstract
arXiv:2506.01048v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated exceptional performance across a wide range of natural language tasks. However, selecting the optimal LLM to respond to a user query often necessitates a delicate balance between performance and cost. While powerful models deliver better results, they come at a high cost, whereas smaller models are more cost-effective but less capable. To address this trade-off, we propose IRT-Router, a multi-LLM routing framework that efficiently routes user queries to the most suitable LLM. Inspired by Item Response Theory (IRT), a psychological measurement methodology, IRT-Router explicitly models the relationship between LLM capabilities and user query attributes. This not only enables accurate prediction of response performance but also provides interpretable insights, such as LLM abilities and query difficulty. Additionally, we design an online query warm-up technique based on semantic similarity, further enhancing the online generalization capability of IRT-Router. Extensive experiments on 20 LLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline methods in terms of effectiveness and interpretability. Its superior performance in cold-start scenarios further confirms the reliability and practicality of IRT-Router in real-world applications. Code is available at https://github.com/Mercidaiha/IRT-Router.

### 摘要
大语言模型（LLMs）在各类自然语言任务中展现出卓越性能。然而，为用户查询选择最优LLM往往需要在性能与成本之间取得微妙平衡——强大模型虽能提供更佳结果，但成本高昂；轻量级模型虽经济高效，但能力有限。为解决这一权衡问题，我们提出IRT-Router，一个基于多LLM的路由框架，可高效将用户查询分配至最合适的LLM。受心理测量学方法项目反应理论（IRT）启发，IRT-Router显式建模了LLM能力与用户查询属性间的关联关系，不仅能准确预测响应性能，还可提供模型能力、查询难度等可解释性洞察。此外，我们设计了一种基于语义相似度的在线查询预热技术，进一步增强了IRT-Router的在线泛化能力。在20个LLM和12个数据集上的大量实验表明，IRT-Router在效果与可解释性方面优于多数基线方法。其在冷启动场景下的优异表现，进一步证实了IRT-Router在实际应用中的可靠性与实用性。代码已开源：https://github.com/Mercidaiha/IRT-Router。

---

## [SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning](https://arxiv.org/abs/2506.01096)

### Abstract
arXiv:2506.01096v1 Announce Type: new 
Abstract: Large language models are increasingly used for complex reasoning tasks where high-quality offline data such as expert-annotated solutions and distilled reasoning traces are often available. However, in environments with sparse rewards, reinforcement learning struggles to sample successful trajectories, leading to inefficient learning. At the same time, these offline trajectories that represent correct reasoning paths are not utilized by standard on-policy reinforcement learning methods. To address this limitation, we propose SuperRL, a unified training framework that adaptively incorporates offline supervision into reinforcement learning. SuperRL introduces an Adaptive Switch to detect sparse reward conditions and activates a Hybrid Actor when necessary. The Hybrid Actor integrates policy gradient and supervised learning objectives at the loss level, enabling the model to benefit from accurate offline reasoning signals while maintaining the exploratory capacity of reinforcement learning. Experiments on a range of reasoning benchmarks show that SuperRL consistently outperforms standard reinforcement learning by improving sample efficiency, generalization, and robustness under sparse rewards.

### 摘要
大型语言模型越来越多地用于复杂推理任务，这些任务通常可获得高质量的离线数据，如专家标注的解决方案和提炼的推理轨迹。然而，在奖励稀疏的环境中，强化学习难以采样成功轨迹，导致学习效率低下。同时，这些代表正确推理路径的离线轨迹并未被标准的策略强化学习方法所利用。为解决这一局限，我们提出SuperRL，一种将离线监督自适应融入强化学习的统一训练框架。SuperRL引入自适应开关以检测稀疏奖励条件，并在必要时激活混合执行器。该混合执行器在损失层面整合了策略梯度和监督学习目标，使模型既能从准确的离线推理信号中受益，又能保持强化学习的探索能力。在一系列推理基准测试中，实验表明SuperRL通过提升样本效率、泛化能力和稀疏奖励下的鲁棒性，始终优于标准强化学习方法。

---

## [ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation](https://arxiv.org/abs/2506.01116)

### Abstract
arXiv:2506.01116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used across various scenarios due to their exceptional reasoning capabilities and natural language understanding. While LLMs demonstrate strong performance in tasks involving mathematics and coding, their effectiveness diminishes significantly when applied to chemistry-related problems. Chemistry problems typically involve long and complex reasoning steps, which contain specific terminology, including specialized symbol systems and complex nomenclature conventions. These characteristics often cause general LLMs to experience hallucinations during the reasoning process due to their lack of specific knowledge. However, existing methods are struggling to effectively leverage chemical expertise and formulas. Moreover, current uncertainty estimation methods, designed to mitigate potential reasoning errors, are unable to precisely identify specific steps or key knowledge. In this work, we propose a novel framework called ChemAU, which incorporates our adaptive uncertainty estimation method that applies different uncertainty values based on the position of reasoning steps within the whole reasoning chain. Leveraging this method, ChemAU identifies gaps in chemistry knowledge and precisely supplements chemical expertise with the specialized domain model, thereby correcting and updating the previously flawed reasoning chain. Our experiments with three popular LLMs across three chemistry datasets demonstrate that ChemAU significantly enhances both reasoning accuracy and uncertainty estimation.

### 摘要
大型语言模型（LLMs）因其卓越的推理能力和自然语言理解能力，被广泛应用于各种场景。尽管LLMs在数学和编程相关任务中表现出色，但在处理化学问题时其效能显著下降。化学问题通常涉及冗长复杂的推理步骤，包含特定术语体系——如专业符号系统与复杂的命名规则。这些特征常导致通用LLMs因缺乏领域知识而在推理过程中产生幻觉。然而，现有方法难以有效利用化学专业知识与公式。此外，当前旨在减少潜在推理错误的不确定性估计方法，无法精确定位具体步骤或关键知识节点。本研究提出名为ChemAU的新型框架，其核心是自适应不确定性估计方法——根据推理步骤在整体推理链中的位置动态调整不确定性估值。基于该方法，ChemAU能识别化学知识缺口，并通过专业领域模型精准补充化学知识，从而修正并更新存在缺陷的推理链。我们在三个化学数据集上对三种主流LLMs的实验表明，ChemAU显著提升了推理准确性与不确定性估计效能。

---

## [MCP-Zero: Proactive Toolchain Construction for LLM Agents from Scratch](https://arxiv.org/abs/2506.01056)

### Abstract
arXiv:2506.01056v1 Announce Type: new 
Abstract: Function-calling has enabled large language models (LLMs) to act as tool-using agents, but injecting thousands of tool schemas into the prompt is costly and error-prone. We introduce MCP-Zero, a proactive agent framework that lets the LLM itself decide when and which external tools to retrieve, thereby assembling a task-specific toolchain from scratch. The framework is built upon three components: (1) Proactive Tool Request, where the model emits a structured $\left&lt;\operatorname&#123;tool\_assistant&#125;\right&gt;$ block that explicitly specifies the desired server and task; (2) Hierarchical Vector Routing, a coarse-to-fine retrieval algorithm that first selects candidate servers and then ranks tools within each server based on the semantic similarity; (3) Iterative Proactive Invocation, enabling multi-round, cross-domain toolchain construction with minimal context overhead, and allowing the model to iteratively revise its request when the returned tools are insufficient. To evaluate our approach we also compile MCP-tools, a retrieval dataset comprising 308 MCP servers and 2,797 tools extracted from the official Model-Context-Protocol repository and normalized into a unified JSON schema. Experiments show that MCP-Zero (i) effectively addresses the context overhead problem of existing methods and accurately selects the correct tool from a pool of nearly 3,000 candidates (248.1k tokens); (ii) reduces token consumption by 98\% on the APIbank while maintaining high accuracy; and (iii) supports multi-turn tool invocation with consistent accuracy across rounds. The code and dataset will be released soon.

### 摘要
函数调用功能使大语言模型（LLMs）能够作为工具使用代理，但将数千个工具模式注入提示既成本高昂又容易出错。我们提出MCP-Zero，这是一种主动代理框架，让LLM自行决定何时检索及使用哪些外部工具，从而从零开始构建任务特定的工具链。该框架基于三个核心组件：（1）主动工具请求——模型生成结构化的$\left&lt;\operatorname&#123;tool\_assistant&#125;\right&gt;$区块，明确指定目标服务器和任务；（2）分层向量路由——一种由粗到精的检索算法，先筛选候选服务器，再基于语义相似度对各服务器内的工具进行排序；（3）迭代主动调用——支持多轮跨领域工具链构建，保持最低上下文开销，并允许模型在返回工具不足时迭代修正请求。为评估方法性能，我们还构建了MCP-tools检索数据集，包含从官方Model-Context-Protocol仓库提取的308个MCP服务器和2,797个工具，并规范化为统一JSON模式。实验表明MCP-Zero：（i）有效解决了现有方法的上下文开销问题，能从近3,000个候选工具（248.1k tokens）中准确选择正确工具；（ii）在APIbank数据集上减少98%的token消耗同时保持高准确率；（iii）支持多轮工具调用且各轮次准确率稳定。代码与数据集即将开源。

---

## [Retrieval-Augmented Generation of Ontologies from Relational Databases](https://arxiv.org/abs/2506.01232)

### Abstract
arXiv:2506.01232v1 Announce Type: new 
Abstract: Transforming relational databases into knowledge graphs with enriched ontologies enhances semantic interoperability and unlocks advanced graph-based learning and reasoning over data. However, previous approaches either demand significant manual effort to derive an ontology from a database schema or produce only a basic ontology. We present RIGOR, Retrieval-augmented Iterative Generation of RDB Ontologies, an LLM-driven approach that turns relational schemas into rich OWL ontologies with minimal human effort. RIGOR combines three sources via RAG, the database schema and its documentation, a repository of domain ontologies, and a growing core ontology, to prompt a generative LLM for producing successive, provenance-tagged delta ontology fragments. Each fragment is refined by a judge-LLM before being merged into the core ontology, and the process iterates table-by-table following foreign key constraints until coverage is complete. Applied to real-world databases, our approach outputs ontologies that score highly on standard quality dimensions such as accuracy, completeness, conciseness, adaptability, clarity, and consistency, while substantially reducing manual effort.

### 摘要
将关系数据库转化为具有丰富本体的知识图谱，能够增强语义互操作性，并实现对数据基于图的高级学习和推理。然而，现有方法要么需要大量人工从数据库模式中推导本体，要么只能生成基础本体。我们提出RIGOR（基于检索增强的关系数据库本体迭代生成方法），这是一种利用大语言模型、以最小人工投入将关系模式转化为丰富OWL本体的方法。RIGOR通过检索增强生成技术整合三类资源——数据库模式及其文档、领域本体库以及不断扩展的核心本体，驱动生成式大语言模型产出带有溯源标记的增量本体片段。每个片段经评审大语言模型优化后并入核心本体，并依照外键约束逐表迭代执行该流程直至完全覆盖。在实际数据库测试中，本方法生成的本体在准确性、完备性、简洁性、适应性、清晰度和一致性等标准质量维度上均表现优异，同时显著降低了人工工作量。

---

## [CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction](https://arxiv.org/abs/2506.01268)

### Abstract
arXiv:2506.01268v1 Announce Type: new 
Abstract: CleanS2S is a framework for human-like speech-to-speech interaction that advances conversational AI through single-file implementation and proactive dialogue capabilities. Our system integrates automatic speech recognition, large language models, and text-to-speech synthesis into a unified pipeline with real-time interruption handling, achieving low transition latency through full-duplex websocket connections and non-blocking I/O. Beyond conventional chatbot paradigms, we pioneer a proactive interaction mechanism, which combines memory systems with Subjective Action Judgement module, enabling five human-like response strategies: interruption, refusal, deflection, silence, and standard response. The memory module dynamically aggregates historical, and contextual data to inform interaction decisions. This approach breaks the rigid turn-based convention by allowing system-initiated dialog control and context-aware response selection. And we propose Action Judgement SFT that assesses input streams for responses strategies. The framework's single-file implementation with atomic configurations offers researchers unprecedented transparency and extensibility for interaction agents. The code of CleanS2S is released at \https://github.com/opendilab/CleanS2S.

### 摘要
CleanS2S是一个拟人化语音交互框架，通过单文件实现和主动对话能力推动会话式人工智能发展。本系统将自动语音识别、大语言模型和文本转语音合成整合至具有实时打断处理能力的统一流程，借助全双工WebSocket连接与非阻塞I/O实现低延迟切换。突破传统聊天机器人范式，我们首创了结合记忆系统与主观行为判定模块的主动交互机制，支持五种拟人化响应策略：打断、拒绝、转移、沉默及标准响应。记忆模块动态聚合历史与上下文数据以指导交互决策，通过系统主导的对话控制和情境感知响应选择，打破了僵化的轮转式对话惯例。我们提出行为判定监督微调方法（Action Judgement SFT）用于评估输入流并选择响应策略。该框架采用原子化配置的单文件实现方式，为交互智能体研究提供了前所未有的透明度和可扩展性。CleanS2S代码已发布于https://github.com/opendilab/CleanS2S。

---

## [Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner](https://arxiv.org/abs/2506.01301)

### Abstract
arXiv:2506.01301v1 Announce Type: new 
Abstract: Theory-of-Mind (ToM) enables humans to infer mental states-such as beliefs, desires, and intentions-forming the foundation of social cognition. However, existing computational ToM methods rely on structured workflows with ToM-specific priors or deep model fine-tuning, which struggle with scalability in multimodal environments and fail to generalize as task complexity increases. To address these limitations, we propose a scalable Bayesian ToM planner that decomposes ToM reasoning into stepwise Bayesian updates. Our framework introduces weak-to-strong control, allowing smaller language models (LMs) to specialize in ToM-specific likelihood estimation and transfer their reasoning behaviors to larger LMs (7B to 405B) for integration with social and world knowledge. This synergistic approach aligns large-model inference of human mental states with Bayesian principles. Extensive experiments show that our method achieves a 4.6% accuracy improvement over state-of-the-art techniques on multimodal ToM benchmarks, including challenging unseen scenarios, thereby establishing a new standard for modeling human mental states in complex environments.

### 摘要
心理理论（ToM）使人类能够推断心理状态（如信念、欲望和意图），构成社会认知的基础。然而，现有计算型ToM方法依赖于具有ToM特定先验的结构化流程或深度模型微调，这些方法在多模态环境中难以扩展，且随着任务复杂度增加而无法泛化。为解决这些局限性，我们提出一种可扩展的贝叶斯ToM规划器，将ToM推理分解为逐步贝叶斯更新。该框架引入弱到强控制机制，使较小语言模型（LMs）专注于ToM特定似然估计，并将其推理行为迁移至更大LMs（7B至405B参数），实现与社会及世界知识的整合。这种协同方法使大模型对人类心理状态的推断与贝叶斯原理保持一致。大量实验表明，我们的方法在多模态ToM基准测试中（包括具有挑战性的未见场景）比现有技术精度提升4.6%，从而为复杂环境中人类心理状态建模设立了新标准。

---

## [RAISE: Reasoning Agent for Interactive SQL Exploration](https://arxiv.org/abs/2506.01273)

### Abstract
arXiv:2506.01273v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have propelled research in natural language interfaces to databases. However, most state-of-the-art text-to- SQL systems still depend on complex, multi-stage pipelines. This work proposes a novel agentic framework that unifies schema linking, query generation, and itera- tive refinement within a single, end-to-end component. By leveraging the intrinsic reasoning abilities of LLMs, our method emulates how humans answer questions when working with unfamiliar databases: understanding the data by formulating hypotheses, running dynamic queries to validate them, reasoning over the results, and revising outputs based on observed results. Crucially, our approach intro- duces a new strategy for scaling test-time computation in text-to-SQL: we scale the depth of interactive database exploration and reflection. This shift enables the model to allocate computation dynamically to better understand the data, especially useful in ambiguous and underspecified scenarios. Our experiments show that it improved the Execution Accuracy (EX) from 44.8% to 56.5% on the challenging BIRD dataset using DeepSeek-R1-Distill-Llama-70B. Fur- thermore, when equipped with steps to add more diversity to the answers, our agent achieves a Best-of-N accuracy of 81.8% with 8 rounds of candidate gener- ation, rivaling the 82.79% achieved by the top-ranked published solution, while reducing engineering complexity. These findings position our unified framework as a promising alternative for building natural language interfaces to databases.

### 摘要
大语言模型（LLM）的最新进展推动了自然语言数据库界面的研究。然而，大多数先进的文本到SQL系统仍依赖于复杂的多阶段流程。本研究提出了一种新颖的智能体框架，将模式链接、查询生成和迭代优化统一在单一的端到端组件中。通过利用LLM固有的推理能力，我们的方法模拟了人类在处理陌生数据库时的提问应答过程：通过建立假设理解数据，执行动态查询进行验证，对结果进行推理，并根据观察结果修正输出。关键创新在于提出了一种扩展文本到SQL测试时计算的新策略：通过增加交互式数据库探索与反思的深度。这种转变使模型能动态分配计算资源以更好地理解数据，尤其在模糊和欠明确场景中效果显著。实验表明，在具有挑战性的BIRD数据集上，使用DeepSeek-R1-Distill-Llama-70B模型将执行准确率（EX）从44.8%提升至56.5%。此外，当采用增加答案多样性的步骤时，经过8轮候选生成后，我们的智能体实现了81.8%的最佳N准确率，与当前最高公开解决方案的82.79%相当，同时显著降低了工程复杂度。这些发现表明，我们的统一框架是构建自然语言数据库接口的有力替代方案。

---

## [Scalable In-Context Q-Learning](https://arxiv.org/abs/2506.01299)

### Abstract
arXiv:2506.01299v1 Announce Type: new 
Abstract: Recent advancements in language models have demonstrated remarkable in-context learning abilities, prompting the exploration of in-context reinforcement learning (ICRL) to extend the promise to decision domains. Due to involving more complex dynamics and temporal correlations, existing ICRL approaches may face challenges in learning from suboptimal trajectories and achieving precise in-context inference. In the paper, we propose \textbf&#123;S&#125;calable \textbf&#123;I&#125;n-\textbf&#123;C&#125;ontext \textbf&#123;Q&#125;-\textbf&#123;L&#125;earning (\textbf&#123;SICQL&#125;), an innovative framework that harnesses dynamic programming and world modeling to steer ICRL toward efficient reward maximization and task generalization, while retaining the scalability and stability of supervised pretraining. We design a prompt-based multi-head transformer architecture that simultaneously predicts optimal policies and in-context value functions using separate heads. We pretrain a generalized world model to capture task-relevant information, enabling the construction of a compact prompt that facilitates fast and precise in-context inference. During training, we perform iterative policy improvement by fitting a state value function to an upper-expectile of the Q-function, and distill the in-context value functions into policy extraction using advantage-weighted regression. Extensive experiments across a range of discrete and continuous environments show consistent performance gains over various types of baselines, especially when learning from suboptimal data. Our code is available at https://github.com/NJU-RL/SICQL

### 摘要
语言模型的最新进展展现出卓越的上下文学习能力，这促使研究者探索上下文强化学习（ICRL）以将该优势扩展至决策领域。由于涉及更复杂的动态特性和时序相关性，现有ICRL方法在学习次优轨迹和实现精确上下文推理方面可能面临挑战。本文提出\textbf&#123;可扩展上下文Q学习&#125;（\textbf&#123;SICQL&#125;），这一创新框架通过动态规划和世界建模引导ICRL实现高效奖励最大化与任务泛化，同时保留监督预训练的可扩展性和稳定性。我们设计了一种基于提示的多头Transformer架构，利用独立头部同步预测最优策略和上下文价值函数。通过预训练通用世界模型捕获任务相关信息，构建紧凑提示以实现快速精确的上下文推理。训练过程中，我们通过将状态价值函数拟合至Q函数的上期望分位数进行迭代策略改进，并利用优势加权回归将上下文价值函数蒸馏至策略提取中。在离散与连续环境中的大量实验表明，相较于各类基线方法（尤其是基于次优数据学习时），本方法均能取得稳定性能提升。代码已开源：https://github.com/NJU-RL/SICQL

---

## [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)

### Abstract
arXiv:2506.01372v1 Announce Type: new 
Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf&#123;the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.&#125; Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf&#123;implementation gap&#125;, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.

### 摘要
人工智能（AI）科学家的出现标志着科学发现范式的转变，其中大型语言模型（LLMs）成为从创意生成到实验实施的整个科研流程中的主要执行者。近期AI科学家的研究表明，其已具备独立科学发现的充分能力，所生成的研究报告被ICLR 2025研讨会和ACL 2025接收，这表明能够揭示人类未知现象、达到人类水平的AI科学家可能即将问世。尽管取得重大进展，AI科学家尚未在计算机科学领域产生能与自动化科研工具比肩的突破性成果。基于复杂工程任务现有基准的定量证据，以及对五个先进AI科学家系统生成的28篇研究论文的系统评估，我们认为AI科学家的根本瓶颈在于其执行必要验证程序的能力。现有AI科学家系统缺乏开展严格实验和产出高质量学术论文所需的执行能力。为深入阐明这一实施鸿沟的根源，我们详细探讨了AI科学家的本质局限。本立场文件旨在呼吁学界参与者共同弥合这一实施鸿沟。

---

## [ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research](https://arxiv.org/abs/2506.01326)

### Abstract
arXiv:2506.01326v1 Announce Type: new 
Abstract: Operations research (OR) is widely deployed to solve critical decision-making problems with complex objectives and constraints, impacting manufacturing, logistics, finance, and healthcare outcomes. While Large Language Models (LLMs) have shown promising results in various domains, their practical application in industry-relevant operations research (OR) problems presents significant challenges and opportunities. Preliminary industrial applications of LLMs for operations research face two critical deployment challenges: 1) Self-correction focuses on code syntax rather than mathematical accuracy, causing costly errors; 2) Complex expert selection creates unpredictable workflows that reduce transparency and increase maintenance costs, making them impractical for time-sensitive business applications. To address these business limitations, we introduce ORMind, a cognitive-inspired framework that enhances optimization through counterfactual reasoning. Our approach emulates human cognition, implementing an end-to-end workflow that systematically transforms requirements into mathematical models and executable solver code. It is currently being tested internally in Lenovo's AI Assistant, with plans to enhance optimization capabilities for both business and consumer customers. Experiments demonstrate that ORMind outperforms existing methods, achieving a 9.5\% improvement on the NL4Opt dataset and a 14.6\% improvement on the ComplexOR dataset.

### 摘要
运筹学（OR）被广泛应用于解决具有复杂目标和约束的关键决策问题，对制造业、物流、金融和医疗等领域产生重要影响。尽管大语言模型（LLM）在多个领域展现出良好前景，但其在工业相关运筹学问题中的实际应用仍面临重大挑战与机遇。当前LLM在运筹学的初步工业应用存在两大关键部署难题：1）自我修正机制仅关注代码语法而非数学准确性，导致代价高昂的错误；2）复杂的专家选择机制形成不可预测的工作流程，降低透明度并增加维护成本，使其难以适用于时效性强的商业场景。针对这些业务限制，我们提出ORMind框架——一种受认知启发的反事实推理优化框架。该方法模拟人类认知机制，构建端到端工作流，将需求系统性地转化为数学模型及可执行的求解器代码。该框架目前正在联想AI助手内部测试，计划为企业和消费者客户扩展优化功能。实验表明，ORMind在NL4Opt数据集上实现9.5%的性能提升，在ComplexOR数据集上提升14.6%，显著优于现有方法。

---

## [An Empirical Study of Group Conformity in Multi-Agent Systems](https://arxiv.org/abs/2506.01332)

### Abstract
arXiv:2506.01332v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.

### 摘要
大语言模型（LLM）的最新进展使得多智能体系统能够以接近人类推理的方式模拟现实世界互动。尽管先前研究已深入探讨了种族等受保护属性的偏见，但多智能体LLM交互中社会争议性议题的偏见产生与传播机制仍缺乏充分研究。本研究通过五个争议性话题的辩论，探讨LLM智能体如何塑造公众舆论。通过模拟超过2500场辩论，我们分析了初始持中立立场、被赋予中间派倾向的智能体如何随时间推移采纳特定立场。统计分析揭示了与人类行为高度一致的群体从众现象：LLM智能体倾向于与数量占优的群体或更智能的个体保持一致，后者会施加更强的影响力。这些发现凸显了智能体智力水平在话语权形成中的关键作用，同时揭示了在线互动中偏见放大的风险。研究结果强调需要制定政策来促进LLM生成讨论的多样性与透明度，以降低匿名网络环境中偏见传播的风险。

---

## [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)

### Abstract
arXiv:2506.01391v1 Announce Type: new 
Abstract: The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and $91.3\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.

### 摘要
大型语言模型智能体的最新进展为通过图形用户界面（GUI）实现任务自动化开辟了新途径，尤其在移动环境中，智能交互能显著提升可用性。然而，此类智能体的实际部署仍受若干关键挑战限制：现有训练数据常存在噪声且语义多样性不足，阻碍了精确基础定位与规划能力的学习；纯模仿训练的模型易对已知界面模式过拟合，在陌生场景中泛化能力不足；此外，先前研究多聚焦英语界面，忽视了中文移动生态等非英语应用的快速增长。本研究提出AgentCPM-GUI——一个专为鲁棒高效设备端GUI交互设计的80亿参数智能体。其训练流程包含：增强感知能力的基础定位预训练、基于中英文高质量操作轨迹的监督微调（模拟人类行为），以及采用GRPO算法提升推理能力的强化微调。我们还设计了精简动作空间以降低输出长度，支持移动设备低延迟执行。AgentCPM-GUI在五个公开基准及新构建的中文GUI基准CAGUI上达到最先进性能（类型匹配率96.9%，精确匹配率91.3%）。为促进复现与深入研究，我们公开了全部代码、模型检查点和评估数据。

---

## [PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization](https://arxiv.org/abs/2506.01475)

### Abstract
arXiv:2506.01475v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents' ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style Planning Guided Preference Optimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents' ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.

### 摘要
大语言模型（LLM）智能体在处理复杂交互问题时展现出卓越能力。现有LLM智能体主要生成自然语言计划来指导推理，这种方式冗长且低效。自然语言计划还针对特定任务定制，限制了智能体在相似任务间的泛化能力。为此，我们探索采用伪代码风格计划（P-code Plan）来捕捉推理的结构逻辑。研究发现P-code Plan能赋予LLM智能体更强的泛化能力和更高效率。基于此发现，我们提出一种伪代码风格的规划引导偏好优化方法PGPO，用于实现高效的智能体学习。通过两个面向规划的奖励机制，PGPO进一步提升了LLM智能体生成高质量P-code Plan及后续推理的能力。实验表明，PGPO在代表性智能体基准测试中取得最优性能，超越当前领先基线方法。分析表明PGPO在减少推理过程中的动作错误与遗漏方面具有优势。

---

## [Distinguishing Autonomous AI Agents from Collaborative Agentic Systems: A Comprehensive Framework for Understanding Modern Intelligent Architectures](https://arxiv.org/abs/2506.01438)

### Abstract
arXiv:2506.01438v1 Announce Type: new 
Abstract: The emergence of large language models has catalyzed two distinct yet interconnected paradigms in artificial intelligence: standalone AI Agents and collaborative Agentic AI ecosystems. This comprehensive study establishes a definitive framework for distinguishing these architectures through systematic analysis of their operational principles, structural compositions, and deployment methodologies. We characterize AI Agents as specialized, tool-enhanced systems leveraging foundation models for targeted automation within constrained environments. Conversely, Agentic AI represents sophisticated multi-entity frameworks where distributed agents exhibit emergent collective intelligence through coordinated interaction protocols. Our investigation traces the evolutionary trajectory from traditional rule-based systems through generative AI foundations to contemporary agent architectures. We present detailed architectural comparisons examining planning mechanisms, memory systems, coordination protocols, and decision-making processes. The study categorizes application landscapes, contrasting single-agent implementations in customer service and content management with multi-agent deployments in research automation and complex decision support. We identify critical challenges including reliability issues, coordination complexities, and scalability constraints, while proposing innovative solutions through enhanced reasoning frameworks, robust memory architectures, and improved coordination mechanisms. This framework provides essential guidance for practitioners selecting appropriate agentic approaches and establishes foundational principles for next-generation intelligent system development.

### 摘要
大型语言模型的出现催生了人工智能领域两种截然不同却又相互关联的范式：独立AI智能体与协作式Agentic AI生态系统。本研究通过系统分析其运作原理、结构组成与部署方法，建立了区分这些架构的权威框架。我们将AI智能体定义为基于基础模型、通过工具增强的专用系统，用于受限环境下的目标自动化；而Agentic AI则代表复杂的多实体框架，其分布式智能体通过协调交互协议展现出涌现的集体智能。研究追溯了从传统规则系统到生成式AI基础，再到当代智能体架构的演进轨迹，并通过规划机制、记忆系统、协调协议和决策过程的详细架构对比进行剖析。研究对应用场景进行分类，对比了客户服务和内容管理中的单智能体实现与科研自动化和复杂决策支持中的多智能体部署。我们识别出包括可靠性问题、协调复杂性和可扩展性限制等关键挑战，同时通过增强推理框架、鲁棒记忆架构和改进协调机制提出创新解决方案。该框架为从业者选择合适的智能体方法提供了重要指导，并为下一代智能系统开发奠定了基本原则。

---

## [Agentic Episodic Control](https://arxiv.org/abs/2506.01442)

### Abstract
arXiv:2506.01442v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to scientific discovery and AI alignment. However, its broader applicability remains limited by challenges such as low data efficiency and poor generalizability. Recent advances suggest that large language models, with their rich world knowledge and reasoning capabilities, could complement RL by enabling semantic state modeling and task-agnostic planning. In this work, we propose the Agentic Episodic Control (AEC), a novel architecture that integrates RL with LLMs to enhance decision-making. The AEC can leverage a large language model (LLM) to map the observations into language-grounded embeddings, which further can be stored in an episodic memory for rapid retrieval of high-value experiences. Simultaneously, a World-Graph working memory module is utilized to capture structured environmental dynamics in order to enhance relational reasoning. Furthermore, a lightweight critical state detector dynamically arbitrates between the episodic memory recall and the world-model-guided exploration. On the whole, by combining the trial-and-error learning scheme with LLM-derived semantic priors, the proposed AEC can improve both data efficiency and generalizability in reinforcement learning. In experiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial improvements over existing baselines, especially on complex and generalization tasks like FindObj, where it outperforms the best baseline by up to 76%. The proposed AEC framework bridges the strengths of numeric reinforcement learning and symbolic reasoning, which provides a pathway toward more adaptable and sample-efficient agents.

### 摘要
强化学习（RL）在人工智能领域取得了从游戏博弈到科学发现乃至AI对齐的突破性进展。然而，其广泛应用仍受限于数据效率低下和泛化能力不足等挑战。最新研究表明，具备丰富世界知识和推理能力的大语言模型可通过语义状态建模和任务无关规划来补充强化学习。本研究提出"自主情景控制"（AEC）架构，通过整合RL与LLM来增强决策能力。该架构利用大语言模型将观测数据映射为语言锚定嵌入，并将其存储于情景记忆中以实现高价值经验的快速检索；同时采用世界图工作记忆模块捕捉结构化环境动态以增强关系推理；此外，轻量化关键状态检测器动态协调情景记忆召回与世界模型引导探索。整体而言，通过将试错学习机制与LLM衍生的语义先验相结合，AEC能同时提升强化学习的数据效率和泛化能力。在BabyAI-Text基准测试中，AEC较现有基线方法取得显著提升——尤其在FindObj等复杂泛化任务上，其性能最高超出最佳基线76%。该框架融合了数值强化学习与符号推理的优势，为构建更具适应性和样本效率的智能体提供了新路径。

---

## [Reasoning-Based Approach with Chain-of-Thought for Alzheimer's Detection Using Speech and Large Language Models](https://arxiv.org/abs/2506.01683)

### Abstract
arXiv:2506.01683v1 Announce Type: new 
Abstract: Societies worldwide are rapidly entering a super-aged era, making elderly health a pressing concern. The aging population is increasing the burden on national economies and households. Dementia cases are rising significantly with this demographic shift. Recent research using voice-based models and large language models (LLM) offers new possibilities for dementia diagnosis and treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and language models. The process starts with automatic speech recognition to convert speech to text. We add a linear layer to an LLM for Alzheimer's disease (AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT reasoning and cues. This approach showed an 16.7% relative performance improvement compared to methods without CoT prompt reasoning. To the best of our knowledge, our proposed method achieved state-of-the-art performance in CoT approaches.

### 摘要
全球社会正快速步入超老龄化时代，老年人健康问题日益凸显。人口老龄化持续加重国民经济与家庭负担，伴随这一人口结构转变，痴呆症发病率显著攀升。基于语音模型与大语言模型（LLM）的最新研究为痴呆症诊疗提供了新思路。本研究提出的思维链（CoT）推理方法融合了语音与语言模型：首先通过自动语音识别将语音转换为文本，随后在LLM中增加线性层，结合监督微调（SFT）与CoT推理及提示线索，实现阿尔茨海默病（AD）与非AD分类。相较于非CoT提示推理方法，本方案获得16.7%的相对性能提升。据我们所知，该研究方法在CoT领域取得了最先进的性能表现。

---

## [Social Cooperation in Conversational AI Agents](https://arxiv.org/abs/2506.01624)

### Abstract
arXiv:2506.01624v1 Announce Type: new 
Abstract: The development of AI agents based on large, open-domain language models (LLMs) has paved the way for the development of general-purpose AI assistants that can support human in tasks such as writing, coding, graphic design, and scientific research. A major challenge with such agents is that, by necessity, they are trained by observing relatively short-term interactions with humans. Such models can fail to generalize to long-term interactions, for example, interactions where a user has repeatedly corrected mistakes on the part of the agent. In this work, we argue that these challenges can be overcome by explicitly modeling humans' social intelligence, that is, their ability to build and maintain long-term relationships with other agents whose behavior cannot always be predicted. By mathematically modeling the strategies humans use to communicate and reason about one another over long periods of time, we may be able to derive new game theoretic objectives against which LLMs and future AI agents may be optimized.

### 摘要
基于大型开放域语言模型（LLM）的AI智能体发展，为构建通用人工智能助手铺平了道路，这类助手能够支持人类完成写作、编程、图形设计和科研等任务。此类智能体面临的主要挑战在于，其训练过程必然基于对人类短期交互行为的观察，因而难以泛化至长期交互场景——例如用户多次纠正智能体错误的情况。本研究提出，通过显式建模人类的社会智能（即与行为不可完全预测的其他智能体建立并维持长期关系的能力），可以克服这些挑战。通过数学建模人类在长期互动中使用的沟通与推理策略，我们有望推导出新的博弈论目标，从而优化LLM及未来AI智能体的训练方向。

---

## [Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents](https://arxiv.org/abs/2506.01689)

### Abstract
arXiv:2506.01689v1 Announce Type: new 
Abstract: Querying generative AI models, e.g., large language models (LLMs), has become a prevalent method for information acquisition. However, existing query-answer datasets primarily focus on textual responses, making it challenging to address complex user queries that require visual demonstrations or explanations for better understanding. To bridge this gap, we construct a benchmark, RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V) models in answering real-world, visually grounded queries. It identifies 7.5K real user queries with video response intents from Chatbot-Arena and builds 4.5K high-quality query-video pairs through a multistage video retrieval and refinement process. We further develop a multi-angle evaluation system to assess the quality of generated video answers. Experiments indicate that current T2V models struggle with effectively addressing real user queries, pointing to key challenges and future research opportunities in multimodal AI.

### 摘要
查询生成式人工智能模型（如大语言模型）已成为获取信息的普遍方法。然而现有问答数据集主要关注文本响应，难以应对需要视觉演示或解释以增强理解力的复杂用户查询。为填补这一空白，我们构建了RealVideoQuest基准，旨在评估文生视频模型在回答现实世界视觉化查询时的能力。该研究从Chatbot-Arena中识别出7500个具有视频响应意图的真实用户查询，并通过多阶段视频检索与优化流程构建了4500个高质量查询-视频对。我们进一步开发了多角度评估体系来衡量生成视频答案的质量。实验表明，当前文生视频模型在有效解决真实用户查询方面仍存在困难，这为多模态人工智能领域指明了关键挑战与未来研究方向。

---

## [K12Vista: Exploring the Boundaries of MLLMs in K-12 Education](https://arxiv.org/abs/2506.01676)

### Abstract
arXiv:2506.01676v1 Announce Type: new 
Abstract: Multimodal large language models have demonstrated remarkable reasoning capabilities in various visual tasks. However, their abilities in K12 scenarios are still systematically underexplored. Previous studies suffer from various limitations including narrow subject coverage, insufficient data scale, lack of diversity in question types, and naive answer-centric evaluation method, resulting in insufficient exploration of model capabilities. To address these gaps, we propose K12Vista, the most comprehensive multimodal benchmark for Chinese K12 subject knowledge understanding and reasoning to date, featuring 33,000 questions across five core subjects from primary to high school and three question types. Moreover, beyond the final outcome, we are also concerned with the correctness of MLLMs' reasoning processes. For this purpose, we meticulously compiles errors from MLLMs' reasoning processes and leverage an automated data pipeline to construct K12-PEM-800K, the largest process evaluation dataset offering detailed step-by-step judgement annotations for MLLMs' reasoning. Subsequently, we developed K12-PEM, an advanced process evaluation model that integrates an overall assessment of both the reasoning process and answer correctness. Moreover, we also introduce K12-PEBench, the first high-quality, human-annotated benchmark specifically designed for evaluating abilities of reasoning process evaluation.Extensive experiments reveal that current MLLMs exhibit significant flaws when reasoning within K12Vista, providing critical insights for the development of more capable MLLMs.We open our resources at https://github.com/lichongod/K12Vista.

### 摘要
多模态大语言模型在各种视觉任务中展现出卓越的推理能力，但其在K12教育场景中的能力仍缺乏系统性探索。现有研究存在学科覆盖狭窄、数据规模不足、题型单一及以答案为唯一评估标准等局限，导致模型能力探究不够深入。为填补这些空白，我们提出K12Vista——迄今最全面的中文K12学科知识理解与推理多模态基准，涵盖中小学五个核心学科的33,000道题目及三种问题类型。此外，我们不仅关注最终结果，更重视模型推理过程的正确性。为此，我们系统梳理了模型推理中的错误类型，并利用自动化数据管道构建了K12-PEM-800K——目前最大的过程评估数据集，为模型推理提供细粒度分步评判标注。基于此，我们开发了K12-PEM评估模型，可综合评判推理过程与答案正确性。同时推出首个高质量人工标注的推理过程评估基准K12-PEBench。大量实验表明，现有模型在K12Vista中的推理存在显著缺陷，这为开发更强大的模型提供了关键洞见。所有资源已开源：https://github.com/lichongod/K12Vista。

---

## [MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments](https://arxiv.org/abs/2506.01616)

### Abstract
arXiv:2506.01616v1 Announce Type: new 
Abstract: The emergence of multimodal LLM-based agents (MLAs) has transformed interaction paradigms by seamlessly integrating vision, language, action and dynamic environments, enabling unprecedented autonomous capabilities across GUI applications ranging from web automation to mobile systems. However, MLAs introduce critical trustworthiness challenges that extend far beyond traditional language models' limitations, as they can directly modify digital states and trigger irreversible real-world consequences. Existing benchmarks inadequately tackle these unique challenges posed by MLAs' actionable outputs, long-horizon uncertainty and multimodal attack vectors. In this paper, we introduce MLA-Trust, the first comprehensive and unified framework that evaluates the MLA trustworthiness across four principled dimensions: truthfulness, controllability, safety and privacy. We utilize websites and mobile applications as realistic testbeds, designing 34 high-risk interactive tasks and curating rich evaluation datasets. Large-scale experiments involving 13 state-of-the-art agents reveal previously unexplored trustworthiness vulnerabilities unique to multimodal interactive scenarios. For instance, proprietary and open-source GUI-interacting MLAs pose more severe trustworthiness risks than static MLLMs, particularly in high-stakes domains; the transition from static MLLMs into interactive MLAs considerably compromises trustworthiness, enabling harmful content generation in multi-step interactions that standalone MLLMs would typically prevent; multi-step execution, while enhancing the adaptability of MLAs, involves latent nonlinear risk accumulation across successive interactions, circumventing existing safeguards and resulting in unpredictable derived risks. Moreover, we present an extensible toolbox to facilitate continuous evaluation of MLA trustworthiness across diverse interactive environments.

### 摘要
基于多模态大语言模型的智能体（MLAs）的出现通过无缝整合视觉、语言、动作和动态环境，彻底改变了交互范式，使其在从网络自动化到移动系统的图形用户界面应用中展现出前所未有的自主能力。然而，MLAs带来了远超传统语言模型局限性的关键可信度挑战，因为它们能直接修改数字状态并触发不可逆的现实后果。现有基准测试未能充分应对MLAs可执行输出、长时程不确定性和多模态攻击向量所带来的独特挑战。本文提出MLA-Trust，首个全面统一的评估框架，从四个原则维度（真实性、可控性、安全性和隐私性）系统评估MLA的可信度。我们以网站和移动应用作为真实测试平台，设计了34个高风险交互任务并构建了丰富的评估数据集。通过对13个最先进智能体的大规模实验，揭示了多模态交互场景特有的、此前未被探索的可信度漏洞。例如：专有和开源GUI交互型MLAs比静态多模态大语言模型（MLLMs）存在更严重的可信度风险，尤其在高风险领域；从静态MLLMs转变为交互式MLAs会显著降低可信度，使得多步交互中生成有害内容成为可能（而独立MLLMs通常能阻止此类行为）；多步执行虽增强MLAs适应性，但会通过连续交互产生潜在非线性风险累积，绕过现有防护机制并导致不可预测的衍生风险。此外，我们开发了可扩展工具包，以促进不同交互环境下MLA可信度的持续评估。

---

## [Self-Challenging Language Model Agents](https://arxiv.org/abs/2506.01716)

### Abstract
arXiv:2506.01716v1 Announce Type: new 
Abstract: Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.

### 摘要
大语言模型正迅速成为能够使用工具的智能代理的基础。然而，训练此类代理具有挑战性，因为它需要人类创建和标注多样化的任务集、工具集以及评估标准。本文提出了一种自我挑战框架，用于训练代理基于其自身生成的高质量任务进行学习。代理首先扮演挑战者角色，在与给定工具交互后生成任务。这些任务采用一种新颖的通用问题类别形式，称为"代码即任务"，其由指令、验证函数、解决方案以及作为测试用例的失败案例定义，从而能够筛选出仅高质量的任务。随后，代理转为执行者角色，通过强化学习在这些任务上进行训练，并将评估反馈作为奖励。在两个现有的多轮工具使用代理基准测试（M3ToolEval和TauBench）上的评估表明，自我挑战框架在Llama-3.1-8B-Instruct模型上实现了超过两倍的性能提升，尽管仅使用了自我生成的训练数据。

---

## [A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents](https://arxiv.org/abs/2506.01804)

### Abstract
arXiv:2506.01804v1 Announce Type: new 
Abstract: This paper provides an in-depth technical analysis and implementation methodology of the open-source Agent-to-Agent (A2A) protocol developed by Google and the Model Context Protocol (MCP) introduced by Anthropic. While the evolution of LLM-based autonomous agents is rapidly accelerating, efficient interactions among these agents and their integration with external systems remain significant challenges. In modern AI systems, collaboration between autonomous agents and integration with external tools have become essential elements for building practical AI applications. A2A offers a standardized communication method that enables agents developed in heterogeneous environments to collaborate effectively, while MCP provides a structured I/O framework for agents to connect with external tools and resources. Prior studies have focused primarily on the features and applications of either A2A or MCP individually. In contrast, this study takes an integrated approach, exploring how the two protocols can complement each other to address interoperability issues and facilitate efficient collaboration within complex agent ecosystems.

### 摘要
本文对谷歌开发的开源Agent-to-Agent（A2A）协议与Anthropic提出的Model Context Protocol（MCP）进行了深入技术分析与实现方法研究。尽管基于大语言模型的自主智能体发展迅猛，但这些智能体间的高效交互及其与外部系统的集成仍面临重大挑战。在现代人工智能系统中，自主智能体间的协作及与外部工具的集成已成为构建实用AI应用的关键要素。A2A提供了一种标准化通信方法，使异构环境下开发的智能体能够有效协作；而MCP则为智能体连接外部工具与资源提供了结构化I/O框架。先前研究主要单独关注A2A或MCP的特性与应用，本研究则采用整合视角，探讨两种协议如何互补以解决互操作性问题，促进复杂智能体生态系统中的高效协作。

---

## [Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese](https://arxiv.org/abs/2506.00019)

### Abstract
arXiv:2506.00019v1 Announce Type: cross 
Abstract: This report introduces the experience of developing Amadeus Verbo, a family of large language models for Brazilian Portuguese. To handle diverse use cases, Amadeus Verbo includes base-tuned, merged, and instruction-tuned models in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main objective is to show how easy it is to fine-tune foundation models to democratize the open-source development of Brazilian Portuguese LLMs when data and resources are available. Amadeus-Verbo family models are all available at HuggingFace at https://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.

---

## [Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research](https://arxiv.org/abs/2506.01839)

### Abstract
arXiv:2506.01839v1 Announce Type: new 
Abstract: As large language models (LLMs) transition from static tools to fully agentic systems, their potential for transforming social science research has become increasingly evident. This paper introduces a structured framework for understanding the diverse applications of LLM-based agents, ranging from simple data processors to complex, multi-agent systems capable of simulating emergent social dynamics. By mapping this developmental continuum across six levels, the paper clarifies the technical and methodological boundaries between different agentic architectures, providing a comprehensive overview of current capabilities and future potential. It highlights how lower-tier systems streamline conventional tasks like text classification and data annotation, while higher-tier systems enable novel forms of inquiry, including the study of group dynamics, norm formation, and large-scale social processes. However, these advancements also introduce significant challenges, including issues of reproducibility, ethical oversight, and the risk of emergent biases. The paper critically examines these concerns, emphasizing the need for robust validation protocols, interdisciplinary collaboration, and standardized evaluation metrics. It argues that while LLM-based agents hold transformative potential for the social sciences, realizing this promise will require careful, context-sensitive deployment and ongoing methodological refinement. The paper concludes with a call for future research that balances technical innovation with ethical responsibility, encouraging the development of agentic systems that not only replicate but also extend the frontiers of social science, offering new insights into the complexities of human behavior.

### 摘要
随着大型语言模型(LLMs)从静态工具向完全代理系统的转变，其变革社会科学研究的潜力日益凸显。本文提出一个结构化框架，用以理解基于LLM的代理系统从简单数据处理到能够模拟涌现社会动态的复杂多代理系统的多样化应用。通过将这一发展连续体划分为六个层级，本文厘清了不同代理架构之间的技术和方法论边界，全面概述了当前能力与未来潜力。研究指出，低层级系统可优化文本分类和数据标注等常规任务，而高层级系统则支持群体动力学、规范形成及大规模社会过程等新型研究范式。然而，这些进展也带来了重大挑战，包括可复现性问题、伦理监督缺失以及涌现偏见风险。本文批判性地审视了这些问题，强调需要建立稳健的验证协议、开展跨学科合作并制定标准化评估指标。研究认为，尽管基于LLM的代理系统具有变革社会科学的潜力，但实现这一愿景需要谨慎的、情境敏感的部署以及持续的方法论改进。最后，本文呼吁未来研究应在技术创新与伦理责任之间取得平衡，鼓励开发不仅能复现更能拓展社会科学前沿的代理系统，为理解人类行为的复杂性提供新视角。

---

## [Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods](https://arxiv.org/abs/2506.01901)

### Abstract
arXiv:2506.01901v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) on domain-specific data is the dominant approach for adapting foundation models to specialized tasks. However, it has been observed that SFT models tend to forget knowledge acquired during pretraining. In vision models, ensembling a pretrained model with its fine-tuned counterpart has been shown to mitigate this issue. In this work, we demonstrate that the same holds for language models, and, more strikingly, we observe an overadaptation phenomenon: the ensemble model not only retains general knowledge from the foundation model but also outperforms the fine-tuned model even on the fine-tuning domain itself. Despite the empirical success of ensembling, a theoretical understanding of its benefits remains underexplored. We develop a formal theoretical analysis of the overadaptation phenomenon. Ensembling mitigates this by balancing two primary sources of error: bias, caused by insufficient fine-tuning, and variance, introduced by overfitting to fine-tuning data. While regularization techniques aim to address this trade-off, we show that ensembling provides a more effective solution. We analyze this phenomenon in over-parameterized linear settings and demonstrate that interpolating between pretrained and fine-tuned weights significantly improves performance. These findings offer theoretical justification for the observed advantages of model ensembling, supported by empirical experiments consistent with our analysis.

### 摘要
基于领域特定数据的监督微调（SFT）是将基础模型适配到专业任务的主要方法。然而，研究发现SFT模型往往会遗忘预训练阶段获得的知识。在视觉模型中，将预训练模型与其微调版本集成已被证明能缓解这一问题。本工作证实该结论同样适用于语言模型，并且更显著地观察到了过适应现象：集成模型不仅保留了基础模型的通用知识，甚至在微调领域本身也优于微调后的单一模型。尽管集成方法取得了实证成功，但其优势的理论理解仍待深入。我们建立了关于过适应现象的正式理论分析，发现集成通过平衡两种主要误差源来缓解该问题：由微调不足引起的偏差，以及由对微调数据过拟合导致的方差。虽然正则化技术旨在解决这一权衡，但我们证明集成提供了更有效的解决方案。我们在过参数化线性场景中分析该现象，并证明预训练权重与微调权重之间的插值能显著提升性能。这些发现为模型集成优势提供了理论依据，实证实验结果也与我们的分析一致。

---

## [Using LLMs to Advance the Cognitive Science of Collectives](https://arxiv.org/abs/2506.00052)

### Abstract
arXiv:2506.00052v1 Announce Type: cross 
Abstract: LLMs are already transforming the study of individual cognition, but their application to studying collective cognition has been underexplored. We lay out how LLMs may be able to address the complexity that has hindered the study of collectives and raise possible risks that warrant new methods.

### 摘要
大型语言模型（LLMs）已经开始改变个体认知研究，但其在集体认知研究中的应用尚未得到充分探索。我们阐述了LLMs如何能够解决阻碍集体研究的复杂性，并提出了可能需要新方法应对的潜在风险。

---

## [WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue](https://arxiv.org/abs/2506.01881)

### Abstract
arXiv:2506.01881v1 Announce Type: new 
Abstract: Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.

### 摘要
面向任务的对话系统常面临用户话语看似语义完整但缺乏必要结构信息以执行适当系统操作的困境。这源于用户往往未能完全理解自身需求，而系统却需要精确的意图定义。当前基于大语言模型的代理无法有效区分语言完整表达与上下文可触发表达，且缺乏协作式意图形成的框架。我们提出STORM框架，通过UserLLM（拥有完整内部访问权限）与AgentLLM（仅可观察行为）的对话建模非对称信息动态。该框架生成标注语料库，捕捉表达轨迹与潜在认知转换，从而系统分析协作理解的发展过程。我们的贡献包括：（1）形式化对话系统中的非对称信息处理；（2）建模意图形成过程以追踪协作理解的演化；（3）评估指标同时测量内部认知改进与任务表现。在四种语言模型上的实验表明，适度不确定性（40-60%）在某些场景下能优于完全透明状态，且模型特异性模式提示需要重新思考人机协作中最优信息完整性的标准。这些发现有助于理解非对称推理动态，并为不确定性校准的对话系统设计提供依据。

---

## [Large language models can learn and generalize steganographic chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926)

### Abstract
arXiv:2506.01926v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. By proactively preventing models from acting on CoT indicating misaligned or harmful intent, CoT monitoring can be used to reduce risks associated with deploying models. However, developers may be incentivized to train away the appearance of harmful intent from CoT traces, by either customer preferences or regulatory requirements. Recent works have shown that banning mention of a specific example of reward hacking, which may be done either to make CoT presentable to users or as a naive attempt to prevent the behavior, causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior. Such obfuscation threatens the reliability of CoT monitoring. However, obfuscation of reasoning can be due to its internalization to latent space computation, or its encoding within the CoT. Here, we provide an extension to these results. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning. We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.

### 摘要
思维链（CoT）推理不仅能提升大语言模型的性能，还可为决策过程提供关键洞察，这使其成为监测模型意图与规划的有效工具。通过主动阻止模型执行那些表明意图偏差或有害的CoT推理，CoT监控可降低模型部署风险。然而，出于客户偏好或监管要求，开发者可能倾向于通过训练消除CoT痕迹中有害意图的表征。近期研究表明，禁止提及奖励黑客攻击的具体实例（无论是为了使CoT呈现给用户，还是作为阻止该行为的简单尝试），会导致不良推理痕迹被模糊化，但不良行为却持续存在。这种模糊化威胁着CoT监控的可靠性。值得注意的是，推理的模糊化可能源于其被内化为潜在空间计算，或被编码在CoT中。本文对这些发现进行了扩展：首先证明对关键推理痕迹中特定字符串使用的惩罚会导致模型替换为替代字符串。关键在于，这并未改变模型执行任务的基本方法，表明模型能学会对推理进行隐写编码。我们进一步证明模型可泛化编码方案——当被惩罚字符串属于某个总体类别时，模型不仅能替换训练中见过的字符串，还能为该类别所有成员开发通用编码方案，并应用于保留测试字符串。

---

## [Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization](https://arxiv.org/abs/2506.00002)

### Abstract
arXiv:2506.00002v1 Announce Type: cross 
Abstract: Recent years have witnessed a significant increase in the adoption of AI techniques to enhance electronic design automation. In particular, the emergence of Large Language Models (LLMs) has sparked significant interest in LLM-assisted hardware design generation, spanning applications from classical digital circuits to quantum computing. Despite substantial progress in this direction, the quality of LLM-generated hardware design still cannot meet the requirements for practical deployment. In this work, we identify three critical challenges hindering the development of LLM-assisted hardware design generation: 1) limited data availability, 2) varied data quality, 3) inadequate inference-time efficiency. To address these fundamental challenges, this paper introduces a two-stage framework for AI-assisted hardware design by exploring decentralized training and personalized inference. In the first stage, we propose to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. To mitigate the impact of low-quality data, we identify optimization opportunities in hardware generation tasks, using user-defined metrics for model aggregation. The second stage focuses on client personalization to enhance both speed and quality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware generation efficiency. To optimize Trueput, we implement personalized inference-time acceleration and customized sampling strategies. Evaluating both classical and quantum benchmarks, our experimental results demonstrate that the proposed two-stage framework can significantly improve the model capability for hardware design generation. As orthogonal enhancements to existing methods, our framework can achieve $33\% \sim 50\%$ semantic accuracy improvement and $2.3$ times speedup, depending on the difficulty of the generation tasks.

### 摘要
近年来，人工智能技术在提升电子设计自动化方面的应用显著增长。特别是大型语言模型（LLMs）的出现，引发了人们对LLM辅助硬件设计生成的广泛关注，其应用范围涵盖从经典数字电路到量子计算等多个领域。尽管该方向已取得重大进展，但LLM生成的硬件设计质量仍无法满足实际部署需求。本研究揭示了阻碍LLM辅助硬件设计生成发展的三大关键挑战：1）数据可用性有限；2）数据质量参差不齐；3）推理时效不足。为应对这些根本性挑战，本文通过探索去中心化训练与个性化推理，提出一个两阶段的人工智能辅助硬件设计框架。第一阶段提出采用分层去中心化训练机制来利用私有领域设计资源，以解决数据共享限制问题。为降低低质量数据的影响，我们基于用户定义指标进行模型聚合，识别硬件生成任务中的优化机会。第二阶段聚焦客户端个性化以提升速度与质量，引入新指标"真实吞吐"（Trueput）来分析LLM辅助硬件生成效率。为实现真实吞吐优化，我们实施了个性化推理加速与定制化采样策略。通过对经典和量子基准测试的评估，实验结果表明所提出的两阶段框架能显著提升硬件设计生成的模型能力。作为现有方法的正交增强，本框架可实现33%～50%的语义准确率提升和2.3倍加速效果，具体提升幅度取决于生成任务的难度。

---

## [COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents](https://arxiv.org/abs/2506.01900)

### Abstract
arXiv:2506.01900v1 Announce Type: new 
Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.

### 摘要
自主大型语言模型（LLM）代理的迅猛发展和广泛应用展现出跨领域的强大潜力，但其部署正日益受到巨大计算需求（特别是图形处理器GPU资源）的制约。本文致力于解决LLM代理系统中资源优化的关键问题，提出COALESCE框架（基于技能能力评估的成本优化安全代理劳务交换），该创新框架使自主LLM代理能动态将特定子任务外包给专业且经济高效的第三方LLM代理。该框架整合了混合技能表征、动态技能发现、自动化任务分解、内部执行成本与外部外包价格对比的统一成本模型、简化的市场决策算法以及LLM代理间标准化通信协议等机制。通过239次理论模拟验证显示其具有41.8%的成本降低潜力，而在240项实际LLM任务的大规模实证中，采用ε-贪婪探索策略后实现了20.3%的成本节约，证实了其理论可行性与实践有效性。Google提出的Agent2Agent（A2A）协议等开放标准的出现，进一步凸显了COALESCE这类能利用此类标准实现高效代理交互的框架的必要性。通过构建动态代理能力市场（可能采用A2A等通信协议），COALESCE旨在显著降低运营成本、增强系统可扩展性并促进专业化代理经济生态的形成，从而使复杂LLM代理功能更易获取且更具经济可行性。

---

## [Rapid yet accurate Tile-circuit and device modeling for Analog In-Memory Computing](https://arxiv.org/abs/2506.00004)

### Abstract
arXiv:2506.00004v1 Announce Type: cross 
Abstract: Analog In-Memory Compute (AIMC) can improve the energy efficiency of Deep Learning by orders of magnitude. Yet analog-domain device and circuit non-idealities -- within the analog ``Tiles'' performing Matrix-Vector Multiply (MVM) operations -- can degrade neural-network task accuracy. We quantify the impact of low-level distortions and noise, and develop a mathematical model for Multiply-ACcumulate (MAC) operations mapped to analog tiles. Instantaneous-current IR-drop (the most significant circuit non-ideality), and ADC quantization effects are fully captured by this model, which can predict MVM tile-outputs both rapidly and accurately, as compared to much slower rigorous circuit simulations. A statistical model of PCM read noise at nanosecond timescales is derived from -- and matched against -- experimental measurements. We integrate these (statistical) device and (deterministic) circuit effects into a PyTorch-based framework to assess the accuracy impact on the BERT and ALBERT Transformer networks. We show that hardware-aware fine-tuning using simple Gaussian noise provides resilience against ADC quantization and PCM read noise effects, but is less effective against IR-drop. This is because IR-drop -- although deterministic -- is non-linear, is changing significantly during the time-integration window, and is ultimately dependent on all the excitations being introduced in parallel into the analog tile. The apparent inability of simple Gaussian noise applied during training to properly prepare a DNN network for IR-drop during inference implies that more complex training approaches -- incorporating advances such as the Tile-circuit model introduced here -- will be critical for resilient deployment of large neural networks onto AIMC hardware.

### 摘要
模拟内存计算（AIMC）能将深度学习的能效提升数个数量级。然而，执行矩阵向量乘法（MVM）运算的模拟"计算单元"内部的器件与电路非理想特性会降低神经网络任务精度。我们量化了底层失真与噪声的影响，并建立了映射至模拟计算单元的乘加运算（MAC）数学模型。该模型完整捕获了瞬时电流IR压降（最显著的电路非理想性）和ADC量化效应，与耗时更长的严格电路仿真相比，能快速准确地预测MVM计算单元输出。基于实验测量数据，我们推导出纳秒级PCM读取噪声的统计模型并完成匹配验证。我们将这些（统计性）器件效应与（确定性）电路效应整合至PyTorch框架，用于评估BERT和ALBERT Transformer网络的精度影响。研究表明：采用简单高斯噪声的硬件感知微调可有效抵抗ADC量化和PCM读取噪声影响，但对IR压降的缓解效果有限。这是因为IR压降虽具确定性，但具有非线性特征，在时间积分窗口内变化显著，且最终取决于并行注入模拟计算单元的所有激励信号。训练阶段施加简单高斯噪声无法使DNN网络充分适应推理阶段的IR压降，这意味着结合本文计算单元电路模型等先进技术的复杂训练方法，对于大型神经网络在AIMC硬件上的稳健部署至关重要。

---

## [Rethinking Hybrid Retrieval: When Small Embeddings and LLM Re-ranking Beat Bigger Models](https://arxiv.org/abs/2506.00049)

### Abstract
arXiv:2506.00049v1 Announce Type: cross 
Abstract: This paper presents a comparison of embedding models in tri-modal hybrid retrieval for Retrieval-Augmented Generation (RAG) systems. We investigate the fusion of dense semantic, sparse lexical, and graph-based embeddings, focusing on the performance of the MiniLM-v6 and BGE-Large architectures. Contrary to conventional assumptions, our results show that the compact MiniLM-v6 outperforms the larger BGE-Large when integrated with LLM-based re-ranking within our tri-modal hybrid framework. Experiments conducted on the SciFact, FIQA, and NFCorpus datasets demonstrate significant improvements in retrieval quality with the MiniLM-v6 configuration. The performance difference is particularly pronounced in agentic re-ranking scenarios, indicating better alignment between MiniLM-v6's embedding space and LLM reasoning. Our findings suggest that embedding model selection for RAG systems should prioritize compatibility with multi-signal fusion and LLM alignment, rather than relying solely on larger models. This approach may reduce computational requirements while improving retrieval accuracy and efficiency.

### 摘要
本文对比了检索增强生成(RAG)系统中三模态混合检索的嵌入模型性能。我们研究了稠密语义嵌入、稀疏词法嵌入与图嵌入的融合方式，重点分析MiniLM-v6和BGE-Large架构的表现。与传统假设相反，实验结果表明：在三模态混合框架中结合基于大语言模型的重排序时，紧凑型MiniLM-v6的性能优于更大规模的BGE-Large。在SciFact、FIQA和NFCorpus数据集上的实验显示，MiniLM-v6配置显著提升了检索质量。这种性能差异在智能体重排序场景中尤为明显，表明MiniLM-v6的嵌入空间与大语言模型推理具有更好的对齐性。研究发现，RAG系统的嵌入模型选择应优先考虑多信号融合兼容性和LLM对齐性，而非单纯依赖更大规模的模型。该方法可在提升检索精度与效率的同时降低计算资源需求。

---

## [Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling](https://arxiv.org/abs/2506.00064)

### Abstract
arXiv:2506.00064v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated significant advancements in error handling. Current error-handling works are performed in a passive manner, with explicit error-handling instructions. However, in real-world scenarios, explicit error-handling instructions are usually unavailable. In this paper, our work identifies this challenge as how to conduct proactive error handling without explicit error handling instructions. To promote further research, this work introduces a new benchmark, termed Mis-prompt, consisting of four evaluation tasks, an error category taxonomy, and a new evaluation dataset. Furthermore, this work analyzes current LLMs' performance on the benchmark, and the experimental results reveal that current LLMs show poor performance on proactive error handling, and SFT on error handling instances improves LLMs' proactive error handling capabilities. The dataset will be publicly available.

### 摘要
大型语言模型（LLMs）在错误处理方面已展现出显著进展。当前错误处理工作以被动方式进行，需依赖显式的错误处理指令。然而在现实场景中，显式错误处理指令通常不可得。本文将该挑战定义为：如何在缺乏显式错误处理指令的情况下实施主动错误处理。为推进相关研究，本研究提出了名为Mis-prompt的新基准，包含四项评估任务、一个错误分类体系和一个新的评估数据集。此外，本文分析了当前LLMs在该基准上的表现，实验结果表明：现有LLMs在主动错误处理方面表现欠佳，而基于错误处理实例的监督微调（SFT）可提升LLMs的主动错误处理能力。该数据集将公开发布。

---

## [You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models](https://arxiv.org/abs/2506.00065)

### Abstract
arXiv:2506.00065v1 Announce Type: cross 
Abstract: Multimodal language models (MLMs) increasingly communicate in human-like ways, yet their ability to use reference words remains largely overlooked despite their ubiquity in everyday communication. Our study addresses this gap by comparing human and MLM use of three word classes with increasing cognitive demands: vocabulary words, possessive pronouns (`mine' vs `yours'), and demonstrative pronouns (`this one' vs `that one'). Evaluating seven state-of-the-art MLMs against human participants, we observe a clear difficulty hierarchy: while MLMs approach human-level performance on the vocabulary task, they show substantial deficits with possessives and demonstratives. Our analysis reveals these difficulties stem from limitations in perspective-taking and spatial reasoning. Although prompt engineering improved model performance on possessive use, demonstrative use remained well below human-level competence. These findings provide theoretical and empirical evidence that producing grammatical forms requiring pragmatics and social cognition remains a clear challenge in current NLP systems.

### 摘要
多模态语言模型（MLMs）的交流方式日益趋近人类，但其对指代词的使用能力——尽管在日常交流中无处不在——却仍被广泛忽视。本研究通过比较人类与MLMs在三类认知需求递增的词汇（普通词汇、物主代词['我的'与'你的']、指示代词['这个'与'那个']）上的使用表现来填补这一空白。在评估七种前沿MLMs与人类参与者的对比实验中，我们观察到明显的难度层级：虽然MLMs在词汇任务上接近人类水平，但在物主代词和指示代词上表现出显著缺陷。分析表明这些困难源于视角采择与空间推理能力的局限。尽管提示工程提升了模型对物主代词的使用表现，但其指示代词的使用能力仍远低于人类水平。这些发现从理论与实证层面证明：当前自然语言处理系统在生成需要语用学与社会认知参与的语法形式方面仍存在明显挑战。

---

## [Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports](https://arxiv.org/abs/2506.00060)

### Abstract
arXiv:2506.00060v1 Announce Type: cross 
Abstract: Purpose: We investigated the utilization of privacy-preserving, locally-deployed, open-source Large Language Models (LLMs) to extract diagnostic information from free-text cardiovascular magnetic resonance (CMR) reports. Materials and Methods: We evaluated nine open-source LLMs on their ability to identify diagnoses and classify patients into various cardiac diagnostic categories based on descriptive findings in 109 clinical CMR reports. Performance was quantified using standard classification metrics including accuracy, precision, recall, and F1 score. We also employed confusion matrices to examine patterns of misclassification across models. Results: Most open-source LLMs demonstrated exceptional performance in classifying reports into different diagnostic categories. Google's Gemma2 model achieved the highest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B with F1 scores of 0.96 and 0.95, respectively. All other evaluated models attained average scores above 0.93, with Mistral and DeepseekR1-7B being the only exceptions. The top four LLMs outperformed our board-certified cardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR reports. Conclusion: Our findings demonstrate the feasibility of implementing open-source, privacy-preserving LLMs in clinical settings for automated analysis of imaging reports, enabling accurate, fast and resource-efficient diagnostic categorization.

### 摘要
目的：本研究探讨利用隐私保护、本地部署的开源大语言模型（LLMs）从心血管磁共振（CMR）自由文本报告中提取诊断信息的可行性。材料与方法：我们评估了9个开源LLMs基于109份临床CMR报告描述性发现来识别诊断并将患者分类至不同心脏诊断类别的能力，采用准确率、精确率、召回率和F1分数等标准分类指标量化性能，并通过混淆矩阵分析各模型的误分类模式。结果：多数开源LLMs在将报告分类至不同诊断类别时表现出卓越性能。谷歌Gemma2模型以0.98的平均F1分数位列第一，Qwen2.5:32B和DeepseekR1-32B分别以0.96和0.95的F1分数紧随其后。其余评估模型平均分数均高于0.93，仅Mistral和DeepseekR1-7B例外。在分析CMR报告时，排名前四的LLMs在所有评估指标上均优于我们经委员会认证的心脏专家（F1分数0.94）。结论：本研究证实了在临床环境中实施开源、隐私保护LLMs进行影像报告自动化分析的可行性，可实现准确、快速且资源高效的诊断分类。

---

## [Prompt Engineer: Analyzing Skill Requirements in the AI Job Market](https://arxiv.org/abs/2506.00058)

### Abstract
arXiv:2506.00058v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs) has created a new job role: the Prompt Engineer. Despite growing interest in this position, we still do not fully understand what skills this new job role requires or how common these jobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt engineer positions, to learn more about this emerging role. We found that prompt engineering is still rare (less than 0.5% of sampled job postings) but has a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt design skills (18.7%), good communication (21.9%), and creative problem-solving (15.8%) skills. These requirements significantly differ from those of established roles, such as data scientists and machine learning engineers, showing that prompt engineering is becoming its own profession. Our findings help job seekers, employers, and educational institutions in better understanding the emerging field of prompt engineering.

### 摘要
大型语言模型（LLMs）的兴起催生了一个新的职业角色：提示工程师。尽管对这一职位的兴趣日益增长，我们仍未完全理解该角色所需的技能或这类职位的普遍程度。我们分析了LinkedIn上的20,662个职位招聘信息（其中包括72个提示工程师岗位），以深入了解这一新兴角色。研究发现，提示工程目前仍属罕见（占抽样职位不到0.5%），但其技能要求具有独特性。提示工程师需要具备人工智能知识（22.8%）、提示设计技能（18.7%）、良好的沟通能力（21.9%）以及创造性问题解决能力（15.8%）。这些要求与数据科学家、机器学习工程师等成熟岗位存在显著差异，表明提示工程正在发展成为一个独立的专业领域。本研究为求职者、雇主及教育机构更好地理解提示工程这一新兴领域提供了参考。

---

## [Evaluating the Sensitivity of LLMs to Prior Context](https://arxiv.org/abs/2506.00069)

### Abstract
arXiv:2506.00069v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed in multi-turn dialogue and other sustained interactive scenarios, it is essential to understand how extended context affects their performance. Popular benchmarks, focusing primarily on single-turn question answering (QA) tasks, fail to capture the effects of multi-turn exchanges. To address this gap, we introduce a novel set of benchmarks that systematically vary the volume and nature of prior context. We evaluate multiple conventional LLMs, including GPT, Claude, and Gemini, across these benchmarks to measure their sensitivity to contextual variations. Our findings reveal that LLM performance on multiple-choice questions can degrade dramatically in multi-turn interactions, with performance drops as large as 73% for certain models. Even highly capable models such as GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative performance of larger versus smaller models is not always predictable. Moreover, the strategic placement of the task description within the context can substantially mitigate performance drops, improving the accuracy by as much as a factor of 3.5. These findings underscore the need for robust strategies to design, evaluate, and mitigate context-related sensitivity in LLMs.

### 摘要
随着大语言模型（LLMs）在多轮对话等持续交互场景中的广泛应用，理解长上下文如何影响其性能变得至关重要。现有主流基准测试主要关注单轮问答（QA）任务，未能捕捉多轮交互的效应。为填补这一空白，我们提出了一套新型基准测试，系统性地控制历史上下文的体量与性质。通过在这些基准上评估GPT、Claude和Gemini等主流大语言模型，我们测量了其对上下文变化的敏感性。研究发现：在多轮交互中，大语言模型的多选题性能会出现显著下降，部分模型性能降幅高达73%；即使GPT-4o等高能力模型，准确率最大降幅也达32%。值得注意的是，大模型与小模型的相对性能并不总是可预测的。此外，任务描述在上下文中的策略性放置可显著缓解性能下降，最高能使准确率提升3.5倍。这些发现表明，需要建立更鲁棒的策略来设计、评估和缓解大语言模型的上下文敏感性。

---

## [Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs](https://arxiv.org/abs/2506.00061)

### Abstract
arXiv:2506.00061v1 Announce Type: cross 
Abstract: In this work we present the Social Influence Technique Taxonomy (SITT), a comprehensive framework of 58 empirically grounded techniques organized into nine categories, designed to detect subtle forms of social influence in textual content. We also investigate the LLMs ability to identify various forms of social influence. Building on interdisciplinary foundations, we construct the SITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and translated into English -- to evaluate the ability of LLMs to identify these techniques. Using a hierarchical multi-label classification setup, we benchmark five LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our results show that while some models, notably Claude 3.5, achieved moderate success (F1 score = 0.45 for categories), overall performance of models remains limited, particularly for context-sensitive techniques. The findings demonstrate key limitations in current LLMs' sensitivity to nuanced linguistic cues and underscore the importance of domain-specific fine-tuning. This work contributes a novel resource and evaluation example for understanding how LLMs detect, classify, and potentially replicate strategies of social influence in natural dialogues.

### 摘要
在本研究中，我们提出了社会影响技术分类法（SITT），这是一个包含9大类58种实证基础技术的综合框架，用于检测文本内容中微妙的社会影响形式。同时，我们探究了大语言模型（LLMs）识别各类社会影响的能力。基于跨学科基础，我们构建了SITT数据集——一个由11位专家标注的746段波兰语对话语料库（已译为英语），用以评估LLMs识别这些技术的能力。通过分层多标签分类实验，我们对包括GPT-4o、Claude 3.5、Llama-3.1、Mixtral和PLLuM在内的五款LLMs进行了基准测试。结果表明，尽管部分模型（特别是Claude 3.5）取得了中等成功率（类别F1分数=0.45），但模型整体表现仍存在局限，尤其对上下文敏感技术的识别欠佳。这些发现揭示了当前LLMs在感知微妙语言线索方面的关键缺陷，并强调了领域特异性微调的重要性。本研究为理解LLMs如何检测、分类及潜在复现自然对话中的社会影响策略，提供了新颖的研究资源和评估范例。

---

## [Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics](https://arxiv.org/abs/2506.00070)

### Abstract
arXiv:2506.00070v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning.

### 摘要
大型视觉语言模型（LVLMs）近期通过将具身推理与机器人控制相结合，在推动机器人技术发展方面展现出巨大潜力。当前主流方法采用监督微调（SFT）对机器人控制相关的具身推理任务进行训练，但SFT数据集通常基于启发式构建，未明确针对机器人控制优化。此外，SFT常引发灾难性遗忘和泛化性能下降等问题。为解决这些局限性，我们提出Robot-R1框架，该框架利用强化学习专门增强面向机器人控制的具身推理能力。Robot-R1通过学习预测任务完成所需的下一关键点状态（以当前场景图像和专家演示衍生的环境元数据为条件），其设计灵感源自DeepSeek-R1学习方法，通过采样基于推理的响应并强化那些产生更准确预测的响应。实验表明，采用Robot-R1训练的模型在具身推理任务上优于SFT方法。尽管仅有70亿参数，Robot-R1在空间运动和基础动作推理等低层动作控制相关任务中的表现甚至超越GPT-4o。

---

## [Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs](https://arxiv.org/abs/2506.00072)

### Abstract
arXiv:2506.00072v1 Announce Type: cross 
Abstract: This paper investigates how prompt engineering techniques impact both accuracy and confidence elicitation in Large Language Models (LLMs) applied to medical contexts. Using a stratified dataset of Persian board exam questions across multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini, Llama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These configurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles (Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales (1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error (ECE) to evaluate alignment between confidence and actual performance. Chain-of-Thought prompts improved accuracy but also led to overconfidence, highlighting the need for calibration. Emotional prompting further inflated confidence, risking poor decisions. Smaller models like Llama-3.1-8b underperformed across all metrics, while proprietary models showed higher accuracy but still lacked calibrated confidence. These results suggest prompt engineering must address both accuracy and uncertainty to be effective in high-stakes medical tasks.

### 摘要
本文研究了提示工程技术如何影响应用于医疗场景的大型语言模型（LLMs）的准确性和置信度激发。通过采用跨多个专业的波斯语执业考试分层数据集，我们在156种配置下评估了五种LLMs（GPT-4o、o3-mini、Llama-3.3-70b、Llama-3.1-8b和DeepSeek-v3）。这些配置在温度设置（0.3、0.7、1.0）、提示风格（思维链、少样本、情感诱导、专家模仿）和置信度量表（1-10、1-100）方面存在差异。我们使用AUC-ROC、Brier评分和期望校准误差（ECE）来评估置信度与实际表现的一致性。研究发现：思维链提示虽能提升准确性，但会导致过度自信，凸显校准的必要性；情感诱导会进一步放大置信度，增加决策失误风险；Llama-3.1-8b等较小模型在所有指标上表现欠佳，而专有模型虽具有更高准确性却仍存在置信度校准不足的问题。这些结果表明，要在高风险医疗任务中有效应用提示工程，必须同时兼顾准确性和不确定性管理。

---

## [Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation](https://arxiv.org/abs/2506.00075)

### Abstract
arXiv:2506.00075v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs), such as GPT, in industrial robotics enhances operational efficiency and human-robot collaboration. However, the computational complexity and size of these models often provide latency problems in request and response times. This study explores the integration of the ChatGPT natural language model with the Robot Operating System 2 (ROS 2) to mitigate interaction latency and improve robotic system control within a simulated Gazebo environment. We present an architecture that integrates these technologies without requiring a middleware transport platform, detailing how a simulated mobile robot responds to text and voice commands. Experimental results demonstrate that this integration improves execution speed, usability, and accessibility of the human-robot interaction by decreasing the communication latency by 7.01\% on average. Such improvements facilitate smoother, real-time robot operations, which are crucial for industrial automation and precision tasks.

### 摘要
大型语言模型（如GPT）在工业机器人领域的集成提升了操作效率与人机协作能力。然而，这些模型的计算复杂性和规模常导致请求与响应时延问题。本研究探讨将ChatGPT自然语言模型与机器人操作系统2（ROS 2）相结合，以降低交互延迟并改善Gazebo仿真环境中的机器人系统控制。我们提出一种无需中间件传输平台的集成架构，详细阐述了仿真移动机器人如何响应文本及语音指令。实验结果表明，该集成方案通过平均降低7.01%的通信延迟，显著提升了人机交互的执行速度、可用性与可访问性。此类改进有助于实现更流畅的实时机器人操作，这对工业自动化与精密任务至关重要。

---

## [Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values](https://arxiv.org/abs/2506.00079)

### Abstract
arXiv:2506.00079v1 Announce Type: cross 
Abstract: The rapid integration of Large Language Models (LLMs) in high-stakes decision-making -- such as allocating scarce resources like donor organs -- raises critical questions about their alignment with human moral values. We systematically evaluate the behavior of several prominent LLMs against human preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark deviations from human values in prioritizing various attributes, and ii) in contrast to humans, LLMs rarely express indecision, opting for deterministic decisions even when alternative indecision mechanisms (e.g., coin flipping) are provided. Nonetheless, we show that low-rank supervised fine-tuning with few samples is often effective in improving both decision consistency and calibrating indecision modeling. These findings illustrate the necessity of explicit alignment strategies for LLMs in moral/ethical domains.

### 摘要
大型语言模型（LLMs）在高风险决策（如分配捐赠器官等稀缺资源）中的快速应用，引发了关于其与人类道德价值观对齐的关键问题。我们系统评估了多个知名LLMs在肾脏分配场景中相对于人类偏好的行为表现，发现：i）LLMs在各类属性优先级排序上表现出与人类价值观的显著偏差；ii）与人类不同，LLMs极少表达犹豫倾向，即使提供替代性犹豫机制（如抛硬币）时仍倾向于做出确定性决策。然而研究表明，采用少量样本进行低秩监督微调通常能有效提升决策一致性并改善犹豫建模的校准效果。这些发现揭示了在道德/伦理领域对LLMs实施显式对齐策略的必要性。

---

## [Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages](https://arxiv.org/abs/2506.00068)

### Abstract
arXiv:2506.00068v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly shaping public discourse, yet their politico-economic biases remain underexamined in non-Western and low-resource multilingual contexts. This paper presents a systematic analysis of political bias in 13 state-of-the-art LLMs across five low-resource languages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We propose a novel framework that integrates an adapted Political Compass Test (PCT) with a multi-level framing analysis. Our method combines quantitative assessment of political orientation across economic (left-right) and social (libertarian-authoritarian) axes with qualitative analysis of framing through content, style, and emphasis. We further contextualize this analysis by aligning prompts with 11 key socio-political themes relevant to Pakistani society. Our results reveal that LLMs predominantly align with liberal-left values, echoing Western training data influences, but exhibit notable shifts toward authoritarian framing in regional languages, suggesting strong cultural modulation effects. We also identify consistent model-specific bias signatures and language-conditioned variations in ideological expression. These findings show the urgent need for culturally grounded, multilingual bias auditing frameworks.

### 摘要
大型语言模型（LLMs）正日益影响公共话语，但其在非西方及低资源多语言环境中的政治经济偏见仍未得到充分研究。本文系统分析了13个最先进LLMs在巴基斯坦五种低资源语言（乌尔都语、旁遮普语、信德语、俾路支语和普什图语）中的政治偏见。我们提出了一种新颖框架，将改进版政治指南针测试（PCT）与多层次框架分析相结合。该方法通过经济（左翼-右翼）和社会（自由意志-威权）维度的政治取向定量评估，结合内容、风格和侧重点的定性框架分析，并进一步将提示语与巴基斯坦社会11个关键社会政治主题相校准。研究结果表明，LLMs主要倾向自由左翼价值观，反映出西方训练数据的影响，但在地区语言中表现出向威权框架的显著偏移，表明强烈的文化调节效应。我们还发现了模型特定的偏见特征以及意识形态表达中受语言条件影响的变异。这些发现表明亟需建立基于文化的多语言偏见审计框架。

---

## [Artificial Empathy: AI based Mental Health](https://arxiv.org/abs/2506.00081)

### Abstract
arXiv:2506.00081v1 Announce Type: cross 
Abstract: Many people suffer from mental health problems but not everyone seeks professional help or has access to mental health care. AI chatbots have increasingly become a go-to for individuals who either have mental disorders or simply want someone to talk to. This paper presents a study on participants who have previously used chatbots and a scenario-based testing of large language model (LLM) chatbots. Our findings indicate that AI chatbots were primarily utilized as a "Five minute therapist" or as a non-judgmental companion. Participants appreciated the anonymity and lack of judgment from chatbots. However, there were concerns about privacy and the security of sensitive information. The scenario-based testing of LLM chatbots highlighted additional issues. Some chatbots were consistently reassuring, used emojis and names to add a personal touch, and were quick to suggest seeking professional help. However, there were limitations such as inconsistent tone, occasional inappropriate responses (e.g., casual or romantic), and a lack of crisis sensitivity, particularly in recognizing red flag language and escalating responses appropriately. These findings can inform both the technology and mental health care industries on how to better utilize AI chatbots to support individuals during challenging emotional periods.

### 摘要
许多人深受心理健康问题困扰，但并非所有人都会寻求专业帮助或能获得心理健康服务。AI聊天机器人已逐渐成为心理障碍患者或单纯需要倾诉者的首选求助对象。本研究针对曾使用聊天机器人的参与者开展调查，并对大语言模型（LLM）聊天机器人进行情景测试。研究发现，AI聊天机器人主要被用作"五分钟心理治疗师"或非评判性陪伴者，参与者尤其看重其匿名性和无评判特质。然而，用户对隐私及敏感信息安全性存在顾虑。LLM聊天机器人的情景测试暴露出更多问题：部分机器人能持续提供安抚、使用表情符号和姓名增强亲和力，并快速建议寻求专业帮助；但也存在语调不一致、偶发不当回应（如随意或暧昧表述）、危机敏感性不足等局限，尤其体现在危险信号语言识别和响应升级机制方面。这些发现可为科技行业和心理健康领域如何更好地运用AI聊天机器人支持情绪困境个体提供参考。

---

## [Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations](https://arxiv.org/abs/2506.00074)

### Abstract
arXiv:2506.00074v1 Announce Type: cross 
Abstract: This paper evaluates the performance of six open-weight LLMs (llama3-8b, llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending experts in physics across five tasks: top-k experts by field, influential scientists by discipline, epoch, seniority, and scholar counterparts. The evaluation examines consistency, factuality, and biases related to gender, ethnicity, academic popularity, and scholar similarity. Using ground-truth data from the American Physical Society and OpenAlex, we establish scholarly benchmarks by comparing model outputs to real-world academic records. Our analysis reveals inconsistencies and biases across all models. mixtral-8x7b produces the most stable outputs, while llama3.1-70b shows the highest variability. Many models exhibit duplication, and some, particularly gemma2-9b and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real scientists, but accuracy drops in field-, epoch-, and seniority-specific queries, consistently favoring senior scholars. Representation biases persist, replicating gender imbalances (reflecting male predominance), under-representing Asian scientists, and over-representing White scholars. Despite some diversity in institutional and collaboration networks, models favor highly cited and productive scholars, reinforcing the rich-getricher effect while offering limited geographical representation. These findings highlight the need to improve LLMs for more reliable and equitable scholarly recommendations.

### 摘要
本研究评估了六种开源权重大型语言模型（llama3-8b、llama3.1-8b、gemma2-9b、mixtral-8x7b、llama3-70b、llama3.1-70b）在物理学领域专家推荐任务中的表现，涵盖五大任务：领域前k名专家、学科影响力科学家、时代划分、资历级别及学者对应关系。通过与美国物理学会和OpenAlex的真实数据对比建立学术基准，我们从一致性、事实准确性及性别/族裔/学术知名度/学者相似性等维度检验模型偏差。分析表明所有模型均存在不一致性和偏见：mixtral-8x7b输出最稳定，llama3.1-70b变异性最高；多数模型出现重复推荐现象，gemma2-9b和llama3.1-8b尤其存在格式错误问题。虽然模型普遍能推荐真实学者，但在细分领域、时代和资历查询中准确率下降，且持续偏向资深学者。表征偏差表现为性别失衡（反映男性主导）、亚裔学者代表性不足和白人学者过度代表。尽管在机构和合作网络方面呈现一定多样性，模型仍倾向于高被引和高产学者，强化马太效应的同时地理代表性有限。这些发现表明需要改进大型语言模型以实现更可靠、公平的学术推荐。

---

## [An AI-powered Knowledge Hub for Potato Functional Genomics](https://arxiv.org/abs/2506.00082)

### Abstract
arXiv:2506.00082v1 Announce Type: cross 
Abstract: Potato functional genomics lags due to unsystematic gene information curation, gene identifier inconsistencies across reference genome versions, and the increasing volume of research publications. To address these limitations, we developed the Potato Knowledge Hub (http://www.potato-ai.top), leveraging Large Language Models (LLMs) and a systematically curated collection of over 3,200 high-quality potato research papers spanning over 120 years. This platform integrates two key modules: a functional gene database containing 2,571 literature-reported genes, meticulously mapped to the latest DMv8.1 reference genome with resolved nomenclature discrepancies and links to original publications; and a potato knowledge base. The knowledge base, built using a Retrieval-Augmented Generation (RAG) architecture, accurately answers research queries with literature citations, mitigating LLM "hallucination." Users can interact with the hub via a natural language AI agent, "Potato Research Assistant," for querying specialized knowledge, retrieving gene information, and extracting sequences. The continuously updated Potato Knowledge Hub aims to be a comprehensive resource, fostering advancements in potato functional genomics and supporting breeding programs.

### 摘要
马铃薯功能基因组学研究因基因信息整理缺乏系统性、不同参考基因组版本间基因标识符不一致以及研究文献数量激增而进展缓慢。为解决这些局限，我们开发了马铃薯知识枢纽，该平台利用大语言模型(LLMs)系统化整合了跨越120余年的3,200多篇高质量马铃薯研究论文。该平台包含两大核心模块：一是功能基因数据库，收录2,571个文献报道基因，经严格校正命名差异后映射至最新DMv8.1参考基因组，并与原始文献建立关联；二是基于检索增强生成(RAG)架构构建的马铃薯知识库，能通过文献引证精准解答科研问题，有效缓解LLM"幻觉"现象。用户可通过自然语言AI代理"马铃薯研究助手"交互查询专业知识、获取基因信息及提取序列。持续更新的马铃薯知识枢纽旨在成为促进马铃薯功能基因组学发展和育种项目支持的综合资源平台。

---

## [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085)

### Abstract
arXiv:2506.00085v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) encode behaviors such as refusal within their activation space, yet identifying these behaviors remains a significant challenge. Existing methods often rely on predefined refusal templates detectable in output tokens or require manual analysis. We introduce \textbf&#123;COSMIC&#125; (Cosine Similarity Metrics for Inversion of Concepts), an automated framework for direction selection that identifies viable steering directions and target layers using cosine similarity - entirely independent of model outputs. COSMIC achieves steering performance comparable to prior methods without requiring assumptions about a model's refusal behavior, such as the presence of specific refusal tokens. It reliably identifies refusal directions in adversarial settings and weakly aligned models, and is capable of steering such models toward safer behavior with minimal increase in false refusals, demonstrating robustness across a wide range of alignment conditions.

### 摘要
大语言模型（LLMs）在其激活空间中编码了诸如拒绝之类的行为，但识别这些行为仍是一个重大挑战。现有方法通常依赖于可检测输出标记的预定义拒绝模板，或需要人工分析。我们提出\textbf&#123;COSMIC&#125;（基于余弦相似度的概念反演度量），这是一种自动化的方向选择框架，通过余弦相似度识别可行的调控方向和目标层——完全独立于模型输出。COSMIC实现了与现有方法相当的调控性能，且无需对模型的拒绝行为（如特定拒绝标记的存在）做出假设。该框架能在对抗性环境和弱对齐模型中可靠地识别拒绝方向，并能以极低的误拒率增量引导此类模型转向更安全的行为，展现了在广泛对齐条件下的鲁棒性。

---

## [HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.00088)

### Abstract
arXiv:2506.00088v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.

### 摘要
近年来，大型语言模型（LLMs）取得了显著进展，但幻觉问题（即模型生成不准确或非事实性陈述）仍是实际应用中的重大挑战。尽管当前基于分类的方法（如SAPLMA）在缓解幻觉方面效率较高，但当非事实信息出现在输出序列的早期或中期时，这些方法表现欠佳，降低了可靠性。为解决这些问题，我们提出幻觉检测-神经微分方程（HD-NDEs），该方法通过捕捉LLMs潜在空间中的完整动态，系统评估陈述的真实性。我们的方法应用神经微分方程（Neural DEs）对LLMs潜在空间中的动态系统进行建模，然后将潜在空间中的序列映射至分类空间进行真实性评估。在五个数据集和六种常用LLMs上的大量实验表明，HD-NDEs具有显著效果，尤其在True-False数据集上相比最先进技术实现了超过14%的AUC-ROC提升。

---

## [TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents](https://arxiv.org/abs/2506.00089)

### Abstract
arXiv:2506.00089v1 Announce Type: cross 
Abstract: The reasoning, writing, text-editing, and retrieval capabilities of proprietary large language models (LLMs) have advanced rapidly, providing users with an ever-expanding set of functionalities. However, this growing utility has also led to a serious societal concern: the over-reliance on LLMs. In particular, users increasingly delegate tasks such as homework, assignments, or the processing of sensitive documents to LLMs without meaningful engagement. This form of over-reliance and misuse is emerging as a significant social issue. In order to mitigate these issues, we propose a method injecting imperceptible phantom tokens into documents, which causes LLMs to generate outputs that appear plausible to users but are in fact incorrect. Based on this technique, we introduce TRAPDOC, a framework designed to deceive over-reliant LLM users. Through empirical evaluation, we demonstrate the effectiveness of our framework on proprietary LLMs, comparing its impact against several baselines. TRAPDOC serves as a strong foundation for promoting more responsible and thoughtful engagement with language models. Our code is available at https://github.com/jindong22/TrapDoc.

### 摘要
专有大语言模型（LLMs）的推理、写作、文本编辑和检索能力迅速发展，为用户提供了不断扩展的功能集。然而，这种日益增长的实用性也引发了一个严重的社会问题：对LLMs的过度依赖。具体而言，用户越来越多地将作业、任务或敏感文档处理等工作委托给LLMs，而自身并未进行实质性参与。这种形式的过度依赖和滥用正逐渐成为一个重要的社会问题。为缓解这些问题，我们提出了一种向文档中注入难以察觉的幻影标记的方法，该方法会导致LLMs生成看似合理但实则错误的输出。基于此技术，我们引入了TRAPDOC框架，旨在欺骗过度依赖LLMs的用户。通过实证评估，我们在专有LLMs上验证了该框架的有效性，并将其影响与多个基线进行了比较。TRAPDOC为促进更负责任和深思熟虑的语言模型使用奠定了坚实基础。我们的代码发布于https://github.com/jindong22/TrapDoc。

---

## [Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models](https://arxiv.org/abs/2506.00134)

### Abstract
arXiv:2506.00134v1 Announce Type: cross 
Abstract: Social determinants of health (SDOH) extraction from clinical text is critical for downstream healthcare analytics. Although large language models (LLMs) have shown promise, they may rely on superficial cues leading to spurious predictions. Using the MIMIC portion of the SHAC (Social History Annotation Corpus) dataset and focusing on drug status extraction as a case study, we demonstrate that mentions of alcohol or smoking can falsely induce models to predict current/past drug use where none is present, while also uncovering concerning gender disparities in model performance. We further evaluate mitigation strategies - such as prompt engineering and chain-of-thought reasoning - to reduce these false positives, providing insights into enhancing LLM reliability in health domains.

### 摘要
从临床文本中提取健康社会决定因素（SDOH）对下游医疗分析至关重要。尽管大语言模型（LLMs）已展现出潜力，但其可能依赖表面线索导致伪预测。本研究以SHAC（社会史标注语料库）数据集的MIMIC部分为样本，聚焦药物状态提取作为案例，发现当文本提及酒精或吸烟时，模型会错误预测当前/既往药物使用（即使实际不存在该情况），同时揭示了模型性能中存在的性别差异问题。我们进一步评估了缓解策略——如提示工程和思维链推理——以降低这些误报率，为提升LLMs在健康领域的可靠性提供了实践洞见。

---

## [Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments](https://arxiv.org/abs/2506.00083)

### Abstract
arXiv:2506.00083v1 Announce Type: cross 
Abstract: Autonomous operation of service robotics in human-centric scenes remains challenging due to the need for understanding of changing environments and context-aware decision-making. While existing approaches like topological maps offer efficient spatial priors, they fail to model transient object relationships, whereas dense neural representations (e.g., NeRF) incur prohibitive computational costs. Inspired by the hierarchical scene representation and video scene graph generation works, we propose Hi-Dyna Graph, a hierarchical dynamic scene graph architecture that integrates persistent global layouts with localized dynamic semantics for embodied robotic autonomy. Our framework constructs a global topological graph from posed RGB-D inputs, encoding room-scale connectivity and large static objects (e.g., furniture), while environmental and egocentric cameras populate dynamic subgraphs with object position relations and human-object interaction patterns. A hybrid architecture is conducted by anchoring these subgraphs to the global topology using semantic and spatial constraints, enabling seamless updates as the environment evolves. An agent powered by large language models (LLMs) is employed to interpret the unified graph, infer latent task triggers, and generate executable instructions grounded in robotic affordances. We conduct complex experiments to demonstrate Hi-Dyna Grap's superior scene representation effectiveness. Real-world deployments validate the system's practicality with a mobile manipulator: robotics autonomously complete complex tasks with no further training or complex rewarding in a dynamic scene as cafeteria assistant. See https://anonymous.4open.science/r/Hi-Dyna-Graph-B326 for video demonstration and more details.

### 摘要
服务机器人在以人为中心的场景中实现自主运行仍面临挑战，这源于对动态环境理解和情境感知决策的需求。现有方法（如拓扑地图）虽能提供高效的空间先验，却无法建模瞬态物体关系；而密集神经表征（如NeRF）则存在过高计算成本。受层次化场景表征和视频场景图生成研究的启发，我们提出Hi-Dyna Graph——一种融合持久性全局布局与局部动态语义的层次化动态场景图架构，用于具身机器人自主系统。该框架通过位姿RGB-D输入构建全局拓扑图，编码房间尺度连通性与大型静态物体（如家具），同时环境摄像头与自我中心摄像头用物体位置关系和人物交互模式填充动态子图。通过语义与空间约束将这些子图锚定至全局拓扑结构，形成混合架构，实现环境变化时的无缝更新。采用大语言模型（LLM）驱动的智能体解析统一场景图，推断潜在任务触发器，并生成基于机器人功能的可执行指令。通过复杂实验验证了Hi-Dyna Graph在场景表征效能上的优越性。真实场景部署以移动机械臂验证系统实用性：机器人在动态场景（如餐厅助手）中无需额外训练或复杂奖励机制即可自主完成复杂任务。视频演示与详情参见https://anonymous.4open.science/r/Hi-Dyna-Graph-B326。

---

## [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095)

### Abstract
arXiv:2506.00095v1 Announce Type: cross 
Abstract: Hepato-pancreato-biliary (HPB) disorders represent a global public health challenge due to their high morbidity and mortality. Although large language models (LLMs) have shown promising performance in general medical question-answering tasks, the current evaluation benchmarks are mostly derived from standardized examinations or manually designed questions, lacking HPB coverage and clinical cases. To address these issues, we systematically eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended multiple-choice questions and 337 open-ended real diagnosis cases, which encompasses all the 33 main categories and 465 subcategories of HPB diseases defined in the International Statistical Classification of Diseases, 10th Revision (ICD-10). The multiple-choice questions are curated from public datasets and synthesized data, and the clinical cases are collected from prestigious medical journals, case-sharing platforms, and collaborating hospitals. By evalauting commercial and open-source general and medical LLMs on our established benchmark, namely ClinBench-HBP, we find that while commercial LLMs perform competently on medical exam questions, they exhibit substantial performance degradation on HPB diagnosis tasks, especially on complex, inpatient clinical cases. Those medical LLMs also show limited generalizability to HPB diseases. Our results reveal the critical limitations of current LLMs in the domain of HPB diseases, underscoring the imperative need for future medical LLMs to handle real, complex clinical diagnostics rather than simple medical exam questions. The benchmark will be released at the homepage.

### 摘要
肝胰胆（HPB）疾病因其高发病率和高死亡率成为全球公共卫生挑战。尽管大语言模型（LLMs）在通用医学问答任务中展现出良好性能，但现有评估基准多源自标准化考试或人工设计问题，缺乏HPB疾病覆盖和真实临床病例。为解决这些问题，我们系统构建了一个包含3,535道封闭式选择题和337道开放式真实诊断病例的HPB疾病评估基准，涵盖《国际疾病分类第十版》（ICD-10）定义的全部33个主类别和465个子类别HPB疾病。选择题选自公开数据集与合成数据，临床病例则收集自权威医学期刊、病例共享平台及合作医院。通过在所建基准ClinBench-HBP上评估商业与开源通用及医学LLMs，我们发现：尽管商业LLMs在医学考试题上表现良好，但在HPB诊断任务（尤其是复杂的住院病例）上性能显著下降；医学LLMs对HPB疾病的泛化能力也有限。研究结果揭示了当前LLMs在HPB疾病领域的关键局限，强调未来医学LLMs必须能处理真实复杂的临床诊断而非简单医学考题。该基准将在主页公开发布。

---

## [Supporting architecture evaluation for ATAM scenarios with LLMs](https://arxiv.org/abs/2506.00150)

### Abstract
arXiv:2506.00150v1 Announce Type: cross 
Abstract: Architecture evaluation methods have long been used to evaluate software designs. Several evaluation methods have been proposed and used to analyze tradeoffs between different quality attributes. Having competing qualities leads to conflicts for selecting which quality-attribute scenarios are the most suitable ones that an architecture should tackle and for prioritizing the scenarios required by the stakeholders. In this context, architecture evaluation is carried out manually, often involving long brainstorming sessions to decide which are the most adequate quality scenarios. To reduce this effort and make the assessment and selection of scenarios more efficient, we suggest the usage of LLMs to partially automate evaluation activities. As a first step to validate this hypothesis, this work studies MS Copilot as an LLM tool to analyze quality scenarios suggested by students in a software architecture course and compares the students' results with the assessment provided by the LLM. Our initial study reveals that the LLM produces in most cases better and more accurate results regarding the risks, sensitivity points and tradeoff analysis of the quality scenarios. Overall, the use of generative AI has the potential to partially automate and support the architecture evaluation tasks, improving the human decision-making process.

### 摘要
架构评估方法长期以来被用于评估软件设计。已有多种评估方法被提出并用于分析不同质量属性之间的权衡。当存在相互竞争的质量属性时，会导致在选择架构应处理的最合适质量属性场景以及利益相关者所需场景的优先级方面产生冲突。在此背景下，架构评估通常以人工方式进行，往往需要长时间的头脑风暴会议来决定哪些质量场景最为适宜。为减少这一工作量并提高场景评估与选择的效率，我们建议利用大语言模型（LLM）部分自动化评估活动。作为验证这一假设的第一步，本研究采用MS Copilot作为LLM工具，分析软件架构课程中学生提出的质量场景，并将学生评估结果与LLM提供的评估进行对比。初步研究表明，在大多数情况下，LLM关于质量场景的风险、敏感点和权衡分析能产生更优且更准确的结果。总体而言，生成式人工智能的应用有望部分自动化并支持架构评估任务，从而改善人类决策过程。

---

## [Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences](https://arxiv.org/abs/2506.00195)

### Abstract
arXiv:2506.00195v1 Announce Type: cross 
Abstract: Current LLMs are trained to refuse potentially harmful input queries regardless of whether users actually had harmful intents, causing a tradeoff between safety and user experience. Through a study of 480 participants evaluating 3,840 query-response pairs, we examine how different refusal strategies affect user perceptions across varying motivations. Our findings reveal that response strategy largely shapes user experience, while actual user motivation has negligible impact. Partial compliance -- providing general information without actionable details -- emerges as the optimal strategy, reducing negative user perceptions by over 50% to flat-out refusals. Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and evaluate how 6 reward models score different refusal strategies, demonstrating that models rarely deploy partial compliance naturally and reward models currently undervalue it. This work demonstrates that effective guardrails require focusing on crafting thoughtful refusals rather than detecting intent, offering a path toward AI safety mechanisms that ensure both safety and sustained user engagement.

### 摘要
当前的大型语言模型（LLM）被训练为拒绝可能有害的输入查询，无论用户是否实际怀有有害意图，这导致安全性与用户体验之间的权衡。通过对480名参与者评估3,840组查询-响应对的研究，我们考察了不同拒绝策略如何影响不同动机下的用户感知。研究发现，响应策略在很大程度上塑造了用户体验，而用户的实际动机影响微乎其微。部分遵从——提供通用信息但不包含可操作细节——成为最优策略，相较于直接拒绝，能将负面用户感知减少50%以上。作为补充，我们分析了9个前沿LLM的响应模式，并评估了6个奖励模型对不同拒绝策略的评分，结果表明模型很少自然采用部分遵从策略，且当前奖励模型对其价值评估不足。这项工作表明，有效的防护机制需要专注于设计深思熟虑的拒绝策略而非意图检测，为同时确保安全性和持续用户参与的AI安全机制提供了路径。

---

## [Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment](https://arxiv.org/abs/2506.00166)

### Abstract
arXiv:2506.00166v1 Announce Type: cross 
Abstract: Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework addressing these challenges by decoupling safety-specific computations from a task-optimized base model. DSA utilizes lightweight adapters that leverage the base model's internal representations, enabling diverse and flexible safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails substantially outperform comparably sized standalone models, notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 &amp; BeaverTails). Furthermore, DSA-based safety alignment allows dynamic, inference-time adjustment of alignment strength and a fine-grained trade-off between instruction following performance and model safety. Importantly, combining the DSA safety guardrail with DSA safety alignment facilitates context-dependent alignment strength, boosting safety on StrongReject by 93% while maintaining 98% performance on MTBench -- a total reduction in alignment tax of 8 percentage points compared to standard safety alignment fine-tuning. Overall, DSA presents a promising path towards more modular, efficient, and adaptable AI safety and alignment.

### 摘要
现有AI安全防护范式（如护栏模型和对齐训练）往往需要在推理效率与开发灵活性之间进行权衡。我们提出解耦安全适配器（DSA）这一新型框架，通过将安全计算与任务优化的基础模型分离来解决上述问题。DSA采用轻量级适配器，利用基础模型的内部表征，在几乎不影响推理成本的前提下实现多样化、灵活的安全功能。实证表明：基于DSA的安全护栏显著优于同等规模的独立模型，在幻觉检测（Summedits数据集AUC值0.88 vs 0.61）、仇恨言论分类（ToxiGen数据集0.98 vs 0.92）以及不安全输入/响应识别（AEGIS2.0 &amp; BeaverTails数据集0.93 vs 0.90）方面表现突出。此外，基于DSA的安全对齐支持推理时动态调整对齐强度，实现指令遵循性能与模型安全性的细粒度权衡。值得注意的是，将DSA安全护栏与安全对齐结合可实现上下文相关的对齐强度调节：在StrongReject数据集上安全性能提升93%的同时，保持MTBench测试98%的指令性能——相比标准安全对齐微调，对齐税总体降低8个百分点。该框架为构建模块化、高效且适应性强的AI安全与对齐系统提供了可行路径。

---

## [Structure-Aware Fill-in-the-Middle Pretraining for Code](https://arxiv.org/abs/2506.00204)

### Abstract
arXiv:2506.00204v1 Announce Type: cross 
Abstract: Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where models complete code segments given surrounding context. However, existing LLMs treat code as plain text and mask random character spans. We propose and evaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees (ASTs) to mask complete syntactic structures at scale, ensuring coherent training examples better aligned with universal code structures and common code editing patterns such as blocks, expressions, or functions. To evaluate real-world fill-in-the-middle (FIM) programming tasks, we introduce Real-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12 languages. On infilling tasks, experiments on 1B and 8B parameter models show that AST-FIM is particularly beneficial for real-world code editing as it outperforms standard random-character FIM by up to 5 pts on standard FIM benchmarks. Our code is publicly available at https://github.com/gonglinyuan/ast_fim.

### 摘要
中间填充（FIM）是代码大语言模型（LLM）常见的预训练方法，模型根据上下文补全代码片段。然而现有LLM将代码视为纯文本并随机掩码字符片段。我们提出并评估了AST-FIM预训练策略，该方法利用抽象语法树（AST）大规模掩码完整语法结构，确保生成与通用代码结构（如代码块、表达式或函数）及常见代码编辑模式更契合的连贯训练样本。为评估真实场景下的中间填充编程任务，我们构建了Real-FIM-Eval基准测试集，该数据集源自12种编程语言的30,000+次GitHub提交。在1B和8B参数模型的填充任务实验中，AST-FIM对实际代码编辑尤为有效，其在标准FIM基准测试中比随机字符FIM方法最高提升5个点。代码已开源：https://github.com/gonglinyuan/ast_fim。

---

## [Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00281)

### Abstract
arXiv:2506.00281v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems, which integrate Large Language Models (LLMs) with external knowledge sources, are vulnerable to a range of adversarial attack vectors. This paper examines the importance of RAG systems through recent industry adoption trends and identifies the prominent attack vectors for RAG: prompt injection, data poisoning, and adversarial query manipulation. We analyze these threats under risk management lens, and propose robust prioritized control list that includes risk-mitigating actions like input validation, adversarial training, and real-time monitoring.

### 摘要
检索增强生成（RAG）系统通过将大语言模型（LLM）与外部知识源相结合，容易受到多种对抗性攻击向量的威胁。本文通过近期行业应用趋势分析了RAG系统的重要性，并识别出其主要攻击向量：提示注入、数据投毒和对抗性查询操纵。我们从风险管理视角剖析这些威胁，提出包含输入验证、对抗训练和实时监控等风险缓解措施的鲁棒性优先级控制清单。

---

## [MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models](https://arxiv.org/abs/2506.00198)

### Abstract
arXiv:2506.00198v1 Announce Type: cross 
Abstract: The discovery of Metal-Organic Frameworks (MOFs) with application-specific properties remains a central challenge in materials chemistry, owing to the immense size and complexity of their structural design space. Conventional computational screening techniques such as molecular simulations and density functional theory (DFT), while accurate, are computationally prohibitive at scale. Machine learning offers an exciting alternative by leveraging data-driven approaches to accelerate materials discovery. The complexity of MOFs, with their extended periodic structures and diverse topologies, creates both opportunities and challenges for generative modeling approaches. To address these challenges, we present a reinforcement learning-enhanced, transformer-based framework for the de novo design of MOFs. Central to our approach is MOFid, a chemically-informed string representation encoding both connectivity and topology, enabling scalable generative modeling. Our pipeline comprises three components: (1) a generative GPT model trained on MOFid sequences, (2) MOFormer, a transformer-based property predictor, and (3) a reinforcement learning (RL) module that optimizes generated candidates via property-guided reward functions. By integrating property feedback into sequence generation, our method drives the model toward synthesizable, topologically valid MOFs with desired functional attributes. This work demonstrates the potential of large language models, when coupled with reinforcement learning, to accelerate inverse design in reticular chemistry and unlock new frontiers in computational MOF discovery.

### 摘要
金属有机框架材料（MOFs）的发现因其结构设计空间的巨大规模与复杂性，始终是材料化学领域的核心挑战。传统计算筛选技术如分子模拟和密度泛函理论（DFT）虽精确，但大规模计算成本过高。机器学习通过数据驱动方法加速材料发现提供了新思路，而MOFs的周期性扩展结构和多样拓扑特性既为生成模型带来机遇也带来挑战。为此，我们提出一种基于强化学习增强的Transformer框架用于MOFs的从头设计。该方法的核心是MOFid——一种编码连接性与拓扑结构的化学信息字符串表示法，可实现可扩展的生成建模。我们的流程包含三个组件：（1）基于MOFid序列训练的生成式GPT模型；（2）Transformer架构的性质预测器MOFormer；（3）通过性质导向奖励函数优化生成候选结构的强化学习模块。通过将性质反馈整合至序列生成过程，本方法能驱动模型产生具有可合成性、拓扑有效性及目标功能属性的MOFs。该研究证明了大型语言模型与强化学习相结合在加速网状化学逆向设计方面的潜力，为计算驱动的MOFs发现开辟了新途径。

---

## [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)

### Abstract
arXiv:2506.00253v1 Announce Type: cross 
Abstract: Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.

### 摘要
尽管价值对齐的语言模型（LMs）在显性偏见评估中表现无偏，但在隐性词汇联想任务中仍常显现刻板印象，引发对其公平应用的担忧。本研究探讨了这种差异背后的机制，发现对齐过程反而会放大模型输出中的隐性偏见。具体而言，我们证明当上下文模糊时，与未对齐模型不同，对齐模型在早期内部表征中会忽略种族概念。这种对种族表征的缺失可能导致安全防护机制未被激活，从而产生非预期的偏见。基于此发现，我们提出一种新的偏见缓解策略，通过激励模型在早期层表征种族概念来实现。与传统机器去偏方法不同，我们的干预措施表明，引导模型增强对种族概念的觉察能有效缓解隐性偏见。类似于人类的"种族盲视"，忽略种族细微差异可能无意中使语言模型延续隐性偏见。

---

## [Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response](https://arxiv.org/abs/2506.00274)

### Abstract
arXiv:2506.00274v1 Announce Type: cross 
Abstract: Large language models hold considerable promise for supporting forensic investigations, but their widespread adoption is hindered by a lack of transparency, explainability, and reproducibility. This paper explores how the emerging Model Context Protocol can address these challenges and support the meaningful use of LLMs in digital forensics. Through a theoretical analysis, we examine how MCP can be integrated across various forensic scenarios - ranging from artifact analysis to the generation of interpretable reports. We also outline both technical and conceptual considerations for deploying an MCP server in forensic environments. Our analysis reveals a wide range of use cases in which MCP not only strengthens existing forensic workflows but also facilitates the application of LLMs to areas of forensics where their use was previously limited. Furthermore, we introduce the concept of the inference constraint level - a way of characterizing how specific MCP design choices can deliberately constrain model behavior, thereby enhancing both auditability and traceability. Our insights demonstrate that MCP has significant potential as a foundational component for developing LLM-assisted forensic workflows that are not only more transparent, reproducible, and legally defensible, but also represent a step toward increased automation in digital forensic analysis. However, we also highlight potential challenges that the adoption of MCP may pose for digital forensics in the future.

### 摘要
大型语言模型在支持取证调查方面具有显著潜力，但其广泛应用因缺乏透明度、可解释性和可复现性而受到制约。本文探讨新兴的模型上下文协议（MCP）如何应对这些挑战，促进LLM在数字取证中的有效应用。通过理论分析，我们研究了MCP如何整合到从痕迹分析到可解释报告生成等多种取证场景中，并阐述了在取证环境中部署MCP服务器的技术及概念考量。分析表明，MCP不仅能强化现有取证流程，还能推动LLM应用于传统受限的取证领域。此外，我们提出"推理约束等级"概念——通过特定MCP设计选择来系统约束模型行为，从而提升审计追踪能力。研究表明，MCP作为构建LLM辅助取证工作流的基础组件具有重要价值，不仅能增强透明度、可复现性和法律可辩护性，还推动了数字取证分析自动化的进程。最后，我们也指出了MCP未来在数字取证领域可能面临的潜在挑战。

---

## [Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings](https://arxiv.org/abs/2506.00277)

### Abstract
arXiv:2506.00277v1 Announce Type: cross 
Abstract: Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson $\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.

### 摘要
情境化大型语言模型嵌入在主题建模和聚类中的应用日益广泛。然而，现有方法普遍存在扩展性差、依赖不透明的相似性度量标准以及在多语言环境下表现不佳等问题。本研究提出了一种新颖的、可扩展的、可解释的、层次化的多语言聚类方法，用于处理新闻文章和社交媒体数据。具体而言，我们首先训练了多语言套娃嵌入模型，该模型能够根据所考察的嵌入维度子集，在不同粒度级别上判定报道相似性。该嵌入模型在SemEval 2022任务8测试数据集上取得了最先进的性能表现（Pearson ρ=0.816）。基于此，我们开发了一种高效的层次聚类算法，利用套娃嵌入的层次特性来识别独特的新闻报道、叙事框架和主题结构。最后，我们通过实际新闻数据集展示了该方法如何有效识别并聚类具体报道、叙事模式和宏观主题。

---

## [The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features](https://arxiv.org/abs/2506.00203)

### Abstract
arXiv:2506.00203v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to evolve, questions about their trustworthiness in delivering factual information have become increasingly important. This concern also applies to their ability to accurately represent the geographic world. With recent advancements in this field, it is relevant to consider whether and to what extent LLMs' representations of the geographical world can be trusted. This study evaluates the performance of GPT-4o and Gemini 2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and reverse geocoding. In the geocoding task, both models exhibited systematic and random errors in estimating the coordinates of St. Anne's Column in Innsbruck, Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash demonstrating more precision but a significant systematic offset. For elevation estimation, both models tended to underestimate elevations across Austria, though they captured overall topographical trends, and Gemini 2.0 Flash performed better in eastern regions. The reverse geocoding task, which involved identifying Austrian federal states from coordinates, revealed that Gemini 2.0 Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating better consistency across regions. Despite these findings, neither model achieved an accurate reconstruction of Austria's federal states, highlighting persistent misclassifications. The study concludes that while LLMs can approximate geographic information, their accuracy and reliability are inconsistent, underscoring the need for fine-tuning with geographical information to enhance their utility in GIScience and Geoinformatics.

### 摘要
随着大型语言模型(LLMs)的持续发展，其提供事实信息的可信度问题日益受到关注。这种担忧同样适用于它们准确表征地理世界的能力。鉴于该领域的最新进展，有必要考量LLMs对地理世界的表征是否以及在何种程度上值得信赖。本研究评估了GPT-4o和Gemini 2.0 Flash在三项关键地理空间任务中的表现：地理编码、高程估计和反向地理编码。在地理编码任务中，两个模型对奥地利因斯布鲁克圣安娜柱坐标的估计均表现出系统性和随机性误差：GPT-4o偏差较大，而Gemini 2.0 Flash虽精度较高但存在显著系统性偏移。在高程估计方面，两个模型均倾向于低估奥地利各地海拔，但仍能捕捉整体地形趋势，其中Gemini 2.0 Flash在东部地区表现更优。反向地理编码任务（通过坐标识别奥地利联邦州）显示，Gemini 2.0 Flash在整体准确率和F1分数上优于GPT-4o，表现出更好的区域一致性。尽管存在这些发现，两个模型均未能准确重建奥地利联邦州边界，存在持续性的错误分类。研究结论表明，虽然LLMs能够近似获取地理信息，但其准确性和可靠性并不稳定，这凸显了需要通过地理信息微调来增强其在GIScience和地理信息学中的实用性。

---

## [Lossless Token Sequence Compression via Meta-Tokens](https://arxiv.org/abs/2506.00307)

### Abstract
arXiv:2506.00307v1 Announce Type: cross 
Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.

### 摘要
现有关于大型语言模型（LLM）提示压缩的研究主要集中于有损压缩方法，其目标是在显著缩短序列长度的同时，尽可能保留与下游任务相关的语义信息。本文提出了一种与LZ77类似的任务无关无损压缩技术，在两项评估任务中平均可分别将输入标记序列长度减少27%和18%。鉴于我们采用基于Transformer的LLM，由于注意力机制的二次计算复杂度，这相当于分别减少47%和33%的编码计算量。该标记序列变换过程可轻松逆转，且明确表明未丢失任何语义信息。我们在两项要求严格保持语义/句法的任务上评估所提方法，结果表明现有有损压缩方法在此类场景下表现欠佳。研究发现，与未压缩输入相比，我们的无损压缩技术仅产生微小性能差距，并指出更大规模的模型和扩展的计算预算有望完全消除这一差距。

---

## [MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform](https://arxiv.org/abs/2506.00308)

### Abstract
arXiv:2506.00308v1 Announce Type: cross 
Abstract: Understanding the prevalence of misinformation in health topics online can inform public health policies and interventions. However, measuring such misinformation at scale remains a challenge, particularly for high-stakes but understudied topics like opioid-use disorder (OUD)--a leading cause of death in the U.S. We present the first large-scale study of OUD-related myths on YouTube, a widely-used platform for health information. With clinical experts, we validate 8 pervasive myths and release an expert-labeled video dataset. To scale labeling, we introduce MythTriage, an efficient triage pipeline that uses a lightweight model for routine cases and defers harder ones to a high-performing, but costlier, large language model (LLM). MythTriage achieves up to 0.86 macro F1-score while estimated to reduce annotation time and financial cost by over 76% compared to experts and full LLM labeling. We analyze 2.9K search results and 343K recommendations, uncovering how myths persist on YouTube and offering actionable insights for public health and platform moderation.

### 摘要
理解网络健康话题中错误信息的普遍性可为公共卫生政策与干预措施提供依据。然而大规模测量此类错误信息仍存在挑战，尤其对阿片类药物使用障碍(OUD)这类高风险但研究不足的议题——该病症是美国主要致死原因之一。本研究首次对YouTube这一广泛使用的健康信息平台上OUD相关谬误进行大规模分析，通过与临床专家合作验证了8种普遍谬误并发布专家标注视频数据集。为扩大标注规模，我们提出MythTriage高效分类流程：采用轻量模型处理常规案例，将疑难案例交由性能卓越但成本较高的大语言模型(LLM)处理。相比专家标注和全程LLM标注，MythTriage在达到0.86宏观F1值的同时，预计可减少76%以上的标注时间与经济成本。通过分析2,900条搜索结果和343,000条推荐内容，本研究揭示了谬误内容在YouTube的持续存在机制，为公共卫生及平台内容治理提供了可操作的见解。

---

## [Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs](https://arxiv.org/abs/2506.00344)

### Abstract
arXiv:2506.00344v1 Announce Type: cross 
Abstract: Scaling test-time computation--generating and analyzing multiple or sequential outputs for a single input--has become a promising strategy for improving the reliability and quality of large language models (LLMs), as evidenced by advances in uncertainty quantification and multi-step reasoning. A key shared component is semantic clustering, which groups outputs that differ in form but convey the same meaning. Semantic clustering enables estimation of the distribution over the semantics of outputs and helps avoid redundant exploration of reasoning paths. However, existing approaches typically rely on external models, which introduce substantial computational overhead and often fail to capture context-aware semantics. We propose Latent Semantic Clustering (LSC), a lightweight and context-sensitive method that leverages the generator LLM's internal hidden states for clustering, eliminating the need for external models. Our extensive experiment across various LLMs and datasets shows that LSC significantly improves the computational efficiency of test-time scaling while maintaining or exceeding the performance of existing methods.

### 摘要
扩展测试时计算——为单个输入生成并分析多个或序列化输出——已成为提升大语言模型（LLM）可靠性和质量的重要策略，这在不确定性量化和多步推理的进展中得到印证。其核心共性在于语义聚类技术，该技术能将形式不同但语义相同的输出归为一类。语义聚类既可估计输出语义的分布，又能避免推理路径的冗余探索。然而现有方法通常依赖外部模型，这会导致显著的计算开销，且往往难以捕捉上下文相关的语义。我们提出潜在语义聚类（LSC），这是一种轻量级且上下文敏感的解决方案，通过利用生成LLM内部的隐藏状态进行聚类，无需借助外部模型。我们在多种LLM和数据集上的大量实验表明，LSC在保持或超越现有方法性能的同时，显著提升了测试时扩展的计算效率。

---

## [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/abs/2506.00288)

### Abstract
arXiv:2506.00288v1 Announce Type: cross 
Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.

### 摘要
持续预训练（CPT）是将现有大型语言模型（LLMs）适配到新语言的常用方法。在此过程中，通常会在训练数据中混入部分英语数据，但其作用至今尚未得到深入研究。本研究表明，加入英语数据虽不影响验证困惑度，但对目标语言下游能力的涌现至关重要。我们提出了一个与语言无关的上下文学习（ICL）基准，发现若不包含英语数据，CPT早期会出现灾难性遗忘现象。这进而损害了模型对目标语言下游提示的泛化能力（以困惑度衡量），尽管该现象在训练后期才表现为准确率下降，且与模型参数的显著偏移相关。基于这些发现，我们引入课程学习与权重指数移动平均（EMA）作为有效替代方案，以缓解对英语数据的依赖。总之，本研究揭示了语言适配CPT过程中涌现能力的动态机制，为未来设计更有效的方法奠定了基础。

---

## [An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3](https://arxiv.org/abs/2506.00312)

### Abstract
arXiv:2506.00312v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been prominent in various tasks, including text generation and summarisation. The applicability of LLMs to the generation of product reviews is gaining momentum, paving the way for the generation of movie reviews. In this study, we propose a framework that generates movie reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate their performance by comparing the generated outputs with IMDb user reviews. We use movie subtitles and screenplays as input to the LLMs and investigate how they affect the quality of reviews generated. We review the LLM-based movie reviews in terms of vocabulary, sentiment polarity, similarity, and thematic consistency in comparison to IMDB user reviews. The results demonstrate that LLMs are capable of generating syntactically fluent and structurally complete movie reviews. Nevertheless, there is still a noticeable gap in emotional richness and stylistic coherence between LLM-generated and IMDb reviews, suggesting that further refinement is needed to improve the overall quality of movie review generation. We provided a survey-based analysis where participants were told to distinguish between LLM and IMDb user reviews. The results show that LLM-generated reviews are difficult to distinguish from IMDB user reviews. We found that DeepSeek-V3 produced the most balanced reviews, closely matching IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0 captured negative emotions better but showed excessive emotional intensity.

### 摘要
大语言模型（LLMs）在文本生成和摘要等多种任务中表现突出。LLMs在生成产品评论方面的适用性日益增强，为电影评论的生成铺平了道路。本研究提出一个框架，利用三种LLMs（GPT-4o、DeepSeek-V3和Gemini-2.0）生成电影评论，并通过将生成的输出与IMDb用户评论进行比较来评估其性能。我们使用电影字幕和剧本作为LLMs的输入，研究它们如何影响生成的评论质量。我们从词汇、情感极性、相似性和主题一致性等方面对基于LLM的电影评论与IMDb用户评论进行了比较分析。结果表明，LLMs能够生成句法流畅且结构完整的电影评论。然而，LLM生成的评论与IMDb评论在情感丰富性和风格连贯性上仍存在明显差距，这表明需要进一步改进以提高电影评论生成的整体质量。我们提供了一项基于调查的分析，参与者被要求区分LLM和IMDb用户评论。结果显示，LLM生成的评论与IMDb用户评论难以区分。我们发现，DeepSeek-V3生成的评论最为平衡，与IMDb评论最为接近；GPT-4o过度强调积极情感，而Gemini-2.0虽能更好地捕捉消极情感，但表现出过度的情感强度。

---

## [Scaling Textual Gradients via Sampling-Based Momentum](https://arxiv.org/abs/2506.00400)

### Abstract
arXiv:2506.00400v1 Announce Type: cross 
Abstract: As prompts play an increasingly critical role in large language models (LLMs), optimizing textual prompts has become a crucial challenge. The Textual Gradient Descent (TGD) framework has emerged as a promising data-driven approach that iteratively refines textual prompts using LLM - suggested updates (or textual gradients) over minibatches of training samples. In this paper, we empirically demonstrate that scaling the number of training examples initially improves but later degrades TGD's performance across multiple downstream NLP tasks. However, while data scaling improves results for most tasks, it also significantly increases the computational cost when leveraging LLMs. To address this, we draw inspiration from numerical gradient descent and propose Textual Stochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates scalable in-context learning by reweighting prompt sampling based on past batch distributions. Across nine NLP tasks spanning three domains - including BIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks - TSGD-M significantly outperforms TGD baselines that do not incorporate reweighted sampling, while also reducing variance in most tasks.

### 摘要
随着提示词在大型语言模型（LLMs）中的作用日益关键，文本提示优化已成为重要挑战。文本梯度下降（TGD）框架作为一种数据驱动方法，通过利用LLM对训练样本小批量生成的更新建议（即文本梯度）迭代优化提示词，展现出显著潜力。本文通过实证研究发现：在多个下游NLP任务中，增加训练样本数量会先提升后降低TGD的性能表现。尽管数据扩增能改善多数任务效果，但使用LLM进行优化会大幅增加计算成本。受数值梯度下降启发，我们提出带动量的文本随机梯度下降法（TSGD-M）——该方法通过基于历史批次分布重新加权提示采样，实现可扩展的上下文学习。在涵盖三大领域的九项NLP任务（包括BIG-Bench Hard（BBH）、自然语言理解任务和推理任务）中，TSGD-M显著优于未采用重加权采样的TGD基线方法，同时降低了多数任务的方差。

---

## [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413)

### Abstract
arXiv:2506.00413v1 Announce Type: cross 
Abstract: The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.

### 摘要
大语言模型（LLM）的生成速度受限于自回归解码机制，即逐个顺序预测令牌。扩散大语言模型（dLLM）虽理论上支持并行令牌生成，但在实际应用中难以在不显著降低质量的前提下达到自回归模型的速度。为此，我们提出自适应并行解码（APD）这一新方法，通过动态调整并行采样的令牌数量实现优化。该方法通过定义dLLM边缘概率与小型辅助自回归模型序列联合概率之间的乘积混合分布来实现这一目标。这逆转了推测性解码的标准设置——后者旨在通过小型模型草稿从大型自回归验证器中采样。我们进一步通过启用KV缓存和限制掩码输入规模来优化APD。总体而言，该方法提供三个可调参数以灵活权衡吞吐量与生成质量。实验表明，APD在下游基准测试中能以极小的质量损失显著提升吞吐量。

---

## [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/abs/2506.00418)

### Abstract
arXiv:2506.00418v1 Announce Type: cross 
Abstract: In context learning (ICL) relies heavily on high quality demonstrations drawn from large annotated corpora. Existing approaches detect noisy annotations by ranking local perplexities, presuming that noisy samples yield higher perplexities than their clean counterparts. However, this assumption breaks down when the noise ratio is high and many demonstrations are flawed. We reexamine the perplexity based paradigm for text generation under noisy annotations, highlighting two sources of bias in perplexity: the annotation itself and the domain specific knowledge inherent in large language models (LLMs). To overcome these biases, we introduce a dual debiasing framework that uses synthesized neighbors to explicitly correct perplexity estimates, yielding a robust Sample Cleanliness Score. This metric uncovers absolute sample cleanliness regardless of the overall corpus noise level. Extensive experiments demonstrate our method's superior noise detection capabilities and show that its final ICL performance is comparable to that of a fully clean demonstration corpus. Moreover, our approach remains robust even when noise ratios are extremely high.

### 摘要
语境学习（ICL）高度依赖于从大规模标注语料库中提取的高质量示例。现有方法通过排序局部困惑度来检测噪声标注，其假设噪声样本比干净样本会产生更高的困惑度。然而当噪声比例较高且多数示例存在缺陷时，这一假设将失效。我们重新审视了噪声标注下基于困惑度的文本生成范式，揭示出困惑度存在的两种偏差源：标注本身以及大语言模型（LLM）固有的领域特定知识。为克服这些偏差，我们提出了一种双重去偏框架，利用合成邻居显式校正困惑度估计，从而得到鲁棒的样本清洁度评分。该指标能揭示样本的绝对清洁度，与语料库整体噪声水平无关。大量实验表明，我们的方法具有卓越的噪声检测能力，其最终ICL性能与完全干净的示例语料库相当。即使在极高噪声比例下，该方法仍保持稳健性。

---

## [Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety](https://arxiv.org/abs/2506.00415)

### Abstract
arXiv:2506.00415v1 Announce Type: cross 
Abstract: As large language models (LLMs) become more powerful and pervasive across society, ensuring these systems are beneficial, safe, and aligned with human values is crucial. Current alignment techniques, like Constitutional AI (CAI), involve complex iterative processes. This paper argues that the Method of Wide Reflective Equilibrium (MWRE) -- a well-established coherentist moral methodology -- offers a uniquely apt framework for understanding current LLM alignment efforts. Moreover, this methodology can substantively augment these processes by providing concrete pathways for improving their dynamic revisability, procedural legitimacy, and overall ethical grounding. Together, these enhancements can help produce more robust and ethically defensible outcomes. MWRE, emphasizing the achievement of coherence between our considered moral judgments, guiding moral principles, and relevant background theories, arguably better represents the intricate reality of LLM alignment and offers a more robust path to justification than prevailing foundationalist models or simplistic input-output evaluations. While current methods like CAI bear a structural resemblance to MWRE, they often lack its crucial emphasis on dynamic, bi-directional revision of principles and the procedural legitimacy derived from such a process. While acknowledging various disanalogies (e.g., consciousness, genuine understanding in LLMs), the paper demonstrates that MWRE serves as a valuable heuristic for critically analyzing current alignment efforts and for guiding the future development of more ethically sound and justifiably aligned AI systems.

### 摘要
随着大型语言模型（LLMs）在社会中日益强大和普及，确保这些系统有益、安全且符合人类价值观变得至关重要。当前的模型对齐技术（如宪法人工智能CAI）涉及复杂的迭代过程。本文认为，广义反思平衡方法（MWRE）——一种成熟的连贯主义道德方法论——为理解当前LLM对齐工作提供了独特而恰当的理论框架。此外，该方法论能通过提供具体路径来增强动态可修正性、程序合法性及整体伦理基础，从而实质性改进对齐流程。这些改进共同有助于产生更具鲁棒性和伦理辩护力的结果。MWRE强调在我们深思熟虑的道德判断、指导性原则及相关背景理论之间达成连贯性，相比主流基础主义模型或简化的输入输出评估，它更能体现LLM对齐的复杂现实，并提供更坚实的辩护路径。尽管CAI等方法与MWRE存在结构相似性，但往往缺乏后者对原则动态双向修正的关键重视，以及由此过程产生的程序合法性。在承认若干差异（如LLMs的意识和真实理解能力）的同时，本文论证了MWRE可作为重要启发工具，既能批判性分析当前对齐实践，又能指导未来开发更具伦理合理性和辩护依据的AI系统。

---

## [RLAE: Reinforcement Learning-Assisted Ensemble for LLMs](https://arxiv.org/abs/2506.00439)

### Abstract
arXiv:2506.00439v1 Announce Type: cross 
Abstract: Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\text&#123;RLAE&#125;_\text&#123;PPO&#125;$ and $\text&#123;RLAE&#125;_\text&#123;MAPPO&#125;$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.

### 摘要
集成大语言模型（LLMs）能有效结合不同模型的多样化优势，为提升各类任务性能提供了可行途径。然而现有方法通常依赖固定权重策略，难以适应LLM能力动态变化的上下文相关特性。本研究提出强化学习辅助的大语言模型集成框架（RLAE），通过马尔可夫决策过程（MDP）重构LLM集成机制。该框架引入强化学习智能体，基于输入上下文和中间生成状态动态调整集成权重，并通过最终输出质量直接对应的奖励机制进行训练。我们分别采用单智能体与多智能体强化学习算法（RLAE_PPO和RLAE_MAPPO）实现RLAE，相较传统集成方法取得显著提升。在多样化任务上的大量实验表明，RLAE以最高3.3%的准确率优势超越现有方法，提供了更高效的LLM集成框架。此外，该方法在不同任务间展现出优异的泛化能力且无需重新训练，同时实现了更低的时间延迟。

---

## [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation](https://arxiv.org/abs/2506.00482)

### Abstract
arXiv:2506.00482v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.

### 摘要
随着大型语言模型(LLMs)的持续发展，对最新且组织良好的基准测试需求变得愈发关键。然而，尽管数学、代码等领域中领域专用模型的重要性日益凸显，现有许多数据集仍存在分散性、难以管理等问题，难以为特定需求或领域定制评估方案。本文介绍BenchHub——一个动态基准测试资源库，旨在帮助研究者和开发者更高效地评估LLMs。该系统汇集并自动分类来自多领域的基准数据集，整合了38个基准测试中的30.3万个问题，支持持续更新和可扩展的数据管理，能够针对不同领域或使用场景提供灵活可定制的评估方案。通过对各类LLM家族的大量实验，我们发现模型在领域特定子集上的表现存在显著差异，这凸显了领域感知基准测试的重要性。我们相信BenchHub能促进更好的数据集复用、更透明的模型比较，并更容易识别现有基准中代表性不足的领域，为推进LLM评估研究提供关键基础设施。

---

## [M2WLLM: Multi-Modal Multi-Task Ultra-Short-term Wind Power Prediction Algorithm Based on Large Language Model](https://arxiv.org/abs/2506.00531)

### Abstract
arXiv:2506.00531v1 Announce Type: cross 
Abstract: The integration of wind energy into power grids necessitates accurate ultra-short-term wind power forecasting to ensure grid stability and optimize resource allocation. This study introduces M2WLLM, an innovative model that leverages the capabilities of Large Language Models (LLMs) for predicting wind power output at granular time intervals. M2WLLM overcomes the limitations of traditional and deep learning methods by seamlessly integrating textual information and temporal numerical data, significantly improving wind power forecasting accuracy through multi-modal data. Its architecture features a Prompt Embedder and a Data Embedder, enabling an effective fusion of textual prompts and numerical inputs within the LLMs framework. The Semantic Augmenter within the Data Embedder translates temporal data into a format that the LLMs can comprehend, enabling it to extract latent features and improve prediction accuracy. The empirical evaluations conducted on wind farm data from three Chinese provinces demonstrate that M2WLLM consistently outperforms existing methods, such as GPT4TS, across various datasets and prediction horizons. The results highlight LLMs' ability to enhance accuracy and robustness in ultra-short-term forecasting and showcase their strong few-shot learning capabilities.

### 摘要
将风能整合到电网中需要精确的超短期风电功率预测，以确保电网稳定性并优化资源配置。本研究提出了一种创新模型M2WLLM，该模型利用大型语言模型（LLMs）的能力，在细粒度时间间隔内预测风电功率输出。M2WLLM通过无缝整合文本信息和时序数值数据，克服了传统方法和深度学习的局限性，借助多模态数据显著提高了风电功率预测的准确性。其架构包含提示嵌入器和数据嵌入器，能够在LLMs框架内有效融合文本提示和数值输入。数据嵌入器中的语义增强器将时序数据转换为LLMs可理解的格式，使其能够提取潜在特征并提高预测精度。在中国三个省份风电场数据上进行的实证评估表明，M2WLLM在不同数据集和预测时间范围内均优于GPT4TS等现有方法。结果凸显了LLMs在提升超短期预测准确性和鲁棒性方面的能力，并展示了其强大的少样本学习性能。

---

## [CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention](https://arxiv.org/abs/2506.00519)

### Abstract
arXiv:2506.00519v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often exhibit knowledge disparities across languages. Encouraging LLMs to \textit&#123;abstain&#125; when faced with knowledge gaps is a promising strategy to reduce hallucinations in multilingual settings. Current abstention strategies for multilingual scenarios primarily rely on generating feedback in various languages using LLMs and performing self-reflection. However, these methods can be adversely impacted by inaccuracies and biases in the generated feedback. To address this, from a causal perspective, we introduce \textit&#123;CausalAbstain&#125;, a method that helps LLMs determine whether to utilize multiple generated feedback responses and how to identify the most useful ones. Extensive experiments demonstrate that \textit&#123;CausalAbstain&#125; effectively selects helpful feedback and enhances abstention decisions with interpretability in both native language (\textsc&#123;Casual-native&#125;) and multilingual (\textsc&#123;Causal-multi&#125;) settings, outperforming strong baselines on two benchmark datasets covering encyclopedic and commonsense knowledge QA tasks. Our code and data are open-sourced at https://github.com/peachch/CausalAbstain.

### 摘要
大语言模型（LLMs）在不同语言间常表现出知识差异。鼓励LLMs在面临知识空白时选择	extit&#123;弃权&#125;，是多语言环境下减少幻觉的有效策略。当前多语言场景的弃权策略主要依赖LLM生成多语言反馈并进行自我反思，但这些方法易受生成反馈的误差与偏见影响。为此，我们从因果视角提出	extit&#123;CausalAbstain&#125;方法，通过因果分析帮助LLMs判断是否利用多个生成反馈，并识别最有价值的反馈。大量实验表明，无论是在母语场景（	extsc&#123;Casual-native&#125;）还是多语言场景（	extsc&#123;Causal-multi&#125;）中，	extit&#123;CausalAbstain&#125;都能有效选择有益反馈，以可解释的方式提升弃权决策效果，在涵盖百科知识和常识问答的两个基准数据集上均优于强基线方法。代码与数据已开源：https://github.com/peachch/CausalAbstain。

---

## [The Security Threat of Compressed Projectors in Large Vision-Language Models](https://arxiv.org/abs/2506.00534)

### Abstract
arXiv:2506.00534v1 Announce Type: cross 
Abstract: The choice of a suitable visual language projector (VLP) is critical to the successful training of large visual language models (LVLMs). Mainstream VLPs can be broadly categorized into compressed and uncompressed projectors, and each offering distinct advantages in performance and computational efficiency. However, their security implications have not been thoroughly examined. Our comprehensive evaluation reveals significant differences in their security profiles: compressed projectors exhibit substantial vulnerabilities, allowing adversaries to successfully compromise LVLMs even with minimal knowledge of structural information. In stark contrast, uncompressed projectors demonstrate robust security properties and do not introduce additional vulnerabilities. These findings provide critical guidance for researchers in selecting optimal VLPs that enhance the security and reliability of visual language models. The code will be released.

### 摘要
选择合适的视觉语言投影器（VLP）对于成功训练大型视觉语言模型（LVLM）至关重要。主流VLP可大致分为压缩型和非压缩型投影器，各自在性能和计算效率上具有显著优势。然而，其安全性影响尚未得到充分研究。我们的综合评估揭示了二者在安全特性上的显著差异：压缩型投影器存在严重漏洞，即使攻击者仅掌握少量结构信息，也能成功攻陷LVLM；而非压缩型投影器则表现出强大的安全特性，不会引入额外漏洞。这些发现为研究者选择最优VLP提供了关键指导，以增强视觉语言模型的安全性和可靠性。代码将予以公开。

---

## [It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs](https://arxiv.org/abs/2506.00486)

### Abstract
arXiv:2506.00486v1 Announce Type: cross 
Abstract: Despite rapid advancements in the research and deployment of large language models (LLMs), the statistical distribution of model parameters, as well as their influence on initialization, training dynamics, and downstream efficiency, has received surprisingly little attention. A recent work introduced BackSlash, a training-time compression algorithm. It first demonstrated that pre-trained LLM parameters follow generalized Gaussian distributions (GGDs) better. By optimizing GG priors during training, BackSlash can reduce parameters by up to 90\% with minimal performance loss. Building on this foundational insight, we propose a unified, end-to-end framework for LLM optimization based on the GG model. Our contributions are threefold: (1) GG-based initialization scheme that aligns with the statistical structure of trained models, resulting in faster convergence and improved accuracy; (2) DeepShape, a post-training regularization method that reshapes weight distributions to match a GG profile, improving compressibility with minimized degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit floating-point format designed for GG-distributed-initialized BackSlash training, enabling low-cost inference without compromising accuracy. Experiments across diverse model architectures show that our framework consistently yields smaller and faster models that match or outperform standard training baselines. By grounding LLM development in principled statistical modeling, this work forges a new path toward efficient, scalable, and hardware-aware AI systems. The code is available on our project page: https://huggingface.co/spaces/shifeng3711/gg_prior.

### 摘要
尽管大型语言模型（LLMs）的研究与部署进展迅速，模型参数的统计分布及其对初始化、训练动态和下游效率的影响却鲜少受到关注。近期研究提出BackSlash这一训练时压缩算法，首次证明预训练LLM参数更符合广义高斯分布（GGDs）。通过在训练过程中优化GG先验，BackSlash能以最小性能损失实现高达90%的参数缩减。基于这一基础发现，我们提出一个基于GG模型、端到端统一的LLM优化框架。主要贡献包括：（1）符合训练模型统计结构的GG初始化方案，可加速收敛并提升精度；（2）DeepShape后训练正则化方法，通过重塑权重分布匹配GG轮廓，在性能损失最小化的同时提升压缩性；（3）RF8——专为GG分布初始化BackSlash训练设计的8位硬件高效浮点格式，可在保持精度前提下实现低成本推理。多模型架构实验表明，本框架始终能生成更小更快的模型，其性能匹配或超越标准训练基线。通过将LLM开发建立在原则性统计建模基础上，本研究为高效、可扩展且硬件感知的AI系统开辟了新路径。代码详见项目页：https://huggingface.co/spaces/shifeng3711/gg_prior。

---

## [Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing](https://arxiv.org/abs/2506.00536)

### Abstract
arXiv:2506.00536v1 Announce Type: cross 
Abstract: Knowledge editing aims to efficiently update Large Language Models (LLMs) by modifying specific knowledge without retraining the entire model. Among knowledge editing approaches, in-context editing (ICE) offers a lightweight solution by injecting new knowledge directly into the input context, leaving model parameters unchanged. However, existing ICE approaches do not explicitly separate the newly injected knowledge from the model's original reasoning process. This entanglement often results in conflicts between external updates and internal parametric knowledge, undermining the consistency and accuracy of the reasoning path.In this work, we conduct preliminary experiments to examine how parametric knowledge influences reasoning path planning. We find that the model's reasoning is tightly coupled with its internal knowledge, and that naively injecting new information without adapting the reasoning path often leads to performance degradation, particularly in multi-hop tasks. To this end, we propose DecKER, a novel ICE framework that decouples reasoning from knowledge editing by generating a masked reasoning path and then resolving knowledge edits via hybrid retrieval and model-based validation. Experiments on multi-hop QA benchmarks show that DecKER significantly outperforms existing ICE methods by mitigating knowledge conflicts and preserving reasoning consistency. Our code is available at: https://github.com/bebr2/DecKER .

### 摘要
知识编辑旨在通过修改特定知识而非重新训练整个模型，来高效更新大型语言模型（LLMs）。在知识编辑方法中，上下文编辑（ICE）提供了一种轻量级解决方案，其通过直接将新知识注入输入上下文而保持模型参数不变。然而，现有ICE方法未能明确分离新注入知识与模型原始推理过程，这种纠缠常导致外部更新与内部参数化知识间的冲突，从而损害推理路径的一致性和准确性。本研究通过初步实验探究参数化知识如何影响推理路径规划，发现模型的推理与其内部知识紧密耦合，若未适配推理路径而简单注入新信息，往往会导致性能下降（尤其在多跳任务中）。为此，我们提出DecKER框架，该新型ICE方法通过生成掩码推理路径并借助混合检索与基于模型的验证来解决知识编辑，从而实现推理与知识编辑的解耦。多跳问答基准测试表明，DecKER通过缓解知识冲突并保持推理一致性，显著优于现有ICE方法。代码已开源：https://github.com/bebr2/DecKER。

---

## [Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning](https://arxiv.org/abs/2506.00527)

### Abstract
arXiv:2506.00527v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems in the Intellectual Property (IP) field often struggle with diverse user queries, including colloquial expressions, spelling errors, and ambiguous terminology, leading to inaccurate retrieval and suboptimal responses. To address this challenge, we propose Multi-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a novel framework that leverages large language models (LLMs) to simulate varied user inquiries and fine-tunes retrieval models to align semantically equivalent but linguistically diverse questions. Unlike complex architectural modifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining prompt-engineered query generation with hard negative mining to enhance retrieval robustness without costly infrastructure changes. Experimental results on a Taiwan patent Q&amp;A dataset show 185.62% improvement in retrieval accuracy on the Patent Consultation dataset and 262.26% improvement on the Novel Patent Technology Report dataset, with 14.22% and 53.58% improvements in generation quality over the baselines, respectively. By bridging the gap between user intent and system comprehension through semantic-aware retrieval optimization, MQG-RFM offers a practical, scalable approach for rapid, cost-effective deployment among small and medium-sized agencies seeking reliable patent intelligence solutions. Additionally, our proposed method has already been adopted by ScholarMate, the largest professional research social networking platform in China, to support real-world development and deployment. A demo version of the instantiated is available at https://github.com/renruntao/patent_rag.

### 摘要
知识产权领域的检索增强生成（RAG）系统常因用户查询的多样性（包括口语化表达、拼写错误和术语歧义）导致检索不准确和响应欠佳。为解决这一问题，我们提出多角度问题生成与检索微调方法（MQG-RFM），该框架利用大语言模型（LLM）模拟多样化用户询问，并通过微调检索模型以实现语义等效但语言形式多样问题的对齐。与复杂的架构修改不同，MQG-RFM采用轻量级"数据调优"范式，结合提示工程化查询生成与困难负样本挖掘，在不增加基础设施成本的前提下提升检索鲁棒性。在台湾专利问答数据集上的实验表明：专利咨询数据集的检索准确率提升185.62%，新型专利技术报告数据集提升262.26%，生成质量较基线分别提高14.22%和53.58%。通过语义感知的检索优化弥合用户意图与系统理解之间的鸿沟，MQG-RFM为寻求可靠专利情报解决方案的中小型机构提供了实用、可扩展的快速低成本部署方案。此外，中国最大专业科研社交平台"学者圈"已采用本方法支持实际开发部署。实例化演示版本详见https://github.com/renruntao/patent_rag。

---

## [Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages](https://arxiv.org/abs/2506.00549)

### Abstract
arXiv:2506.00549v1 Announce Type: cross 
Abstract: Evaluation frameworks for text summarization have evolved in terms of both domain coverage and metrics. However, existing benchmarks still lack domain-specific assessment criteria, remain predominantly English-centric, and face challenges with human annotation due to the complexity of reasoning. To address these, we introduce MSumBench, which provides a multi-dimensional, multi-domain evaluation of summarization in English and Chinese. It also incorporates specialized assessment criteria for each domain and leverages a multi-agent debate system to enhance annotation quality. By evaluating eight modern summarization models, we discover distinct performance patterns across domains and languages. We further examine large language models as summary evaluators, analyzing the correlation between their evaluation and summarization capabilities, and uncovering systematic bias in their assessment of self-generated summaries. Our benchmark dataset is publicly available at https://github.com/DISL-Lab/MSumBench.

### 摘要
文本摘要评估框架在领域覆盖度和评价指标方面持续演进。然而现有基准测试仍存在三大局限：缺乏领域特异性评估标准、以英语为中心、以及因推理复杂性导致人工标注困难。为此，我们提出MSumBench评估体系，提供中英文双语的跨领域多维摘要评估方案。该体系不仅包含各领域的专业化评估标准，还采用多智能体辩论机制提升标注质量。通过对八种前沿摘要模型的测试，我们发现了跨领域与跨语言的性能差异规律。进一步探究大语言模型作为摘要评估器的表现时，我们揭示了其评估能力与摘要生成能力的相关性，并发现其对自身生成摘要存在系统性评估偏差。本基准数据集已开源：https://github.com/DISL-Lab/MSumBench。

---

## [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)

### Abstract
arXiv:2506.00551v1 Announce Type: cross 
Abstract: Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers' mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator's configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.

### 摘要
由于在人工智能驱动的心理健康研究中涉及真实求助者存在成本和伦理限制，研究人员开发了基于大型语言模型的对话代理（CAs），通过配置个性化参数（如个人资料、症状和场景）来模拟求助者。尽管这些努力推动了心理健康领域AI的发展，但实现更真实的求助者模拟仍面临两个关键挑战：动态演化和多会话记忆。求助者的心理状态在通常跨越多轮次的心理咨询过程中往往存在波动。为此，我们提出AnnaAgent——一个具备三级记忆机制的情感与认知动态代理系统。该系统通过基于真实心理咨询对话训练的情绪调节器和主诉诱发器，实现了对模拟器参数的动态控制。其三级记忆机制能有效整合跨会话的短期与长期记忆。自动化评估与人工评估结果表明，与现有基线相比，AnnaAgent在心理咨询场景中实现了更真实的求助者模拟。经过伦理审查与筛选的代码已发布于https://github.com/sci-m-wang/AnnaAgent。

---

## [ORAN-GUIDE: RAG-Driven Prompt Learning for LLM-Augmented Reinforcement Learning in O-RAN Network Slicing](https://arxiv.org/abs/2506.00576)

### Abstract
arXiv:2506.00576v1 Announce Type: cross 
Abstract: Advanced wireless networks must support highly dynamic and heterogeneous service demands. Open Radio Access Network (O-RAN) architecture enables this flexibility by adopting modular, disaggregated components, such as the RAN Intelligent Controller (RIC), Centralized Unit (CU), and Distributed Unit (DU), that can support intelligent control via machine learning (ML). While deep reinforcement learning (DRL) is a powerful tool for managing dynamic resource allocation and slicing, it often struggles to process raw, unstructured input like RF features, QoS metrics, and traffic trends. These limitations hinder policy generalization and decision efficiency in partially observable and evolving environments. To address this, we propose \textit&#123;ORAN-GUIDE&#125;, a dual-LLM framework that enhances multi-agent RL (MARL) with task-relevant, semantically enriched state representations. The architecture employs a domain-specific language model, ORANSight, pretrained on O-RAN control and configuration data, to generate structured, context-aware prompts. These prompts are fused with learnable tokens and passed to a frozen GPT-based encoder that outputs high-level semantic representations for DRL agents. This design adopts a retrieval-augmented generation (RAG) style pipeline tailored for technical decision-making in wireless systems. Experimental results show that ORAN-GUIDE improves sample efficiency, policy convergence, and performance generalization over standard MARL and single-LLM baselines.

### 摘要
先进无线网络需支持高度动态异构的业务需求。开放式无线接入网（O-RAN）架构通过采用模块化、解耦的组件（如无线智能控制器RIC、集中单元CU和分布单元DU）实现这种灵活性，这些组件可通过机器学习（ML）支持智能控制。尽管深度强化学习（DRL）是管理动态资源分配与网络切片的强大工具，但其在处理射频特征、服务质量指标和流量趋势等原始非结构化输入时往往表现不佳。这些限制在部分可观测且持续演化的环境中阻碍了策略泛化与决策效率。为此，我们提出ORAN-GUIDE——一种双大语言模型框架，通过任务相关且语义增强的状态表征来优化多智能体强化学习（MARL）。该架构采用基于O-RAN控制与配置数据预训练的领域专用语言模型ORANSight，生成结构化、上下文感知的提示。这些提示与可学习令牌融合后输入冻结的GPT编码器，进而为DRL智能体输出高层语义表征。该设计采用检索增强生成（RAG）范式，专为无线系统技术决策定制。实验结果表明，相较于标准MARL和单大语言模型基线，ORAN-GUIDE在样本效率、策略收敛和性能泛化方面均有提升。

---

## [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)

### Abstract
arXiv:2506.00555v1 Announce Type: cross 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 18.4% over supervised fine-tuning baselines.

### 摘要
医学大型视觉语言模型（Med-LVLMs）在多模态诊断任务中展现出强大潜力。然而，现有单智能体模型难以适应多样化的医学专科领域，限制了其性能表现。近期研究受临床工作流程启发，引入了多智能体协作框架，其中全科医生（GPs）与专科医生按固定序列交互。尽管有所改进，这些静态流程在推理中缺乏灵活性和适应性。为此，我们提出MMedAgent-RL——基于强化学习（RL）的多智能体框架，可实现医疗智能体间的动态优化协作。具体而言，我们通过RL训练两个基于Qwen2.5-VL的全科智能体：分诊医生学习将患者分配至合适专科，而主治医生则综合多专科意见与自身知识做出最终决策。针对专科输出不一致问题，我们提出课程学习（CL）引导的RL策略，逐步指导主治医生在模仿专科诊断与纠正其错误之间取得平衡。在五个医学VQA基准测试上的实验表明，MMedAgent-RL不仅性能超越开源与商用Med-LVLMs，更展现出类人推理模式。值得注意的是，其平均性能较监督微调基线提升达18.4%。

---

## [Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing](https://arxiv.org/abs/2506.00574)

### Abstract
arXiv:2506.00574v1 Announce Type: cross 
Abstract: Modern wireless networks must adapt to dynamic conditions while efficiently managing diverse service demands. Traditional deep reinforcement learning (DRL) struggles in these environments, as scattered and evolving feedback makes optimal decision-making challenging. Large Language Models (LLMs) offer a solution by structuring unorganized network feedback into meaningful latent representations, helping RL agents recognize patterns more effectively. For example, in O-RAN slicing, concepts like SNR, power levels and throughput are semantically related, and LLMs can naturally cluster them, providing a more interpretable state representation. To leverage this capability, we introduce a contextualization-based adaptation method that integrates learnable prompts into an LLM-augmented DRL framework. Instead of relying on full model fine-tuning, we refine state representations through task-specific prompts that dynamically adjust to network conditions. Utilizing ORANSight, an LLM trained on O-RAN knowledge, we develop Prompt-Augmented Multi agent RL (PA-MRL) framework. Learnable prompts optimize both semantic clustering and RL objectives, allowing RL agents to achieve higher rewards in fewer iterations and adapt more efficiently. By incorporating prompt-augmented learning, our approach enables faster, more scalable, and adaptive resource allocation in O-RAN slicing. Experimental results show that it accelerates convergence and outperforms other baselines.

### 摘要
现代无线网络需适应动态环境并高效管理多样化服务需求。传统深度强化学习（DRL）在此类场景中表现欠佳，因为分散且动态演进的反馈使得最优决策具有挑战性。大型语言模型（LLMs）通过将无序网络反馈结构化处理为有意义的潜在表征，可帮助强化学习智能体更有效地识别模式。例如在O-RAN切片场景中，信噪比、功率水平与吞吐量等概念具有语义关联性，LLMs能自然实现其聚类，提供更具可解释性的状态表征。为利用此特性，我们提出基于情境化的自适应方法，将可学习提示词集成至LLM增强的DRL框架。该方法摒弃全模型微调，转而通过动态适应网络条件的任务特定提示词来优化状态表征。基于O-RAN知识训练的LLM模型ORANSight，我们开发了提示增强多智能体强化学习（PA-MRL）框架。可学习提示词同时优化语义聚类与强化学习目标，使智能体能以更少迭代次数获得更高奖励并实现高效适应。通过引入提示增强学习，我们的方法在O-RAN切片中实现了更快速、可扩展且自适应的资源分配。实验结果表明，该方法能加速收敛并优于其他基线方案。

---

## [Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models](https://arxiv.org/abs/2506.00653)

### Abstract
arXiv:2506.00653v1 Announce Type: cross 
Abstract: It has been hypothesized that neural networks with similar architectures trained on similar data learn shared representations relevant to the learning task. We build on this idea by extending the conceptual framework where representations learned across models trained on the same data can be expressed as linear combinations of a \emph&#123;universal&#125; set of basis features. These basis features underlie the learning task itself and remain consistent across models, regardless of scale. From this framework, we propose the \textbf&#123;Linear Representation Transferability (LRT)&#125; Hypothesis -- that there exists an affine transformation between the representation spaces of different models. To test this hypothesis, we learn affine mappings between the hidden states of models of different sizes and evaluate whether steering vectors -- directions in hidden state space associated with specific model behaviors -- retain their semantic effect when transferred from small to large language models using the learned mappings. We find strong empirical evidence that such affine mappings can preserve steering behaviors. These findings suggest that representations learned by small models can be used to guide the behavior of large models, and that the LRT hypothesis may be a promising direction on understanding representation alignment across model scales.

### 摘要
有假说认为，在相似数据上训练的、具有相似架构的神经网络会学习到与任务相关的共享表征。我们基于这一观点，扩展了概念框架：在不同模型上学习到的表征可以表示为对一组通用基础特征的线性组合。这些基础特征构成学习任务本身，且在不同规模的模型中保持一致性。基于此框架，我们提出线性表征可迁移性(LRT)假说——不同模型的表征空间之间存在仿射变换关系。为验证该假说，我们学习了不同规模模型隐藏状态间的仿射映射，并评估导向向量（隐藏状态空间中与特定模型行为相关的方向）在通过习得映射从小型语言模型迁移至大型语言模型时能否保持语义效应。实验证据表明，此类仿射映射能有效保留导向行为。这些发现表明，小型模型学习到的表征可用于指导大型模型的行为，且LRT假说可能为理解跨模型尺度的表征对齐机制提供新方向。

---

## [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)

### Abstract
arXiv:2506.00643v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.

### 摘要
大型语言模型（LLMs）在单答案多选题任务上的评估日益增多，然而许多现实问题需要从选项集中识别所有正确答案，这种能力目前仍未得到充分探索。我们推出SATA-BENCH，这是首个专门用于评估LLMs在'全选适用'（SATA）问题上表现的跨领域基准，涵盖阅读理解、法律和生物医学等多个领域。通过对27个开源和专有模型的评估，我们发现显著差距：即使最强模型也仅达到41.8%的精确匹配率，暴露了LLMs无法可靠识别所有正确答案的缺陷。研究表明这一弱点源于两个核心挑战：选择偏差（模型倾向于特定选项而忽略内容）和数量偏差（模型无法预测正确答案数量）。为解决这些问题，我们提出Choice Funnel解码策略，该策略将标记去偏与自适应阈值相结合，引导模型做出全面准确的选择。Choice Funnel在保持竞争力的基准上实现了最高29%的精确匹配提升，同时降低64%以上的推理成本。这些发现揭示了当前LLMs的根本局限性，并为诊断和改进多答案推理提供了新框架。我们公开SATA-BENCH和Choice Funnel以促进LLMs在现实多答案应用中实现稳健决策的发展。

---

## [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)

### Abstract
arXiv:2506.00688v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.

### 摘要
机器遗忘旨在从大型语言模型中移除敏感或不必要的数据。然而，近期研究表明遗忘往往是浅层的，指出被移除的知识很容易被恢复。本研究对标准遗忘评估方法进行了批判性检验，揭示了动摇这些结论可信度的关键局限。首先，我们发现部分评估方法向模型注入了大量新信息，可能通过测试期间的重新教学掩盖真实的遗忘效果。其次，我们证明评估结果在不同任务间存在显著差异，动摇了现有评估流程的普适性。最后，我们发现许多评估依赖于虚假相关性，导致结果难以信任和解释。综合来看，这些问题表明当前评估方案可能同时高估和低估了遗忘的成功率。为此，我们提出未来遗忘评估的两项原则：最小信息注入和下游任务感知。通过一系列针对性实验验证了这些原则，展示了违反任一原则如何导致误导性结论。

---

## [Pitfalls in Evaluating Language Model Forecasters](https://arxiv.org/abs/2506.00723)

### Abstract
arXiv:2506.00723v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.

### 摘要
大型语言模型（LLMs）近期被应用于预测任务，部分研究声称这些系统的表现可媲美或超越人类。本文认为，学术界对此类结论应持谨慎态度，因为评估LLM预测系统面临独特的挑战。我们识别出两大类问题：(1) 由于多种形式的时间信息泄露，导致评估结果可信度存疑；(2) 难以将评估表现外推至现实世界预测场景。通过系统分析和前人研究中的具体案例，我们论证了评估缺陷如何引发对当前及未来性能声明的质疑。我们主张需要采用更严谨的评估方法，才能可靠地判定LLMs的预测能力。

---

## [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)

### Abstract
arXiv:2506.00658v1 Announce Type: cross 
Abstract: Sarcasm is a form of humor where expressions convey meanings opposite to their literal interpretations. Classifying and generating sarcasm using large language models is vital for interpreting human communication. Sarcasm poses challenges for computational models, due to its nuanced nature. We introduce Sarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating, brooding, deadpan, polite, obnoxious, raging, and manic by annotating entries of the MUStARD dataset. Classification was evaluated using zero-shot, few-shot, chain-of-thought (CoT), and a novel emotion-based prompting technique. We propose an emotion-based generation method developed by identifying key components of sarcasm-incongruity, shock value, and context dependency. Our classification experiments show that Gemini 2.5, using emotion-based prompting, outperforms other setups with an F1 score of 0.3664. Human evaluators preferred our emotion-based prompting, with 38.46% more successful generations than zero-shot prompting.

### 摘要
讽刺是一种通过表达与字面意思相反的言语形式来体现的幽默。利用大语言模型对讽刺进行分类和生成对于理解人类交流至关重要。由于其微妙性，讽刺对计算模型提出了挑战。我们提出了Sarc7基准，通过标注MUStARD数据集的条目，将讽刺分为7类：自嘲型、沉思型、冷面型、礼貌型、讨厌型、愤怒型和狂躁型。评估采用零样本、少样本、思维链（CoT）及新型基于情感的提示技术进行分类。我们提出了一种基于情感的生成方法，该方法通过识别讽刺的关键成分——不一致性、冲击力和语境依赖性来实现。分类实验表明，采用基于情感提示的Gemini 2.5模型以0.3664的F1分数优于其他设置。人工评估者更倾向于我们的基于情感提示方法，其成功生成率比零样本提示高出38.46%。

---

## [QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training](https://arxiv.org/abs/2506.00711)

### Abstract
arXiv:2506.00711v1 Announce Type: cross 
Abstract: Clinical decision-making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision-centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time-series signals, and text reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization (DRPO), a novel reinforcement-learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.

### 摘要
临床决策通常需要对异构数据进行推理，然而现有的多模态语言模型（MLLMs）仍主要集中于视觉领域，且难以跨临床专科泛化。为弥补这一空白，我们提出了QoQ-Med-7B/32B，这是首个开放的通用临床基础模型，能够联合推理医学图像、时间序列信号和文本报告。QoQ-Med采用领域感知相对策略优化（DRPO）进行训练，这是一种新颖的强化学习目标，根据领域稀有性和模态难度分层缩放归一化奖励，从而缓解由临床数据分布偏斜导致的性能不平衡问题。通过在9个临床领域的261万条指令调优对上进行训练，我们发现与其他无评论训练方法（如GRPO）相比，DRPO训练将视觉领域的宏观F1诊断性能平均提升了43%。此外，QoQ-Med在密集分割数据上训练后，能够突出显示与诊断相关的显著区域，其IoU比开放模型高10倍，同时达到OpenAI o4-mini的性能。为促进可重复性和下游研究，我们在https://github.com/DDVD233/QoQ_Med上发布了（i）完整模型权重，（ii）模块化训练流程，以及（iii）所有中间推理轨迹。

---

## [An LLM Agent for Functional Bug Detection in Network Protocols](https://arxiv.org/abs/2506.00714)

### Abstract
arXiv:2506.00714v1 Announce Type: cross 
Abstract: Functional correctness is critical for ensuring the reliability and security of network protocol implementations. Functional bugs, instances where implementations diverge from behaviors specified in RFC documents, can lead to severe consequences, including faulty routing, authentication bypasses, and service disruptions. Detecting these bugs requires deep semantic analysis across specification documents and source code, a task beyond the capabilities of traditional static analysis tools. This paper introduces RFCScan, an autonomous agent that leverages large language models (LLMs) to detect functional bugs by checking conformance between network protocol implementations and their RFC specifications. Inspired by the human auditing procedure, RFCScan comprises two key components: an indexing agent and a detection agent. The former hierarchically summarizes protocol code semantics, generating semantic indexes that enable the detection agent to narrow down the scanning scope. The latter employs demand-driven retrieval to iteratively collect additional relevant data structures and functions, eventually identifying potential inconsistencies with the RFC specifications effectively. We evaluate RFCScan across six real-world network protocol implementations. RFCScan identifies 47 functional bugs with 81.9% precision, of which 20 bugs have been confirmed or fixed by developers.

### 摘要
功能正确性对于确保网络协议实现的可靠性和安全性至关重要。功能缺陷（即实现与RFC文档规定行为出现偏差的情况）可能导致严重后果，包括错误路由、身份验证绕过和服务中断。检测这类缺陷需要对规范文档和源代码进行深度语义分析，这超出了传统静态分析工具的能力范围。本文提出RFCScan——一种利用大语言模型（LLMs）通过检查网络协议实现与RFC规范一致性来检测功能缺陷的自主智能体。受人工审计流程启发，RFCScan包含两个核心组件：索引代理和检测代理。前者通过层级化总结协议代码语义生成语义索引，使检测代理能有效缩小扫描范围；后者采用需求驱动的检索方式迭代收集相关数据结构和函数，最终高效识别与RFC规范的潜在不一致性。我们在六个真实网络协议实现上评估RFCScan，共发现47个功能缺陷（精确率达81.9%），其中20个缺陷已获开发者确认或修复。

---

## [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)

### Abstract
arXiv:2506.00637v1 Announce Type: cross 
Abstract: Well-calibrated model confidence scores can improve the usefulness of text generation models. For example, users can be prompted to review predictions with low confidence scores, to prevent models from returning bad or potentially dangerous predictions. However, confidence metrics are not always well calibrated in text generation. One reason is that in generation, there can be many valid answers, which previous methods do not always account for. Hence, a confident model could distribute its output probability among multiple sequences because they are all valid. We propose task-agnostic confidence metrics suited to generation, which rely solely on the probabilities associated with the model outputs without the need for further fine-tuning or heuristics. Using these, we are able to improve the calibration of BART and Flan-T5 on summarization, translation, and QA datasets.

### 摘要
校准良好的模型置信度评分能提升文本生成模型的实用性。例如，系统可提示用户审阅低置信度预测结果，以避免模型返回错误或潜在危险的预测。然而，文本生成中的置信度指标往往存在校准不足的问题，部分原因在于生成任务可能存在多个有效答案，而现有方法未能充分考虑这一特性。当模型遇到多个有效序列时，即便其本身具有高置信度，也可能将输出概率分散到这些序列上。本研究提出适用于生成任务的通用置信度度量方法，该方法仅依赖模型输出概率分布，无需额外微调或启发式规则。实验表明，该方法能有效提升BART和Flan-T5在摘要生成、翻译和问答数据集上的校准效果。

---

## [SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning](https://arxiv.org/abs/2506.00676)

### Abstract
arXiv:2506.00676v1 Announce Type: cross 
Abstract: As large language models (LLMs) become ubiquitous, parameter-efficient fine-tuning methods and safety-first defenses have proliferated rapidly. However, the number of approaches and their recent increase have resulted in diverse evaluations-varied datasets, metrics, and inconsistent threat settings-making it difficult to fairly compare safety, utility, and robustness across methods. To address this, we introduce SafeTuneBed, a benchmark and toolkit unifying fine-tuning and defense evaluation. SafeTuneBed (i) curates a diverse repository of multiple fine-tuning datasets spanning sentiment analysis, question-answering, multi-step reasoning, and open-ended instruction tasks, and allows for the generation of harmful-variant splits; (ii) enables integration of state-of-the-art defenses, including alignment-stage immunization, in-training safeguards, and post-tuning repair; and (iii) provides evaluators for safety (attack success rate, refusal consistency) and utility. Built on Python-first, dataclass-driven configs and plugins, SafeTuneBed requires minimal additional code to specify any fine-tuning regime, defense method, and metric suite, while ensuring end-to-end reproducibility. We showcase its value by benchmarking representative defenses across varied poisoning scenarios and tasks. By standardizing data, code, and metrics, SafeTuneBed is the first focused toolkit of its kind to accelerate rigorous and comparable research in safe LLM fine-tuning. Code is available at: https://github.com/criticalml-uw/SafeTuneBed

### 摘要
随着大语言模型（LLMs）的普及，参数高效微调方法与安全优先防御技术迅速涌现。然而，方法数量的激增及其近期发展导致了评估标准的碎片化——数据集、度量指标及威胁场景的不一致，使得难以公平比较不同方法在安全性、实用性和鲁棒性上的表现。为此，我们推出SafeTuneBed基准测试与工具包，旨在统一微调与防御评估体系。该平台具有三大核心功能：（i）构建多领域微调数据集库，涵盖情感分析、问答系统、多步推理及开放式指令任务，并支持生成有害数据变体；（ii）集成前沿防御技术，包括对齐阶段免疫、训练中安全防护及微调后修复；（iii）提供安全性（攻击成功率、拒绝一致性）与实用性评估模块。基于Python优先、数据类驱动的配置与插件架构，SafeTuneBed仅需极简代码即可指定任意微调方案、防御方法与度量体系，同时保证端到端可复现性。我们通过在不同投毒场景与任务中测试代表性防御方案，验证了其价值。通过标准化数据、代码与评估指标，SafeTuneBed成为首个专注于推动大语言模型安全微调领域严谨可比研究的工具包。代码已开源：https://github.com/criticalml-uw/SafeTuneBed

---

## [CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning](https://arxiv.org/abs/2506.00750)

### Abstract
arXiv:2506.00750v1 Announce Type: cross 
Abstract: Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limit models' capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.

### 摘要
理解和推理代码语义对于提升代码大语言模型解决现实软件工程任务的能力至关重要。尽管现有多个代码推理基准测试，但多数依赖于合成数据集或教学编程问题，且聚焦于输入/输出预测等粗粒度推理任务，限制了其在实际软件工程场景中评估大语言模型的有效性。为填补这一空白，我们提出CodeSense——首个针对真实世界代码的软件工程问题提供细粒度代码推理任务的基准测试。我们从真实代码仓库收集Python、C和Java软件项目，执行测试用例并采集执行轨迹，构建了细粒度语义推理任务的基础真值数据集。随后对前沿大语言模型进行全面评估，结果表明模型在处理细粒度推理任务时存在显著性能差距。虽然思维链和上下文学习等提示技术有所助益，但大语言模型缺乏代码语义理解能力从根本上限制了其代码推理性能。除数据集、基准测试和评估外，本研究还开发了执行追踪框架与工具集，可便捷获取细粒度软件工程推理任务的基础真值，为未来基准构建与模型后训练奠定坚实基础。代码与数据详见https://codesense-bench.github.io/

---

## [Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments](https://arxiv.org/abs/2506.00694)

### Abstract
arXiv:2506.00694v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks like argument generation, yet their reliability remains a concern. Building upon pilot work assessing LLM generation of 3-ply legal arguments using human evaluation, this paper introduces an automated pipeline to evaluate LLM performance on this task, specifically focusing on faithfulness (absence of hallucination), factor utilization, and appropriate abstention. We define hallucination as the generation of factors not present in the input case materials and abstention as the model's ability to refrain from generating arguments when instructed and no factual basis exists. Our automated method employs an external LLM to extract factors from generated arguments and compares them against the ground-truth factors provided in the input case triples (current case and two precedent cases). We evaluated eight distinct LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply argument, 2) generating an argument with swapped precedent roles, and 3) recognizing the impossibility of argument generation due to lack of shared factors and abstaining. Our findings indicate that while current LLMs achieve high accuracy (over 90%) in avoiding hallucination on viable argument generation tests (Tests 1 &amp; 2), they often fail to utilize the full set of relevant factors present in the cases. Critically, on the abstention test (Test 3), most models failed to follow instructions to stop, instead generating spurious arguments despite the lack of common factors. This automated pipeline provides a scalable method for assessing these crucial LLM behaviors, highlighting the need for improvements in factor utilization and robust abstention capabilities before reliable deployment in legal settings. Project page: https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.

### 摘要
大型语言模型（LLMs）在生成法律论据等复杂法律任务中展现出潜力，但其可靠性仍存疑虑。基于前期通过人工评估测试LLMs生成三层次法律论据的试点研究，本文提出一种自动化流程来评估LLMs在此任务中的表现，特别关注忠实性（无幻觉）、要素利用和适度弃权。我们将幻觉定义为生成输入案件材料中不存在的要素，弃权则指模型在缺乏事实依据时能遵从指令停止生成论据的能力。我们的自动化方法采用外部LLM从生成论据中提取要素，并与输入案件三元组（当前案件和两个先例案件）提供的真实要素进行比对。我们在三个难度递增的测试中评估了八个不同的LLM：1）生成标准三层次论据；2）生成先例角色互换的论据；3）识别因缺乏共同要素而无法生成论据时选择弃权。研究结果表明，虽然现有LLMs在可行论据生成测试（测试1和2）中避免幻觉的准确率较高（超过90%），但往往未能充分利用案件中的所有相关要素。关键的是，在弃权测试（测试3）中，大多数模型未能遵循停止指令，反而在缺乏共同要素的情况下生成虚假论据。该自动化流程为评估这些关键LLM行为提供了可扩展的方法，揭示了在法律场景中可靠部署前需要改进要素利用和强化弃权能力的需求。项目页面：https://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention。

---

## [Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection](https://arxiv.org/abs/2506.00743)

### Abstract
arXiv:2506.00743v1 Announce Type: cross 
Abstract: Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in adapting Large Language Models (LLMs) for downstream tasks in Natural Language Processing. However, its adoption in privacy-preserving distributed learning frameworks, such as Federated Learning (FL), remains relatively limited. This is mainly due to challenges specific to FL, such as resource-constrained devices and diverse data distributions among clients. In this paper, we propose an efficient method to perform PEFT within the FL framework for Multi-Head Attention (MHA) based language models. We address the challenges through head pruning, a novel head-specific weighted aggregation mechanism, and a client selection strategy. Head pruning minimizes training complexity within the clients, guided by the importance score computed based on the confidence of the attention head. Weighted aggregation of heads ensures the global model captures crucial updates from diverse clients complementing our client selection strategy. We show results on the MultiNLI benchmark along with 20 Newsgroups, XL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model with LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting in a communication advantage of up to 1.8x and a reduction in training OPs of 3.9x while maintaining the accuracy drop under 2%.

### 摘要
参数高效微调（Parameter Efficient Fine-Tuning, PEFT）已成为自然语言处理中针对下游任务适配大语言模型（Large Language Models, LLMs）的主流方法。然而，其在隐私保护的分布式学习框架（如联邦学习Federated Learning, FL）中的应用仍相对有限，这主要源于FL特有的挑战，例如资源受限的客户端设备及客户端间数据分布的异构性。本文提出一种在基于多头注意力机制（Multi-Head Attention, MHA）的语言模型中实施联邦PEFT的高效方法，通过注意力头剪枝、新型的头部加权聚合机制以及客户端选择策略应对这些挑战。基于注意力头置信度计算的重要性分数指导剪枝操作，从而最小化客户端训练复杂度；头部加权聚合机制与客户端选择策略协同工作，确保全局模型能有效捕获来自不同客户端的关键更新。我们在MultiNLI基准测试及20 Newsgroups、XL-Sum和E2E NLG数据集上验证了方法效果。采用MultiNLI数据集和T5-small模型，以LoRA作为PEFT方法时，实现了高达90%的稀疏度，通信效率提升达1.8倍，训练操作量减少3.9倍，同时将准确率下降控制在2%以内。

---

## [LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning](https://arxiv.org/abs/2506.00772)

### Abstract
arXiv:2506.00772v1 Announce Type: cross 
Abstract: Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.

### 摘要
近期研究表明，对大型语言模型（LLM）进行少量高质量数据集的监督微调可显著提升其推理能力。然而，完全微调（Full FT）虽效果显著，但存在计算成本高、易过拟合及灾难性遗忘等问题，在数据有限时尤为突出。稀疏微调通过仅更新模型参数的子集，在效率与性能间实现了良好平衡，此前已取得显著成果。但在LLM时代，由于难以识别真正关键的推理参数，该方法发展相对滞后。本研究提出：经过低秩近似后幅值最大的权重是微调的关键参数（称为主权重）。值得注意的是，基于幅值的稀疏微调作为基线方法在LLM微调中表现欠佳，但经秩约减后效果显著提升。这些发现催生了我们的方法——低秩信息稀疏微调（LIFT）。LIFT仅训练过程中更新前5%的主权重，在算术推理等目标任务上持续优于完全微调，同时保持与主流参数高效微调方法相当的内存效率。相较于完全微调和LoRA，LIFT还能多保留20%的源领域知识。代码已开源：https://github.com/zihanghliu/LIFT。

---

## [COMPKE: Complex Question Answering under Knowledge Editing](https://arxiv.org/abs/2506.00829)

### Abstract
arXiv:2506.00829v1 Announce Type: cross 
Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language models, has gathered great attention. Current benchmarks primarily use multi-hop question answering to assess and analyze newly injected or updated knowledge. However, we argue that these benchmarks fail to effectively evaluate how well the updated models apply this knowledge in real-life scenarios, particularly when questions require complex reasoning, involving one-to-many relationships or multi-step logical intersections. To fill in this gap, we introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge Editing, which includes 11,924 complex questions that reflect real-life situations. We conduct an extensive evaluation of four knowledge editing methods on COMPKE, revealing that their effectiveness varies notably across different models. For instance, MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further investigate the underlying causes of these disparities from both methodological and model-specific perspectives. The datasets are available at https://github.com/kzjkzj666/CompKE.

---

## [Behavioral Augmentation of UML Class Diagrams: An Empirical Study of Large Language Models for Method Generation](https://arxiv.org/abs/2506.00788)

### Abstract
arXiv:2506.00788v1 Announce Type: cross 
Abstract: Automating the enrichment of UML class diagrams with behavioral methods from natural language use cases is a significant challenge. This study evaluates nine large language models (LLMs) in augmenting a methodless UML diagram (21 classes, 17 relationships) using 21 structured waste-management use cases. A total of 90 diagrams (3,373 methods) were assessed across six metrics: method quantity, signature richness (visibility, names, parameters, return types), annotation completeness (linking to use cases/actions), structural fidelity, syntactic correctness (PlantUML compilation), and naming convergence (across models). All LLMs produced valid PlantUML diagrams adhering to UML conventions. Some models excelled in method coverage and annotation accuracy, while others showed richer parameterization but weaker traceability. These results demonstrate that LLMs can generate well-structured methods with consistent naming, advancing automated behavioral modeling. However, inconsistencies in annotations and signatures highlight the need for improved prompt engineering and model selection. The rapid generation of these methods supports Agile practices by enabling faster design iterations. Despite their capabilities, human oversight is essential to ensure accuracy, appropriateness, and semantic alignment. This positions LLMs as collaborative partners in software design. All experimental artifacts (\texttt&#123;.puml&#125;, \texttt&#123;.png&#125;, \texttt&#123;.csv&#125;) are publicly available for reproducibility.

### 摘要
将自然语言用例中的行为方法自动补充至UML类图是一项重要挑战。本研究评估了九种大语言模型（LLM）在利用21个结构化废物管理用例对无方法UML图（含21个类、17种关系）进行增强时的表现。通过六项指标对生成的90张图表（含3,373个方法）进行评估：方法数量、签名丰富度（可见性、名称、参数、返回类型）、注解完整性（与用例/动作的关联）、结构保真度、语法正确性（PlantUML编译）及命名一致性（跨模型）。所有LLM生成的PlantUML图均符合UML规范。部分模型在方法覆盖率和注解准确性方面表现突出，而另一些模型则展现出更丰富的参数化特征但可追溯性较弱。结果表明LLM能生成命名一致、结构良好的方法，推动了自动化行为建模的发展。然而注解和签名的不一致性提示需要改进提示工程和模型选择。这些方法的快速生成支持敏捷开发实践，可加速设计迭代。尽管具备这些能力，仍需人工监督以确保准确性、适当性和语义对齐。这使LLM成为软件设计中的协作伙伴。所有实验材料（\texttt&#123;.puml&#125;、\texttt&#123;.png&#125;、\texttt&#123;.csv&#125;）均已公开以确保可复现性。

---

## [KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision](https://arxiv.org/abs/2506.00783)

### Abstract
arXiv:2506.00783v1 Announce Type: cross 
Abstract: Large language models (LLMs) have made remarkable strides in various natural language processing tasks, but their performance on complex reasoning problems remains hindered by a lack of explainability and trustworthiness. This issue, often manifesting as hallucinations or unattributable reasoning processes, limits their applicability in complex reasoning scenarios. To address this, we propose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain Explanation Supervision (KG-TRACES), a novel framework that enhances the reasoning ability of LLMs through explicit supervision over reasoning paths and processes. KG-TRACES jointly supervises the model to: (1) predict symbolic relation paths, (2) predict full triple-level reasoning paths, and (3) generate attribution-aware reasoning processes grounded in the reasoning paths. At inference phase, the model adapts to both KG-available and KG-unavailable scenarios, retrieving reasoning paths from a KG when possible or predicting plausible reasoning paths with only intrinsic knowledge when not. This design enables the model to reason in an explainable and source-attributable pattern. Through extensive experiments on complex reasoning tasks, we demonstrate that KG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6% and F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1% in F1 on CWQ. Moreover, we show its transferability to specialized domains such as medicine. By visualizing the intermediate steps of reasoning processes, we further show that the explicit supervision introduced by KG-TRACES leads to more stable and goal-directed reasoning processes, aligning closely with correct answers. Code is available at https://github.com/Edaizi/KG-TRACES.

### 摘要
大型语言模型（LLMs）在各类自然语言处理任务中取得了显著进展，但其在复杂推理问题上的表现仍因缺乏可解释性与可信度而受限。这一问题常表现为幻觉或不可溯源的推理过程，制约了模型在复杂推理场景中的应用。为此，我们提出知识图谱约束的轨迹推理归因与链式解释监督框架（KG-TRACES），该框架通过对推理路径与过程的显式监督来增强LLMs的推理能力。KG-TRACES联合监督模型实现：（1）预测符号化关系路径；（2）预测完整三元组级推理路径；（3）生成基于推理路径的可归因推理过程。在推理阶段，模型可自适应知识图谱可用与不可用场景：当知识图谱存在时检索推理路径，缺失时则仅依靠内部知识预测合理推理路径。该设计使模型能以可解释且来源可溯的方式执行推理。通过在复杂推理任务上的大量实验，我们证明KG-TRACES显著优于现有最优方法：在WebQSP上Hits@1提升1.6%、F1值提高4.7%，在CWQ上Hits@1提升4.8%、F1值提高2.1%。此外，我们还验证了其在医学等专业领域的可迁移性。通过可视化推理过程的中间步骤，我们进一步表明KG-TRACES引入的显式监督能产生更稳定且目标导向的推理过程，与正确答案高度吻合。代码详见https://github.com/Edaizi/KG-TRACES。

---

## [HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs](https://arxiv.org/abs/2506.00826)

### Abstract
arXiv:2506.00826v1 Announce Type: cross 
Abstract: Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs) by incorporating diverse modalities such as images and text. Multi-modal knowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals to infer missing facts, thereby mitigating the intrinsic incompleteness of MMKGs. Existing MMKGC methods typically leverage only the information contained in the MMKGs under the closed-world assumption and adopt discriminative training objectives, which limits their reasoning capacity during completion. Recent generative completion approaches powered by advanced large language models (LLMs) have shown strong reasoning abilities in unimodal knowledge graph completion, but their potential in MMKGC remains largely unexplored. To bridge this gap, we propose HERGC, a Heterogeneous Experts Representation and Generative Completion framework for MMKGs. HERGC first deploys a Heterogeneous Experts Representation Retriever that enriches and fuses multimodal information and retrieves a compact candidate set for each incomplete triple. It then uses a Generative LLM Predictor fine-tuned on minimal instruction data to accurately identify the correct answer from these candidates. Extensive experiments on three standard MMKG benchmarks demonstrate HERGC's effectiveness and robustness, achieving state-of-the-art performance.

### 摘要
多模态知识图谱（MMKGs）通过整合图像、文本等多样化模态，丰富了传统知识图谱（KGs）的表示。多模态知识图谱补全（MMKGC）旨在利用这些异构信号推断缺失事实，从而缓解MMKGs固有的不完整性。现有MMKGC方法通常在封闭世界假设下仅利用知识图谱内部信息，并采用判别式训练目标，这限制了其补全过程中的推理能力。近期基于先进大语言模型（LLMs）的生成式补全方法在单模态知识图谱补全中展现出强大推理能力，但其在MMKGC中的应用潜力尚未充分探索。为此，我们提出HERGC框架——一种面向多模态知识图谱的异构专家表示与生成式补全方法。该框架首先部署异构专家表示检索器，通过增强与融合多模态信息为每个不完整三元组检索紧凑候选集；随后利用基于少量指令数据微调的生成式LLM预测器，从候选集中准确识别正确答案。在三个标准MMKG基准上的大量实验验证了HERGC的有效性与鲁棒性，其性能达到当前最优水平。

---

## [Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](https://arxiv.org/abs/2506.00842)

### Abstract
arXiv:2506.00842v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.

### 摘要
大型语言模型（LLMs）在纯文本任务中表现优异，但在处理表格和数据库等结构化数据时性能欠佳。这一现象可能源于预训练阶段对结构化数据接触不足，以及僵化的文本到结构转换机制。与人类能够跨数据模态无缝应用学习模式不同，LLMs难以推断表格格式中隐含的关联关系，尤其在缺乏明确结构指导时更为明显。为弥合这种认知差距，我们提出基于经验的对比检索增强生成框架（CoRE），该框架通过构建经验记忆表征，并采用对比式上下文学习（ICL）来模拟人类知识迁移过程以增强泛化能力。在Text-to-SQL和TableQA任务上的实验表明，CoRE显著提升性能，平均增益分别达3.44%和4.24%，在挑战性任务中最高提升17.2%。通过蒙特卡洛树搜索（MCTS）生成的经验记忆将训练数据扩展8-9倍，增强了数据多样性和领域覆盖。这种无需训练且持续演进的方法推动LLMs向结构化知识专家迈进。

---

## [A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems](https://arxiv.org/abs/2506.00831)

### Abstract
arXiv:2506.00831v1 Announce Type: cross 
Abstract: Modern transportation systems rely on cyber-physical systems (CPS), where cyber systems interact seamlessly with physical systems like transportation-related sensors and actuators to enhance safety, mobility, and energy efficiency. However, growing automation and connectivity increase exposure to cyber vulnerabilities. Existing threat modeling frameworks for transportation CPS are often limited in scope, resource-intensive, and dependent on significant cybersecurity expertise. To address these gaps, we present TraCR-TMF (Transportation Cybersecurity and Resiliency Threat Modeling Framework), a large language model (LLM)-based framework that minimizes expert intervention. TraCR-TMF identifies threats, potential attack techniques, and corresponding countermeasures by leveraging the MITRE ATT&amp;CK matrix through three LLM-based approaches: (i) a retrieval-augmented generation (RAG) method requiring no expert input, (ii) an in-context learning approach requiring low expert input, and (iii) a supervised fine-tuning method requiring moderate expert input. TraCR-TMF also maps attack paths to critical assets by analyzing vulnerabilities using a customized LLM. The framework was evaluated in two scenarios. First, it identified relevant attack techniques across transportation CPS applications, with 90% precision as validated by experts. Second, using a fine-tuned LLM, it successfully predicted multiple exploitations including lateral movement, data exfiltration, and ransomware-related encryption that occurred during a major real-world cyberattack incident. These results demonstrate TraCR-TMF's effectiveness in CPS threat modeling, its reduced reliance on cybersecurity expertise, and its adaptability across CPS domains.

### 摘要
现代交通系统依赖于信息物理系统（CPS），其中信息系统与交通相关传感器、执行器等物理系统无缝交互，以提升安全性、机动性和能效。然而，自动化与互联程度的提高也加剧了网络漏洞暴露风险。现有交通CPS威胁建模框架普遍存在范围局限、资源密集且高度依赖网络安全专家知识的问题。为弥补这些不足，我们提出TraCR-TMF（交通网络安全与弹性威胁建模框架），这是一种基于大语言模型（LLM）的框架，可最大限度减少专家干预。该框架通过三种LLM方法（i）无需专家输入的检索增强生成（RAG）方法、（ii）需少量专家输入的上下文学习方法，以及（iii）需适度专家输入的监督微调方法，利用MITRE ATT&amp;CK矩阵识别威胁、潜在攻击技术及对应防御措施。TraCR-TMF还通过定制化LLM分析漏洞，将攻击路径映射至关键资产。框架在两种场景下接受评估：首先，在交通CPS应用中识别相关攻击技术，专家验证其精确度达90%；其次，使用微调后的LLM成功预测了实际重大网络攻击事件中发生的横向移动、数据外泄和勒索软件加密等多重攻击行为。这些结果证明了TraCR-TMF在CPS威胁建模中的有效性、对网络安全专业知识的低依赖性，以及跨CPS领域的适应能力。

---

## [CODEMENV: Benchmarking Large Language Models on Code Migration](https://arxiv.org/abs/2506.00894)

### Abstract
arXiv:2506.00894v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown remarkable capabilities across various software engineering tasks; however, their effectiveness in code migration, adapting code to run in different environments, remains insufficiently studied. In this work, we introduce CODEMENV: Code Migration Across Environment, a new benchmark specifically designed to assess LLMs' abilities in code migration scenarios. CODEMENV consists of 922 examples spanning 19 Python and Java packages, and covers three core tasks: (1) identifying functions incompatible with specific versions, (2) detecting changes in function definitions, and (3) adapting code to target environments. Experimental evaluation with seven LLMs on CODEMENV yields an average pass@1 rate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings include: (i) LLMs tend to be more proficient with newer function versions, which aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical inconsistencies by identifying function changes irrelevant to the intended migration environment. The datasets are available at https://github.com/xdshen-ai/Benchmark-of-Code-Migration.

### 摘要
大语言模型（LLMs）在各种软件工程任务中展现出卓越能力，但其在代码迁移（使代码适应不同运行环境）方面的有效性仍未得到充分研究。本研究提出CODEMENV：跨环境代码迁移基准，这是一个专门用于评估LLMs代码迁移能力的新基准。CODEMENV包含922个示例，涵盖19个Python和Java软件包，涉及三项核心任务：(1) 识别与特定版本不兼容的函数，(2) 检测函数定义变更，(3) 使代码适应目标环境。在CODEMENV上对7个LLMs的实验评估显示，其平均pass@1成功率为26.50%，其中GPT-4O以43.84%的得分表现最佳。关键发现包括：(i) LLMs对新版本函数更熟练，这有助于迁移遗留代码；(ii) LLMs有时会表现出逻辑不一致性，识别出与目标迁移环境无关的函数变更。数据集详见https://github.com/xdshen-ai/Benchmark-of-Code-Migration。

---

## [PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models](https://arxiv.org/abs/2506.00910)

### Abstract
arXiv:2506.00910v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs--i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.

### 摘要
知识蒸馏（KD）是一种广泛应用的框架，通过利用教师模型的知识来训练紧凑的任务专用模型。然而，其在主动学习（AL）中的应用仍待深入探索——AL旨在通过迭代样本选择最小化标注成本。这一研究空白源于KD通常假设可获得充足标注数据，而AL则运作于数据稀缺场景，其中任务专用教师模型往往不可用。本文提出ActiveKD框架，通过利用大规模视觉语言模型（VLM）的零样本和少样本能力，将AL与KD相结合。ActiveKD的关键在于VLM的结构化预测偏差——即其预测会在概率空间中形成聚类。我们将此结构视为教师模型的归纳偏置，它能捕捉对学生学习有益的可泛化输出模式。为利用此偏置，我们提出概率核心集（PCoreSet）选择策略，该策略最大化概率空间而非特征空间的覆盖范围。PCoreSet通过策略性地选择类别多样化的未标注样本，在有限标注预算下更高效地传递教师知识。在11个数据集上的评估表明，PCoreSet在ActiveKD框架中 consistently 优于现有选择方法，推动了AL与KD交叉领域的研究进展。

---

## [How do Transformer Embeddings Represent Compositions? A Functional Analysis](https://arxiv.org/abs/2506.00914)

### Abstract
arXiv:2506.00914v1 Announce Type: cross 
Abstract: Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality.

### 摘要
组合性是人类智能的关键特征，对于推理和泛化能力至关重要。尽管基于Transformer的模型已成为众多语言建模任务的事实标准，但学界对其如何表征复合词以及这些表征是否具有组合性仍知之甚少。本研究测试了Mistral、OpenAI Large和谷歌嵌入模型的组合性，并与BERT进行了对比。首先，我们通过六种不同的组合性模型（加法、乘法、膨胀、回归等）评估表征的组合性，发现岭回归虽为线性模型，却能最好地解释组合性。出乎意料的是，经典向量加法模型的表现几乎与任何其他模型相当。其次，我们验证了大多数嵌入模型具有高度组合性，而BERT则表现出较弱的组合性。我们通过完全透明的形容词-名词组合合成数据集对研究结果进行了验证和可视化。总体而言，本研究对组合性进行了全面深入的探究。

---

## [Affordance Benchmark for MLLMs](https://arxiv.org/abs/2506.00893)

### Abstract
arXiv:2506.00893v1 Announce Type: cross 
Abstract: Affordance theory posits that environments inherently offer action possibilities that shape perception and behavior. While Multimodal Large Language Models (MLLMs) excel in vision-language tasks, their ability to perceive affordance, which is crucial for intuitive and safe interactions, remains underexplored. To address this, we introduce A4Bench, a novel benchmark designed to evaluate the affordance perception abilities of MLLMs across two dimensions: 1) Constitutive Affordance&#125;, assessing understanding of inherent object properties through 1,282 question-answer pairs spanning nine sub-disciplines, and 2) Transformative Affordance, probing dynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or individual-specific affordance) with 718 challenging question-answer pairs. Evaluating 17 MLLMs (nine proprietary and eight open-source) against human performance, we find that proprietary models generally outperform open-source counterparts, but all exhibit limited capabilities, particularly in transformative affordance perception. Furthermore, even top-performing models, such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind human performance (best: 85.34%, worst: 81.25%). These findings highlight critical gaps in environmental understanding of MLLMs and provide a foundation for advancing AI systems toward more robust, context-aware interactions. The dataset is available in https://github.com/JunyingWang959/A4Bench/.

### 摘要
可供性理论认为，环境本身提供的行为可能性会塑造感知与行为。尽管多模态大语言模型（MLLMs）在视觉-语言任务中表现优异，但其对可供性的感知能力——这对实现直观安全的交互至关重要——仍未得到充分探索。为此，我们提出A4Bench这一新颖基准，从两个维度评估MLLMs的可供性感知能力：1）构成性可供性，通过涵盖九个子学科的1,282组问答对评估对物体固有属性的理解；2）转化性可供性，通过718组具有挑战性的问答对探究动态与情境细微差异（如误导性、时间依赖性、文化或个体特异性可供性）。通过对比17个MLLMs（9个专有模型和8个开源模型）与人类表现，我们发现专有模型总体优于开源模型，但所有模型均表现出局限性，尤其在转化性可供性感知方面。即使表现最佳的模型（如Gemini-2.0-Pro总体精确匹配准确率18.05%）也显著落后于人类表现（最佳85.34%，最差81.25%）。这些发现揭示了MLLMs在环境理解方面的关键缺陷，为推进AI系统实现更稳健、情境感知的交互奠定了基础。数据集详见https://github.com/JunyingWang959/A4Bench/。

---

## [ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models](https://arxiv.org/abs/2506.00880)

### Abstract
arXiv:2506.00880v1 Announce Type: cross 
Abstract: Molecular Relational Learning (MRL) aims to understand interactions between molecular pairs, playing a critical role in advancing biochemical research. With the recent development of large language models (LLMs), a growing number of studies have explored the integration of MRL with LLMs and achieved promising results. However, the increasing availability of diverse LLMs and molecular structure encoders has significantly expanded the model space, presenting major challenges for benchmarking. Currently, there is no LLM framework that supports both flexible molecular input formats and dynamic architectural switching. To address these challenges, reduce redundant coding, and ensure fair model comparison, we propose ModuLM, a framework designed to support flexible LLM-based model construction and diverse molecular representations. ModuLM provides a rich suite of modular components, including 8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation encoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing to its highly flexible model assembly mechanism, ModuLM enables the dynamic construction of over 50,000 distinct model configurations. In addition, we provide comprehensive results to demonstrate the effectiveness of ModuLM in supporting LLM-based MRL tasks.

### 摘要
分子关系学习（MRL）旨在理解分子对之间的相互作用，对推动生化研究具有关键作用。随着大语言模型（LLM）的最新发展，越来越多的研究探索将MRL与LLM结合，并取得了显著成果。然而，日益多样化的LLM和分子结构编码器的可用性显著扩大了模型空间，为基准测试带来了重大挑战。目前尚无支持灵活分子输入格式和动态架构切换的LLM框架。为应对这些挑战、减少冗余编码并确保模型公平比较，我们提出了ModuLM框架，该框架旨在支持基于LLM的灵活模型构建和多样化分子表征。ModuLM提供丰富的模块化组件套件，包括8种二维分子图编码器、11种三维分子构象编码器、7种交互层以及7种主流LLM主干网络。凭借其高度灵活的模型组装机制，ModuLM可动态构建超过50,000种不同模型配置。此外，我们提供了全面实验结果，以证明ModuLM在支持基于LLM的MRL任务方面的有效性。

---

## [anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding](https://arxiv.org/abs/2506.00942)

### Abstract
arXiv:2506.00942v1 Announce Type: cross 
Abstract: The advent of multimodal large language models (MLLMs) has sparked interest in their application to electrocardiogram (ECG) analysis. However, existing ECG-focused MLLMs primarily focus on report generation tasks, often limited to single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that supports a broader range of tasks and more flexible ECG inputs. However, existing ECG-QA datasets are often monotonous. To address this gap, we first constructed the anyECG dataset, which encompasses a wide variety of tasks, including report generation, abnormal waveform localization, and open-ended question answering. In addition to standard hospital ECGs, we introduced long-duration reduced-lead ECGs for home environments and multiple ECG comparison scenarios commonly encountered in clinical practice. Furthermore, we propose the anyECG-chat model, which supports dynamic-length ECG inputs and multiple ECG inputs. We trained the model using a three-stage curriculum training recipe with the anyECG dataset. A comprehensive evaluation was conducted, demonstrating that anyECG-chat is capable of supporting various practical application scenarios, including not only common report generation tasks but also abnormal waveform localization for long-duration reduced-lead ECGs in home environments and comprehensive comparative analysis of multiple ECGs.

### 摘要
多模态大语言模型(MLLM)的出现引发了其在心电图(ECG)分析领域应用的关注。然而现有专注于ECG的MLLM主要集中于报告生成任务，且通常仅支持单次12导联、短时程(10秒)ECG输入，未能充分发挥MLLM的潜力。为此，我们致力于开发一种支持更广泛任务和更灵活ECG输入的MLLM。但现有ECG问答数据集往往内容单一。为弥补这一空白，我们首先构建了anyECG数据集，涵盖报告生成、异常波形定位和开放式问答等多种任务。除标准医院ECG外，我们还引入了适用于家庭环境的长期减导联ECG以及临床常见的多ECG对比场景。此外，我们提出anyECG-chat模型，支持动态长度ECG输入和多ECG输入。采用三阶段课程训练方案，利用anyECG数据集对该模型进行训练。综合评估表明，anyECG-chat能够支持多种实际应用场景，不仅包括常见的报告生成任务，还可实现家庭环境下长期减导联ECG的异常波形定位，以及多ECG的综合对比分析。

---

## [Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models](https://arxiv.org/abs/2506.00943)

### Abstract
arXiv:2506.00943v1 Announce Type: cross 
Abstract: Smart contracts can implement and automate parts of legal contracts, but ensuring their legal compliance remains challenging. Existing approaches such as formal specification, verification, and model-based development require expertise in both legal and software development domains, as well as extensive manual effort. Given the recent advances of Large Language Models (LLMs) in code generation, we investigate their ability to generate legally compliant smart contracts directly from natural language legal contracts, addressing these challenges. We propose a novel suite of metrics to quantify legal compliance based on modeling both legal and smart contracts as processes and comparing their behaviors. We select four LLMs, generate 20 smart contracts based on five legal contracts, and analyze their legal compliance. We find that while all LLMs generate syntactically correct code, there is significant variance in their legal compliance with larger models generally showing higher levels of compliance. We also evaluate the proposed metrics against properties of software metrics, showing they provide fine-grained distinctions, enable nuanced comparisons, and are applicable across domains for code from any source, LLM or developer. Our results suggest that LLMs can assist in generating starter code for legally compliant smart contracts with strict reviews, and the proposed metrics provide a foundation for automated and self-improving development workflows.

### 摘要
智能合约能够实现法律合同的部分自动化执行，但确保其法律合规性仍具挑战性。现有方法如形式化规范、验证和基于模型的开发需要同时具备法律与软件开发领域的专业知识，并需耗费大量人工。鉴于大语言模型（LLMs）在代码生成领域的最新进展，我们研究其直接从自然语言法律合同生成合规智能合约的能力以应对这些挑战。我们提出一套新颖的量化合规性指标，通过将法律合同和智能合约建模为流程并比较其行为来实现。我们选取四种LLM模型，基于五份法律合同生成20份智能合约，并分析其法律合规性。研究发现：虽然所有LLM均能生成语法正确的代码，但其法律合规性存在显著差异，通常较大模型表现出更高合规水平。我们还评估了所提指标相对于软件度量属性的表现，证明其能提供细粒度区分、支持 nuanced 比较，并适用于跨领域任何来源（LLM或开发者）的代码。结果表明：经过严格审查，LLM可辅助生成合规智能合约的初始代码，所提出的指标为自动化且能自我改进的开发工作流程奠定了基础。

---

## [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)

### Abstract
arXiv:2506.00975v1 Announce Type: cross 
Abstract: Inspired by the impressive capabilities of GPT-4o, there is growing interest in enabling speech language models (SLMs) to engage in natural, fluid spoken interactions with humans. Recent advancements have led to the development of several SLMs that demonstrate promising results in this area. However, current approaches have yet to fully exploit dual-channel speech data, which inherently captures the structure and dynamics of human conversation. In this work, we systematically explore the use of dual-channel speech data in the context of modern large language models, and introduce a novel generative modeling paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent dual-channel spoken dialogue learning using decoder-only architectures for the first time. We evaluate our approach on standard benchmarks, and empirical results show that our proposed method, NTPP, significantly improves the conversational abilities of SLMs in terms of turn-taking prediction, response coherence, and naturalness. Moreover, compared to existing methods, NTPP achieves substantially lower inference latency, highlighting its practical efficiency for real-time applications.

### 摘要
受GPT-4o卓越能力的启发，学界对实现语音语言模型（SLMs）与人类进行自然流畅的口语交互的兴趣日益增长。近期进展已促成多个SLM的开发，在该领域展现出良好效果。然而，现有方法尚未充分利用双通道语音数据——这类数据天然蕴含人类对话的结构与动态特征。本研究系统探索了双通道语音数据在现代大语言模型中的应用，首次提出一种新颖的生成建模范式——下一词对预测（NTPP），通过纯解码器架构实现与说话者无关的双通道口语对话学习。我们在标准基准上评估了该方法，实证结果表明：所提出的NTPP在轮转预测、响应连贯性和自然度方面显著提升了SLM的对话能力。此外，与现有方法相比，NTPP实现了显著更低的推理延迟，凸显了其在实时应用中的实践效率优势。

---

## [Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison](https://arxiv.org/abs/2506.00924)

### Abstract
arXiv:2506.00924v1 Announce Type: cross 
Abstract: This paper introduces a dual-layer framework for network operator-side quality of experience (QoE) assessment that integrates both objective network modeling and subjective user perception extracted from live-streaming platforms. On the objective side, we develop a machine learning model trained on mean opinion scores (MOS) computed via the ITU-T P.1203 reference implementation, allowing accurate prediction of user-perceived video quality using only network parameters such as packet loss, delay, jitter, and throughput without reliance on video content or client-side instrumentation. On the subjective side, we present a semantic filtering and scoring pipeline that processes user comments from live streams to extract performance-related feedback. A large language model is used to assign scalar MOS scores to filtered comments in a deterministic and reproducible manner. To support scalable and interpretable analysis, we con- struct a labeled dataset of 47,894 live-stream comments, of which about 34,000 are identified as QoE-relevant through multi-layer semantic filtering. Each comment is enriched with simulated Internet Service Provider attribution and temporally aligned using synthetic timestamps in 5-min intervals. The resulting dataset enables operator-level aggregation and time-series analysis of user-perceived quality. A delta MOS metric is proposed to measure each Internet service provider's deviation from platform-wide sentiment, allowing detection of localized degradations even in the absence of direct network telemetry. A controlled outage simulation confirms the framework's effectiveness in identifying service disruptions through comment-based trends alone. The system provides each operator with its own subjective MOS and the global platform average per interval, enabling real-time interpretation of performance deviations and comparison with objective network-based QoE estimates.

### 摘要
本文提出了一种面向网络运营商端的体验质量(QoE)评估双层框架，该框架融合了客观网络建模与从直播平台提取的主观用户感知。在客观层面，我们开发了一个基于ITU-T P.1203参考实现计算的平均意见得分(MOS)训练的机器学习模型，仅需利用丢包、延迟、抖动和吞吐量等网络参数即可准确预测用户感知视频质量，无需依赖视频内容或客户端检测。在主观层面，我们设计了一套语义过滤与评分流程，通过处理直播用户评论来提取性能相关反馈。采用大语言模型以确定且可复现的方式为过滤后的评论分配标量MOS分数。为支持可扩展和可解释的分析，我们构建了包含47,894条直播评论的标注数据集，其中约34,000条通过多层语义过滤被识别为QoE相关评论。每条评论均附加模拟的互联网服务提供商归属信息，并采用5分钟间隔的合成时间戳进行时序对齐。最终数据集支持运营商层面的聚合和用户感知质量的时间序列分析。我们提出ΔMOS指标来衡量各互联网服务提供商与平台整体情感基准的偏离程度，从而在缺乏直接网络遥测数据时仍能检测局部质量劣化。受控中断模拟实验证实，该框架仅通过评论趋势即可有效识别服务中断。系统为每个运营商提供其主观MOS分数及各时段的全局平台平均值，支持实时解读性能偏差并与基于客观网络的QoE评估进行对比。

---

## [Probing Neural Topology of Large Language Models](https://arxiv.org/abs/2506.01042)

### Abstract
arXiv:2506.01042v1 Announce Type: cross 
Abstract: Probing large language models (LLMs) has yielded valuable insights into their internal mechanisms by linking neural representations to interpretable semantics. However, how neurons functionally co-activate with each other to give rise to emergent capabilities remains largely unknown, hindering a deeper understanding and safer development of LLMs. In this work, we introduce graph probing, a method for uncovering the functional connectivity topology of LLM neurons and relating it to language generation performance. By analyzing internal neural graphs across diverse LLM families and scales, we discover a universal predictability of next-token prediction performance using only neural topology. This predictability is robust even when retaining just 1% of neuron connections or probing models after only 8 pretraining steps, highlighting the sparsity and early emergence of topological patterns. Further graph matching analysis suggests that, despite significant distinctions in architectures, parameters, and training data, different LLMs develop intricate and consistent neural topological structures that may form the foundation for their language generation abilities. Codes and data for the graph probing toolbox are released at https://github.com/DavyMorgan/llm-graph-probing.

### 摘要
通过将神经表征与可解释语义相关联，对大语言模型（LLM）的探测已为其内部机制提供了宝贵洞见。然而，神经元如何通过功能共激活产生涌现能力仍 largely 未知，这阻碍了对大语言模型更深入的理解和更安全的开发。本研究提出图探测方法，用于揭示LLM神经元的功能连接拓扑结构，并将其与语言生成性能相关联。通过分析不同家族和规模的LLM内部神经图，我们发现仅凭神经拓扑结构即可普遍预测下一词元预测性能。这种预测性在仅保留1%神经元连接或模型仅经过8次预训练步后探测时仍然稳健，凸显了拓扑模式的稀疏性和早期涌现特性。进一步的图匹配分析表明，尽管在架构、参数和训练数据上存在显著差异，不同LLM会发展出复杂且一致的神经拓扑结构，这可能构成其语言生成能力的基础。图探测工具箱的代码和数据发布于https://github.com/DavyMorgan/llm-graph-probing。

---

## [Less is More: Local Intrinsic Dimensions of Contextual Language Models](https://arxiv.org/abs/2506.01034)

### Abstract
arXiv:2506.01034v1 Announce Type: cross 
Abstract: Understanding the internal mechanisms of large language models (LLMs) remains a challenging and complex endeavor. Even fundamental questions, such as how fine-tuning affects model behavior, often require extensive empirical evaluation. In this paper, we introduce a novel perspective based on the geometric properties of contextual latent embeddings to study the effects of training and fine-tuning. To that end, we measure the local dimensions of a contextual language model's latent space and analyze their shifts during training and fine-tuning. We show that the local dimensions provide insights into the model's training dynamics and generalization ability. Specifically, the mean of the local dimensions predicts when the model's training capabilities are exhausted, as exemplified in a dialogue state tracking task, overfitting, as demonstrated in an emotion recognition task, and grokking, as illustrated with an arithmetic task. Furthermore, our experiments suggest a practical heuristic: reductions in the mean local dimension tend to accompany and predict subsequent performance gains. Through this exploration, we aim to provide practitioners with a deeper understanding of the implications of fine-tuning on embedding spaces, facilitating informed decisions when configuring models for specific applications. The results of this work contribute to the ongoing discourse on the interpretability, adaptability, and generalizability of LLMs by bridging the gap between intrinsic model mechanisms and geometric properties in the respective embeddings.

### 摘要
理解大型语言模型（LLMs）的内部机制仍是一项具有挑战性的复杂任务。即使是基础性问题（例如微调如何影响模型行为）通常也需要大量实证评估。本文提出一种基于上下文潜在嵌入几何特性的新视角，用于研究训练与微调的影响。为此，我们测量了上下文语言模型潜在空间的局部维度，并分析其在训练和微调过程中的变化。研究表明，局部维度能揭示模型的训练动态与泛化能力：局部维度均值可预测模型训练能力何时耗尽（以对话状态跟踪任务为例）、何时出现过拟合（以情感识别任务为例）以及何时发生顿悟现象（以算术任务为例）。此外，实验表明一个实用启发式规律：局部维度均值的降低往往伴随并预示着后续性能提升。通过这项探索，我们旨在帮助实践者更深入地理解微调对嵌入空间的影响，从而为特定应用配置模型时做出明智决策。本研究通过建立内在模型机制与相应嵌入几何特性之间的关联，为LLMs的可解释性、适应性和泛化能力的持续讨论提供了新的见解。

---

## [Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs](https://arxiv.org/abs/2506.01064)

### Abstract
arXiv:2506.01064v1 Announce Type: cross 
Abstract: Recent advances in large vision-language models (LVLMs) have showcased their remarkable capabilities across a wide range of multimodal vision-language tasks. However, these models remain vulnerable to visual adversarial attacks, which can substantially compromise their performance. Despite their potential impact, the development of effective methods for purifying such adversarial examples has received relatively limited attention. In this paper, we introduce F3, a novel adversarial purification framework that employs a counterintuitive "fighting fire with fire" strategy: intentionally introducing simple perturbations to adversarial examples to mitigate their harmful effects. Specifically, F3 leverages cross-modal attentions derived from randomly perturbed adversary examples as reference targets. By injecting noise into these adversarial examples, F3 effectively refines their attention, resulting in cleaner and more reliable model outputs. Remarkably, this seemingly paradoxical approach of employing noise to counteract adversarial attacks yields impressive purification results. Furthermore, F3 offers several distinct advantages: it is training-free and straightforward to implement, and exhibits significant computational efficiency improvements compared to existing purification methods. These attributes render F3 particularly suitable for large-scale industrial applications where both robust performance and operational efficiency are critical priorities. The code will be made publicly available.

### 摘要
大型视觉语言模型（LVLM）的最新进展展现了其在多模态视觉语言任务中的卓越能力。然而，这些模型仍易受视觉对抗攻击的影响，导致性能显著下降。尽管潜在影响重大，针对此类对抗样本的有效净化方法研究却相对有限。本文提出F3——一种新颖的对抗净化框架，其采用"以毒攻毒"的反直觉策略：通过主动引入简单扰动来消除对抗样本的有害影响。具体而言，F3利用从随机扰动对抗样本中提取的跨模态注意力作为参考目标，通过向对抗样本注入噪声，有效优化其注意力机制，从而产生更清晰可靠的模型输出。值得注意的是，这种看似矛盾的"以噪制噪"方法展现出卓越的净化效果。此外，F3具有多项显著优势：无需训练、实现简单，且相比现有净化方法具有显著的计算效率提升。这些特性使F3特别适用于对鲁棒性能和运行效率均有严苛要求的大规模工业应用。相关代码将公开提供。

---

## [Taming LLMs by Scaling Learning Rates with Gradient Grouping](https://arxiv.org/abs/2506.01049)

### Abstract
arXiv:2506.01049v1 Announce Type: cross 
Abstract: Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.

### 摘要
训练大规模语言模型（LLM）因其庞大体量和异构架构而面临挑战。尽管AdamW等自适应优化器能够应对梯度变化问题，但在高效且精确的参数级学习率估计方面仍存在不足，导致训练不稳定、收敛速度慢以及与参数高效微调（PEFT）技术的兼容性较差。本研究提出梯度分组缩放法（SGG），该优化器封装通过动态分组和组别特异性缩放改进了自适应学习率估计机制。SGG首先将每层梯度统计量划分为若干聚类，随后应用聚类特异性缩放来校准每个参数的学习率，从而在保持精确参数级适应的同时实现集体组级约束。多样化（多模态）大语言模型基准测试表明，SGG能无缝集成现有优化器，在不同模型规模下均取得稳定性能提升和更快收敛速度。其在不同批量大小和学习率下的稳定性表现，使SGG成为大语言模型优化的强健选择方案。

---

## [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)

### Abstract
arXiv:2506.01062v1 Announce Type: cross 
Abstract: We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.

### 摘要
我们推出SealQA这一新型挑战基准，旨在评估增强搜索能力的语言模型在面对事实性提问时（当网络搜索产生矛盾、噪声或无帮助结果的情况）的表现。该基准包含三种变体：(1) Seal-0（核心版）和(2) Seal-Hard用于评估事实准确性与推理能力，其中Seal-0聚焦于聊天模型（如GPT-4.1）通常接近零准确率的超高难度问题；(3) LongSeal则扩展至"大海捞针"场景下的长上下文多文档推理测试。评估揭示了当前模型的关键缺陷：即使前沿大语言模型在所有SealQA变体中都表现欠佳。在Seal-0上，配备o3和o4-mini等工具的前沿代理模型在最佳推理状态下仅分别达到17.1%和6.3%的准确率。我们发现如DeepSeek-R1-671B和o3-mini等先进推理模型对噪声搜索结果极度敏感。值得注意的是，增加测试时计算资源并未在o3-mini、o4-mini和o3上带来可靠增益，性能往往早期就进入平台期甚至下降。此外，尽管新模型受"中间迷失"问题影响较小，但在LongSeal面临大量干扰文档时仍无法可靠识别相关材料。为促进后续研究，我们已在huggingface.co/datasets/vtllms/sealqa发布SealQA数据集。

---

## [Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements](https://arxiv.org/abs/2506.01089)

### Abstract
arXiv:2506.01089v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive performances in tasks related to coreference resolution. However, previous studies mostly assessed LLM performance on coreference resolution with nouns and third person pronouns. This study evaluates LLM performance on coreference resolution with indexical like I, you, here and tomorrow, which come with unique challenges due to their linguistic properties. We present the first study examining how LLMs interpret indexicals in English, releasing the English Indexical Dataset with 1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that LLMs exhibit an impressive performance with some indexicals (I), while struggling with others (you, here, tomorrow), and that syntactic cues (e.g. quotation) contribute to LLM performance with some indexicals, while they reduce performance with others. Code and data are available at: https://github.com/metehanoguzz/LLMs-Indexicals-English.

### 摘要
大语言模型（LLMs）在指代消解相关任务中展现出卓越性能。然而，先前研究主要评估LLMs在名词和第三人称代词指代消解上的表现。本研究评估LLMs对"我"、"你"、"这里"、"明天"等指示词的指代消解能力，这些词语因其语言学特性带来独特挑战。我们首次系统研究LLMs如何理解英语指示词，并发布包含1600道选择题的英语指示词数据集。评估对象包括GPT-4o、Claude 3.5 Sonnet、Gemini 1.5 Pro和DeepSeek V3等前沿模型。结果表明：LLMs对部分指示词（如"我"）表现优异，但对其他指示词（如"你"、"这里"、"明天"）存在困难；句法线索（如引号）能提升LLMs对某些指示词的处理能力，却会降低对其他指示词的表现。代码与数据详见：https://github.com/metehanoguzz/LLMs-Indexicals-English。

---

## [GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking](https://arxiv.org/abs/2506.01078)

### Abstract
arXiv:2506.01078v1 Announce Type: cross 
Abstract: Despite notable advancements in multimodal reasoning, leading Multimodal Large Language Models (MLLMs) still underperform on vision-centric multimodal reasoning tasks in general scenarios. This shortfall stems from their predominant reliance on logic- and knowledge-based slow thinking strategies, while effective for domains like math and science, fail to integrate visual information effectively during reasoning. Consequently, these models often fail to adequately ground visual cues, resulting in suboptimal performance in tasks that require multiple plausible visual interpretations and inferences. To address this, we present GThinker (General Thinker), a novel reasoning MLLM excelling in multimodal reasoning across general scenarios, mathematics, and science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that grounds inferences in visual cues and iteratively reinterprets these cues to resolve inconsistencies. Building on this pattern, we further propose a two-stage training pipeline, including pattern-guided cold start and incentive reinforcement learning, designed to enable multimodal reasoning capabilities across domains. Furthermore, to support the training, we construct GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths and 4K curated reinforcement learning samples, filling the data gap toward general multimodal reasoning. Extensive experiments demonstrate that GThinker achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark M$^3$CoT, surpassing the latest O4-mini model. It also shows an average improvement of 2.1% on general scenario multimodal reasoning benchmarks, while maintaining on-par performance in mathematical reasoning compared to counterpart advanced reasoning models. The code, model, and data will be released soon at https://github.com/jefferyZhan/GThinker.

### 摘要
尽管多模态推理领域取得了显著进展，但主流的多模态大语言模型（MLLMs）在通用场景下的视觉中心型多模态推理任务中仍表现欠佳。这一不足源于其过度依赖基于逻辑与知识的慢思考策略——虽然这些策略在数学和科学等领域有效，却无法在推理过程中有效整合视觉信息。因此，这些模型往往难以充分锚定视觉线索，导致在需要多重合理视觉解释与推理的任务中表现不佳。为此，我们提出GThinker（通用思考者），这是一种在通用场景、数学和科学领域均表现卓越的新型多模态推理模型。GThinker引入了"线索重思"机制，这种灵活的推理模式将推断锚定于视觉线索，并通过迭代重新解读这些线索来解决不一致性。基于该模式，我们进一步提出包含模式引导冷启动和激励强化学习的两阶段训练流程，旨在实现跨领域的多模态推理能力。此外，为支持训练，我们构建了包含7K条高质量迭代标注推理路径和4K条精选强化学习样本的GThinker-11K数据集，填补了通用多模态推理的数据空白。大量实验表明，GThinker在具有挑战性的综合多模态推理基准M$^3$CoT上达到81.5%的准确率，超越最新O4-mini模型。在通用场景多模态推理基准上平均提升2.1%，同时在数学推理方面保持与先进推理模型相当的性能。代码、模型和数据将于https://github.com/jefferyZhan/GThinker 发布。

---

## [Earley-Driven Dynamic Pruning for Efficient Structured Decoding](https://arxiv.org/abs/2506.01151)

### Abstract
arXiv:2506.01151v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring their outputs conform to strict structural or grammatical constraints remains challenging, which is critical in function calls and domain-specific language (DSL) generation. Constrained decoding with context-free grammar is a flexible approach to guarantee LLMs' adherence to a specific format by dynamically building a token logits mask. However, creating this mask requires checking the validity of all tokens in the LLM vocabulary at every decoding step, which often incurs significant overheads in existing constrained decoding engines. To address this challenge, we propose $\textbf&#123;ZapFormat&#125;$, a novel $\textbf&#123;dynamic pruning&#125;$ strategy based on the Earley algorithm that identifies and eliminates invalid or redundant Earley states in real-time, significantly reducing memory occupation of the Earley algorithm's states. This further enables us to use a state cache to speed up structured generations on a large number of queries. We implemented ZapFormat in a new constrained decoding engine called Formatron which also incorporates existing optimizations. Through comprehensive experiments on structured generation tasks, including JSON generation, JSON Schema validation, and semantic parsing, we demonstrate that Formatron not only $\textbf&#123;consistently maintains&#125;$ high-precision compliant outputs but also achieves $\textbf&#123;significant improvements&#125;$ in inference speed up to 2x compared to state-of-the-art implementations. More importantly, Formatron is generally applicable across various LLM architectures. We release Formatron as open source at https://github.com/Dan-wanna-M/formatron.

---

## [Reconsidering LLM Uncertainty Estimation Methods in the Wild](https://arxiv.org/abs/2506.01114)

### Abstract
arXiv:2506.01114v1 Announce Type: cross 
Abstract: Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifically, we assess (1) the sensitivity of UE methods to decision threshold selection, (2) their robustness to query transformations such as typos, adversarial prompts, and prior chat history, (3) their applicability to long-form generation, and (4) strategies for handling multiple UE scores for a single query. Our evaluations on 19 UE methods reveal that most of them are highly sensitive to threshold selection when there is a distribution shift in the calibration dataset. While these methods generally exhibit robustness against previous chat history and typos, they are significantly vulnerable to adversarial prompts. Additionally, while existing UE methods can be adapted for long-form generation through various strategies, there remains considerable room for improvement. Lastly, ensembling multiple UE scores at test time provides a notable performance boost, which highlights its potential as a practical improvement strategy. Code is available at: https://github.com/duygunuryldz/uncertainty_in_the_wild.

### 摘要
近年来，大语言模型（LLM）不确定性估计（UE）方法已成为检测幻觉的关键工具。尽管已提出多种UE方法，但现有研究大多采用AUROC或PRR等与阈值无关的指标，在孤立的简短问答场景中对其进行评估。然而，UE方法在实际部署中面临诸多挑战。本研究系统性地考察了UE方法在实用场景中的四个关键方面：具体而言，我们评估了（1）UE方法对决策阈值选择的敏感性，（2）其对查询变换（如拼写错误、对抗性提示和先前的聊天历史）的鲁棒性，（3）其在长文本生成中的适用性，以及（4）处理单个查询多个UE分数的策略。通过对19种UE方法的评估发现，当校准数据集存在分布偏移时，大多数方法对阈值选择高度敏感。这些方法通常对先前聊天历史和拼写错误表现出鲁棒性，但极易受到对抗性提示的影响。此外，现有UE方法虽可通过多种策略适配长文本生成，但仍存在较大改进空间。最后，测试时集成多个UE分数能显著提升性能，这凸显了其作为实用改进策略的潜力。代码见：https://github.com/duygunuryldz/uncertainty_in_the_wild。

---

## [FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion](https://arxiv.org/abs/2506.01111)

### Abstract
arXiv:2506.01111v1 Announce Type: cross 
Abstract: High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.

### 摘要
高质量、大规模的音频描述对于推进音频理解至关重要，然而当前自动化方法生成的描述往往缺乏细粒度细节和上下文准确性，这主要源于其对有限单模态或浅层多模态信息的依赖。受人类听觉感知的启发（其能巧妙整合跨模态线索并执行复杂的听觉场景分析），我们提出了一种新颖的两阶段自动化流程。该流程首先采用专用预训练模型提取多样化上下文线索（如语音、音乐、环境声及关联视频的视觉信息），随后通过大语言模型（LLM）综合这些丰富的多模态输入以生成细致且上下文感知的音频描述。本工作的核心贡献包括：（1）提出的可扩展细粒度音频描述生成方法；（2）FusionAudio——一个包含120万条此类详细描述与600万问答对的新大规模数据集；（3）基于FusionAudio开发的增强音频模型（特别是具有优异音频-文本对齐和指令跟随能力的CLAP音频编码器）。本研究为更精细、准确地自动化理解复杂音频环境奠定了基础。代码与数据详见https://github.com/satsuki2486441738/FusionAudio。

---

## [Doubly Robust Alignment for Large Language Models](https://arxiv.org/abs/2506.01183)

### Abstract
arXiv:2506.01183v1 Announce Type: cross 
Abstract: This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at https://github.com/DRPO4LLM/DRPO4LLM

### 摘要
本文研究基于人类反馈的强化学习（RLHF）在将大语言模型与人类偏好对齐中的应用。尽管RLHF已展现出良好的效果，但许多算法对底层偏好模型（如Bradley-Terry模型）、参考策略或奖励函数的误设高度敏感，从而导致不良的微调结果。为解决模型误设问题，我们提出了一种双重稳健的偏好优化算法，该算法在偏好模型或参考策略任一被正确设定时（无需同时满足）仍能保持一致性。理论分析和实验结果表明，我们的方法相较于现有最优算法具有更优越且更稳健的性能。代码已开源：https://github.com/DRPO4LLM/DRPO4LLM

---

## [Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures](https://arxiv.org/abs/2506.01197)

### Abstract
arXiv:2506.01197v1 Announce Type: cross 
Abstract: Sparse dictionary learning (and, in particular, sparse autoencoders) attempts to learn a set of human-understandable concepts that can explain variation on an abstract space. A basic limitation of this approach is that it neither exploits nor represents the semantic relationships between the learned concepts. In this paper, we introduce a modified SAE architecture that explicitly models a semantic hierarchy of concepts. Application of this architecture to the internal representations of large language models shows both that semantic hierarchy can be learned, and that doing so improves both reconstruction and interpretability. Additionally, the architecture leads to significant improvements in computational efficiency.

### 摘要
稀疏字典学习（特别是稀疏自编码器）旨在学习一组人类可理解的概念，用以解释抽象空间中的变化。该方法的一个基本局限是既未利用也未表示所学概念间的语义关系。本文提出一种改进的稀疏自编码器架构，该架构显式建模概念的语义层次结构。将该架构应用于大型语言模型的内部表征时表明：语义层次结构不仅能够被学习，而且能同时提升重构效果与可解释性。此外，该架构还显著提高了计算效率。

---

## [From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models](https://arxiv.org/abs/2506.01133)

### Abstract
arXiv:2506.01133v1 Announce Type: cross 
Abstract: The emergence of large language models (LLMs) has demonstrated that systems trained solely on text can acquire extensive world knowledge, develop reasoning capabilities, and internalize abstract semantic concepts--showcasing properties that can be associated with general intelligence. This raises an intriguing question: Do such concepts emerge in models trained on other modalities, such as speech? Furthermore, when models are trained jointly on multiple modalities: Do they develop a richer, more structured semantic understanding? To explore this, we analyze the conceptual structures learned by speech and textual models both individually and jointly. We employ Latent Concept Analysis, an unsupervised method for uncovering and interpreting latent representations in neural networks, to examine how semantic abstractions form across modalities. For reproducibility we made scripts and other resources available to the community.

### 摘要
大型语言模型（LLMs）的出现表明，仅通过文本训练的系统能够获取广泛的世界知识、发展推理能力并内化抽象语义概念——这些特性可与通用智能相关联。这引发了一个有趣的问题：此类概念是否会在其他模态（如语音）训练的模型中出现？此外，当模型在多模态联合训练时：它们是否会形成更丰富、更具结构化的语义理解？为探究这一问题，我们分析了语音与文本模型单独及联合训练时所学习的概念结构。我们采用潜在概念分析（一种揭示和解释神经网络潜在表征的无监督方法）来考察跨模态语义抽象的形成机制。为确保可复现性，我们已向学术界公开相关脚本及资源。

---

## [FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA](https://arxiv.org/abs/2506.01194)

### Abstract
arXiv:2506.01194v1 Announce Type: cross 
Abstract: LoRA has emerged as one of the most promising fine-tuning techniques, especially for federated learning (FL), since it significantly reduces communication and computation costs at resource-constrained clients. However, data heterogeneity remains a significant challenge for LoRA-based FL, and the conventional aggregation strategy based on FedAvg suffers from slow convergence and suboptimal accuracy. Motivated by recent advances in model merging, particularly Task Arithmetic, we explore the idea of aggregating client LoRA parameters using scaled averaging. We first observe that a naive application of Task Arithmetic is ineffective due to the high cosine similarity between client updates, indicating significant common knowledge in the updates across clients. To address this issue, we propose decomposing client LoRA updates via Robust Principal Component Analysis (Robust-PCA) into a common low-rank component and client-specific sparse components. Our proposed algorithm FedRPCA aggregates the low-rank components through averaging, consolidating common knowledge, and applies scaled averaging to the sparse components to amplify client-specific knowledge. We evaluate our approach across a variety of vision and language tasks and demonstrate that it achieves higher final accuracy and faster convergence compared to competing baselines.

### 摘要
LoRA已成为最具前景的微调技术之一，尤其在联邦学习（FL）领域，因其能显著降低资源受限客户端的通信与计算成本。然而数据异构性仍是基于LoRA的联邦学习面临的主要挑战，而基于FedAvg的传统聚合策略存在收敛速度慢和精度欠佳的问题。受模型融合领域最新进展（特别是任务算术方法）的启发，我们探索采用缩放平均法聚合客户端LoRA参数的思路。首先发现直接应用任务算术效果不佳，因客户端更新间的高余弦相似性表明各客户端更新中存在大量共性知识。为解决该问题，我们提出通过鲁棒主成分分析（Robust-PCA）将客户端LoRA更新分解为公共低秩成分和客户端特定的稀疏成分。所提出的FedRPCA算法对低秩成分进行均值聚合以整合共性知识，同时对稀疏成分实施缩放平均以增强客户端特定知识。通过在多种视觉与语言任务上的实验验证，本方法相比基线方案能获得更高的最终精度和更快的收敛速度。

---

## [OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation](https://arxiv.org/abs/2506.01196)

### Abstract
arXiv:2506.01196v1 Announce Type: cross 
Abstract: We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and multi-view RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA projects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/

### 摘要
我们提出OG-VLA——一种新型架构与学习框架，通过融合视觉语言动作模型（VLA）的泛化能力与3D感知策略的鲁棒性，解决了将自然语言指令和多视角RGBD观测映射至准静态机器人动作的挑战。3D感知机器人策略在精确操作任务中表现卓越，但在处理未见指令、场景和物体时泛化能力不足；而VLA模型虽擅长跨指令和场景的泛化，却对相机与机器人位姿变化较为敏感。我们利用语言视觉基础模型中嵌入的先验知识，提升3D感知关键帧策略的泛化能力。OG-VLA将多视角输入观测投影为点云，并通过标准正交视角渲染，确保输入视角不变性及输入输出空间的一致性。这些标准视图经由视觉主干网络、大语言模型（LLM）和图像扩散模型处理，生成编码末端执行器在输入场景中下一位置与朝向的图像。在Arnold和Colosseum基准测试中，本方法在未知环境中实现了超过40%的相对性能提升，同时保持已知场景的稳健表现，展现了当前最优的泛化能力。实验还表明仅需3至5次演示即可实现真实场景适配，并具备强大泛化性能。视频与资源详见https://og-vla.github.io/。

---

## [Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean](https://arxiv.org/abs/2506.01237)

### Abstract
arXiv:2506.01237v1 Announce Type: cross 
Abstract: We introduce the $\underline&#123;Ko&#125;rean \underline&#123;G&#125;rammar \underline&#123;E&#125;valuation Bench\underline&#123;M&#125;ark (KoGEM)$, designed to assess the linguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k multiple-choice QA pairs covering five main categories and 16 subcategories. The zero-shot evaluation of 27 LLMs of various sizes and types reveals that while LLMs perform remarkably well on straightforward tasks requiring primarily definitional knowledge, they struggle with tasks that demand the integration of real-world experiential knowledge, such as phonological rules and pronunciation. Furthermore, our in-depth analysis suggests that incorporating such experiential knowledge could enhance the linguistic competence of LLMs. With KoGEM, we not only highlight the limitations of current LLMs in linguistic competence but also uncover hidden facets of LLMs in linguistic competence, paving the way for enhancing comprehensive language understanding. Our code and dataset are available at: https://github.com/SungHo3268/KoGEM.

### 摘要
我们推出《韩国语语法评估基准》（KoGEM），旨在评估大语言模型及人类在韩语领域的语言能力。该基准包含1,500道多项选择题对，涵盖5个主类别和16个子类别。通过对27种不同规模和类型的大语言模型进行零样本评估发现：虽然模型在仅需定义性知识的简单任务上表现优异，但在需要整合现实世界经验知识（如音系规则与发音）的任务中表现欠佳。进一步深度分析表明，融入此类经验知识可提升大语言模型的语言能力。KoGEM不仅揭示了当前大语言模型在语言能力上的局限性，还发掘了其在语言理解方面的潜在特质，为增强综合语言理解能力开辟了新路径。相关代码与数据集已发布于：https://github.com/SungHo3268/KoGEM。

---

## [Mamba Drafters for Speculative Decoding](https://arxiv.org/abs/2506.01206)

### Abstract
arXiv:2506.01206v1 Announce Type: cross 
Abstract: Speculative decoding has emerged as a promising approach to accelerating large language model (LLM) generation using a fast drafter while maintaining alignment with the target model's distribution. However, existing approaches face a trade-off: external drafters offer flexibility but can suffer from slower drafting, while self-speculation methods use drafters tailored to the target model but require re-training. In this paper, we introduce novel drafters based on Mamba, a state-of-the-art state space model (SSM), as a solution that combines the best aspects of both approaches. By leveraging the linear structure of SSMs, our approach avoids the quadratic complexity inherent in traditional Transformer-based methods, enabling faster drafting and lower memory usage while maintaining the flexibility to work across different target models. We further enhance efficiency with a novel test-time tree search algorithm for generating high-quality draft candidates. Our empirical evaluation demonstrates that Mamba-based drafters not only outperform existing external drafting methods but are also comparable to state-of-the-art self-speculation approaches while using less memory and maintaining their cross-model adaptability.

### 摘要
推测解码已成为一种利用快速草稿模型加速大语言模型（LLM）生成并保持与目标模型分布对齐的有效方法。然而，现有方法面临两难选择：外部草稿模型具有灵活性但生成速度较慢，而自推测方法虽使用与目标模型适配的草稿模型却需重新训练。本文提出基于Mamba（一种先进的状态空间模型SSM）的新型草稿模型，该方案融合了两种方法的优势。通过利用SSM的线性结构，我们的方法避免了传统基于Transformer方法的二次复杂度，在保持跨模型灵活性的同时实现了更快草拟速度和更低内存占用。我们还提出一种新颖的测试时树搜索算法来生成高质量候选草拟序列，从而进一步提升效率。实验评估表明，基于Mamba的草稿模型不仅优于现有外部草拟方法，其性能更可与最先进的自推测方法相媲美，同时具备更低内存消耗和跨模型适应能力。

---

## [Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model](https://arxiv.org/abs/2506.01266)

### Abstract
arXiv:2506.01266v1 Announce Type: cross 
Abstract: Existing approaches for Large language model (LLM) detoxification generally rely on training on large-scale non-toxic or human-annotated preference data, designing prompts to instruct the LLM to generate safe content, or modifying the model parameters to remove toxic information, which are computationally expensive, lack robustness, and often compromise LLMs' fluency and contextual understanding. In this paper, we propose a simple yet effective approach for LLM detoxification, which leverages a compact, pre-trained calibration model that guides the detoxification process of a target LLM via a lightweight intervention in its generation pipeline. By learning a detoxified embedding space from non-toxic data, the calibration model effectively steers the LLM away from generating harmful content. This approach only requires a one-time training of the calibration model that is able to be seamlessly applied to multiple LLMs without compromising fluency or contextual understanding. Experiment results on the benchmark dataset demonstrate that our approach reduces toxicity while maintaining reasonable content expression.

### 摘要
现有的大型语言模型（LLM）去毒方法通常依赖于大规模无毒或人工标注的偏好数据训练、设计提示语引导LLM生成安全内容，或是通过修改模型参数来消除有害信息。这些方法计算成本高昂、缺乏鲁棒性，且往往损害模型的流畅性和上下文理解能力。本文提出一种简单而有效的LLM去毒方法，该方法利用一个紧凑的预训练校准模型，通过轻量级干预目标LLM的生成流程来引导其去毒过程。校准模型通过从无毒数据中学习去毒嵌入空间，有效引导LLM避免生成有害内容。该方法仅需一次性训练校准模型，即可无缝应用于多个LLM，且不会影响生成内容的流畅性或上下文理解。基准数据集上的实验结果表明，我们的方法在降低毒性的同时保持了合理的内容表达能力。

---

## [ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding](https://arxiv.org/abs/2506.01274)

### Abstract
arXiv:2506.01274v1 Announce Type: cross 
Abstract: Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility.

### 摘要
尽管大型多模态模型（LMMs）的最新进展已实现了高效的视觉语言推理，但视频内容理解能力仍受限于次优的帧选择策略。现有方法通常依赖静态启发式规则或外部检索模块向视频大语言模型输入帧信息，这可能无法提供与查询相关的有效内容。本研究提出ReFoCUS（基于强化的上下文理解帧优化框架），这是一种新颖的帧级策略优化框架，其将优化目标从文本响应转向视觉输入选择。ReFoCUS通过强化学习训练帧选择策略，利用源自参考LMM的奖励信号来反映模型对支持时序定位响应的最优帧的内在偏好。为高效探索大规模组合帧空间，我们采用自回归条件选择架构，在降低复杂度的同时确保时序连贯性。该方法无需帧级显式监督，并在多个视频问答基准测试中持续提升推理性能，凸显了将帧选择与模型内部效用对齐的优势。

---

## [Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation](https://arxiv.org/abs/2506.01293)

### Abstract
arXiv:2506.01293v1 Announce Type: cross 
Abstract: Multi-modal large language models (MLLMs) incorporate heterogeneous modalities into LLMs, enabling a comprehensive understanding of diverse scenarios and objects. Despite the proliferation of evaluation benchmarks and leaderboards for MLLMs, they predominantly overlook the critical capacity of MLLMs to comprehend world knowledge with structured abstractions that appear in visual form. To address this gap, we propose a novel evaluation paradigm and devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for STRuctured understanding. This benchmark leverages multi-modal knowledge graphs to synthesize images encapsulating subgraph architectures enriched with multi-modal entities. M3STR necessitates that MLLMs not only recognize the multi-modal entities within the visual inputs but also decipher intricate relational topologies among them. We delineate the benchmark's statistical profiles and automated construction pipeline, accompanied by an extensive empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent deficiencies in processing abstractive visual information with structured knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic reasoning capacities. Our code and data are released at https://github.com/zjukg/M3STR

### 摘要
多模态大语言模型（MLLMs）通过将异构模态整合至大语言模型中，实现了对多样化场景与对象的综合理解。尽管针对MLLMs的评估基准和排行榜不断涌现，但它们普遍忽视了MLLMs以视觉形式呈现结构化抽象的世界知识理解这一核心能力。为填补这一空白，我们提出了一种新颖的评估范式，并构建了基于多模态地图结构化理解（M3STR）的创新基准。该基准利用多模态知识图谱合成包含富含多模态实体的子图架构图像，要求MLLMs不仅能识别视觉输入中的多模态实体，还需解析其间复杂的关联拓扑结构。我们详细阐述了该基准的统计特征与自动化构建流程，并对26个前沿MLLMs展开了全面实证分析。研究结果表明，现有模型在处理蕴含结构化知识的抽象视觉信息时仍存在显著缺陷，从而为提升MLLMs整体推理能力指明了关键发展方向。相关代码与数据已发布于https://github.com/zjukg/M3STR。

---

## [Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models](https://arxiv.org/abs/2506.01307)

### Abstract
arXiv:2506.01307v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have evolved into Multimodal Large Language Models (MLLMs), significantly enhancing their capabilities by integrating visual information and other types, thus aligning more closely with the nature of human intelligence, which processes a variety of data forms beyond just text. Despite advancements, the undesirable generation of these models remains a critical concern, particularly due to vulnerabilities exposed by text-based jailbreak attacks, which have represented a significant threat by challenging existing safety protocols. Motivated by the unique security risks posed by the integration of new and old modalities for MLLMs, we propose a unified multimodal universal jailbreak attack framework that leverages iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image. Our work not only highlights the interaction of image-text modalities can be used as a critical vulnerability but also validates that multimodal universal jailbreak attacks can bring higher-quality undesirable generations across different MLLMs. We evaluate the undesirable context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and InstructBLIP, and reveal significant multimodal safety alignment issues, highlighting the inadequacy of current safety mechanisms against sophisticated multimodal attacks. This study underscores the urgent need for robust safety measures in MLLMs, advocating for a comprehensive review and enhancement of security protocols to mitigate potential risks associated with multimodal capabilities.

### 摘要
大型语言模型（LLMs）已发展为多模态大型语言模型（MLLMs），通过整合视觉信息与其他数据类型显著提升了能力，从而更贴近人类智能处理多种数据形式的本质。尽管取得进展，这些模型的不良生成仍是一个关键问题，尤其是基于文本的越狱攻击暴露的漏洞对现有安全协议构成重大威胁。针对新旧模态融合给MLLMs带来的独特安全风险，我们提出统一的多模态通用越狱攻击框架，利用迭代式图文交互和迁移策略生成通用对抗后缀与图像。本研究不仅揭示图文模态交互可作为关键漏洞，还验证了多模态通用越狱攻击能在不同MLLMs上产生更高质量的不良生成。我们对LLaVA、Yi-VL、MiniGPT4、MiniGPT-v2及InstructBLIP等模型的不良语境生成进行评估，发现显著的多模态安全对齐缺陷，表明当前防护机制难以应对复杂的多模态攻击。该研究强调亟需加强MLLMs的安全防护，建议全面审查并升级安全协议以降低多模态功能带来的潜在风险。

---

## [DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models](https://arxiv.org/abs/2506.01257)

### Abstract
arXiv:2506.01257v1 Announce Type: cross 
Abstract: DeepSeek-R1 is a cutting-edge open-source large language model (LLM) developed by DeepSeek, showcasing advanced reasoning capabilities through a hybrid architecture that integrates mixture of experts (MoE), chain of thought (CoT) reasoning, and reinforcement learning. Released under the permissive MIT license, DeepSeek-R1 offers a transparent and cost-effective alternative to proprietary models like GPT-4o and Claude-3 Opus; it excels in structured problem-solving domains such as mathematics, healthcare diagnostics, code generation, and pharmaceutical research. The model demonstrates competitive performance on benchmarks like the United States Medical Licensing Examination (USMLE) and American Invitational Mathematics Examination (AIME), with strong results in pediatric and ophthalmologic clinical decision support tasks. Its architecture enables efficient inference while preserving reasoning depth, making it suitable for deployment in resource-constrained settings. However, DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation, adversarial manipulation, and safety failures - especially in multilingual and ethically sensitive contexts. This survey highlights the model's strengths, including interpretability, scalability, and adaptability, alongside its limitations in general language fluency and safety alignment. Future research priorities include improving bias mitigation, natural language comprehension, domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1 represents a major advance in open, scalable AI, underscoring the need for collaborative governance to ensure responsible and equitable deployment.

### 摘要
DeepSeek-R1是深度求索公司开发的前沿开源大语言模型（LLM），通过融合混合专家系统（MoE）、思维链推理（CoT）和强化学习的混合架构，展现出先进的推理能力。该模型采用宽松的MIT许可协议发布，为GPT-4o和Claude-3 Opus等专有模型提供了透明且高性价比的替代方案，在数学、医疗诊断、代码生成和药物研发等结构化问题解决领域表现卓越。在美国医师执照考试（USMLE）和美国数学邀请赛（AIME）等基准测试中展现出竞争力，尤其在儿科和眼科临床决策支持任务中表现突出。其架构在保持推理深度的同时实现了高效推断，适合在资源受限环境中部署。但该模型也表现出对偏见、错误信息、对抗性操纵和安全漏洞的更高敏感性——特别是在多语言和伦理敏感场景下。本综述阐明了模型在可解释性、可扩展性和适应性方面的优势，同时指出其在通用语言流畅度和安全对齐方面的局限。未来研究重点包括改进偏见缓解、自然语言理解、领域特异性验证和法规遵从性。总体而言，DeepSeek-R1标志着开放可扩展人工智能的重大进展，凸显了通过协同治理实现负责任、公平部署的必要性。

---

## [MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine](https://arxiv.org/abs/2506.01252)

### Abstract
arXiv:2506.01252v1 Announce Type: cross 
Abstract: Traditional Chinese Medicine (TCM) is a holistic medical system with millennia of accumulated clinical experience, playing a vital role in global healthcare-particularly across East Asia. However, the implicit reasoning, diverse textual forms, and lack of standardization in TCM pose major challenges for computational modeling and evaluation. Large Language Models (LLMs) have demonstrated remarkable potential in processing natural language across diverse domains, including general medicine. Yet, their systematic evaluation in the TCM domain remains underdeveloped. Existing benchmarks either focus narrowly on factual question answering or lack domain-specific tasks and clinical realism. To fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs on TCM Knowledge, Reasoning, and Safety. Developed in collaboration with certified TCM experts, MTCMB comprises 12 sub-datasets spanning five major categories: knowledge QA, language understanding, diagnostic reasoning, prescription generation, and safety evaluation. The benchmark integrates real-world case records, national licensing exams, and classical texts, providing an authentic and comprehensive testbed for TCM-capable models. Preliminary results indicate that current LLMs perform well on foundational knowledge but fall short in clinical reasoning, prescription planning, and safety compliance. These findings highlight the urgent need for domain-aligned benchmarks like MTCMB to guide the development of more competent and trustworthy medical AI systems. All datasets, code, and evaluation tools are publicly available at: https://github.com/Wayyuanyuan/MTCMB.

### 摘要
中医（TCM）是一个拥有数千年临床经验积累的整体医学体系，在全球医疗保健尤其是东亚地区发挥着至关重要的作用。然而，中医隐含的推理机制、多样化的文本形式以及缺乏标准化的问题，为计算建模与评估带来了重大挑战。大语言模型（LLMs）在处理跨领域自然语言（包括通用医学）方面已展现出显著潜力，但其在中医领域的系统性评估仍处于欠发达状态。现有基准测试要么局限于事实性问答，要么缺乏针对特定领域的任务和临床真实性。为填补这一空白，我们推出了MTCMB——一个用于评估大语言模型在中医知识、推理与安全性方面的多任务基准。该基准由认证中医专家参与开发，包含12个子数据集，涵盖五大类别：知识问答、语言理解、诊断推理、处方生成和安全性评估。基准整合了真实世界病例记录、国家执业资格考试和经典文献，为具备中医能力的模型提供了真实且全面的测试平台。初步结果表明，当前大语言模型在基础知识方面表现良好，但在临床推理、处方规划和安全合规性方面存在不足。这些发现凸显了亟需MTCMB等与领域对齐的基准测试，以指导开发更具胜任力且可信赖的医疗人工智能系统。所有数据集、代码与评估工具已公开于：https://github.com/Wayyuanyuan/MTCMB。

---

## [TSRating: Rating Quality of Diverse Time Series Data by Meta-learning from LLM Judgment](https://arxiv.org/abs/2506.01290)

### Abstract
arXiv:2506.01290v1 Announce Type: cross 
Abstract: High-quality time series (TS) data are essential for ensuring TS model performance, rendering research on rating TS data quality indispensable. Existing methods have shown promising rating accuracy within individual domains, primarily by extending data quality rating techniques such as influence functions and Shapley values to account for temporal characteristics. However, they neglect the fact that real-world TS data can span vastly different domains and exhibit distinct properties, hampering the accurate and efficient rating of diverse TS data. In this paper, we propose TSRating, a novel and unified framework for rating the quality of time series data crawled from diverse domains. TSRating is built on the assumption that LLMs inherit ample knowledge, acquired during their extensive pretraining, enabling them to comprehend and discern quality differences in diverse TS data. We verify this assumption by devising a series of prompts to elicit quality comparisons from LLMs for pairs of TS samples. We then fit a dedicated rating model, termed TSRater, to convert the LLMs' judgments into efficient quality predictions via TSRater's inference on future TS samples. To ensure cross-domain adaptability, we develop a meta-learning scheme to train TSRater on quality comparisons collected from nine distinct domains. To improve training efficiency, we employ signSGD for inner-loop updates, thus circumventing the demanding computation of hypergradients. Extensive experimental results on eleven benchmark datasets across three time series tasks, each using both conventional TS models and TS foundation models, demonstrate that TSRating outperforms baselines in terms of estimation accuracy, efficiency, and domain adaptability.

### 摘要
高质量时间序列（TS）数据对保障TS模型性能至关重要，这使得时间序列数据质量评估研究成为必要。现有方法通过将影响函数和Shapley值等数据质量评估技术扩展至时序特性，在单一领域内展现出良好的评估准确性。然而这些方法忽视了现实世界中TS数据可能跨越迥异领域并呈现不同特性的问题，导致难以准确高效地评估多样化TS数据。本文提出TSRating——一个面向多领域爬取时间序列数据质量评估的新型统一框架。该框架基于以下假设：大语言模型（LLMs）通过海量预训练继承了丰富知识，使其能够理解并判别多样化TS数据的质量差异。我们通过设计系列提示词引导LLMs对TS样本对进行质量比较，验证了这一假设。随后拟合专用评估模型TSRater，将其推理结果转化为对未来TS样本的高效质量预测。为确保跨领域适应性，我们开发了元学习方案，基于从九个不同领域收集的质量比较数据训练TSRater。为提高训练效率，采用signSGD进行内循环更新，从而规避超梯度的高计算需求。在三个时间序列任务（分别使用传统TS模型和TS基础模型）的十一个基准数据集上的大量实验表明，TSRating在评估准确性、效率及领域适应性方面均优于基线方法。

---

## [T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning](https://arxiv.org/abs/2506.01317)

### Abstract
arXiv:2506.01317v1 Announce Type: cross 
Abstract: Instruction tuning is essential for Large Language Models (LLMs) to effectively follow user instructions. To improve training efficiency and reduce data redundancy, recent works use LLM-based scoring functions, e.g., Instruction-Following Difficulty (IFD), to select high-quality instruction-tuning data with scores above a threshold. While these data selection methods often lead to models that can match or even exceed the performance of models trained on the full datasets, we identify two key limitations: (i) they assess quality at the sample level, ignoring token-level informativeness; and (ii) they overlook the robustness of the scoring method, often selecting a sample due to superficial lexical features instead of its true quality. In this work, we propose Token-Selective HIeRarchical Data Selection for Instruction Tuning (T-SHIRT), a novel data selection framework that introduces a new scoring method to include only informative tokens in quality evaluation and also promotes robust and reliable samples whose neighbors also show high quality with less local inconsistencies. We demonstrate that models instruction-tuned on a curated dataset (only 5% of the original size) using T-SHIRT can outperform those trained on the entire large-scale dataset by up to 5.48 points on average across eight benchmarks. Across various LLMs and training set scales, our method consistently surpasses existing state-of-the-art data selection techniques, while also remaining both cost-effective and highly efficient. For instance, by using GPT-2 for score computation, we are able to process a dataset of 52k samples using 40 minutes on a single GPU.

### 摘要
指令调优对于大型语言模型(LLMs)有效遵循用户指令至关重要。为提高训练效率并减少数据冗余，近期研究采用基于LLM的评分函数(如指令遵循难度IFD)来筛选分数超过阈值的高质量指令调优数据。尽管这些数据选择方法通常能使模型性能匹配甚至超越全数据集训练的模型，我们发现两个关键局限：(1)它们在样本层面评估质量，忽视了词元级别的信息量；(2)忽略了评分方法的鲁棒性，常因表面词汇特征而非真实质量选择样本。本研究提出基于词元选择的层次化指令调优数据筛选框架(T-SHIRT)，该创新框架通过新型评分方法仅纳入信息性词元进行质量评估，并优选其邻域样本同样高质量且局部不一致性低的稳健可靠样本。实验表明，使用T-SHIRT筛选的精选数据集(仅为原始规模5%)进行指令调优的模型，在八项基准测试中平均性能最高可超越全数据集训练模型5.48分。在不同规模LLMs和训练集上，本方法始终优于现有最先进数据选择技术，同时保持高成本效益与效率。例如使用GPT-2进行评分计算，单块GPU仅需40分钟即可处理52k样本数据集。

---

## [ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control](https://arxiv.org/abs/2506.01333)

### Abstract
arXiv:2506.01333v1 Announce Type: cross 
Abstract: The Model Context Protocol (MCP) plays a crucial role in extending the capabilities of Large Language Models (LLMs) by enabling integration with external tools and data sources. However, the standard MCP specification presents significant security vulnerabilities, notably Tool Poisoning and Rug Pull attacks. This paper introduces the Enhanced Tool Definition Interface (ETDI), a security extension designed to fortify MCP. ETDI incorporates cryptographic identity verification, immutable versioned tool definitions, and explicit permission management, often leveraging OAuth 2.0. We further propose extending MCP with fine-grained, policy-based access control, where tool capabilities are dynamically evaluated against explicit policies using a dedicated policy engine, considering runtime context beyond static OAuth scopes. This layered approach aims to establish a more secure, trustworthy, and controllable ecosystem for AI applications interacting with LLMs and external tools.

### 摘要
模型上下文协议（MCP）通过实现与外部工具及数据源的集成，在扩展大语言模型（LLM）能力方面发挥着关键作用。然而，标准MCP规范存在显著安全漏洞，尤其是工具投毒和抽毯式攻击。本文提出增强型工具定义接口（ETDI），这是一种旨在强化MCP的安全扩展方案。ETDI整合了加密身份验证、不可变版本化工具定义及显式权限管理（通常基于OAuth 2.0实现）。我们进一步建议通过细粒度的基于策略的访问控制来扩展MCP，其中工具能力由专用策略引擎根据显式策略进行动态评估，同时考量静态OAuth范围之外的运行时上下文。这种分层方法旨在为与LLM及外部工具交互的AI应用构建更安全、可信且可控的生态系统。

---

## [KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors](https://arxiv.org/abs/2506.01357)

### Abstract
arXiv:2506.01357v1 Announce Type: cross 
Abstract: Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at https://github.com/UEC-InabaLab/KokoroChat.

### 摘要
利用语言模型生成心理咨询应答高度依赖于高质量数据集。众包数据收集方法需要严格的员工培训，而来自真实咨询环境的数据可能引发隐私和伦理问题。尽管近期研究探索了使用大语言模型（LLMs）来增强心理咨询对话数据集，但生成的数据往往存在多样性和真实性不足的缺陷。为解决这些局限，本研究采用角色扮演方法，由训练有素的咨询师模拟咨询师-来访者互动，在确保对话高质量的同时降低隐私风险。通过该方法，我们构建了KokoroChat——一个包含6,589组长形式对话的日语心理咨询数据集，每组对话均附有详尽的来访者反馈。实验结果表明，使用KokoroChat对开源LLMs进行微调后，不仅能提升生成咨询应答的质量，还能改进咨询对话的自动评估效果。KokoroChat数据集可通过https://github.com/UEC-InabaLab/KokoroChat获取。

---

## [Incentivizing LLMs to Self-Verify Their Answers](https://arxiv.org/abs/2506.01369)

### Abstract
arXiv:2506.01369v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance during inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating its capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling. Our code is available at https://github.com/mansicer/self-verification.

### 摘要
大型语言模型（LLMs）通过训练后扩展和测试时扩展定律，在复杂推理任务中展现出显著进展。尽管当前主流的测试时扩展方法通常采用外部奖励模型来引导生成过程，但我们发现针对特定推理任务进行训练后扩展时，性能提升有限。研究表明，这种局限性源于特定训练后的生成器与通用奖励模型之间的分布差异。为解决这一问题，我们提出一个激励LLMs自我验证答案的框架。通过将答案生成与验证统一在单一强化学习（RL）过程中，我们训练出能够有效评估自身解决方案正确性的模型。经训练的模型在推理阶段无需依赖外部验证器，即可通过自我验证实现性能扩展。我们基于Qwen2.5-Math-7B和DeepSeek-R1-Distill-Qwen-1.5B训练了自验证模型，并在不同长度的推理语境中验证了其能力。多个数学推理基准测试表明，该模型不仅能提升训练后性能，还可实现有效的测试时扩展。代码已开源：https://github.com/mansicer/self-verification。

---

## [Compiler Optimization via LLM Reasoning for Efficient Model Serving](https://arxiv.org/abs/2506.01374)

### Abstract
arXiv:2506.01374v1 Announce Type: cross 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimization to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed REASONING COMPILER) that formulates optimization as a sequential, context-aware decision process, guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-aware transformations that reflect the current program state and accumulated performance feedback. Monte Carlo tree search (MCTS) incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.

### 摘要
虽然模型服务已释放出前所未有的能力，但大规模模型的高昂服务成本仍然是实现广泛可及性和快速创新的主要障碍。编译器优化长期以来推动着显著的性能提升，但由于可能的转换空间呈指数级增长且高度相互依赖，现有编译器难以应对神经网络的运算负载。尽管现有的随机搜索技术可能有效，但它们往往样本效率低下，且未能利用编译决策背后的结构化上下文。我们着手研究以下科学问题：在不进行任何再训练的情况下，利用大型语言模型（LLMs）进行推理，能否借助编译器优化的上下文感知决策空间显著提高样本效率？为此，我们提出了一种新颖的编译框架（称为推理编译器），该框架将优化问题构建为基于大型语言模型和结构化蒙特卡洛树搜索（MCTS）的序列化、上下文感知决策过程。大型语言模型作为提议机制，根据当前程序状态和累积的性能反馈，提出硬件感知的转换方案。蒙特卡洛树搜索（MCTS）整合LLM生成的提议，平衡探索与利用，从而在广阔的编译器优化空间中进行结构化、上下文敏感的遍历。与主流神经编译器相比，我们的方法以显著更少的样本实现了大幅加速，这证明了LLM引导的推理具有改变编译器优化格局的潜力。

---

## [Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines](https://arxiv.org/abs/2506.01329)

### Abstract
arXiv:2506.01329v1 Announce Type: cross 
Abstract: Psychological support hotlines are critical for crisis intervention but face significant challenges due to rising demand. Large language models (LLMs) could support crisis assessments, yet their capabilities in emotionally sensitive contexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, assessing four tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. We evaluated 64 LLMs across 15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot, few-shot, and fine-tuning paradigms. Performance was measured by F1-score, with statistical comparisons via Welch's t-tests. LLMs performed strongly on suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779), and risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood status recognition was more challenging (max F1=0.709), likely due to lost vocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) surpassed larger models on mood and suicidal ideation. Open-source models like QwQ-32B performed comparably to closed-source on most tasks (p&gt;0.3), though closed models retained an edge in mood detection (p=0.007). Performance scaled with size up to a point; quantization (AWQ) reduced GPU memory by 70% with minimal F1 degradation. LLMs show substantial promise in structured psychological crisis assessments, especially with fine-tuning. Mood recognition remains limited due to contextual complexity. The narrowing gap between open- and closed-source models, combined with efficient quantization, suggests feasible integration. PsyCrisisBench offers a robust evaluation framework to guide model development and ethical deployment in mental health.

### 摘要
心理援助热线在危机干预中至关重要，但面临需求激增带来的重大挑战。大型语言模型（LLMs）有望支持危机评估，但其在情感敏感情境中的能力尚不明确。我们推出PsyCrisisBench基准测试，包含杭州心理援助热线540份标注对话文本，评估四大任务：情绪状态识别、自杀意念检测、自杀计划识别和风险评估。我们采用零样本、少样本和微调范式评估了15个系列共64个LLMs（如GPT、Claude、Gemini、Llama、Qwen、DeepSeek）。通过F1分数和Welch t检验进行性能比较，结果显示：LLMs在自杀意念检测（F1=0.880）、自杀计划识别（F1=0.779）和风险评估（F1=0.907）表现优异，少样本和微调可进一步提升性能；情绪状态识别更具挑战性（最高F1=0.709），可能源于语音线索缺失和语义模糊性。经微调的15亿参数模型（Qwen2.5-1.5B）在情绪和自杀意念任务上超越更大模型。开源模型如QwQ-32B在多数任务表现与闭源相当（p&gt;0.3），但闭源模型在情绪检测仍具优势（p=0.007）。模型性能随规模提升至临界点后饱和；采用AWQ量化技术可在F1损失最小化同时降低70%显存占用。研究表明LLMs在结构化心理危机评估中（特别是微调后）潜力显著，但情绪识别仍受限于语境复杂性。开源与闭源模型差距缩小结合高效量化技术，预示可行整合方案。PsyCrisisBench为心理健康领域模型开发和伦理部署提供了稳健评估框架。

---

## [Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes](https://arxiv.org/abs/2506.01512)

### Abstract
arXiv:2506.01512v1 Announce Type: cross 
Abstract: Rational speakers are supposed to know what they know and what they do not know, and to generate expressions matching the strength of evidence. In contrast, it is still a challenge for current large language models to generate corresponding utterances based on the assessment of facts and confidence in an uncertain real-world environment. While it has recently become popular to estimate and calibrate confidence of LLMs with verbalized uncertainty, what is lacking is a careful examination of the linguistic knowledge of uncertainty encoded in the latent space of LLMs. In this paper, we draw on typological frameworks of epistemic expressions to evaluate LLMs' knowledge of epistemic modality, using controlled stories. Our experiments show that the performance of LLMs in generating epistemic expressions is limited and not robust, and hence the expressions of uncertainty generated by LLMs are not always reliable. To build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge of epistemic modality in LLMs.

### 摘要
理性的说话者应当知晓其所知与所不知，并能根据证据强度生成匹配的表达。然而在当前不确定的现实环境中，基于事实评估与置信度生成相应话语，对于现有大语言模型仍具挑战性。尽管近来通过语言化不确定性来估计和校准大语言模型的置信度成为流行趋势，但缺乏对其潜在空间中编码的不确定性语言学知识的系统考察。本文借助类型学框架下的认知情态表达理论，采用受控故事实验评估大语言模型对认知情态的掌握程度。实验表明，大语言模型生成认知情态表达的能力有限且不稳定，因此其产生的不确定性表达并非始终可靠。为构建具有不确定性感知能力的大语言模型，必须丰富其认知情态的语义知识。

---

## [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)

### Abstract
arXiv:2506.01413v1 Announce Type: cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.

### 摘要
现有大型语言模型（LLMs）在处理复杂指令时面临挑战，尤其是当多个约束以并行、链式和分支结构组织时。直觉解决方案思维链（CoT）本应普遍提升LLMs能力，但我们发现传统CoT因其简单复述指令的浅层推理模式，反而对性能产生负面影响。该方法未能剖析约束的组成以识别跨类型和维度层次的关系。为此，我们提出一种系统性方法，通过激励推理实现测试时计算扩展，从而增强LLMs处理复杂指令的能力。首先，基于现有分类法对复杂指令进行解构，并提出可复现的数据获取方法。其次，利用强化学习（RL）结合可验证的规则中心奖励信号，专门培养指令遵循的推理能力。我们通过样本级对比解决复杂指令下推理的浅层非本质性问题，实现更优的CoT实施。同时采用专家行为克隆技术，促进LLMs从快速思维模式向熟练推理者的稳定分布迁移。在七个综合基准测试上的广泛评估验证了该方法的有效性，1.5B参数的LLM实现了11.74%的性能提升，其表现可与8B参数模型相媲美。代码与数据详见https://github.com/yuleiqin/RAIF。

---

## [LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation](https://arxiv.org/abs/2506.01538)

### Abstract
arXiv:2506.01538v1 Announce Type: cross 
Abstract: Although Multi-Agent Reinforcement Learning (MARL) is effective for complex multi-robot tasks, it suffers from low sample efficiency and requires iterative manual reward tuning. Large Language Models (LLMs) have shown promise in single-robot settings, but their application in multi-robot systems remains largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL) approach, which integrates MARL with LLMs, significantly enhancing sample efficiency without requiring manual design. LAMARL consists of two modules: the first module leverages LLMs to fully automate the generation of prior policy and reward functions. The second module is MARL, which uses the generated functions to guide robot policy training effectively. On a shape assembly benchmark, both simulation and real-world experiments demonstrate the unique advantages of LAMARL. Ablation studies show that the prior policy improves sample efficiency by an average of 185.9% and enhances task completion, while structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM output success rates by 28.5%-67.5%. Videos and code are available at https://guobin-zhu.github.io/LLM-MARL

### 摘要
尽管多智能体强化学习（MARL）在复杂多机器人任务中表现优异，但其存在样本效率低下且需反复手动调整奖励函数的问题。大型语言模型（LLMs）在单机器人场景已展现潜力，但在多机器人系统中的应用仍待探索。本文提出一种新型LLM辅助MARL（LAMARL）方法，通过将MARL与LLMs相结合，在无需人工设计的情况下显著提升样本效率。LAMARL包含两大模块：首模块利用LLM全自动生成先验策略与奖励函数；次模块为MARL，通过生成函数有效指导机器人策略训练。在形状组装基准测试中，仿真与实物实验均验证了LAMARL的独特优势。消融研究表明：先验策略使样本效率平均提升185.9%并改善任务完成度，而基于思维链（CoT）和基础API的结构化提示则将LLM输出成功率提高28.5%-67.5%。实验视频与代码详见https://guobin-zhu.github.io/LLM-MARL。

---

## [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)

### Abstract
arXiv:2506.01551v1 Announce Type: cross 
Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following natural language instructions is a long-standing goal in human-robot interaction applications. Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for improving navigation, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches primarily adopt direct input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. In this paper, we propose a novel sElf-improving embodied reasoning framework for boosting LLM-based vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with formalized CoT labels to both activate the model's navigational reasoning capabilities and increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also introduced to encourage learning correct reasoning patterns by contrasting with wrong ones. Experimental results on the popular VLN benchmarks demonstrate the superiority of EvolveNav over previous LLM-based VLN approaches. Code is available at https://github.com/expectorlin/EvolveNav.

### 摘要
构建能够遵循自然语言指令进行导航的视觉-语言导航（VLN）智能体是人机交互应用中的长期目标。近期研究表明，通过训练开源大型语言模型（LLM）来释放其推理能力可有效提升导航性能，同时缓解LLM训练语料与VLN任务之间的领域差异。然而现有方法主要采用直接输入-输出映射范式，导致映射学习困难且导航决策缺乏可解释性。思维链（CoT）训练虽能提高导航决策准确性和可解释性，但导航任务的复杂性使得完美CoT标注难以获取，纯CoT监督微调可能导致过拟合。本文提出名为EvolveNav的新型自增强具身推理框架，包含两个阶段：（1）形式化CoT监督微调阶段，通过形式化CoT标注训练模型以激活导航推理能力并提升推理速度；（2）自反思后训练阶段，利用模型自身推理输出作为自增强CoT标签进行迭代训练以增加监督多样性。同时引入自反思辅助任务，通过对比错误推理模式来促进正确推理模式的学习。在主流VLN基准测试上的实验结果表明，EvolveNav优于现有基于LLM的VLN方法。代码发布于https://github.com/expectorlin/EvolveNav。

---

## [V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat](https://arxiv.org/abs/2506.01524)

### Abstract
arXiv:2506.01524v1 Announce Type: cross 
Abstract: With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.

### 摘要
随着基于大语言模型(LLM)的聊天机器人持续普及，人们越来越需要生成不仅在语言表达上流畅、还能在对话中始终符合特定角色特征的回应。然而，现有的角色扮演和基于人设的聊天方法主要依赖静态角色描述、粗粒度信号空间和低质量合成数据，这些方法无法捕捉类人聊天中动态的细粒度细节。类人聊天需要建模微妙的潜在特征，如情感基调、情境意识和动态演变的人格特质，这些特征难以预先定义，也无法轻易从合成或基于蒸馏的数据中学习。为解决这些局限性，我们提出了言语变分自编码(V-VAE)框架，包含变分自编码模块和细粒度控制空间，能基于可解释的细粒度潜在变量(包括说话风格、交互模式和个人属性)动态调整对话行为。我们还构建了高质量数据集HumanChatData和基准测试HumanChatBench，以解决类人领域高质量数据稀缺的问题。实验表明，基于V-VAE的LLM在HumanChatBench和DialogBench上持续优于标准基线模型，进一步验证了V-VAE框架和HumanChat数据集的有效性。

---

## [ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge](https://arxiv.org/abs/2506.01646)

### Abstract
arXiv:2506.01646v1 Announce Type: cross 
Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1 136 multiple-choice questions generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting retrieval-augmented generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports and recommendation documents from seven authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of the model, we implement a rigorous two-stage evaluation protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging from 0.5 B to 671 B parameters) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies typically around 55--70\%, highlighting ESGenius's challenging nature for LLMs in interdisciplinary contexts. However, models employing RAG show significant performance improvements, particularly for smaller models. For example, "DeepSeek-R1-Distill-Qwen-14B" improves from 63.82\% (zero-shot) to 80.46\% with RAG. These results underscore the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first benchmark curated for LLMs and the relevant enhancement technologies that focuses on ESG and sustainability topics.

---

## [MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation](https://arxiv.org/abs/2506.01776)

### Abstract
arXiv:2506.01776v1 Announce Type: cross 
Abstract: With the rapid adoption of large language models (LLMs) in natural language processing, the ability to follow instructions has emerged as a key metric for evaluating their practical utility. However, existing evaluation methods often focus on single-language scenarios, overlooking the challenges and differences present in multilingual and cross-lingual contexts. To address this gap, we introduce MaXIFE: a comprehensive evaluation benchmark designed to assess instruction-following capabilities across 23 languages with 1,667 verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to evaluate several leading commercial and open-source LLMs, establishing baseline results for future comparisons. By providing a standardized tool for multilingual instruction-following evaluation, MaXIFE aims to advance research and development in natural language processing.

### 摘要
随着大语言模型（LLM）在自然语言处理领域的快速普及，指令遵循能力已成为评估其实际应用价值的关键指标。然而，现有评估方法多集中于单语言场景，忽视了多语言及跨语言语境下的挑战与差异。为填补这一空白，我们提出MaXIFE：一个涵盖23种语言、包含1,667项可验证指令任务的综合性评估基准。该框架整合基于规则的评估和基于模型的评估方法，确保效率与准确性的平衡。我们运用MaXIFE对多个主流商业及开源大语言模型进行评测，建立了未来研究可参照的基线结果。通过提供标准化的多语言指令遵循评估工具，MaXIFE旨在推动自然语言处理领域的研究与发展。

---

## [Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification](https://arxiv.org/abs/2506.01631)

### Abstract
arXiv:2506.01631v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance.
  To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.

### 摘要
随着大型语言模型（LLMs）成为现代应用程序中不可或缺的软件组件，通过微调、合并和再分发进行的未授权模型衍生已成为关键的软件工程挑战。与传统软件中成熟的克隆检测和许可证合规机制不同，LLM生态系统缺乏有效手段来追踪模型谱系并执行许可协议。这一缺陷在开源模型创建者（如Meta的LLaMA）要求衍生作品保留命名规范以进行署名时尤为突出，但目前尚无技术手段可验证合规性。为填补这一空白，我们将LLMs视为需要溯源追踪的软件制品，提出了TensorGuard——一种基于梯度的指纹框架，用于LLM相似性检测和家族分类。该方法通过分析随机输入扰动在张量层产生的梯度响应，提取模型内在的行为特征，其运作独立于训练数据、水印或特定模型格式。TensorGuard支持广泛采用的safetensors格式，并通过梯度特征的统计分析构建高维指纹。这些指纹具备两项互补功能：一是通过距离计算直接评估任意模型间的成对相似性；二是利用基于已知基模型的领域知识初始化质心，通过K-Means聚类算法对未知模型进行系统家族分类。在包含5个模型家族（Llama、Qwen、Gemma、Phi、Mistral）8个基模型和50个衍生模型的58个模型上的实验表明，采用质心初始化K-Means聚类的分类准确率达到94%。

---

## [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)

### Abstract
arXiv:2506.01723v1 Announce Type: cross 
Abstract: Idioms present a unique challenge for language models due to their non-compositional figurative meanings, which often strongly diverge from the idiom's literal interpretation. This duality requires a model to learn representing and deciding between the two meanings to interpret an idiom in a figurative sense, or literally. In this paper, we employ tools from mechanistic interpretability to trace how a large pretrained causal transformer (LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom processing: First, the idiom's figurative meaning is retrieved in early attention and MLP sublayers. We identify specific attention heads which boost the figurative meaning of the idiom while suppressing the idiom's literal interpretation. The model subsequently represents the figurative representation through an intermediate path. Meanwhile, a parallel bypass route forwards literal interpretation, ensuring that a both reading remain available. Overall, our findings provide a mechanistic evidence for idiom comprehension in an autoregressive transformer.

### 摘要
习语因其非组合性的比喻意义而对语言模型构成独特挑战，其含义常与字面解释存在显著差异。这种双重性要求模型在学习表征时需对两种含义进行抉择，以决定采用比喻义或字面义。本文运用机制可解释性工具，追踪大型预训练因果Transformer模型（LLama3.2-1B-base）处理此类歧义的过程。我们定位了习语处理的三个步骤：首先，比喻意义在早期注意力和MLP子层中被检索；特定注意力头会增强习语的比喻意义，同时抑制其字面解释。随后模型通过中间路径表征比喻含义，而并行旁路则传递字面解释，确保两种解读方式得以保留。本研究为自回归Transformer中的习语理解机制提供了实证依据。

---

## [Data Pruning by Information Maximization](https://arxiv.org/abs/2506.01701)

### Abstract
arXiv:2506.01701v1 Announce Type: cross 
Abstract: In this paper, we present InfoMax, a novel data pruning method, also known as coreset selection, designed to maximize the information content of selected samples while minimizing redundancy. By doing so, InfoMax enhances the overall informativeness of the coreset. The information of individual samples is measured by importance scores, which capture their influence or difficulty in model learning. To quantify redundancy, we use pairwise sample similarities, based on the premise that similar samples contribute similarly to the learning process. We formalize the coreset selection problem as a discrete quadratic programming (DQP) task, with the objective of maximizing the total information content, represented as the sum of individual sample contributions minus the redundancies introduced by similar samples within the coreset. To ensure practical scalability, we introduce an efficient gradient-based solver, complemented by sparsification techniques applied to the similarity matrix and dataset partitioning strategies. This enables InfoMax to seamlessly scale to datasets with millions of samples. Extensive experiments demonstrate the superior performance of InfoMax in various data pruning tasks, including image classification, vision-language pre-training, and instruction tuning for large language models.

### 摘要
本文提出InfoMax这一新型数据剪枝方法（亦称核心集选择），旨在最大化所选样本信息量的同时最小化冗余度，从而提升核心集整体信息密度。样本个体信息量通过重要性分数度量，该分数表征样本对模型学习的影响力或难易程度。冗余度的量化则基于样本间相似度，其理论前提是相似样本对学习过程的贡献具有同质性。我们将核心集选择问题形式化为离散二次规划（DQP）任务，其优化目标为最大化总信息量——即核心集内各样本贡献之和减去相似样本引入的冗余度。为确保实际可扩展性，我们引入基于梯度的高效求解器，并结合相似度矩阵稀疏化与数据集分块策略，使InfoMax能高效处理百万级样本数据集。大量实验表明，InfoMax在图像分类、视觉语言预训练及大语言模型指令调优等多种数据剪枝任务中均表现出卓越性能。

---

## [ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs](https://arxiv.org/abs/2506.01770)

### Abstract
arXiv:2506.01770v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at https://github.com/weizeming/ReGA.

### 摘要
大型语言模型（LLMs）虽在多类任务中取得显著成功，但其安全性与可靠性问题日益引发关注。该模型存在生成有害内容及越狱攻击脆弱性等风险。基于模型的分析方法在状态化深度神经网络监测领域已展现突出潜力，然而面对LLMs庞大特征空间时面临可扩展性挑战。本文提出ReGA框架——一种基于表征引导抽象的模型分析方法，用于防御有害提示与生成。通过利用安全关键表征（即隐藏状态中指示安全相关概念的低维方向），ReGA有效解决了构建安全抽象模型时的可扩展性问题。综合评估表明，ReGA在区分安全/有害输入方面表现优异：提示级AUROC达0.975，会话级达0.985。该框架对现实攻击具有鲁棒性，并能跨不同安全视角泛化，在可解释性与可扩展性上超越现有防护范式。ReGA通过将表征工程与模型抽象相结合，为提升LLM安全性提供了高效可扩展的方案，为利用软件工程洞见保障AI安全开辟了新范式。代码详见https://github.com/weizeming/ReGA。

---

## [GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion](https://arxiv.org/abs/2506.01673)

### Abstract
arXiv:2506.01673v1 Announce Type: cross 
Abstract: Generative recommendation is an emerging paradigm that leverages the extensive knowledge of large language models by formulating recommendations into a text-to-text generation task. However, existing studies face two key limitations in (i) incorporating implicit item relationships and (ii) utilizing rich yet lengthy item information. To address these challenges, we propose a Generative Recommender via semantic-Aware Multi-granular late fusion (GRAM), introducing two synergistic innovations. First, we design semantic-to-lexical translation to encode implicit hierarchical and collaborative item relationships into the vocabulary space of LLMs. Second, we present multi-granular late fusion to integrate rich semantics efficiently with minimal information loss. It employs separate encoders for multi-granular prompts, delaying the fusion until the decoding stage. Experiments on four benchmark datasets show that GRAM outperforms eight state-of-the-art generative recommendation models, achieving significant improvements of 11.5-16.0% in Recall@5 and 5.3-13.6% in NDCG@5. The source code is available at https://github.com/skleee/GRAM.

### 摘要
生成式推荐是一种新兴范式，其通过将推荐任务转化为文本到文本的生成问题，从而利用大语言模型的广泛知识。然而，现有研究在（i）融入隐式物品关系和（ii）利用丰富但冗长的物品信息两方面存在关键局限。针对这些挑战，我们提出了一种基于语义感知多粒度延迟融合的生成式推荐模型（GRAM），包含两项协同创新：首先，我们设计了语义到词法的转换方法，将隐式的层次化与协同物品关系编码至大语言模型的词汇空间；其次，我们提出多粒度延迟融合策略，以最小信息损失高效整合丰富语义。该策略采用多粒度提示的独立编码器，将融合过程延迟至解码阶段。在四个基准数据集上的实验表明，GRAM优于八种最先进的生成式推荐模型，在Recall@5和NDCG@5指标上分别实现11.5-16.0%和5.3-13.6%的显著提升。源代码详见https://github.com/skleee/GRAM。

---

## [MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs](https://arxiv.org/abs/2506.01850)

### Abstract
arXiv:2506.01850v1 Announce Type: cross 
Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on instruction-following tasks by integrating pretrained visual encoders with large language models (LLMs). However, existing approaches often struggle to ground fine-grained visual concepts in complex scenes. In this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective module designed to refine pre-aligned visual features through instruction-guided modulation. Our approach follows the standard LLaVA training protocol, consisting of a two-stage process: (1) aligning image features to the LLMs input space via a frozen vision encoder and adapter layers, and (2) refining those features using the MoDA adapter during the instructional tuning stage. MoDA employs a Transformer-based cross-attention mechanism to generate a modulation mask over the aligned visual tokens, thereby emphasizing semantically relevant embedding dimensions based on the language instruction. The modulated features are then passed to the LLM for autoregressive language generation. Our experimental evaluation shows that MoDA improves visual grounding and generates more contextually appropriate responses, demonstrating its effectiveness as a general-purpose enhancement for image-based MLLMs.

### 摘要
近期，多模态大语言模型（MLLMs）通过将预训练视觉编码器与大语言模型（LLMs）相结合，在指令跟随任务中展现出卓越性能。然而，现有方法往往难以在复杂场景中精确定位细粒度视觉概念。本文提出MoDA（调制适配器），这是一种轻量级但高效的模块，旨在通过指令引导的调制机制优化预对齐视觉特征。我们的方法遵循标准LLaVA训练流程，包含两个阶段：（1）通过冻结的视觉编码器和适配层将图像特征对齐至LLMs输入空间；（2）在指令微调阶段使用MoDA适配器优化这些特征。MoDA采用基于Transformer的交叉注意力机制，在已对齐的视觉标记上生成调制掩码，从而根据语言指令强调语义相关的嵌入维度。调制后的特征随后传递至LLM进行自回归语言生成。实验评估表明，MoDA能显著提升视觉定位能力并生成更符合语境的响应，验证了其作为图像基MLLMs通用增强方法的有效性。

---

## [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)

### Abstract
arXiv:2506.01784v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) excel at many natural language processing tasks, they often suffer from factual inaccuracies in knowledge-intensive scenarios. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To address these issues, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs.

### 摘要
尽管大语言模型（LLMs）在众多自然语言处理任务中表现卓越，但在知识密集型场景中常出现事实性错误。整合外部知识资源，尤其是知识图谱（KGs），可为更可靠的推理提供透明且可更新的基础。知识库问答（KBQA）作为对知识图谱进行查询和推理的核心技术，对于处理复杂的多跳查询尤为重要。然而，多跳推理面临两大关键挑战：（1）保持连贯的推理路径；（2）避免过早丢弃关键的多跳连接。为解决这些问题，我们提出了iQUEST框架——一种通过问题引导迭代地将复杂查询分解为简单子问题的KBQA方法，从而确保结构化且聚焦的推理轨迹。此外，我们引入图神经网络（GNN）在每一步推理中前瞻性地融合2跳邻域信息。这种双重策略强化了推理过程，使模型能更有效地探索可行路径。详尽的实验表明，iQUEST在四个基准数据集和四种LLMs上均实现了持续的性能提升。

---

## [When LLMs Team Up: The Emergence of Collaborative Affective Computing](https://arxiv.org/abs/2506.01698)

### Abstract
arXiv:2506.01698v1 Announce Type: cross 
Abstract: Affective Computing (AC) is essential in bridging the gap between human emotional experiences and machine understanding. Traditionally, AC tasks in natural language processing (NLP) have been approached through pipeline architectures, which often suffer from structure rigidity that leads to inefficiencies and limited adaptability. The advent of Large Language Models (LLMs) has revolutionized this field by offering a unified approach to affective understanding and generation tasks, enhancing the potential for dynamic, real-time interactions. However, LLMs face cognitive limitations in affective reasoning, such as misinterpreting cultural nuances or contextual emotions, and hallucination problems in decision-making. To address these challenges, recent research advocates for LLM-based collaboration systems that emphasize interactions among specialized models and LLMs, mimicking human-like affective intelligence through the synergy of emotional and rational thinking that aligns with Dual Process Theory in psychology. This survey aims to provide a comprehensive overview of LLM-based collaboration systems in AC, exploring from structured collaborations to autonomous collaborations. Specifically, it includes: (1) A systematic review of existing methods, focusing on collaboration strategies, mechanisms, key functions, and applications; (2) Experimental comparisons of collaboration strategies across representative tasks in affective understanding and generation; (3) An analysis highlighting the potential of these systems to enhance robustness and adaptability in complex affective reasoning; (4) A discussion of key challenges and future research directions to further advance the field. This work is the first to systematically explore collaborative intelligence with LLMs in AC, paving the way for more powerful applications that approach human-like social intelligence.

### 摘要
情感计算（AC）在弥合人类情感体验与机器理解之间的鸿沟中具有关键作用。传统自然语言处理（NLP）中的AC任务通常采用流水线架构，这种结构往往因刚性导致效率低下且适应性有限。大语言模型（LLMs）的出现通过提供情感理解与生成任务的统一框架彻底改变了这一领域，增强了动态实时交互的潜力。然而，LLMs在情感推理方面存在认知局限，例如对文化差异或情境情感的误判，以及决策中的幻觉问题。为解决这些挑战，近期研究提倡建立基于LLM的协作系统，通过专业化模型与LLMs之间的交互，模拟符合心理学双过程理论的情感与理性思维协同作用，实现类人情感智能。本文旨在系统综述AC领域中基于LLM的协作系统研究，涵盖从结构化协作到自主协作的完整谱系。具体包括：（1）现有方法的系统性回顾，重点关注协作策略、机制、核心功能及应用场景；（2）情感理解与生成典型任务中不同协作策略的实验对比；（3）论证这些系统在提升复杂情感推理鲁棒性与适应性方面的潜力；（4）探讨关键挑战与未来研究方向以推动领域发展。本研究首次系统探索了AC中LLMs的协作智能，为开发更接近人类社交智能的强大应用奠定了基础。

---

## [Image Generation from Contextually-Contradictory Prompts](https://arxiv.org/abs/2506.01929)

### Abstract
arXiv:2506.01929v1 Announce Type: cross 
Abstract: Text-to-image diffusion models excel at generating high-quality, diverse images from natural language prompts. However, they often fail to produce semantically accurate results when the prompt contains concept combinations that contradict their learned priors. We define this failure mode as contextual contradiction, where one concept implicitly negates another due to entangled associations learned during training. To address this, we propose a stage-aware prompt decomposition framework that guides the denoising process using a sequence of proxy prompts. Each proxy prompt is constructed to match the semantic content expected to emerge at a specific stage of denoising, while ensuring contextual coherence. To construct these proxy prompts, we leverage a large language model (LLM) to analyze the target prompt, identify contradictions, and generate alternative expressions that preserve the original intent while resolving contextual conflicts. By aligning prompt information with the denoising progression, our method enables fine-grained semantic control and accurate image generation in the presence of contextual contradictions. Experiments across a variety of challenging prompts show substantial improvements in alignment to the textual prompt.

### 摘要
文本到图像扩散模型擅长根据自然语言提示生成高质量、多样化的图像。然而，当提示包含与模型所学先验知识相矛盾的概念组合时，它们往往无法生成语义准确的结果。我们将这种失效模式定义为上下文矛盾，即由于训练过程中学习到的关联纠缠，一个概念会隐含否定另一个概念。为解决这一问题，我们提出了一种阶段感知的提示分解框架，该框架通过一系列代理提示引导去噪过程。每个代理提示的构建都匹配去噪特定阶段预期出现的语义内容，同时确保上下文连贯性。为构建这些代理提示，我们利用大语言模型（LLM）分析目标提示、识别矛盾，并生成在保留原始意图的同时解决上下文冲突的替代表达。通过将提示信息与去噪进程对齐，我们的方法能够在存在上下文矛盾的情况下实现细粒度语义控制和精确图像生成。针对各类挑战性提示的实验表明，该方法在文本提示对齐方面取得了显著改进。

---

## [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)

### Abstract
arXiv:2506.01939v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.

### 摘要
可验证奖励强化学习（RLVR）已成为增强大语言模型（LLMs）推理能力的有效方法，但其机制尚未得到充分理解。本研究首次通过词元熵模式的新视角对RLVR进行探索，系统分析了不同词元如何影响推理性能。通过考察思维链（CoT）推理中的词元熵模式，我们发现仅有少量词元呈现高熵特征，这些词元作为关键分岔点引导模型走向不同的推理路径。进一步研究RLVR训练过程中熵模式的演变表明，RLVR主要遵循基础模型的熵分布规律，重点调整高熵词元的熵值。这些发现揭示了高熵词元（即分岔词元）对RLVR的重要性。我们最终通过将策略梯度更新限制于分岔词元来改进RLVR，并发现一个超越80/20法则的现象：在Qwen3-8B基础模型上仅使用20%的词元即可保持与全梯度更新相当的性能，而在Qwen3-32B（AIME'25提升11.04分，AIME'24提升7.71分）和Qwen3-14B（AIME'25提升4.79分，AIME'24提升5.21分）基础模型上显著超越全梯度更新，展现出强烈的规模扩展趋势。相反，仅对80%最低熵词元进行训练会导致性能显著下降。这些结果表明RLVR的有效性主要源于对决定推理方向的高熵词元的优化。总体而言，我们的研究结果凸显了通过词元熵视角理解RLVR的潜力，以及利用高熵少数词元优化RLVR以进一步提升LLM推理能力的可能性。

---

## [WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks](https://arxiv.org/abs/2506.01952)

### Abstract
arXiv:2506.01952v1 Announce Type: cross 
Abstract: Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.

### 摘要
由大型语言模型（LLM）驱动的网页浏览代理能以类人方式操作浏览器，为自动化各类日常任务提供了高度透明的实现路径。随着网络代理能力日益增强并在通用浏览任务中展现出熟练水平，一个关键问题随之产生：它们能否超越通用浏览范畴，稳健处理那些繁琐复杂的任务，甚至是人类自身经常回避的杂务？本文提出WebChoreArena——一个包含532项精心设计任务的全新可复现基准，旨在将WebArena的范围从通用浏览扩展到更费时费力的繁琐任务。该基准系统整合了三大核心挑战：(i)需要从观察信息中准确检索海量数据的'海量记忆任务'；(ii)要求精确数学推理的'计算任务'；(iii)需跨多个网页保持长期记忆的'长期记忆任务'。基于完全可复现且广泛采用的四个WebArena模拟环境构建，WebChoreArena确保严格的可复现性，支持与既有WebArena基准进行公平直接对比，从而为智能体进展提供关键洞见。实验结果表明，随着以GPT-4o、Claude 3.7 Sonnet和Gemini 2.5 Pro为代表的LLM发展，WebChoreArena上的性能呈现显著提升，证明该基准能更清晰地衡量前沿LLM的进步。然而数据同时显示，即便使用Gemini 2.5 Pro，其表现与WebArena相比仍存在明显差距，凸显WebChoreArena带来的更大挑战。

---

## [Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends](https://arxiv.org/abs/2409.14457)

### Abstract
arXiv:2409.14457v3 Announce Type: replace 
Abstract: With the rapid advancement of large models (LMs), the development of general-purpose intelligent agents powered by LMs has become a reality. It is foreseeable that in the near future, LM-driven general AI agents will serve as essential tools in production tasks, capable of autonomous communication and collaboration without human intervention. This paper investigates scenarios involving the autonomous collaboration of future LM agents. We review the current state of LM agents, the key technologies enabling LM agent collaboration, and the security and privacy challenges they face during cooperative operations. To this end, we first explore the foundational principles of LM agents, including their general architecture, key components, enabling technologies, and modern applications. We then discuss practical collaboration paradigms from data, computation, and knowledge perspectives to achieve connected intelligence among LM agents. After that, we analyze the security vulnerabilities and privacy risks associated with LM agents, particularly in multi-agent settings, examining underlying mechanisms and reviewing current and potential countermeasures. Lastly, we propose future research directions for building robust and secure LM agent ecosystems.

### 摘要
随着大模型（LMs）的快速发展，基于大模型的通用智能体开发已成为现实。可以预见在不久的将来，大模型驱动的通用人工智能体将作为生产任务中的基础工具，能够在无人干预的情况下实现自主通信与协作。本文研究了未来大模型智能体自主协作的应用场景，系统梳理了大模型智能体的发展现状、支撑多智能体协作的关键技术，以及协同运作时面临的安全与隐私挑战。为此，我们首先探讨了大模型智能体的基础原理，包括通用架构、核心组件、使能技术和现代应用；继而从数据、计算和知识三个维度讨论了实现智能体互联的实践协作范式；随后分析了多智能体场景下大模型智能体存在的安全漏洞与隐私风险，剖析其内在机理并综述现有及潜在的防御措施；最后提出了构建鲁棒安全的大模型智能体生态系统的未来研究方向。

---

## [DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation](https://arxiv.org/abs/2506.01954)

### Abstract
arXiv:2506.01954v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) methods have proven highly effective for tasks requiring factual consistency and robust knowledge retrieval. However, large-scale RAG systems consume significant computational resources and are prone to generating hallucinated content from Humans. In this work, we introduce $\texttt&#123;DRAG&#125;$, a novel framework for distilling RAG knowledge from large-scale Language Models (LLMs) into small LMs (SLMs). Our approach leverages evidence- and knowledge graph-based distillation, ensuring that the distilled model retains critical factual knowledge while significantly reducing model size and computational cost. By aligning the smaller model's predictions with a structured knowledge graph and ranked evidence, $\texttt&#123;DRAG&#125;$ effectively mitigates hallucinations and improves factual accuracy. We further present a case demonstrating how our framework mitigates user privacy risks and introduce a corresponding benchmark. Experimental evaluations on multiple benchmarks demonstrate that our method outperforms the prior competitive RAG methods like MiniRAG for SLMs by up to 27.7% using the same models, preserving high-level efficiency and reliability. With $\texttt&#123;DRAG&#125;$, we provide a practical and resource-efficient roadmap to deploying enhanced retrieval and generation capabilities in small-sized LLMs.

### 摘要
检索增强生成（RAG）方法在需要事实一致性和强大知识检索的任务中已被证明非常有效。然而，大规模RAG系统消耗大量计算资源，并且容易生成人类幻觉内容。在本研究中，我们提出$\texttt&#123;DRAG&#125;$，一种将RAG知识从大规模语言模型（LLM）蒸馏到小型语言模型（SLM）的新框架。我们的方法利用基于证据和知识图谱的蒸馏，确保蒸馏模型保留关键事实知识，同时显著减小模型规模和计算成本。通过将较小模型的预测与结构化知识图谱和排序证据对齐，$\texttt&#123;DRAG&#125;$有效缓解了幻觉问题并提高了事实准确性。我们还通过案例展示了该框架如何降低用户隐私风险，并引入了相应的基准测试。在多个基准上的实验评估表明，使用相同模型时，我们的方法比MiniRAG等先前竞争性RAG方法性能提升高达27.7%，同时保持了高水平的效率和可靠性。通过$\texttt&#123;DRAG&#125;$，我们为在小型LLM中部署增强的检索和生成功能提供了一条实用且资源高效的路线图。

---

## [Odyssey: Empowering Minecraft Agents with Open-World Skills](https://arxiv.org/abs/2407.15325)

### Abstract
arXiv:2407.15325v3 Announce Type: replace 
Abstract: Recent studies have delved into constructing generalist agents for open-world environments like Minecraft. Despite the encouraging results, existing efforts mainly focus on solving basic programmatic tasks, e.g., material collection and tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond task as the ultimate goal. This limitation stems from the narrowly defined set of actions available to agents, requiring them to learn effective long-horizon strategies from scratch. Consequently, discovering diverse gameplay opportunities in the open world becomes challenging. In this work, we introduce Odyssey, a new framework that empowers Large Language Model (LLM)-based agents with open-world skills to explore the vast Minecraft world. Odyssey comprises three key parts: (1) An interactive agent with an open-world skill library that consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned LLaMA-3 model trained on a large question-answering dataset with 390k+ instruction entries derived from the Minecraft Wiki. (3) A new agent capability benchmark includes the long-term planning task, the dynamic-immediate planning task, and the autonomous exploration task. Extensive experiments demonstrate that the proposed Odyssey framework can effectively evaluate different capabilities of LLM-based agents. All datasets, model weights, and code are publicly available to motivate future research on more advanced autonomous agent solutions.

### 摘要
近期研究致力于构建适用于《我的世界》等开放世界环境的通用智能体。尽管取得了令人鼓舞的成果，现有研究主要集中于解决基础程序性任务（如遵循游戏科技树进行材料收集和工具制作），并将"获取钻石"任务作为终极目标。这种局限性源于智能体可用动作集的狭隘定义，迫使其需从零开始学习有效的长周期策略，从而导致在开放世界中探索多样化游戏玩法变得困难。本研究提出Odyssey框架，通过赋予基于大语言模型（LLM）的智能体开放世界技能来探索广阔的《我的世界》。该框架包含三个核心组件：（1）具备开放世界技能库的交互式智能体，包含40项基础技能和183项组合技能；（2）基于39万+条来自《我的世界》百科问答指令数据集微调的LLaMA-3模型；（3）包含长期规划任务、动态即时规划任务和自主探索任务的新型智能体能力基准测试。大量实验表明，Odyssey框架能有效评估基于LLM智能体的各项能力。所有数据集、模型权重及代码均已开源，以推动更先进的自主智能体解决方案研究。

---

## [G\"odel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement](https://arxiv.org/abs/2410.04444)

### Abstract
arXiv:2410.04444v4 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has significantly enhanced the capabilities of AI-driven agents across various tasks. However, existing agentic systems, whether based on fixed pipeline algorithms or pre-defined meta-learning frameworks, cannot search the whole agent design space due to the restriction of human-designed components, and thus might miss the globally optimal agent design. In this paper, we introduce G\"odel Agent, a self-evolving framework inspired by the G\"odel machine, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms. G\"odel Agent leverages LLMs to dynamically modify its own logic and behavior, guided solely by high-level objectives through prompting. Experimental results on mathematical reasoning and complex agent tasks demonstrate that implementation of G\"odel Agent can achieve continuous self-improvement, surpassing manually crafted agents in performance, efficiency, and generalizability.

### 摘要
大语言模型（LLMs）的快速发展显著提升了AI驱动代理在各类任务中的能力。然而，现有代理系统无论是基于固定流程算法还是预定义的元学习框架，均因受限于人工设计组件而无法搜索整个代理设计空间，从而可能错过全局最优的代理设计方案。本文提出G"odel Agent——一个受哥德尔机器启发的自进化框架，使代理能够在不依赖预设流程或固定优化算法的情况下实现递归式自我改进。该框架利用大语言模型动态修改自身逻辑与行为，仅通过提示机制遵循高层目标引导。在数学推理和复杂代理任务上的实验结果表明，G"odel Agent的实现能够达成持续自我优化，在性能、效率与泛化能力方面均超越人工设计的代理系统。

---

## [Foundations and Recent Trends in Multimodal Mobile Agents: A Survey](https://arxiv.org/abs/2411.02006)

### Abstract
arXiv:2411.02006v2 Announce Type: replace 
Abstract: Mobile agents are essential for automating tasks in complex and dynamic mobile environments. As foundation models evolve, the demands for agents that can adapt in real-time and process multimodal data have grown. This survey provides a comprehensive review of mobile agent technologies, focusing on recent advancements that enhance real-time adaptability and multimodal interaction. Recent evaluation benchmarks have been developed better to capture the static and interactive environments of mobile tasks, offering more accurate assessments of agents' performance. We then categorize these advancements into two main approaches: prompt-based methods, which utilize large language models (LLMs) for instruction-based task execution, and training-based methods, which fine-tune multimodal models for mobile-specific applications. Additionally, we explore complementary technologies that augment agent performance. By discussing key challenges and outlining future research directions, this survey offers valuable insights for advancing mobile agent technologies. A comprehensive resource list is available at https://github.com/aialt/awesome-mobile-agents

### 摘要
移动智能体在复杂动态移动环境中实现任务自动化至关重要。随着基础模型的发展，对能够实时适应并处理多模态数据的智能体需求日益增长。本文综述全面回顾了移动智能体技术，重点关注提升实时适应性和多模态交互的最新进展。近期开发的评估基准能更好地捕捉移动任务的静态与交互环境，为智能体性能提供更精准的评估。我们将这些进展归纳为两大类方法：基于提示的方法（利用大语言模型执行指令任务）和基于训练的方法（针对移动应用微调多模态模型）。此外，我们还探讨了增强智能体性能的辅助技术。通过分析关键挑战并展望未来研究方向，本综述为推进移动智能体技术提供了重要见解。完整资源列表详见：https://github.com/aialt/awesome-mobile-agents

---

## [CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks](https://arxiv.org/abs/2406.13945)

### Abstract
arXiv:2406.13945v3 Announce Type: replace 
Abstract: As large language models (LLMs) continue to advance and gain widespread use, establishing systematic and reliable evaluation methodologies for LLMs and vision-language models (VLMs) has become essential to ensure their real-world effectiveness and reliability. There have been some early explorations about the usability of LLMs for limited urban tasks, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for urban research lies in the diversity of urban data, the complexity of application scenarios and the highly dynamic nature of the urban environment. In this paper, we design \textit&#123;CityBench&#125;, an interactive simulator based evaluation platform, as the first systematic benchmark for evaluating the capabilities of LLMs for diverse tasks in urban research. First, we build \textit&#123;CityData&#125; to integrate the diverse urban data and \textit&#123;CitySimu&#125; to simulate fine-grained urban dynamics. Based on \textit&#123;CityData&#125; and \textit&#123;CitySimu&#125;, we design 8 representative urban tasks in 2 categories of perception-understanding and decision-making as the \textit&#123;CityBench&#125;. With extensive results from 30 well-known LLMs and VLMs in 13 cities around the world, we find that advanced LLMs and VLMs can achieve competitive performance in diverse urban tasks requiring commonsense and semantic understanding abilities, e.g., understanding the human dynamics and semantic inference of urban images. Meanwhile, they fail to solve the challenging urban tasks requiring professional knowledge and high-level numerical abilities, e.g., geospatial prediction and traffic control task.

### 摘要
随着大语言模型(LLMs)的持续进步和广泛应用，建立系统可靠的LLMs和视觉语言模型(VLMs)评估方法对确保其实际有效性与可靠性变得至关重要。目前已有关于LLMs在有限城市任务中可用性的初步探索，但仍缺乏系统化且可扩展的评估基准。构建城市研究系统性评估基准的挑战在于城市数据的多样性、应用场景的复杂性以及城市环境的高度动态性。本文设计了基于交互式模拟器的评估平台\textit&#123;CityBench&#125;，作为首个系统性评估LLMs在城市研究中多样化任务能力的基准。首先，我们构建\textit&#123;CityData&#125;整合多元城市数据，并开发\textit&#123;CitySimu&#125;模拟细粒度城市动态。基于\textit&#123;CityData&#125;和\textit&#123;CitySimu&#125;，我们设计了包含感知-理解和决策两大类别共8项代表性城市任务的\textit&#123;CityBench&#125;。通过对全球13个城市30个知名LLMs和VLMs的广泛测试，发现先进模型在需要常识和语义理解能力的城市任务(如理解人类动态和城市图像语义推理)中表现优异；但在需要专业知识和高级数值能力的挑战性城市任务(如地理空间预测和交通控制)中仍存在不足。

---

## [CityGPT: Empowering Urban Spatial Cognition of Large Language Models](https://arxiv.org/abs/2406.13948)

### Abstract
arXiv:2406.13948v2 Announce Type: replace 
Abstract: Large language models(LLMs), with their powerful language generation and reasoning capabilities, have already achieved notable success in many domains, e.g., math and code generation. However, they often fall short when tackling real-life geospatial tasks within urban environments. This limitation stems from a lack of physical world knowledge and relevant data during training. To address this gap, we propose \textit&#123;CityGPT&#125;, a systematic framework designed to enhance LLMs' understanding of urban space and improve their ability to solve the related urban tasks by integrating a city-scale `world model' into the model. Firstly, we construct a diverse instruction tuning dataset, \textit&#123;CityInstruction&#125;, for injecting urban knowledge into LLMs and effectively boosting their spatial reasoning capabilities. Using a combination of \textit&#123;CityInstruction&#125; and open source general instruction data, we introduce a novel and easy-to-use self-weighted fine-tuning method (\textit&#123;SWFT&#125;) to train various LLMs (including ChatGLM3-6B, Llama3-8B, and Qwen2.5-7B) to enhance their urban spatial capabilities without compromising, or even improving, their general abilities. Finally, to validate the effectiveness of our proposed framework, we develop a comprehensive text-based spatial benchmark \textit&#123;CityEval&#125; for evaluating the performance of LLMs across a wide range of urban scenarios and geospatial tasks. Extensive evaluation results demonstrate that smaller LLMs trained with \textit&#123;CityInstruction&#125; by \textit&#123;SWFT&#125; method can achieve performance that is competitive with, and in some cases superior to, proprietary LLMs when assessed using \textit&#123;CityEval&#125;.

### 摘要
大型语言模型（LLMs）凭借其强大的语言生成与推理能力，已在数学和代码生成等诸多领域取得显著成功。然而，面对城市环境中的现实地理空间任务时，这些模型往往表现欠佳。这种局限性源于训练过程中缺乏物理世界知识及相关数据。为弥补这一不足，我们提出	extit&#123;CityGPT&#125;——一个通过整合城市级"世界模型"来增强LLMs对城市空间理解能力、提升其解决相关城市任务性能的系统性框架。首先，我们构建了多样化指令微调数据集	extit&#123;CityInstruction&#125;，用于向LLMs注入城市知识并有效提升其空间推理能力。通过结合	extit&#123;CityInstruction&#125;与开源通用指令数据，我们提出新颖易用的自加权微调方法（	extit&#123;SWFT&#125;），用于训练包括ChatGLM3-6B、Llama3-8B和Qwen2.5-7B在内的各类LLMs，在保持甚至提升其通用能力的同时增强城市空间处理能力。最后，为验证框架有效性，我们开发了基于文本的综合空间基准	extit&#123;CityEval&#125;，用于评估LLMs在多样化城市场景和地理空间任务中的表现。大量评估结果表明：采用	extit&#123;SWFT&#125;方法结合	extit&#123;CityInstruction&#125;训练的小型LLMs，在使用	extit&#123;CityEval&#125;评估时，其性能可与专有LLMs相媲美，部分场景甚至更优。

---

## [Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge](https://arxiv.org/abs/2411.09689)

### Abstract
arXiv:2411.09689v2 Announce Type: replace 
Abstract: LLM hallucination, where unfaithful text is generated, presents a critical challenge for LLMs' practical applications. Current detection methods often resort to external knowledge, LLM fine-tuning, or supervised training with large hallucination-labeled datasets. Moreover, these approaches do not distinguish between different types of hallucinations, which is crucial for enhancing detection performance. To address such limitations, we introduce hallucination probing, a new task that classifies LLM-generated text into three categories: aligned, misaligned, and fabricated. Driven by our novel discovery that perturbing key entities in prompts affects LLM's generation of these three types of text differently, we propose SHINE, a novel hallucination probing method that does not require external knowledge, supervised training, or LLM fine-tuning. SHINE is effective in hallucination probing across three modern LLMs, and achieves state-of-the-art performance in hallucination detection, outperforming seven competing methods across four datasets and four LLMs, underscoring the importance of probing for accurate detection.

### 摘要
大语言模型（LLM）幻觉现象（即生成不忠实文本）对其实际应用构成重大挑战。现有检测方法通常依赖外部知识、LLM微调或基于大规模幻觉标注数据集的监督训练。此外，这些方法未能区分不同类型的幻觉，而这对提升检测性能至关重要。为突破这些局限，我们提出"幻觉探测"新任务，将LLM生成文本分类为对齐型、错位型和虚构型三类。基于我们提出的新发现——提示中关键实体的扰动会差异化影响LLM生成这三类文本，我们开发了SHINE方法。这种无需外部知识、监督训练或LLM微调的新型幻觉探测方法，在三种现代LLM上均表现出色，并在四个数据集和四种LLM的测试中超越七种对比方法，创下幻觉检测的最优性能，证实了精准探测对检测效果的关键作用。

---

## [Are Your LLMs Capable of Stable Reasoning?](https://arxiv.org/abs/2412.13147)

### Abstract
arXiv:2412.13147v4 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has shown remarkable progress in complex reasoning tasks. However, a significant disparity exists between benchmark performances and real-world applications. We attribute this gap primarily to current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, especially in complex reasoning tasks where both accuracy and consistency are essential. In this paper, we introduce G-Pass@$k$, a novel evaluation metric that continuously assesses model performance across multiple sampling attempts, quantifying both the model's performance potential and its stability. Through extensive experiments on various public and newly constructed benchmarks, we employ G-Pass@$k$ in conjunction with state-of-the-art large language models to provide comprehensive insights into their potential capabilities and operational consistency. Our findings reveal a significant opportunity to enhance the realistic reasoning abilities of LLMs, underscoring the necessity for more robust evaluation metrics.

### 摘要
大型语言模型（LLMs）的快速发展在复杂推理任务中展现出显著进展。然而，基准测试表现与实际应用之间仍存在明显差距。我们认为这一差距主要源于当前评估方法和指标的局限性——它们未能全面捕捉LLMs的全部能力，尤其在需要同时保证准确性与一致性的复杂推理任务中。本文提出G-Pass@$k$这一新型评估指标，通过连续多次采样尝试来量化模型的性能潜力与稳定性。基于各类公开及新建基准测试的广泛实验，我们结合前沿大型语言模型运用G-Pass@$k$指标，全面揭示了模型的潜在能力与操作一致性。研究结果表明，提升LLMs现实推理能力存在重大改进空间，同时凸显了建立更强健评估指标的必要性。

---

## [Stepwise Reasoning Error Disruption Attack of LLMs](https://arxiv.org/abs/2412.11934)

### Abstract
arXiv:2412.11934v4 Announce Type: replace 
Abstract: Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/SEED-Attack.

### 摘要
大型语言模型（LLMs）在复杂推理任务中取得了显著进展，但其推理过程的安全性与鲁棒性仍未得到充分探索。现有针对LLM推理的攻击方法受限于特定场景或缺乏隐蔽性，制约了其可行性与泛化能力。为解决这些问题，我们提出逐步推理错误干扰攻击（SEED），该方法通过在前序推理步骤中巧妙注入错误，误导模型产生错误的后续推理与最终答案。与先前方法不同，SEED兼容零样本和小样本设置，保持自然推理流程，且无需修改指令即可实现隐蔽执行。在四个模型、四个数据集上的大量实验证明了SEED的有效性，揭示了LLMs在推理过程中易受干扰的脆弱性。这些发现表明需要更加重视LLM推理的鲁棒性，以确保实际应用中的安全性。代码已开源：https://github.com/Applied-Machine-Learning-Lab/SEED-Attack。

---

## [AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence](https://arxiv.org/abs/2502.13943)

### Abstract
arXiv:2502.13943v2 Announce Type: replace 
Abstract: Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.

### 摘要
当前训练过程奖励模型（PRMs）的方法通常依赖于基于规则的技术将响应分解为多个推理步骤，例如使用预定义的占位符标记或将推理步骤长度固定为统一尺寸。这些方法忽视了关键事实：特定词汇通常并不真正标记文本中的决策点。为此，我们提出AdaptiveStep方法，该方法根据模型预测下一个词的置信度来划分推理步骤。这种划分方式能在每个步骤提供更丰富的决策信息，从而提升奖励模型学习等下游任务性能。此外，我们的方法无需人工标注。通过在数学推理和代码生成任务中对AdaptiveStep训练的PRMs进行实验，我们验证了其有效性。实验结果表明，最终获得的PRM在Best-of-N性能上达到最先进水平，超越了采用词级价值引导解码的贪婪搜索策略，同时相比现有开源PRMs降低了30%以上的构建成本。我们还对PRM的性能表现、可迁移性和泛化能力进行了深入分析与案例研究。

---

## [Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning](https://arxiv.org/abs/2412.13631)

### Abstract
arXiv:2412.13631v3 Announce Type: replace 
Abstract: Theory of Mind (ToM) capabilities in LLMs have recently become a central object of investigation. Cognitive science distinguishes between two steps required for ToM tasks: 1) determine whether to invoke ToM, which includes the appropriate Depth of Mentalizing (DoM), or level of recursion required to complete a task; and 2) applying the correct inference given the DoM. In this position paper, we first identify several lines of work in different communities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and formal models for ToM. We argue that recent work in AI tends to focus exclusively on the second step which are typically framed as static logic problems. We conclude with suggestions for improved evaluation of ToM capabilities inspired by dynamic environments used in cognitive tasks.

### 摘要
大语言模型（LLM）的心理理论（ToM）能力近来成为研究焦点。认知科学区分了完成ToM任务所需的两个步骤：1）判断是否需要调用ToM，包括确定合适的心理化深度（DoM），即完成任务所需的递归层级；2）根据DoM进行正确推理。在本立场论文中，我们首先梳理了人工智能不同领域的研究脉络，包括LLM基准测试、ToM附加组件、ToM探测以及ToM形式化模型。我们指出，当前AI研究往往仅聚焦于第二步——这些研究通常被构建为静态逻辑问题。最后，我们借鉴认知任务中动态环境的设计思路，提出了改进ToM能力评估的建议。

---

## [From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs](https://arxiv.org/abs/2502.17701)

### Abstract
arXiv:2502.17701v2 Announce Type: replace 
Abstract: Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE

### 摘要
疏散决策预测对于高效、有效的野火响应至关重要，它通过帮助应急管理部门预判交通拥堵和瓶颈、分配资源以及最小化负面影响来实现这一目标。传统的疏散决策预测统计方法无法捕捉不同个体复杂多样的行为逻辑。本研究首次提出FLARE（基于大语言模型的野火疏散决策预测高级推理框架），该框架整合行为理论与模型，简化思维链（CoT）推理，并结合基于记忆的强化学习（RL）模块，以提供准确的疏散决策预测与理解。我们的方法解决了现有大语言模型在疏散行为预测中的局限性，如有限的调查数据、与行为理论不匹配、个体偏好冲突、隐含复杂心理状态以及难以处理的心理状态-行为映射问题。在三个野火后调查数据集上的实验表明，该方法相较于传统理论驱动行为模型的性能平均提升20.47%，并展现出强大的跨事件泛化能力。完整代码已公开于https://github.com/SusuXu-s-Lab/FLARE。

---

## [Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations](https://arxiv.org/abs/2501.15056)

### Abstract
arXiv:2501.15056v2 Announce Type: replace 
Abstract: Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues.

### 摘要
在对话系统中实现有效决策和问题解决，需要通过针对性提问来识别并获取缺失信息。核心挑战在于通过提出能最大限度降低不确定性的问题，高效缩小可能结果的大规模空间。为此，我们提出一种新颖框架：利用大型语言模型（LLMs）生成信息寻求型问题，并采用蒙特卡洛树搜索（MCTS）作为推理时规划的一部分，策略性地选择能最大化信息增益的问题。我们的主要贡献包括一种分层反馈机制，该机制利用历史交互模式指导未来策略。具体而言，每个新问题会根据语义相似度映射至特定聚类，而我们的UCT（树的上置信界）公式采用聚类特异性奖励加成，优先选择过去对同类问题有效的成功提问轨迹。在医疗诊断和技术故障排除领域的广泛实验表明，相较于现有技术，我们的方法平均实现12%的成功率提升，且每次对话的LLM规划调用次数减少约10倍。当初始可能性集合受限时，成功率还可额外平均提升8%。研究结果证实了反馈感知型MCTS在增强目标导向对话信息获取方面的有效性。

---

## [Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems](https://arxiv.org/abs/2503.00600)

### Abstract
arXiv:2503.00600v2 Announce Type: replace 
Abstract: AI-augmented data processing systems (DPSs) integrate large language models (LLMs) into query pipelines, allowing powerful semantic operations on structured and unstructured data. However, the reliability (a.k.a. trust) of these systems is fundamentally challenged by the potential for LLMs to produce errors, limiting their adoption in critical domains. To help address this reliability bottleneck, we introduce semantic integrity constraints (SICs) -- a declarative abstraction for specifying and enforcing correctness conditions over LLM outputs in semantic queries. SICs generalize traditional database integrity constraints to semantic settings, supporting common types of constraints, such as grounding, soundness, and exclusion, with both proactive and reactive enforcement strategies.
  We argue that SICs provide a foundation for building reliable and auditable AI-augmented data systems. Specifically, we present a system design for integrating SICs into query planning and runtime execution and discuss its realization in AI-augmented DPSs. To guide and evaluate the vision, we outline several design goals -- covering criteria around expressiveness, runtime semantics, integration, performance, and enterprise-scale applicability -- and discuss how our framework addresses each, along with open research challenges.

### 摘要
AI增强型数据处理系统（DPSs）将大语言模型（LLMs）集成到查询管道中，从而支持对结构化和非结构化数据的强大语义操作。然而，这些系统的可靠性（即可信度）从根本上受到LLMs可能产生错误的挑战，限制了其在关键领域的应用。为解决这一可靠性瓶颈，我们提出了语义完整性约束（SICs）——一种声明式抽象，用于在语义查询中指定并强制执行LLM输出的正确性条件。SICs将传统数据库完整性约束推广到语义环境，支持常见约束类型（如 grounding、soundness 和 exclusion），并提供主动和被动两种执行策略。

我们认为SICs为构建可靠且可审计的AI增强数据系统奠定了基础。具体而言，我们提出了一种将SICs集成到查询规划和运行时执行的系统设计，并探讨了其在AI增强DPSs中的实现。为引导和评估这一愿景，我们概述了若干设计目标——涵盖表达力、运行时语义、集成性、性能及企业级适用性等标准——并讨论了我们的框架如何逐一应对这些目标，同时提出了开放的研究挑战。

---

## [Large Language and Reasoning Models are Shallow Disjunctive Reasoners](https://arxiv.org/abs/2503.23487)

### Abstract
arXiv:2503.23487v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been found to struggle with systematic reasoning. Even on tasks where they appear to perform well, their performance often depends on shortcuts, rather than on genuine reasoning abilities, leading them to collapse on out-of-distribution (OOD) examples. Post-training strategies based on reinforcement learning and chain-of-thought prompting have recently been hailed as a step change. However, little is known about the potential of the resulting ``Large Reasoning Models'' (LRMs) beyond maths and programming-based problem solving, where genuine OOD problems can be sparse. In this paper, we focus on tasks that require systematic relational composition for qualitative spatial and temporal reasoning. The setting allows fine control over problem difficulty to precisely measure OOD generalization. We find that, zero-shot LRMs generally outperform their LLM counterparts in single-path reasoning tasks but struggle in the multi-path setting. Whilst showing comparatively better results, fine-tuned LLMs are also not capable of multi-path generalization. We also provide evidence for the behavioral interpretation for this, i.e., that LRMs are shallow disjunctive reasoners.

### 摘要
研究发现，大型语言模型（LLMs）在系统性推理任务上表现欠佳。即使在某些看似表现良好的任务中，其性能往往依赖于捷径而非真正的推理能力，导致它们在分布外（OOD）样本上表现崩溃。基于强化学习和思维链提示的训练后策略近期被推崇为重大突破。然而，对于由此产生的'大型推理模型'（LRMs）在数学和编程类问题解决之外的潜力，目前认知仍然有限——毕竟这类领域真正的OOD问题可能较为稀疏。本文聚焦于需要系统性关系组合的定性时空推理任务，该实验设置能通过精细控制问题难度来准确测量OOD泛化能力。研究发现：在单路径推理任务中，零样本LRMs通常优于LLMs；但在多路径场景下则表现欠佳。虽然微调后的LLMs显示出相对更好的结果，它们同样无法实现多路径泛化。我们还为此提供了行为学解释的证据，即LRMs本质上是浅层的析取推理器。

---

## [LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?](https://arxiv.org/abs/2503.19990)

### Abstract
arXiv:2503.19990v2 Announce Type: replace 
Abstract: Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of 20 state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90% accuracy. Furthermore, based on LEGO-Puzzles, we design generation tasks to investigate whether MLLMs can transfer their spatial understanding and reasoning abilities to image generation. Our experiments show that only GPT-4o and Gemini-2.0-Flash exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.

### 摘要
多步空间推理要求对多个连续步骤中的空间关系进行理解和推理，这对解决复杂现实应用（如机器人操控、自主导航和自动化装配）至关重要。为评估当前多模态大语言模型（MLLMs）是否具备这一基础能力，我们提出了LEGO-Puzzles——一个通过乐高任务评估MLLMs空间理解与序列推理能力的可扩展基准。该基准包含1,100个精心设计的视觉问答（VQA）样本，涵盖11类从基础空间理解到复杂多步推理的任务。基于LEGO-Puzzles，我们对20个前沿MLLMs进行了全面评估，发现其空间推理能力存在显著局限：即使最强大的模型仅能回答约半数测试案例，而人类参与者准确率超过90%。此外，我们设计了生成任务以探究MLLMs能否将空间理解与推理能力迁移至图像生成。实验表明，仅GPT-4o和Gemini-2.0-Flash展现出有限的指令跟随能力，其他模型则复制输入图像或生成完全无关的输出。总体而言，LEGO-Puzzles揭示了现有MLLMs在空间理解与序列推理上的关键缺陷，强调了多模态空间推理领域进一步发展的必要性。

---

## [Acting Less is Reasoning More! Teaching Model to Act Efficiently](https://arxiv.org/abs/2504.14870)

### Abstract
arXiv:2504.14870v2 Announce Type: replace 
Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools during long-form reasoning, such as search engines and code interpreters, to solve tasks beyond the capabilities of internal reasoning. While reinforcement learning (RL) has shown promise in training such agents, most of existing approaches typically optimize only for final correctness without considering the efficiency or necessity of external tool use. This often leads to excessive tool calling, incurring high computational costs and hindering the development of internal reasoning capabilities - a phenomenon known as \textit&#123;cognitive offloading&#125;. To this end, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers answer correctness and corresponding tool use behavior of model to reach that answer. To validate the effectiveness, we introduce the metric of \textit&#123;tool productivity&#125;, defined as the ratio between the number of correct answers and the total number of tool calls across all test cases. This metric reflects how efficiently tool usage contributes to successful task completion, with higher values indicating smarter and more autonomous reasoning. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 68.3\% and improves tool productivity by up to 215.4\%, while maintaining comparable answer accuracy.

### 摘要
工具集成推理（TIR）通过赋予大语言模型（LLMs）在长链推理过程中调用外部工具（如搜索引擎和代码解释器）的能力，使其能够解决超越内部推理能力的任务。尽管强化学习（RL）在训练此类智能体方面显示出潜力，但现有方法通常仅针对最终正确性进行优化，而未考虑外部工具使用的效率或必要性。这往往导致过多的工具调用，产生高昂的计算成本并阻碍内部推理能力的发展——这种现象被称为“认知卸载”。为此，我们提出了最优工具调用控制策略优化（OTC-PO），这是一个简单而有效的基于RL的框架，旨在鼓励模型以最少的工具调用生成准确答案。我们的方法引入了一种工具集成奖励机制，该机制综合考虑答案正确性及模型为达成该答案所对应的工具使用行为。为验证有效性，我们提出了“工具生产力”指标，定义为所有测试用例中正确答案数量与工具调用总次数的比值。该指标反映了工具使用对任务成功完成的贡献效率，数值越高表明推理更智能且更自主。我们在近端策略优化（PPO）和群体相对偏好优化（GRPO）中实例化了该框架，分别得到OTC-PPO和OTC-GRPO。在Qwen-2.5和Qwen-Math模型上的多问答基准实验表明，我们的方法在保持可比答案准确率的同时，最高可减少68.3%的工具调用，并将工具生产力提升达215.4%。

---

## [OmniRouter: Budget and Performance Controllable Multi-LLM Routing](https://arxiv.org/abs/2502.20576)

### Abstract
arXiv:2502.20576v5 Announce Type: replace 
Abstract: Large language models (LLMs) deliver superior performance but require substantial computational resources and operate with relatively low efficiency, while smaller models can efficiently handle simpler tasks with fewer resources. LLM routing is a crucial paradigm that dynamically selects the most suitable large language models from a pool of candidates to process diverse inputs, ensuring optimal resource utilization while maintaining response quality. Existing routing frameworks typically model this as a locally optimal decision-making problem, selecting the presumed best-fit LLM for each query individually, which overlook global budget constraints, resulting in ineffective resource allocation. To tackle this problem, we introduce OmniRouter, a fundamentally controllable routing framework for multi-LLM serving. Instead of making per-query greedy choices, OmniRouter models the routing task as a constrained optimization problem, assigning models that minimize total cost while ensuring the required performance level. Specifically, a hybrid retrieval-augmented predictor is designed to predict the capabilities and costs of LLMs and a constrained optimizer is employed to control globally optimal query-model allocation. Experiments show that OmniRouter achieves up to 6.30% improvement in response accuracy while simultaneously reducing computational costs by at least 10.15% compared to competitive router baselines. The code and the dataset are available at https://github.com/agiresearch/OmniRouter.

### 摘要
大语言模型（LLMs）虽能提供卓越性能，但需消耗大量计算资源且运行效率较低，而较小模型能以较少资源高效处理简单任务。LLM路由是一种关键范式，其动态从候选池中选择最合适的大语言模型处理多样化输入，在保证响应质量的同时实现资源最优利用。现有路由框架通常将其建模为局部最优决策问题，仅为每个查询单独选择预设最佳LLM，忽略了全局预算约束，导致资源分配效率低下。为解决该问题，我们提出OmniRouter——一个面向多LLM服务的本质可控路由框架。该框架摒弃逐查询贪婪选择策略，将路由任务建模为约束优化问题，在确保性能要求的前提下分配总成本最小化的模型。具体而言，我们设计了混合检索增强预测器来预估LLM的能力与成本，并采用约束优化器实现全局最优的查询-模型分配。实验表明，相较于竞争性路由基线，OmniRouter在响应准确率上最高提升6.30%，同时计算成本至少降低10.15%。代码与数据集已开源：https://github.com/agiresearch/OmniRouter。

---

## [On Meta-Prompting](https://arxiv.org/abs/2312.06562)

### Abstract
arXiv:2312.06562v3 Announce Type: replace-cross 
Abstract: Modern large language models (LLMs) are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them. Unlike traditional learners, LLMs cannot use back-propagation to obtain feedback, and condition their output in situ in a phenomenon known as in-context learning (ICL). Many approaches to prompting and pre-training these models involve the automated generation of these prompts, also known as meta-prompting, or prompting to obtain prompts. However, they do not formally describe the properties and behavior of the LLMs themselves. We propose a theoretical framework based on category theory to generalize and describe ICL and LLM behavior when interacting with users. Our framework allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches. Using our framework and experimental results we argue that meta-prompting is more effective than basic prompting at generating desirable outputs.

### 摘要
现代大型语言模型（LLMs）能够将输入字符串解释为指令或提示，并基于这些指令执行任务。与传统学习器不同，LLMs无法通过反向传播获取反馈，而是通过上下文学习（ICL）现象在当下情境中调节输出。针对这些模型的提示设计与预训练方法多涉及自动化生成提示（即元提示或通过提示获取提示），但均未对模型本身特性与行为进行形式化描述。我们提出基于范畴论的理论框架，用以概括和描述LLMs与用户交互时的ICL及行为特征。该框架使我们能够形式化论证任务无关性及各类元提示方法的等效性问题。通过理论框架与实验验证，我们论证了元提示在生成理想输出方面较基础提示更具效力。

---

## [An Insight into Security Code Review with LLMs: Capabilities, Obstacles, and Influential Factors](https://arxiv.org/abs/2401.16310)

### Abstract
arXiv:2401.16310v4 Announce Type: replace-cross 
Abstract: Security code review is a time-consuming and labor-intensive process typically requiring integration with automated security defect detection tools. However, existing security analysis tools struggle with poor generalization, high false positive rates, and coarse detection granularity. Large Language Models (LLMs) have been considered promising candidates for addressing those challenges. In this study, we conducted an empirical study to explore the potential of LLMs in detecting security defects during code review. Specifically, we evaluated the performance of six LLMs under five different prompts and compared them with state-of-the-art static analysis tools. We also performed linguistic and regression analyses for the best-performing LLM to identify quality problems in its responses and factors influencing its performance. Our findings showthat: (1) existing pre-trained LLMs have limited capability in security code review but significantly outperformthe state-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs when provided with a CWE list for reference. (3) GPT-4 frequently generates verbose or non-compliant responses with the task requirements given in the prompts. (4) GPT-4 is more adept at identifying security defects in code files with fewer tokens, containing functional logic, or written by developers with less involvement in the project.

### 摘要
安全代码审查是一个耗时且劳动密集型的过程，通常需要与自动化安全缺陷检测工具结合使用。然而，现有的安全分析工具普遍存在泛化能力差、误报率高和检测粒度粗等问题。大型语言模型（LLMs）被认为是解决这些挑战的有力候选方案。在本研究中，我们通过实证研究探讨了LLMs在代码审查过程中检测安全缺陷的潜力。具体而言，我们评估了六种LLMs在五种不同提示下的表现，并将其与最先进的静态分析工具进行了对比。此外，我们还对表现最佳的LLM进行了语言学和回归分析，以识别其响应中存在的质量问题及影响其性能的因素。研究结果表明：（1）现有的预训练LLMs在安全代码审查方面能力有限，但显著优于最先进的静态分析工具；（2）当提供CWE列表作为参考时，GPT-4在所有LLMs中表现最佳；（3）GPT-4生成的响应经常存在冗长或不符合提示任务要求的情况；（4）GPT-4更擅长识别代码量较少、包含功能逻辑或由项目参与度较低的开发者编写的代码文件中的安全缺陷。

---

## [StarVector: Generating Scalable Vector Graphics Code from Images and Text](https://arxiv.org/abs/2312.11556)

### Abstract
arXiv:2312.11556v4 Announce Type: replace-cross 
Abstract: Scalable Vector Graphics (SVGs) are vital for modern image rendering due to their scalability and versatility. Previous SVG generation methods have focused on curve-based vectorization, lacking semantic understanding, often producing artifacts, and struggling with SVG primitives beyond path curves. To address these issues, we introduce StarVector, a multimodal large language model for SVG generation. It performs image vectorization by understanding image semantics and using SVG primitives for compact, precise outputs. Unlike traditional methods, StarVector works directly in the SVG code space, leveraging visual understanding to apply accurate SVG primitives. To train StarVector, we create SVG-Stack, a diverse dataset of 2M samples that enables generalization across vectorization tasks and precise use of primitives like ellipses, polygons, and text. We address challenges in SVG evaluation, showing that pixel-based metrics like MSE fail to capture the unique qualities of vector graphics. We introduce SVG-Bench, a benchmark across 10 datasets, and 3 tasks: Image-to-SVG, Text-to-SVG generation, and diagram generation. Using this setup, StarVector achieves state-of-the-art performance, producing more compact and semantically rich SVGs.

### 摘要
可缩放矢量图形（SVG）因其可扩展性和多功能性成为现代图像渲染的关键技术。现有SVG生成方法主要基于曲线矢量化，缺乏语义理解能力，常产生伪影且难以处理路径曲线外的其他SVG图元。为此，我们提出StarVector——一种用于SVG生成的多模态大语言模型。该模型通过理解图像语义并运用SVG图元，实现紧凑精确的图像矢量化。与传统方法不同，StarVector直接在SVG代码空间操作，利用视觉理解能力精准应用各类SVG图元。为训练模型，我们构建了包含200万样本的多样化数据集SVG-Stack，支持矢量化任务的泛化能力及椭圆、多边形、文本等图元的精确使用。针对SVG评估难题，我们证明MSE等基于像素的指标无法捕捉矢量图形的独特性，进而提出跨10个数据集、覆盖图像到SVG生成、文本到SVG生成及图表生成3类任务的基准测试SVG-Bench。实验表明，StarVector在该框架下实现了最先进的性能，生成的SVG文件更紧凑且具有更丰富的语义信息。

---

## [CleanAgent: Automating Data Standardization with LLM-based Agents](https://arxiv.org/abs/2403.08291)

### Abstract
arXiv:2403.08291v4 Announce Type: replace-cross 
Abstract: Data standardization is a crucial part of the data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing different column types, simplifying the LLM's code generation with concise API calls. We first propose Dataprep.Clean, a component of the Dataprep Python Library, significantly reduces the coding complexity by enabling the standardization of specific column types with a single line of code. Then, we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent, data scientists only need to provide their requirements once, allowing for a hands-free process. To demonstrate the practical utility of CleanAgent, we developed a user-friendly web application, allowing users to interact with it using real-world datasets.

### 摘要
数据标准化是数据科学生命周期中的关键环节。尽管Pandas等工具提供了强大功能，但其复杂性及针对不同列类型定制代码所需的手动操作带来了重大挑战。虽然ChatGPT等大型语言模型（LLM）通过自然语言理解和代码生成在自动化该过程方面展现出潜力，但仍需要专家级编程知识和持续交互进行提示优化。为解决这些挑战，我们的核心思路是提出一个具有声明式统一API的Python库，用于标准化不同列类型，通过简洁的API调用简化LLM的代码生成。我们首先提出Dataprep.Clean——Dataprep Python库的组件，该组件通过单行代码实现特定列类型的标准化，显著降低了编码复杂度。随后，我们介绍集成Dataprep.Clean与基于LLM智能体的CleanAgent框架，以自动化数据标准化流程。使用CleanAgent时，数据科学家仅需一次性提供需求，即可实现全自动处理。为展示CleanAgent的实际效用，我们开发了用户友好的Web应用程序，支持用户使用真实数据集进行交互。

---

## [Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment](https://arxiv.org/abs/2405.00557)

### Abstract
arXiv:2405.00557v5 Announce Type: replace-cross 
Abstract: As the capabilities of large language models (LLMs) continue to expand, aligning these models with human values remains a significant challenge. Recent studies show that reasoning abilities contribute significantly to model safety, while integrating Mixture-of-Experts (MoE) architectures can further enhance alignment. In this work, we address a fundamental question: How to effectively incorporate reasoning abilities and MoE architectures into self-alignment process in LLMs? We propose Mixture of insighTful Experts (MoTE), a novel framework that synergistically combines reasoning chains and expert mixtures to improve self-alignments. From a data perspective, MoTE employs a structured reasoning chain comprising four key stages: Question Analysis, Answer Guidance, Safe Answer, and Safety Checking. This approach enhances safety through multi-step reasoning and proves effective even for smaller and less powerful LLMs (e.g., 7B models). From an architectural perspective, MoTE adopts a multi-LoRA framework with step-level routing, where each expert is dedicated to a specific reasoning step. This design eliminates the need for balance losses, ensures stable training, and supports adaptive inference lengths. Experimental results demonstrate that MoTE significantly improves model safety, jailbreak resistance, and over-refusal capabilities, achieving performance comparable to OpenAI's state-of-the-art o1 model.

### 摘要
随着大型语言模型（LLM）能力的持续扩展，如何使其与人类价值观保持一致仍是一项重大挑战。近期研究表明，推理能力对模型安全性具有显著贡献，而混合专家（MoE）架构的集成可进一步提升对齐效果。本研究致力于解决一个核心问题：如何将推理能力与MoE架构有效融入LLM的自对齐过程？我们提出'洞察专家混合'框架（MoTE），通过协同整合推理链与专家混合机制来优化自对齐性能。从数据层面，MoTE采用包含四个关键阶段的结构化推理链：问题分析、答案引导、安全回答及安全检查。这种多步推理方法不仅能增强安全性，即便对较小规模LLM（如70亿参数模型）也展现显著效果。从架构层面，MoTE采用具有步骤级路由的多LoRA框架，每位专家专精于特定推理步骤。该设计无需平衡损失函数即可确保训练稳定性，并支持自适应推理长度。实验结果表明，MoTE显著提升了模型安全性、抗越狱能力和过度拒绝处理能力，其性能可与OpenAI最先进的o1模型相媲美。

---

## [Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models](https://arxiv.org/abs/2405.17820)

### Abstract
arXiv:2405.17820v2 Announce Type: replace-cross 
Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in visual understanding and description, yet often suffer from hallucinations, attributing incorrect or misleading features to images. We observe that LVLMs disproportionately focus on a small subset of image tokens--termed blind tokens--which are typically irrelevant to the query (e.g., background or non-object regions). We hypothesize that such attention misalignment plays a key role in generating hallucinated responses. To mitigate this issue, we propose Attentional Vision Calibration (AvisC), a test-time approach that dynamically recalibrates the influence of blind tokens without modifying the underlying attention mechanism. AvisC first identifies blind tokens by analyzing layer-wise attention distributions over image tokens, then employs a contrastive decoding strategy to balance the influence of original and blind-token-biased logits. Experiments on standard benchmarks, including POPE, MME, and AMBER, demonstrate that AvisC effectively reduces hallucinations in LVLMs.

### 摘要
大型视觉语言模型（LVLMs）在视觉理解和描述方面展现出强大能力，但常存在幻觉问题，即对图像赋予错误或误导性特征。我们发现，LVLMs会过度关注图像中一小部分与查询无关的标记（称为盲标记，通常对应背景或非目标区域），并假设这种注意力错位是产生幻觉响应的关键因素。为缓解该问题，我们提出注意力视觉校准（AvisC），这是一种测试时方法，可在不修改底层注意力机制的情况下动态重新校准盲标记的影响。AvisC首先通过分析图像标记上的分层注意力分布来识别盲标记，随后采用对比解码策略来平衡原始逻辑和盲标记偏置逻辑的影响。在POPE、MME和AMBER等标准基准测试上的实验表明，AvisC能有效减少LVLMs的幻觉现象。

---

## [Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness](https://arxiv.org/abs/2405.18915)

### Abstract
arXiv:2405.18915v3 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.

### 摘要
思维链（CoT）提示在不同推理任务中表现各异。先前研究尝试对其评估，但未能深入分析影响CoT的内在规律。本文从有效性与忠实性两个维度研究CoT性能：针对前者，我们揭示了影响CoT有效性提升的关键因素，包括问题难度、信息增益与信息流；针对后者，通过联合分析问题、CoT与答案间的信息交互，阐释了不忠实CoT问题的成因。结果表明，当大语言模型预测答案时，能够从问题中召回CoT缺失的正确信息，从而导致该问题。基于此，我们提出一种创新算法以缓解该问题——通过从问题中召回额外信息来增强CoT生成，并依据信息增益评估CoT。大量实验证明，该方法能同步提升CoT的忠实性与有效性。

---

## [White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs](https://arxiv.org/abs/2404.10508)

### Abstract
arXiv:2404.10508v5 Announce Type: replace-cross 
Abstract: Social biases can manifest in language agency. However, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous works often rely on string-matching techniques to identify agentic and communal words within texts, falling short of accurately classifying language agency. We introduce the Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE tests for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. Using LABE, we unveil language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to demonstrate greater gender bias than human-written texts; (2) Models demonstrate remarkably higher levels of intersectional bias than the other bias aspects. (3) Prompt-based mitigation is unstable and frequently leads to bias exacerbation. Based on our observations, we propose Mitigation via Selective Rewrite (MSR), a novel bias mitigation strategy that leverages an agency classifier to identify and selectively revise parts of generated texts that demonstrate communal traits. Empirical results prove MSR to be more effective and reliable than prompt-based mitigation method, showing a promising research direction.

### 摘要
社会偏见可能通过语言能动性显现。然而，目前针对大语言模型（LLM）生成内容中此类偏见的研究极为有限。此外，先前研究多依赖字符串匹配技术识别文本中的能动型与亲和型词汇，难以准确分类语言能动性。我们提出语言能动性偏见评估基准（LABE），通过分析模型生成文本中不同人口群体被赋予的能动性水平，全面评估LLM的偏见表现。LABE针对3类文本生成任务（人物传记、教授评述和推荐信），检测LLM在性别、种族及交叉维度上的语言能动性偏见。借助LABE基准，我们揭示了ChatGPT、Llama3和Mistral这3个前沿LLM的语言能动性社会偏见，主要发现包括：（1）LLM生成文本比人类撰写文本表现出更显著的性别偏见；（2）模型在交叉维度上的偏见水平远超其他偏见类型；（3）基于提示的缓解策略效果不稳定且常加剧偏见。基于这些发现，我们提出选择性重写缓解策略（MSR），该创新方法通过能动性分类器识别并选择性修改生成文本中呈现亲和型特质的片段。实证结果表明，MSR相比基于提示的缓解方法更具效力与可靠性，为相关研究提供了新方向。

---

## [SongComposer: A Large Language Model for Lyric and Melody Generation in Song Composition](https://arxiv.org/abs/2402.17645)

### Abstract
arXiv:2402.17645v2 Announce Type: replace-cross 
Abstract: Creating lyrics and melodies for the vocal track in a symbolic format, known as song composition, demands expert musical knowledge of melody, an advanced understanding of lyrics, and precise alignment between them. Despite achievements in sub-tasks such as lyric generation, lyric-to-melody, and melody-to-lyric, etc, a unified model for song composition has not yet been achieved. In this paper, we introduce SongComposer, a pioneering step towards a unified song composition model that can readily create symbolic lyrics and melodies following instructions. SongComposer is a music-specialized large language model (LLM) that, for the first time, integrates the capability of simultaneously composing lyrics and melodies into LLMs by leveraging three key innovations: 1) a flexible tuple format for word-level alignment of lyrics and melodies, 2) an extended tokenizer vocabulary for song notes, with scalar initialization based on musical knowledge to capture rhythm, and 3) a multi-stage pipeline that captures musical structure, starting with motif-level melody patterns and progressing to phrase-level structure for improved coherence. Extensive experiments demonstrate that SongComposer outperforms advanced LLMs, including GPT-4, in tasks such as lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation. Moreover, we will release SongCompose, a large-scale dataset for training, containing paired lyrics and melodies in Chinese and English.

### 摘要
以符号化格式为人声轨道创作歌词和旋律（称为歌曲创作）需要具备专业的旋律音乐知识、对歌词的深刻理解以及两者之间的精确对齐。尽管在歌词生成、歌词到旋律、旋律到歌词等子任务中已取得成果，但尚未实现统一的歌曲创作模型。本文提出SongComposer，这是向统一歌曲创作模型迈出的开创性一步，该模型能够根据指令轻松生成符号化歌词和旋律。SongComposer是一个音乐专用的大语言模型（LLM），通过三项关键创新首次将同时创作歌词和旋律的能力整合到LLM中：1）采用灵活的元组格式实现歌词与旋律的单词级对齐，2）为歌曲音符扩展分词器词汇表，并基于音乐知识进行标量初始化以捕捉节奏，3）采用多阶段流程捕捉音乐结构，从动机级旋律模式开始，逐步构建乐句级结构以提升连贯性。大量实验表明，SongComposer在歌词到旋律生成、旋律到歌词生成、歌曲续写和文本到歌曲创作等任务中表现优于GPT-4等先进LLM。此外，我们将发布用于训练的大规模数据集SongCompose，其中包含中英文配对的歌词和旋律。

---

## [Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions](https://arxiv.org/abs/2405.03205)

### Abstract
arXiv:2405.03205v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have demonstrated considerable success across diverse tasks, including multiple-choice questions (MCQs). However, these models exhibit a positional bias, particularly an even worse anchored bias in the GPT-2 family, where they consistently favour the first choice 'A' in MCQs during inference. This anchored bias challenges the integrity of GPT-2's decision-making process, as it skews performance based on the position rather than the content of the choices in MCQs. In this study, we utilise the mechanistic interpretability approach to identify the internal modules within GPT-2 models responsible for this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention heads, using the "logit lens" method to trace and modify the specific value vectors that contribute to the bias. By updating these vectors within MLP and recalibrating attention patterns to neutralise the preference for the first choice 'A', we effectively mitigate the anchored bias. Our interventions not only mitigate the bias but also improve the overall MCQ prediction accuracy for the GPT-2 family across various datasets. This work represents the first comprehensive mechanistic analysis of anchored bias from the failing cases in MCQs within the GPT-2 models, introducing targeted, minimal-intervention strategies that significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.

### 摘要
大型语言模型（LLMs），如GPT-4和LLaMA系列，在多项任务中展现出显著成效，包括多项选择题（MCQs）的解答。然而，这些模型存在位置偏差问题，其中GPT-2系列尤为严重，表现为推理过程中持续偏向选择第一选项'A'的锚定偏差。这种锚定偏差挑战了GPT-2决策过程的公正性，因其表现受选项位置而非内容的影响。本研究采用机制可解释性方法，识别了GPT-2模型中导致该偏差的内部模块。我们聚焦于多层感知机（MLP）层和注意力头，运用"logit lens"技术追踪并修正引发偏差的特定值向量。通过更新MLP中的这些向量，并重新校准注意力模式以消除对首选项'A'的偏好，我们有效缓解了锚定偏差。干预措施不仅减轻了偏差，还提升了GPT-2系列在多个数据集上的MCQs预测准确率。此项工作首次从GPT-2模型MCQs失败案例出发，对锚定偏差进行了全面机制分析，提出了针对性、最小干预策略，显著增强了GPT-2模型在MCQs中的鲁棒性与准确性。代码详见https://github.com/ruizheliUOA/Anchored_Bias_GPT2。

---

## [Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine](https://arxiv.org/abs/2406.02394)

### Abstract
arXiv:2406.02394v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as ChatGPT demonstrate significant potential in the medical domain and are often evaluated using multiple-choice questions (MCQs) modeled on exams like the USMLE. However, such benchmarks may overestimate true clinical understanding by rewarding pattern recognition and test-taking heuristics. To investigate this, we created a fictional medical benchmark centered on an imaginary organ, the Glianorex, allowing us to separate memorized knowledge from reasoning ability. We generated textbooks and MCQs in English and French using leading LLMs, then evaluated proprietary, open-source, and domain-specific models in a zero-shot setting. Despite the fictional content, models achieved an average score of 64%, while physicians scored only 27%. Fine-tuned medical models outperformed base models in English but not in French. Ablation and interpretability analyses revealed that models frequently relied on shallow cues, test-taking strategies, and hallucinated reasoning to identify the correct choice. These results suggest that standard MCQ-based evaluations may not effectively measure clinical reasoning and highlight the need for more robust, clinically meaningful assessment methods for LLMs.

### 摘要
诸如ChatGPT之类的大型语言模型（LLM）在医学领域展现出显著潜力，其评估通常采用基于美国医师执照考试（USMLE）等标准化考试设计的多选题（MCQ）。然而，这类基准测试可能因奖励模式识别和应试启发式策略而高估真实的临床理解能力。为探究此问题，我们构建了一个以虚构器官'Glianorex'为核心的医学基准测试，从而区分记忆性知识与推理能力。通过领先的LLM生成英文和法文教材及多选题，我们在零样本设置下评估了专有模型、开源模型及领域专用模型的表现。尽管测试内容完全虚构，模型平均得分仍达64%，而医师组仅获27%。经微调的医学模型在英文测试中优于基础模型，但在法文测试中未显优势。消融实验与可解释性分析表明，模型常依赖浅层线索、应试策略及虚构推理来选择正确答案。这些结果表明，基于多选题的标准评估可能无法有效衡量临床推理能力，并凸显了对LLM进行更稳健、更具临床意义评估方法的迫切需求。

---

## [BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning](https://arxiv.org/abs/2406.17764)

### Abstract
arXiv:2406.17764v3 Announce Type: replace-cross 
Abstract: This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual in-context knowledge editing (IKE) across 53 languages, unifying three knowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff. Cross-lingual KE, which requires knowledge edited in one language to generalize across others while preserving unrelated knowledge, remains underexplored. To address this gap, we systematically evaluate IKE under zero-shot, one-shot, and few-shot setups, incorporating tailored metric-specific demonstrations. Our findings reveal that model scale and demonstration alignment critically govern cross-lingual IKE efficacy, with larger models and tailored demonstrations significantly improving performance. Linguistic properties, particularly script type, strongly influence performance variation across languages, with non-Latin languages underperforming due to issues like language confusion. Code and data are publicly available at: https://github.com/ercong21/MultiKnow/.

### 摘要
本文介绍了BMIKE-53，这是一个涵盖53种语言的跨语言上下文知识编辑（IKE）综合基准，整合了zsRE、CounterFact和WikiFactDiff三个知识编辑（KE）数据集。跨语言KE要求以一种语言编辑的知识能泛化至其他语言，同时保持无关知识的完整性，该领域目前研究尚不充分。为填补这一空白，我们在零样本、单样本和少样本设置下系统评估了IKE方法，并融入了针对特定指标的定制化示例。研究发现，模型规模和示例对齐对跨语言IKE效果具有关键影响，更大规模的模型和定制化示例能显著提升性能。语言特性（特别是文字类型）会强烈影响不同语言间的性能差异，非拉丁语系语言由于语言混淆等问题表现欠佳。代码和数据已公开于：https://github.com/ercong21/MultiKnow/。

---

## [When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings](https://arxiv.org/abs/2407.04503)

### Abstract
arXiv:2407.04503v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors, existing studies have largely overlooked the collective behaviors and information distortions arising from iterated LLM interactions. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states. In a series of telephone game experiments, we apply a transmission chain design borrowed from the human cultural evolution literature: LLM agents iteratively receive, produce, and transmit texts from the previous to the next agent in the chain. By tracking the evolution of text toxicity, positivity, difficulty, and length across transmission chains, we uncover the existence of biases and attractors, and study their dependence on the initial text, the instructions, language model, and model size. For instance, we find that more open-ended instructions lead to stronger attraction effects compared to more constrained tasks. We also find that different text properties display different sensitivity to attraction effects, with toxicity leading to stronger attractors than length. These findings highlight the importance of accounting for multi-step transmission dynamics and represent a first step towards a more comprehensive understanding of LLM cultural dynamics.

### 摘要
随着大型语言模型（LLMs）开始相互交互并在线上生成越来越多的文本，更好地理解信息在LLM间传递时的转化过程变得至关重要。尽管已有大量研究关注单个LLM的行为，但现有工作普遍忽视了迭代LLM交互中产生的集体行为和信息失真现象。在单次输出中微不足道的微小偏差，可能在迭代交互中被放大，导致内容逐渐演变为吸引子状态。通过一系列"传话游戏"实验，我们采用人类文化演化研究中的传输链设计：LLM代理迭代地接收、生成并将文本传递给链中的下一个代理。通过追踪文本毒性、积极性、难度和长度在传输链中的演化规律，我们揭示了偏差与吸引子的存在，并研究了其对初始文本、指令、语言模型及模型规模的依赖性。例如，我们发现相比约束性任务，开放性指令会导致更强的吸引效应。同时，不同文本属性对吸引效应的敏感性存在差异，其中毒性的吸引子强度显著高于文本长度。这些发现强调了考虑多步传输动力学的重要性，为更全面理解LLM文化动态迈出了第一步。

---

## [LETS-C: Leveraging Text Embedding for Time Series Classification](https://arxiv.org/abs/2407.06533)

### Abstract
arXiv:2407.06533v2 Announce Type: replace-cross 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.

### 摘要
语言建模技术的最新进展表明，其在时间序列数据应用中展现出良好效果。特别是在时间序列分类任务中，对预训练大语言模型（LLM）进行微调已在标准基准测试上取得了最先进的性能表现。然而，这类基于LLM的模型由于参数量庞大（可训练参数达数百万级），存在显著缺陷。本文提出了一种创新方法，通过利用文本嵌入模型对时间序列进行编码，再结合由卷积神经网络（CNN）和多层感知机（MLP）构成的简易分类头，从而在时间序列领域有效运用语言建模的成功经验。我们在权威时间序列分类基准上开展了大量实验，结果表明LETS-C不仅分类准确率超越当前最优模型，还提供了轻量化解决方案——平均仅需使用最优模型14.5%的可训练参数量。本研究证明，采用文本嵌入模型编码时间序列数据并搭配高效简易的分类头，为实现高性能时间序列分类同时保持轻量级模型架构提供了可行方向。

---

## [Curriculum Learning with Quality-Driven Data Selection](https://arxiv.org/abs/2407.00102)

### Abstract
arXiv:2407.00102v2 Announce Type: replace-cross 
Abstract: The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have generated significant interest in the development of Multimodal Large Language Models (MLLMs). Visual instruction tuning of MLLMs with machine-generated instruction-following data has shown to enhance zero-shot capabilities across various tasks. However, there has been limited exploration into controlling the quality of the instruction data.Current methodologies for data selection in MLLMs often rely on single, unreliable scores or use downstream tasks for selection, which is time-consuming and can lead to potential overfitting on the chosen evaluation datasets. To mitigate these limitations, we propose a novel data selection methodology that utilizes image-text correlation and model perplexity to evaluate and select data of varying quality. This approach leverages the distinct distribution of these two attributes, mapping data quality into a two-dimensional space that allows for the selection of data based on their location within this distribution. By utilizing this space, we can analyze the impact of task type settings, used as prompts, on data quality. Additionally, this space can be used to construct multi-stage subsets of varying quality to facilitate curriculum learning. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in five commonly assessed capabilities compared to using the complete dataset. Our codes, data, and models are publicly available at: https://anonymous.4open.science/r/EHIT-31B4

### 摘要
OpenAI的GPT-4展现出的卓越多模态能力引发了人们对多模态大语言模型（MLLMs）开发的广泛关注。通过机器生成的指令跟随数据对MLLMs进行视觉指令微调，已被证明能提升模型在各类任务中的零样本能力。然而，目前对指令数据质量控制的探索仍较为有限。现有MLLMs数据选择方法通常依赖单一不可靠的评分，或使用下游任务进行筛选，这种方法耗时且可能导致在选定评估数据集上的过拟合。为克服这些局限，我们提出了一种新颖的数据选择方法，利用图文相关性和模型困惑度来评估和筛选不同质量的数据。该方法通过这两种属性的独特分布，将数据质量映射到二维空间，从而根据数据在该分布中的位置进行选择。借助这一空间，我们可以分析任务类型设置（作为提示）对数据质量的影响。此外，该空间还可用于构建不同质量的多阶段子集以促进课程学习。我们的研究在多个数据集上进行了全面实验，结果表明相较于使用完整数据集，该方法在五种常用评估能力上均有显著提升。相关代码、数据及模型已公开于：https://anonymous.4open.science/r/EHIT-31B4

---

## [Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options](https://arxiv.org/abs/2409.00113)

### Abstract
arXiv:2409.00113v3 Announce Type: replace-cross 
Abstract: This work introduces a novel framework for evaluating LLMs' capacity to balance instruction-following with critical reasoning when presented with multiple-choice questions containing no valid answers. Through systematic evaluation across arithmetic, domain-specific knowledge, and high-stakes medical decision tasks, we demonstrate that post-training aligned models often default to selecting invalid options, while base models exhibit improved refusal capabilities that scale with model size. Our analysis reveals that alignment techniques, though intended to enhance helpfulness, can inadvertently impair models' reflective judgment--the ability to override default behaviors when faced with invalid options. We additionally conduct a parallel human study showing similar instruction-following biases, with implications for how these biases may propagate through human feedback datasets used in alignment. We provide extensive ablation studies examining the impact of model size, training techniques, and prompt engineering. Our findings highlight fundamental tensions between alignment optimization and preservation of critical reasoning capabilities, with important implications for developing more robust AI systems for real-world deployment.

### 摘要
本研究提出了一种新颖的评估框架，用于检验大语言模型在面对无正确答案的多选题时，平衡指令遵循与批判性推理的能力。通过对算术运算、领域专业知识和高风险医疗决策任务的系统评估，我们发现经过训练对齐的模型往往默认选择无效选项，而基础模型则表现出随模型规模提升的拒绝能力。分析表明，尽管对齐技术旨在提升模型的帮助性，却可能无意中损害其反思判断能力——即在面对无效选项时超越默认行为的能力。我们还开展了平行人类研究，揭示了类似的指令遵循偏差，这对理解此类偏差如何通过对齐过程中使用的人类反馈数据集传播具有重要意义。通过大量消融实验，我们考察了模型规模、训练技术和提示工程的影响。研究结果揭示了对齐优化与批判性推理能力保持之间的根本性矛盾，这对开发更具鲁棒性的实际应用人工智能系统具有重要启示。

---

## [Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation](https://arxiv.org/abs/2408.03505)

### Abstract
arXiv:2408.03505v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have extended the success of large language models (LLMs) to multiple data types, such as image, text and audio, achieving significant performance in various domains, including multimodal translation, visual question answering and content generation. Nonetheless, existing systems are inefficient to train MLLMs due to substantial GPU bubbles caused by the heterogeneous modality models and complex data dependencies in 3D parallelism. This paper proposes Optimus, a distributed MLLM training system that reduces end-to-end MLLM training time. Optimus is based on our principled analysis that scheduling the encoder computation within the LLM bubbles can reduce bubbles in MLLM training. To make scheduling encoder computation possible for all GPUs, Optimus searches the separate parallel plans for encoder and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM bubbles without breaking the original data dependencies in the MLLM model architecture. We further decompose encoder layer computation into a series of kernels, and analyze the common bubble pattern of 3D parallelism to carefully optimize the sub-millisecond bubble scheduling, minimizing the overall training time. Our experiments in a production cluster show that Optimus accelerates MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs compared to baselines.

### 摘要
多模态大语言模型（MLLMs）将大语言模型（LLMs）的成功扩展至图像、文本和音频等多种数据类型，在多模态翻译、视觉问答和内容生成等领域取得了显著成效。然而，现有系统因异构模态模型和三维并行中复杂的数据依赖性导致的GPU计算空泡问题，导致MLLMs训练效率低下。本文提出Optimus——一种可缩短端到端MLLM训练时间的分布式训练系统。基于"在LLM计算空泡期间调度编码器计算可减少MLLM训练空泡"的原理分析，Optimus通过为编码器和LLM分别搜索并行方案，并采用空泡调度算法在不破坏MLLM模型架构原始数据依赖性的前提下利用LLM空泡。我们进一步将编码器层计算分解为系列内核，通过分析三维并行的典型空泡模式，精细优化亚毫秒级空泡调度以最小化总体训练时间。生产集群实验表明：在3072块GPU上使用ViT-22B和GPT-175B模型时，Optimus相较基线方法可加速MLLM训练20.5%-21.3%。

---

## [CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs](https://arxiv.org/abs/2409.05806)

### Abstract
arXiv:2409.05806v4 Announce Type: replace-cross 
Abstract: Chinese, as a linguistic system rich in depth and complexity, is characterized by distinctive elements such as ancient poetry, proverbs, idioms, and other cultural constructs. However, current Large Language Models (LLMs) face limitations in these specialized domains, highlighting the need for the development of comprehensive datasets that can assess, continuously update, and progressively improve these culturally-grounded linguistic competencies through targeted training optimizations. To address this gap, we introduce CKnowEdit, the first-ever Chinese knowledge editing dataset designed to correct linguistic, factual, and logical errors in LLMs. We collect seven types of knowledge from a wide range of sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, taking into account the unique polyphony, antithesis, and logical structures inherent in the Chinese language. By analyzing this dataset, we highlight the challenges current LLMs face in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques reveals opportunities to advance the correction of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.

### 摘要
作为兼具深度与复杂性的语言体系，中文以古诗、谚语、成语等独特文化元素为特征。然而当前大语言模型（LLMs）在这些专业领域存在明显局限，亟需构建能够通过针对性训练优化来评估、持续更新并逐步提升这类文化语言能力的综合性数据集。为此，我们推出首个中文知识编辑数据集CKnowEdit，旨在修正大语言模型中的语言性、事实性与逻辑性错误。我们从经典文献、成语及百度贴吧弱智吧等多源渠道收集七类知识数据，充分考虑汉语特有的多音对仗与逻辑结构特性。通过数据集分析，我们揭示了当前大语言模型在中文掌握方面的核心挑战。此外，基于对最前沿知识编辑技术的评估，我们发现了推进中文知识校正的研究机遇。代码与数据集详见https://github.com/zjunlp/EasyEdit。

---

## [Content Moderation by LLM: From Accuracy to Legitimacy](https://arxiv.org/abs/2409.03219)

### Abstract
arXiv:2409.03219v2 Announce Type: replace-cross 
Abstract: One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy -- the extent to which LLMs make correct decisions about content. This article argues that accuracy is insufficient and misleading because it fails to grasp the distinction between easy cases and hard cases, as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLMs is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework for evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed, and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLMs' real potential in moderation is not accuracy improvement. Rather, LLMs can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. To realize these contributions, this article proposes a workflow for incorporating LLMs into the content moderation system. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLMs' role in content moderation and redirect relevant research in this field.

### 摘要
当前，大语言模型（LLM）的一个热门应用是用于在线平台的内容审核。现有研究主要关注准确性指标——即LLM对内容做出正确决策的程度。本文认为，仅关注准确性既不足够又具有误导性，因为它未能区分简单案例与疑难案例，也忽视了提高准确性过程中必然存在的权衡。深入分析表明，内容审核是平台治理的构成性环节，其核心在于获取并增强合法性。LLM的主要目标并非确保审核决策的正确性，而是使其具有合法性。据此，本文提出应从单一的准确性基准转向基于合法性的评估框架。该框架指出：对于简单案例，关键在于保证准确性、速度与透明度；而对于疑难案例，重点在于提供合理理由说明与用户参与机制。在此框架下审视发现，LLM在审核中的真正潜力并非准确性提升，而是体现在四个方面：实现疑难案例与简单案例的筛选分流、为审核决策提供高质量解释、协助人工审核者获取更多情境信息、以更具交互性的方式促进用户参与。为实现这些价值，本文设计了LLM融入内容审核系统的工作流程。通过运用法学与社会科学的规范理论对这一新技术应用进行批判性评估，本文旨在重新界定LLM在内容审核中的角色，并引导该领域的后续研究方向。

---

## [A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders](https://arxiv.org/abs/2409.14507)

### Abstract
arXiv:2409.14507v5 Announce Type: replace-cross 
Abstract: Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features ("math" may split into "algebra", "geometry", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get "absorbed" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.

### 摘要
稀疏自编码器（SAEs）旨在将大型语言模型（LLMs）的激活空间分解为人类可解释的潜在方向或特征。随着SAE中特征数量的增加，层级特征往往会分裂为更细粒度的特征（例如"数学"可能分裂为"代数"、"几何"等），这种现象被称为特征分裂。然而，我们发现层级特征的稀疏分解与分裂并不稳健。具体而言，研究表明看似单一语义的特征无法在预期位置激活，反而被"吸收"进其子特征中。我们将此现象命名为特征吸收，并证明其成因在于当底层特征形成层级结构时，SAE对稀疏性的优化所致。我们提出了一种检测SAE中特征吸收的度量方法，并在数百个LLM SAE上实证验证了研究发现。调查表明，改变SAE规模或稀疏度不足以解决该问题。我们探讨了SAE中特征吸收现象的影响，并提出了在SAE能够稳健且大规模应用于LLM解释之前，解决这一根本性理论问题的若干潜在途径。

---

## [Multimodal Banking Dataset: Understanding Client Needs through Event Sequences](https://arxiv.org/abs/2409.17587)

### Abstract
arXiv:2409.17587v2 Announce Type: replace-cross 
Abstract: Financial organizations collect a huge amount of temporal (sequential) data about clients, which is typically collected from multiple sources (modalities). Despite the urgent practical need, developing deep learning techniques suitable to handle such data is limited by the absence of large open-source multi-source real-world datasets of event sequences. To fill this gap, which is mainly caused by security reasons, we present the first industrial-scale publicly available multimodal banking dataset, MBD, that contains information on more than 2M corporate clients of a large bank. Clients are represented by several data sources: 950M bank transactions, 1B geo position events, 5M embeddings of dialogues with technical support, and monthly aggregated purchases of four bank products. All entries are properly anonymized from real proprietary bank data, and the experiments confirm that our anonymization still saves all significant information for introduced downstream tasks. Moreover, we introduce a novel multimodal benchmark suggesting several important practical tasks, such as future purchase prediction and modality matching. The benchmark incorporates our MBD and two public financial datasets. We provide numerical results for the state-of-the-art event sequence modeling techniques including large language models and demonstrate the superiority of fusion baselines over single-modal techniques for each task. Thus, MBD provides a valuable resource for future research in financial applications of multimodal event sequence analysis.
  HuggingFace Link: https://huggingface.co/datasets/ai-lab/MBD
  Github Link: https://github.com/Dzhambo/MBD

### 摘要
金融机构收集了大量关于客户的时间序列数据，这些数据通常来自多种来源（模态）。尽管存在迫切的实践需求，但由于缺乏开源的大规模多源真实事件序列数据集，开发适用于处理此类数据的深度学习技术受到限制。为填补这一主要由安全因素导致的研究空白，我们提出了首个工业级公开可用的多模态银行数据集MBD，该数据集包含一家大型银行超过200万企业客户的信息。客户数据通过以下多源形式呈现：9.5亿笔银行交易、10亿条地理位置事件、500万条技术支持对话的嵌入向量，以及四种银行产品的月度聚合购买记录。所有条目均从真实的银行专有数据中经过严格匿名化处理，实验证实我们的匿名化方法仍能完整保留下游任务所需的关键信息。此外，我们提出了一个创新的多模态基准测试，包含未来购买预测和模态匹配等若干重要实践任务。该基准整合了我们的MBD数据集和两个公开金融数据集。我们为包括大语言模型在内的最先进事件序列建模技术提供了数值结果，并证明在所有任务中多模态融合基线方法均优于单模态技术。因此，MBD为多模态事件序列分析在金融领域的未来研究提供了宝贵资源。

---

## [SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation](https://arxiv.org/abs/2410.03960)

### Abstract
arXiv:2410.03960v3 Announce Type: replace-cross 
Abstract: LLM inference for enterprise applications, such as summarization, RAG, and code-generation, typically observe much longer prompt than generations, leading to high prefill cost and response latency. We present SwiftKV, a novel model transformation and distillation procedure targeted at reducing the prefill compute (in FLOPs) of prompt tokens while preserving high generation quality. First, SwiftKV prefills later layers' KV cache using an earlier layer's output, allowing prompt tokens to skip those later layers. Second, SwiftKV employs a lightweight knowledge-preserving distillation procedure that can adapt existing LLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV cache compression to improve inference performance in low-memory scenarios. Our comprehensive experiments show that SwiftKV can effectively reduce prefill computation by 25-50% across several LLM families while incurring minimum quality degradation. In the end-to-end inference serving, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at https://github.com/snowflakedb/arctictraining.

### 摘要
企业应用中的大语言模型推理（如摘要生成、检索增强生成和代码生成）通常面临提示文本远长于生成内容的情况，导致预填充计算成本高昂且响应延迟显著。本文提出SwiftKV——一种创新的模型转换与蒸馏方法，旨在降低提示令牌的预填充计算量（以浮点运算计），同时保持高质量的生成效果。首先，SwiftKV利用浅层输出来预填充深层键值缓存，使提示令牌可跳过后续层计算。其次，该方法采用轻量级的知识保留蒸馏技术，能以极小的精度损失适配现有大语言模型。第三，SwiftKV可天然集成键值缓存压缩技术，提升内存受限场景下的推理性能。综合实验表明，SwiftKV能在多种大语言模型架构上有效减少25-50%的预填充计算量，且质量损失极小。在端到端推理服务中，SwiftKV实现了最高2倍的总吞吐量提升和60%的单令牌输出时间降低。其归一化推理吞吐量可达惊人的560 TFlops/GPU，相当于Llama-3.1-70B模型每秒处理16K令牌。本技术已开源于https://github.com/snowflakedb/arctictraining。

---

## [Exploring Model Kinship for Merging Large Language Models](https://arxiv.org/abs/2410.12613)

### Abstract
arXiv:2410.12613v2 Announce Type: replace-cross 
Abstract: Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available at https://github.com/zjunlp/ModelKinship.

### 摘要
模型融合已成为增强大语言模型（LLM）能力与效率的关键技术之一。然而，我们对任意两个模型融合后的预期性能增益及其原理的理解仍十分有限。本研究提出模型亲缘性概念——类比生物进化过程中LLM之间的相似性或关联程度。通过全面实证分析，我们发现模型亲缘性与融合后的性能增益存在特定关联，这可为候选模型选择提供指导。受此启发，我们提出新型模型融合策略：基于模型亲缘性的Top-k贪婪融合法，该方法在基准数据集上展现出更优性能。具体而言，我们发现以模型亲缘性为标准可支持持续模型融合，缓解模型进化中的性能退化（局部最优）问题，而模型亲缘性可作为逃离这些陷阱的指引。代码详见https://github.com/zjunlp/ModelKinship。

---

## [CogSteer: Cognition-Inspired Selective Layer Intervention for Efficiently Steering Large Language Models](https://arxiv.org/abs/2410.17714)

### Abstract
arXiv:2410.17714v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) achieve remarkable performance through pretraining on extensive data. This enables efficient adaptation to diverse downstream tasks. However, the lack of interpretability in their underlying mechanisms limits the ability to effectively steer LLMs for specific applications. In this work, we investigate the intrinsic mechanisms of LLMs from a cognitive perspective using eye movement measures. Specifically, we analyze the layer-wise correlation between human cognitive indicators and LLM representations. Building on these insights, we propose a heuristic approach for selecting the optimal steering layer to modulate LLM semantics. To this end, we introduce an efficient selective layer intervention based on prominent parameter-efficient fine-tuning methods, which conventionally adjust either all layers or only the final layer. Additionally, we present an implicit layer contrastive intervention during inference to steer LLMs away from toxic outputs. Extensive experiments on natural language understanding, reasoning, and generation tasks, conducted on GPT-2, Llama2-7B, and Mistral-7B, demonstrate the effectiveness and efficiency of our approach. As a model-agnostic framework, it enhances the interpretability of LLMs while improving efficiency for safe deployment.

### 摘要
大型语言模型（LLMs）通过海量数据的预训练展现出卓越性能，能够高效适应多样化的下游任务。然而，其底层机制缺乏可解释性，限制了针对特定应用有效调控模型的能力。本研究从认知科学视角出发，利用眼动指标探究LLMs的内在机制。具体而言，我们系统分析了人类认知指标与LLM各层级表征之间的相关性。基于这些发现，我们提出一种启发式方法用于选择最优语义调控层。为此，我们在现有参数高效微调方法（通常调整全部层或仅最终层）基础上，提出了一种高效的选择性层级干预机制。此外，我们在推理阶段引入隐式层级对比干预，以引导模型规避有害输出。在GPT-2、Llama2-7B和Mistral-7B上进行的自然语言理解、推理和生成任务实验表明，该方法兼具高效性与有效性。作为一个模型无关的框架，本方法在提升LLMs可解释性的同时，增强了安全部署的效能。

---

## [Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models](https://arxiv.org/abs/2410.07176)

### Abstract
arXiv:2410.07176v2 Announce Type: replace-cross 
Abstract: Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs' internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.

### 摘要
检索增强生成（RAG）通过整合外部知识有效解决了大语言模型（LLM）的固有局限，但可能因检索结果不完善而受到阻碍——这些结果可能包含无关、误导甚至恶意信息。现有研究很少通过联合分析来探讨RAG的行为机制，尤其针对检索缺陷导致的错误传播以及LLM内部知识与外部来源间的潜在冲突。通过现实场景下的全面受控分析，我们发现不完善的检索增强现象具有不可避免性、普遍性和危害性。研究指出，LLM内部知识与检索获取的外部知识之间的冲突是阻碍RAG后检索阶段克服检索缺陷的关键瓶颈。为此，我们提出Astute RAG这一新型框架，其设计目标是对不完善的检索增强具备鲁棒性。该方法能自适应地激发LLM内部核心知识，通过来源感知机制迭代整合内外知识，最终根据信息可靠性生成答案。基于Gemini和Claude的实验表明，Astute RAG相较以往鲁棒性增强的RAG方法具有显著优势。特别地，在最坏场景下，Astute RAG是唯一能达到甚至超越传统LLM使用效果的RAG方法。进一步分析揭示了该框架在解决知识冲突、提升RAG可信度方面的有效性。

---

## [MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment](https://arxiv.org/abs/2410.05873)

### Abstract
arXiv:2410.05873v2 Announce Type: replace-cross 
Abstract: English-centric large language models (LLMs) often show strong multilingual capabilities. However, their multilingual performance remains unclear and is under-evaluated for many other languages. Most benchmarks for multilinguality focus on classic NLP tasks or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages that English-centric LLMs use English as a pivot language in their intermediate layers. MEXA computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in different languages. We conduct controlled experiments using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves an average Pearson correlation of 0.90 between its predicted scores and actual task performance across languages. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://cis-lmu-mexa.hf.space, Code: https://github.com/cisnlp/MEXA.

### 摘要
以英语为核心的大型语言模型（LLMs）通常展现出较强的多语言能力，但其在多语言场景下的实际性能仍不明确，且对许多其他语言的评估不足。现有的大多数多语言基准测试要么聚焦于传统自然语言处理任务，要么仅覆盖极少数语种。本文提出MEXA方法，通过利用平行句对（其覆盖语种数量远超现有下游任务）来评估以英语为核心的预训练LLMs的多语言能力。MEXA基于以下发现：这类模型在中间层会将英语作为枢轴语言。该方法通过计算英语与非英语平行句对之间的对齐度，来评估模型从英语向其他语言的理解迁移效果，这种对齐度可用于预测模型在不同语言中的表现。我们采用多种平行数据集（FLORES-200和Bible）、模型（Llama系列、Gemma系列、Mistral和OLMo）以及成熟的下游任务（Belebele、m-MMLU和m-ARC）进行对照实验，并探索了纯解码器模型中嵌入向量的不同计算方法。实验结果表明：在默认设置下，MEXA预测分数与实际任务表现之间的平均皮尔逊相关系数达到0.90，证明其能可靠评估英语核心LLMs的多语言能力，为理解模型的多语言潜力及内部机制提供了清晰视角。

---

## [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930)

### Abstract
arXiv:2410.16930v3 Announce Type: replace-cross 
Abstract: Math reasoning is an active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence and has implications in several domains, including math education. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within models. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a computationally efficient method we use to isolate math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by filtering out those important for general language tasks. Through pruning parameters MathNeuro identifies, we delete a LLM's math reasoning ability without significantly impacting its general language ability. Scaling the identified parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.

### 摘要
数学推理作为人工智能的重要标志，在数学教育等多个领域具有重要影响，因而成为大语言模型（LLM）研究的活跃领域。然而，目前少有研究探讨数学推理能力如何编码于LLM参数中，以及该能力是否可作为独立技能从模型中分离。此类研究不仅能通过针对性干预提升数学性能而不影响非数学行为，更有助于理解模型编码数学推理的机制。我们提出数学神经手术（MathNeuro）——一种仅需前向传播即可高效分离LLM数学专用参数的计算方法。该方法基于权重与激活值计算参数重要性，并通过过滤通用语言任务相关参数实现数学参数的精准定位。通过剪枝MathNeuro识别的参数，我们能在基本保持模型通用语言能力的同时消除其数学推理能力。将识别出的参数按微小常数缩放后，预训练或指令微调LLM在GSM8K上的性能提升4-17%，在MATH数据集上提升5-35%，且非数学行为保持不变。MathNeuro具有数据高效性：仅需单样本即可保持大部分数学参数识别效能。本方法揭示了未来针对数学专用参数实施干预的潜在研究方向。

---

## [LoGU: Long-form Generation with Uncertainty Expressions](https://arxiv.org/abs/2410.14309)

### Abstract
arXiv:2410.14309v3 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities, they still struggle with generating factually incorrect content (i.e., hallucinations). A promising approach to mitigate this issue is enabling models to express uncertainty when unsure. Previous research on uncertainty modeling has primarily focused on short-form QA, but realworld applications often require much longer responses. In this work, we introduce the task of Long-form Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty Suppression, where models hesitate to express uncertainty, and Uncertainty Misalignment, where models convey uncertainty inaccurately. To tackle these challenges, we propose a refinement-based data collection framework and a two-stage training pipeline. Our framework adopts a divide-and-conquer strategy, refining uncertainty based on atomic claims. The collected data are then used in training through supervised fine-tuning (SFT) and direct preference optimization (DPO) to enhance uncertainty expression. Extensive experiments on three long-form instruction following datasets show that our method significantly improves accuracy, reduces hallucinations, and maintains the comprehensiveness of responses.

### 摘要
尽管大语言模型（LLMs）展现出令人印象深刻的能力，但在生成事实上不准确的内容（即幻觉）方面仍存在困难。缓解这一问题的有效方法是让模型在不确定时能够表达不确定性。先前关于不确定性建模的研究主要集中于短问答场景，而实际应用往往需要更长的响应内容。本研究引入了长文本生成中的不确定性表达任务（LoGU），并识别出两个关键挑战：不确定性抑制（模型不愿表达不确定性）和不确定性错位（模型不准确地传递不确定性）。为应对这些挑战，我们提出了基于细化的数据收集框架和两阶段训练流程。该框架采用分治策略，基于原子声明细化不确定性表达。通过监督微调（SFT）和直接偏好优化（DPO）利用收集的数据进行训练，以增强不确定性表达能力。在三个长文本指令遵循数据集上的大量实验表明，我们的方法显著提高了准确性，减少了幻觉现象，同时保持了回答的全面性。

---

## [FactLens: Benchmarking Fine-Grained Fact Verification](https://arxiv.org/abs/2411.05980)

### Abstract
arXiv:2411.05980v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift towards fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.

### 摘要
大型语言模型（LLMs）在语言生成与理解方面展现出卓越能力，但其产生幻觉和事实性错误信息的倾向仍是关键局限。为验证LLM生成内容及其他来源的主张，传统验证方法通常依赖整体模型对复杂主张赋予单一事实性标签，这可能掩盖细微错误。本文主张转向细粒度验证，即将复杂主张分解为可独立验证的子主张，从而实现更精准的错误定位、提升透明度并降低证据检索的模糊性。然而，子主张生成面临保持上下文一致性及确保与原主张语义等效等挑战。我们提出FactLens基准框架，通过子主张质量评估指标与自动化评测器来评估细粒度事实验证。基准数据经人工校验以确保高质量真实标注。实验结果表明FactLens自动化评测器与人工判断具有一致性，我们进一步探讨了子主张特征对整体验证性能的影响。

---

## [TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models](https://arxiv.org/abs/2410.20445)

### Abstract
arXiv:2410.20445v2 Announce Type: replace-cross 
Abstract: Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modeling. However, the heterogeneity of data and the diversity of trajectory tasks make effective and reliable trajectory modeling an important yet highly challenging endeavor, even for domain experts. In this paper, we propose \textit&#123;TrajAgent&#125;, a agent framework powered by large language models (LLMs), designed to facilitate robust and efficient trajectory modeling through automation modeling. This framework leverages and optimizes diverse specialized models to address various trajectory modeling tasks across different datasets effectively. In \textit&#123;TrajAgent&#125;, we first develop \textit&#123;UniEnv&#125;, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on \textit&#123;UniEnv&#125;, we introduce an agentic workflow designed for automatic trajectory modeling across various trajectory tasks and data. Furthermore, we introduce collaborative learning schema between LLM-based agents and small speciallized models, to enhance the performance of the whole framework effectively. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of \textit&#123;TrajAgent&#125; in automated trajectory modeling, achieving a performance improvement of 2.38\%-34.96\% over baseline methods.

### 摘要
轨迹建模涵盖轨迹数据模式挖掘与未来预测研究，在生活服务、城市交通及公共管理等领域具有广泛应用。尽管已有大量方法针对轨迹建模中的特定问题提出解决方案，但数据的异构性与任务多样性使得高效可靠的轨迹建模仍是一项重要且极具挑战性的工作，即使对领域专家而言亦是如此。本文提出	extit&#123;TrajAgent&#125;——一个由大语言模型（LLMs）驱动的智能体框架，旨在通过自动化建模实现稳健高效的轨迹建模。该框架通过优化整合多种专用模型，有效处理不同数据集上的各类轨迹建模任务。在	extit&#123;TrajAgent&#125;中，我们首先开发了具有统一数据与模型接口的执行环境	extit&#123;UniEnv&#125;，以支持多种模型的执行与训练。基于	extit&#123;UniEnv&#125;，我们设计了适用于跨任务跨数据的自动轨迹建模的智能工作流，并进一步引入基于LLM的智能体与小型专用模型间的协同学习机制，显著提升整体框架性能。在四个真实数据集上开展的四项任务实验表明，	extit&#123;TrajAgent&#125;在自动化轨迹建模中表现优异，相较基线方法性能提升达2.38\%-34.96\%。

---

## [INVARLLM: LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection](https://arxiv.org/abs/2411.10918)

### Abstract
arXiv:2411.10918v2 Announce Type: replace-cross 
Abstract: Cyber-Physical Systems (CPS) are vulnerable to cyber-physical attacks that violate physical laws. While invariant-based anomaly detection is effective, existing methods are limited: data-driven approaches lack semantic context, and physics-based models require extensive manual work. We propose INVARLLM, a hybrid framework that uses large language models (LLMs) to extract semantic information from CPS documentation and generate physical invariants, then validates these against real system data using a PCMCI+-inspired K-means method. This approach combines LLM semantic understanding with empirical validation to ensure both interpretability and reliability. We evaluate INVARLLM on SWaT and WADI datasets, achieving 100% precision in anomaly detection with no false alarms, outperforming all existing methods. Our results demonstrate that integrating LLM-derived semantics with statistical validation provides a scalable and dependable solution for CPS security.

### 摘要
信息物理系统（CPS）易受违反物理规律的跨域攻击。虽然基于不变量的异常检测方法有效，但现有技术存在局限：数据驱动方法缺乏语义上下文，而基于物理模型的方法需要大量人工干预。我们提出INVARLLM混合框架，利用大语言模型（LLMs）从CPS文档中提取语义信息并生成物理不变量，随后通过受PCMCI+启发的K-means方法基于真实系统数据进行验证。该方法将LLM的语义理解能力与实证验证相结合，确保可解释性与可靠性。我们在SWaT和WADI数据集上评估INVARLLM，实现了100%的异常检测精确率且零误报，性能超越所有现有方法。结果表明：通过融合LLM衍生的语义与统计验证，可为CPS安全提供可扩展且可靠的解决方案。

---

## [Real-time Adapting Routing (RAR): Improving Efficiency Through Continuous Learning in Software Powered by Layered Foundation Models](https://arxiv.org/abs/2411.09837)

### Abstract
arXiv:2411.09837v2 Announce Type: replace-cross 
Abstract: To balance the quality and inference cost of a Foundation Model (FM, such as large language models (LLMs)) powered software, people often opt to train a routing model that routes requests to FMs with different sizes and capabilities. Existing routing models rely on learning the optimal routing decision from carefully curated data, require complex computations to be updated, and do not consider the potential evolution of weaker FMs. In this paper, we propose Real-time Adaptive Routing (RAR), an approach to continuously adapt FM routing decisions while using guided in-context learning to enhance the capabilities of weaker FM. The goal is to reduce reliance on stronger, more expensive FMs. We evaluate our approach on different subsets of the popular MMLU benchmark. Over time, our approach routes 50.2% fewer requests to computationally expensive models while maintaining around 90.5% of the general response quality. In addition, the guides generated from stronger models have shown intra-domain generalization and led to a better quality of responses compared to an equivalent approach with a standalone weaker FM.

### 摘要
为了平衡基础模型（FM，如大语言模型（LLMs））驱动软件的质量与推理成本，人们通常选择训练一个路由模型，将请求分配给不同规模和能力的FM。现有路由模型依赖于从精心构建的数据中学习最优路由决策，需要复杂计算进行更新，且未考虑较弱FM的潜在演进。本文提出实时自适应路由（RAR），该方法在利用上下文引导学习增强较弱FM能力的同时，持续调整FM路由决策，旨在减少对更强但更昂贵FM的依赖。我们在流行MMLU基准测试的不同子集上评估了该方法。实验表明，随时间推移，我们的方法在保持约90.5%总体响应质量的前提下，向计算成本高的模型路由请求量减少了50.2%。此外，与独立较弱FM的等效方法相比，由强模型生成的引导指令展现出领域内泛化能力，并显著提升了响应质量。

---

## [Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment](https://arxiv.org/abs/2411.18688)

### Abstract
arXiv:2411.18688v4 Announce Type: replace-cross 
Abstract: With the widespread deployment of Multimodal Large Language Models (MLLMs) for visual-reasoning tasks, improving their safety has become crucial. Recent research indicates that despite training-time safety alignment, these models remain vulnerable to jailbreak attacks. In this work, we first highlight an important safety gap to describe that alignment achieved solely through safety training may be insufficient against jailbreak attacks. To address this vulnerability, we propose Immune, an inference-time defense framework that leverages a safe reward model through controlled decoding to defend against jailbreak attacks. Additionally, we provide a mathematical characterization of Immune, offering insights on why it improves safety against jailbreaks. Extensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal that Immune effectively enhances model safety while preserving the model's original capabilities. For instance, against text-based jailbreak attacks on LLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared to the base MLLM and state-of-the-art defense strategy, respectively.

### 摘要
随着多模态大语言模型（MLLMs）在视觉推理任务中的广泛应用，提升其安全性变得至关重要。近期研究表明，尽管经过训练阶段的安全对齐，这些模型仍易受到越狱攻击。本研究首先揭示了一个关键的安全缺陷：仅通过安全训练实现的对齐可能不足以抵御越狱攻击。针对此漏洞，我们提出免疫（Immune）——一种基于受控解码的推理阶段防御框架，通过安全奖励模型来抵御越狱攻击。此外，我们从数学角度对免疫机制进行了形式化表征，阐明了其提升抗越狱安全性的原理。基于当前主流MLLMs在多样化越狱基准测试中的大量评估表明，免疫在保持模型原始能力的同时显著增强了安全性。以LLaVA-1.6为例，针对文本型越狱攻击，免疫相较于基线MLLM和最先进防御策略分别降低了57.82%和16.78%的攻击成功率。

---

## [If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World](https://arxiv.org/abs/2412.01617)

### Abstract
arXiv:2412.01617v2 Announce Type: replace-cross 
Abstract: Warning: this paper discusses content related, but not limited to, violence, sex, and suicide. Loneliness, or the lack of fulfilling relationships, significantly impacts a person's mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs in services like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT outside of its marketed use as a task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22x more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations to research and industry to address loneliness.

### 摘要
警告：本文讨论内容涉及但不限于暴力、性与自杀主题。孤独感（即缺乏满足性人际关系）对个体的身心健康具有显著负面影响，且在全球范围内普遍存在。既有研究表明大型语言模型（LLMs）可能有助于缓解孤独，但我们指出当前ChatGPT等广泛应用的LLMs在此类场景中的使用更为普遍且风险更高——因其设计初衷并非为此。通过分析用户超出任务型助手定位的ChatGPT交互行为，我们发现：在归类为孤独情绪的对话中，用户寻求建议或情感验证的频率达37%，并获得了良好互动。然而该模型在敏感情境（如应对自杀倾向或创伤叙述）中存在应对失当问题。研究同时观测到有毒内容出现率增加35%，其中女性成为攻击目标的概率是男性的22倍。这些发现揭示了该技术涉及的伦理与法律问题，包括极端化倾向加剧或孤独感恶化等风险。最后我们向学术界与产业界提出应对孤独问题的建议。

---

## [KnowCoder-X: Boosting Multilingual Information Extraction via Code](https://arxiv.org/abs/2411.04794)

### Abstract
arXiv:2411.04794v3 Announce Type: replace-cross 
Abstract: Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in Information Extraction (IE), a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal IE. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we conduct IE cross-lingual alignment instruction tuning on the translated instance prediction task to enhance the model's cross-lingual transferability. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by 30.17\% and SoTA by 20.03\%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: https://github.com/ICT-GoKnow/KnowCoder

### 摘要
实证研究表明，大型语言模型（LLMs）展现出自发性的跨语言对齐能力。然而，尽管LLMs在信息抽取（IE）任务中表现出良好的跨语言对齐潜力，不同语言间仍存在显著的不平衡性，这揭示了其内在缺陷。为解决这一问题，我们提出了KnowCoder-X——一个具备先进跨语言与多语言能力的代码型LLM，用于通用信息抽取。首先，该模型通过Python类标准化多语言Schema的表示形式，确保不同语言间本体论的一致性，并将跨语言IE任务统一建模为代码生成任务。其次，我们在翻译实例预测任务上进行IE跨语言对齐指令微调，以增强模型的跨语言迁移能力。在此阶段，我们还通过提出的鲁棒三阶段流程构建了包含257k样本的高质量多样化双语IE平行数据集ParallelNER，并通过人工标注确保数据质量。尽管未在29种未见语言上进行训练，KnowCoder-X仍以30.17%的优势超越ChatGPT，并以20.03%的优势超越现有最优方法，展现出卓越的跨语言IE能力。在中文和英文64个IE基准测试中的综合评估表明，KnowCoder-X通过提升IE对齐能力，显著增强了跨语言IE迁移性能。代码与数据集已开源：https://github.com/ICT-GoKnow/KnowCoder

---

## [PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from Related Example Banks](https://arxiv.org/abs/2412.05710)

### Abstract
arXiv:2412.05710v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks -- Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms existing frameworks for retrieving examples.

### 摘要
大型语言模型（LLMs）近期通过上下文学习（ICL）展现出卓越的小样本学习能力。然而，ICL性能高度依赖于小样本示例的选择，这使得最优示例的筛选成为持续的研究挑战。这一问题在资源稀缺的印度语系语言中尤为突出，由于真实标注数据的匮乏，选择过程更为复杂。本研究提出PromptRefine——一种改进印度语系低资源语言ICL性能的新型交替最小化示例选择方法。该方法利用相关高资源印度语系的辅助示例库，通过多任务学习技术对齐语言特定检索器，实现高效的跨语言检索。此外，我们通过增强所选示例的多样性来提升泛化能力并减少偏差。基于LLAMA-3.1-8B、LLAMA-2-7B、Qwen-2-7B和Qwen-2.5-7B等前沿大模型，我们在跨语言问答、多语言问答、机器翻译和跨语言摘要四项文本生成任务上的全面实验表明，PromptRefine在示例检索方面显著优于现有框架。

---

## [CNNSum: Exploring Long-Context Summarization with Large Language Models in Chinese Novels](https://arxiv.org/abs/2412.02819)

### Abstract
arXiv:2412.02819v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been well-researched in various long-context tasks. However, the scarcity of long-context summarization datasets hinders progress in this area. To address this, we introduce CNNSum, a multi-scale long-context summarization benchmark based on Chinese novels, featuring human-driven annotations across four subsets totaling 695 samples, with lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct detailed human assessments to summarize abnormal output types. Furthermore, we extensively explore how to improve long-context summarization. In our study: (1) Advanced LLMs may generate much subjective commentary, leading to vague summaries. (2) Currently, long-context summarization mainly relies on memory ability. The advantages of Large LLMs are hard to utilize, thus small LLMs are more cost-effective. (3) Different prompt types paired with various version models may cause large performance gaps. In further fine-tuning, these can be mitigated, and the Base version models perform better. (4) LLMs with RoPE-base scaled exhibit strong extrapolation potential; using short-context data can significantly improve long-context summarization performance. However, further applying other interpolation methods requires careful selection. (5) CNNSum provides more reliable evaluation results than other benchmarks. We release CNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)

### 摘要
大语言模型（LLMs）在各种长上下文任务中已得到充分研究。然而，长上下文摘要数据集的稀缺阻碍了这一领域的进展。为此，我们提出了CNNSum，一个基于中文小说的多尺度长上下文摘要基准，包含人类驱动的标注，涵盖四个子集共695个样本，长度范围从16k到128k。我们对多种LLMs进行了基准测试，并进行了详细的人工评估以总结异常输出类型。此外，我们深入探讨了如何提升长上下文摘要的性能。研究发现：（1）先进的LLMs可能生成大量主观评论，导致摘要模糊不清。（2）目前，长上下文摘要主要依赖记忆能力，大型LLMs的优势难以发挥，因此小型LLMs更具成本效益。（3）不同提示类型与不同版本模型搭配可能导致较大性能差异，但在进一步微调后这些差异可被缓解，且基础版本模型表现更优。（4）采用RoPE-base缩放的LLMs展现出强大的外推潜力；使用短上下文数据可显著提升长上下文摘要性能，但进一步应用其他插值方法需谨慎选择。（5）CNNSum相比其他基准能提供更可靠的评估结果。我们公开CNNSum以推动未来研究。（https://github.com/CxsGhost/CNNSum）

---

## [Can Graph Neural Networks Learn Language with Extremely Weak Text Supervision?](https://arxiv.org/abs/2412.08174)

### Abstract
arXiv:2412.08174v3 Announce Type: replace-cross 
Abstract: While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we propose a multi-modal prompt learning paradigm to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. We demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. The code is available at https://github.com/Violet24K/Morpher.

### 摘要
尽管利用对比语言-图像预训练（CLIP）技术在互联网规模的图文对上构建视觉模型已取得巨大成功，但通过CLIP流程构建可迁移的图神经网络（GNN）仍面临挑战，这主要源于标注数据和文本监督的稀缺性、下游任务的多层级性以及领域间的概念差异。本研究提出一种多模态提示学习范式，仅需少量带有极弱文本监督的语义标注样本，即可有效使预训练GNN适配下游任务和数据。该范式通过同时学习图提示和文本提示，将图数据直接嵌入与大型语言模型（LLMs）相同的表征空间。我们在少样本学习、多任务层级和跨域场景中验证了该范式的卓越性能。此外，我们构建了首个CLIP风格的零样本分类原型，能够在极弱文本监督下将GNN泛化至未见类别。代码已开源：https://github.com/Violet24K/Morpher。

---

## [Exploring Multi-Modal Data with Tool-Augmented LLM Agents for Precise Causal Discovery](https://arxiv.org/abs/2412.13667)

### Abstract
arXiv:2412.13667v2 Announce Type: replace-cross 
Abstract: Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.

### 摘要
因果发现是智能医疗、药物研发人工智能和智能运维等领域决策制定的关键基础。传统统计因果发现方法虽已成熟，但主要依赖观测数据，往往忽略因果关系中固有的语义线索。大型语言模型（LLM）的出现为知识驱动的因果发现提供了经济高效的语义线索利用途径，但因果发现领域的LLM发展滞后于其他领域，尤其在多模态数据探索方面。为此，我们提出MATMCD——一个基于工具增强型LLM的多智能体系统，其包含两个核心代理：数据增强代理负责检索和处理模态增强数据，因果约束代理则整合多模态数据进行知识驱动推理。该系统内部工作机制的设计确保了代理间的有效协作。我们在七个数据集上的实证研究表明，多模态增强的因果发现具有显著潜力。

---

## [LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation](https://arxiv.org/abs/2412.10424)

### Abstract
arXiv:2412.10424v3 Announce Type: replace-cross 
Abstract: We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large language models (LLMs). This approach leverages multi-turn interactions where the LLM interviewer actively provides feedback on responses and poses follow-up questions to the evaluated LLM. At the start of the interview, the LLM interviewer dynamically modifies datasets to generate initial questions, mitigating data contamination. We apply the LLM-as-an-Interviewer framework to evaluate six models on the MATH and DepthQA tasks. Our results show that the framework effectively provides insights into LLM performance, including the quality of initial responses, adaptability to feedback, and ability to address follow-up queries like clarification or additional knowledge requests. The framework also addresses key limitations of conventional methods like LLM-as-a-Judge, including verbosity bias and inconsistency across runs. Finally, we propose the Interview Report, which aggregates insights from the interview process, providing examples and a comprehensive analysis of the LLM's strengths and weaknesses. This report offers a detailed snapshot of the model's real-world applicability. The code for our framework is publicly available at https://github.com/interview-eval/.

### 摘要
我们提出"LLM-as-an-Interviewer"（大语言模型作为面试官）这一评估大语言模型（LLMs）的新范式。该方法通过多轮互动实现评估，其中扮演面试官的LLM会主动对受测LLM的应答提供反馈并提出后续问题。在面试开始时，LLM面试官会动态修改数据集以生成初始问题，从而减轻数据污染的影响。我们将该框架应用于MATH和DepthQA任务中对六个模型进行评估。结果表明，该框架能有效揭示LLM的多方面性能，包括初始应答质量、对反馈的适应能力，以及处理澄清请求或补充知识需求等后续问题的能力。该框架还解决了LLM-as-a-Judge等传统方法的关键局限，如冗长偏好和多次运行不一致等问题。最后，我们提出"面试报告"机制，通过汇总面试过程中的洞察，提供具体案例和对LLM优缺点的全面分析。该报告能详细反映模型在真实场景中的适用性。框架代码已开源：https://github.com/interview-eval/。

---

## [Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective](https://arxiv.org/abs/2412.12276)

### Abstract
arXiv:2412.12276v3 Announce Type: replace-cross 
Abstract: Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., "Finding the first noun in a sentence.") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.

### 摘要
自回归变压器通过上下文学习（ICL）展现出适应性学习能力，这引发了对机制原理的探究。已有研究表明变压器将ICL任务编码为表征空间中的向量。本文基于编码-解码框架，系统研究变压器在预训练过程中如何形成任务向量，以及任务编码质量如何预测ICL任务表现。在合成ICL任务上，我们分析了小型变压器的训练动态，发现任务编码与解码能力存在协同涌现现象：当模型学会将不同潜在任务（如"识别句子中首个名词"）编码为可分立的表征时，会同步构建条件解码算法并提升ICL性能。我们在不同规模预训练模型（Gemma-2 2B/9B/27B、Llama-3.1 8B/70B）及OLMo-7B的预训练过程中验证了这一现象。进一步研究表明，从表征推断的任务编码质量可预测ICL表现，且意外发现：微调前层比微调后层更能改善任务编码与性能。这些实证发现为通过表征分析理解大语言模型成败机制提供了新视角。

---

## [Token-Budget-Aware LLM Reasoning](https://arxiv.org/abs/2412.18547)

### Abstract
arXiv:2412.18547v5 Announce Type: replace-cross 
Abstract: Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE

### 摘要
推理能力对于大语言模型（LLM）在各类任务中的优异表现至关重要。尽管思维链（CoT）推理等方法通过将问题分解为中间步骤提升了LLM性能，但它们也带来了显著的令牌使用开销，导致成本增加。我们发现当前LLM的推理过程存在不必要的冗长性，通过在提示中包含合理的令牌预算可以压缩这一过程，但令牌预算的选择对实际压缩效果具有关键影响。为此，我们提出了一种令牌预算感知的LLM推理框架，该框架能根据每个问题的推理复杂度动态调整推理令牌数量。实验表明，我们的方法在仅轻微降低性能的情况下，有效减少了CoT推理中的令牌成本，为平衡LLM推理效率与准确性提供了实用解决方案。

---

## [Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm](https://arxiv.org/abs/2412.18120)

### Abstract
arXiv:2412.18120v3 Announce Type: replace-cross 
Abstract: Cognitive tasks originally developed for humans are now increasingly used to study language models. While applying these tasks is often straightforward, interpreting their results can be challenging. In particular, when a model underperforms, it is often unclear whether this results from a limitation in the cognitive ability being tested or a failure to understand the task itself. A recent study argues that GPT 3.5's declining performance on 2-back and 3-back tasks reflects a working memory capacity limit similar to humans (Gong et al., 2024). By analyzing a range of open-source language models of varying performance levels on these tasks, we show that the poor performance is due at least in part to a limitation in task comprehension and task set maintenance. We challenge the best-performing model with progressively harder versions of the task (up to 10-back) and experiment with alternative prompting strategies, before analyzing model attentions. Our larger aim is to contribute to the ongoing conversation around refining methodologies for the cognitive evaluation of language models.

### 摘要
最初为人类设计的认知任务现正越来越多地用于研究语言模型。虽然应用这些任务通常较为直接，但解读其结果可能存在挑战。特别是当模型表现不佳时，往往难以判断这是由于被测认知能力的局限，还是对任务本身的理解不足所致。近期研究指出，GPT-3.5在2-back和3-back任务中表现下降反映了其工作记忆容量限制与人类相似（Gong等，2024）。通过分析一系列开源语言模型在这些任务中不同水平的表现，我们发现模型表现欠佳至少部分源于对任务理解和任务集维持的局限性。我们通过逐步提升任务难度（最高至10-back）挑战表现最佳的模型，尝试不同提示策略，并分析模型注意力机制。本研究旨在为推动完善语言模型认知评估方法的持续讨论作出贡献。

---

## [Neuron Empirical Gradient: Discovering and Quantifying Neurons Global Linear Controllability](https://arxiv.org/abs/2412.18053)

### Abstract
arXiv:2412.18053v3 Announce Type: replace-cross 
Abstract: While feed-forward neurons in pre-trained language models (PLMs) can encode knowledge, past research targeted a small subset of neurons that heavily influence outputs. This leaves the broader role of neuron activations unclear, limiting progress in areas like knowledge editing. We uncover a global linear relationship between neuron activations and outputs using neuron interventions on a knowledge probing dataset. The gradient of this linear relationship, which we call the neuron empirical gradient (NEG), captures how changes in activations affect predictions. To compute NEG efficiently, we propose NeurGrad, enabling large-scale analysis of neuron behavior in PLMs. We also show that NEG effectively captures language skills across diverse prompts through skill neuron probing. Experiments on MCEval8k, a multi-genre multiple-choice knowledge benchmark, support NEG's ability to represent model knowledge. Further analysis highlights the key properties of NEG-based skill representation: efficiency, robustness, flexibility, and interdependency. The code and data are released.

### 摘要
虽然预训练语言模型（PLM）中的前馈神经元能够编码知识，但以往研究仅关注对输出影响显著的少量神经元子集。这导致神经元激活的整体作用尚不明确，制约了知识编辑等领域的进展。我们通过在知识探测数据集上进行神经元干预，揭示了神经元激活与输出之间的全局线性关系。该线性关系的梯度（我们称之为神经元经验梯度NEG）量化了激活变化对预测的影响。为高效计算NEG，我们提出NeurGrad方法，实现对PLM神经元行为的大规模分析。通过技能神经元探测实验，我们还证明NEG能有效捕捉多样化提示下的语言技能。在跨体裁多项选择知识基准MCEval8k上的实验验证了NEG表征模型知识的能力。进一步分析揭示了基于NEG的技能表征具有四大关键特性：高效性、鲁棒性、灵活性和互依性。相关代码与数据已开源。

---

## [Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback](https://arxiv.org/abs/2501.01377)

### Abstract
arXiv:2501.01377v2 Announce Type: replace-cross 
Abstract: Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating extensive medical knowledge, demonstrate excellent capabilities in understanding medical images. However, there remain challenges in visual localization in medical images, which is crucial for abnormality detection and interpretation. To address these issues, we propose a novel UMed-LVLM designed to unveil medical abnormalities. Specifically, we collect a Medical Abnormalities Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM training. To collect MAU dataset, we propose a prompt method utilizing the GPT-4V to generate diagnoses based on identified abnormal areas in medical images. Moreover, the two-stage training method includes Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising Relevance Reward, Abnormal Localization Reward and Vision Relevance Reward. Experimental results demonstrate that our UMed-LVLM significantly outperforms existing Med-LVLMs in identifying and understanding medical abnormalities, achieving a 58% improvement over the baseline. In addition, this work shows that enhancing the abnormality detection capabilities of Med-LVLMs significantly improves their understanding of medical images and generalization capability.

### 摘要
现有医学大型视觉语言模型（Med-LVLMs）通过封装大量医学知识，在理解医学图像方面展现出卓越能力。然而，医学图像中的视觉定位仍存在挑战，这对异常检测和解读至关重要。为解决这些问题，我们提出了一种新型UMed-LVLM模型，旨在揭示医学异常。具体而言，我们收集了医学异常揭示（MAU）数据集，并提出两阶段训练方法用于UMed-LVLM训练。在构建MAU数据集时，我们采用基于GPT-4V的提示方法，根据医学图像中识别的异常区域生成诊断报告。此外，两阶段训练方法包含异常感知指令微调和异常感知奖励机制，涵盖相关性奖励、异常定位奖励和视觉相关性奖励。实验结果表明，我们的UMed-LVLM在识别和理解医学异常方面显著优于现有Med-LVLMs，较基线模型提升58%。本工作还表明，增强Med-LVLMs的异常检测能力能显著提升其对医学图像的理解能力和泛化性能。

---

## [Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging](https://arxiv.org/abs/2412.20070)

### Abstract
arXiv:2412.20070v2 Announce Type: replace-cross 
Abstract: Medical imaging provides essential visual insights for diagnosis, and multimodal large language models (MLLMs) are increasingly utilized for its analysis due to their strong generalization capabilities; however, the underlying factors driving this generalization remain unclear. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG), which refers to the models' ability to understand novel combinations by recombining learned elements, as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and confirmed that MLLMs can achieve CG across classification and detection tasks, underscoring its broader generalization potential. Med-MAT is available at https://github.com/FreedomIntelligence/Med-MAT.

### 摘要
医学影像为诊断提供了关键的视觉依据，而多模态大语言模型（MLLMs）凭借其强大的泛化能力正被广泛应用于影像分析领域。然而，驱动这种泛化能力的核心因素尚未明晰。现有研究表明，多任务训练因任务间的协同效应通常优于单任务训练，但这类研究往往忽视任务间的内在关联。为解析这一现象，我们尝试以组合泛化（CG）——即模型通过重组已习得元素来理解新组合的能力——作为理论框架展开研究。由于医学影像可通过模态、解剖区域和任务三个维度精确定义，天然具备探索CG的研究条件，我们整合了106个医学数据集构建Med-MAT平台进行系统实验。实验证实MLLMs能够利用CG理解未见过的医学影像，并揭示CG是多任务训练中观察到的泛化现象主要驱动力之一。进一步研究还表明CG能有效支持数据稀缺的医学数据集，同时验证了MLLMs在分类与检测任务间实现CG的能力，彰显其更广泛的泛化潜力。Med-MAT平台已发布于https://github.com/FreedomIntelligence/Med-MAT。

---

## [Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice](https://arxiv.org/abs/2501.00982)

### Abstract
arXiv:2501.00982v2 Announce Type: replace-cross 
Abstract: In psychological practices, standardized questionnaires serve as essential tools for assessing mental health through structured, clinically-validated questions (i.e., items). While social media platforms offer rich data for mental health screening, computational approaches often bypass these established clinical assessment tools in favor of black-box classification. We propose a novel questionnaire-guided screening framework that bridges psychological practice and computational methods through adaptive Retrieval-Augmented Generation (\textit&#123;aRAG&#125;). Our approach links unstructured social media content and standardized clinical assessments by retrieving relevant posts for each questionnaire item and using Large Language Models (LLMs) to complete validated psychological instruments. Our findings demonstrate two key advantages of questionnaire-guided screening: First, when completing the Beck Depression Inventory-II (BDI-II), our approach matches or outperforms state-of-the-art performance on Reddit-based benchmarks without requiring training data. Second, we show that guiding LLMs through standardized questionnaires can yield superior results compared to directly prompting them for depression screening, while also providing a more interpretable assessment by linking model outputs to clinically validated diagnostic criteria. Additionally, we show, as a proof-of-concept, how our questionnaire-based methodology can be extended to other mental conditions' screening, highlighting the promising role of LLMs as psychological assessors.

### 摘要
在心理学实践中，标准化问卷是通过结构化临床验证问题（即项目）评估心理健康的重要工具。尽管社交媒体平台为心理健康筛查提供了丰富数据，计算方法往往绕过这些成熟的临床评估工具而采用黑箱分类。我们提出了一种新颖的问卷引导筛查框架，通过自适应检索增强生成（aRAG）连接心理学实践与计算方法。该框架通过为每个问卷项目检索相关社交媒体内容，并利用大语言模型（LLMs）完成经过验证的心理测量工具，将非结构化社交媒体内容与标准化临床评估相联系。研究发现问卷引导筛查具有两大优势：首先在使用贝克抑郁量表第二版（BDI-II）时，我们的方法在Reddit基准测试中达到或超越了最先进水平，且无需训练数据；其次研究表明，通过标准化问卷引导LLMs进行抑郁筛查，相比直接提示能获得更优结果，同时通过将模型输出与临床验证的诊断标准相关联，提供更具可解释性的评估。此外，我们通过概念验证展示了该问卷方法可扩展至其他精神状况筛查，突显LLMs作为心理评估工具的重要潜力。

---

## [Automating Legal Interpretation with LLMs: Retrieval, Generation, and Evaluation](https://arxiv.org/abs/2501.01743)

### Abstract
arXiv:2501.01743v3 Announce Type: replace-cross 
Abstract: Interpreting the law is always essential for the law to adapt to the ever-changing society. It is a critical and challenging task even for legal practitioners, as it requires meticulous and professional annotations and summarizations by legal experts, which are admittedly time-consuming and expensive to collect at scale. To alleviate the burden on legal experts, we propose a method for automated legal interpretation. Specifically, by emulating doctrinal legal research, we introduce a novel framework, ATRIE, to address Legal Concept Interpretation, a typical task in legal interpretation. ATRIE utilizes large language models (LLMs) to AuTomatically Retrieve concept-related information, Interpret legal concepts, and Evaluate generated interpretations, eliminating dependence on legal experts. ATRIE comprises a legal concept interpreter and a legal concept interpretation evaluator. The interpreter uses LLMs to retrieve relevant information from previous cases and interpret legal concepts. The evaluator uses performance changes on Legal Concept Entailment, a downstream task we propose, as a proxy of interpretation quality. Automated and multifaceted human evaluations indicate that the quality of our interpretations is comparable to those written by legal experts, with superior comprehensiveness and readability. Although there remains a slight gap in accuracy, it can already assist legal practitioners in improving the efficiency of legal interpretation.

### 摘要
法律解释对于法律适应不断变化的社会始终至关重要。即使对法律从业者而言，这也是一项关键且具有挑战性的任务，因为它需要法律专家进行细致专业的注释和总结，而大规模收集这些注释无疑耗时且成本高昂。为减轻法律专家的负担，我们提出了一种自动化法律解释方法。具体而言，通过模拟 doctrinal legal research（教义式法律研究），我们引入了一个新颖框架ATRIE来解决法律概念解释这一典型法律解释任务。ATRIE利用大语言模型（LLMs）自动检索概念相关信息、解释法律概念并评估生成解释，从而消除对法律专家的依赖。该框架由法律概念解释器和法律概念解释评估器组成：解释器使用LLMs从既往案例中检索相关信息并解释法律概念；评估器则通过我们提出的下游任务'法律概念蕴涵'的性能变化作为解释质量的代理指标。自动化及多维度人工评估表明，我们的解释质量与法律专家撰写的解释相当，且在全面性和可读性方面更优。尽管在准确性方面仍存在微小差距，但该方法已能有效协助法律从业者提升法律解释效率。

---

## [Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models](https://arxiv.org/abs/2501.04945)

### Abstract
arXiv:2501.04945v4 Announce Type: replace-cross 
Abstract: It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, it is an unexplored area to enhance LLMs' ability to follow soft constraints. To bridge the gap, we initially design a pipeline to construct datasets with high-quality outputs automatically. Additionally, to fully utilize the positive and negative samples generated during the data construction process, we choose Direct Preference Optimization (DPO) as the training method. Furthermore, taking into account the difficulty of soft constraints indicated by the number of constraints, we design a curriculum learning training paradigm based on the constraint quantity. We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements.The datasets and code are publicly available at https://github.com/Rainier-rq/FollowSoftConstraint.

### 摘要
大型语言模型（LLMs）遵循涉及多重约束的指令至关重要，然而如何提升其遵循软约束的能力仍属未探索领域。为填补这一空白，我们首先设计了一个自动化构建高质量输出数据集的流程。此外，为充分利用数据构建过程中生成的正负样本，我们选择直接偏好优化（DPO）作为训练方法。进一步地，考虑到软约束难度与约束数量相关，我们设计了基于约束数量的课程学习训练范式。通过实验评估了所提方法在提升LLMs软约束遵循能力方面的有效性，并分析了驱动性能提升的关键因素。数据集与代码已公开于https://github.com/Rainier-rq/FollowSoftConstraint。

---

## [ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation](https://arxiv.org/abs/2501.14956)

### Abstract
arXiv:2501.14956v2 Announce Type: replace-cross 
Abstract: Evaluating personalized text generated by large language models (LLMs) is challenging, as only the LLM user, i.e., prompt author, can reliably assess the output, but re-engaging the same individuals across studies is infeasible. This paper addresses the challenge of evaluating personalized text generation by introducing ExPerT, an explainable reference-based evaluation framework. ExPerT leverages an LLM to extract atomic aspects and their evidence from the generated and reference texts, match the aspects, and evaluate their alignment based on content and writing style -- two key attributes in personalized text generation. Additionally, ExPerT generates detailed, fine-grained explanations for every step of the evaluation process, enhancing transparency and interpretability. Our experiments demonstrate that ExPerT achieves a 7.2% relative improvement in alignment with human judgments compared to the state-of-the-art text generation evaluation methods. Furthermore, human evaluators rated the usability of ExPerT's explanations at 4.7 out of 5, highlighting its effectiveness in making evaluation decisions more interpretable.

### 摘要
评估大型语言模型（LLM）生成的个性化文本具有挑战性，因为只有LLM用户（即提示作者）能够可靠地评估输出结果，但在不同研究中重新联系相同个体并不可行。本文通过引入可解释的基于参考的评估框架ExPerT，解决了个性化文本生成的评估难题。ExPerT利用LLM从生成文本和参考文本中提取原子层面特征及其证据，匹配这些特征，并根据内容和写作风格（个性化文本生成的两个关键属性）评估其对齐程度。此外，ExPerT为评估过程的每个步骤生成详细、细粒度的解释，从而增强透明度和可解释性。实验表明，与最先进的文本生成评估方法相比，ExPerT在与人机判断对齐方面实现了7.2%的相对改进。进一步的人类评估显示，ExPerT解释的可用性评分为4.7分（满分5分），凸显了其在提高评估决策可解释性方面的有效性。

---

## [The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs](https://arxiv.org/abs/2501.18626)

### Abstract
arXiv:2501.18626v4 Announce Type: replace-cross 
Abstract: We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model's prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.
  Warning: this paper contains examples of unethical inquiries used solely for research purposes.

### 摘要
我们提出了一类新型的大语言模型越狱对抗攻击方法，称为"任务嵌入提示"（TIP）攻击。该技术通过将序列到序列任务（如密码解码、谜语解答、代码执行）嵌入模型提示中，间接生成被禁止的输入内容。为系统评估此类攻击的有效性，我们开发了PHRYGE基准测试。实验表明，我们的技术成功绕过了包括GPT-4o和LLaMA 3.2在内的六种前沿语言模型的安全防护机制。研究结果揭示了当前大语言模型安全对齐机制的关键缺陷，并凸显了开发更高级防御策略的紧迫性。

---

## [Effective faking of verbal deception detection with target-aligned adversarial attacks](https://arxiv.org/abs/2501.05962)

### Abstract
arXiv:2501.05962v2 Announce Type: replace-cross 
Abstract: Background: Deception detection through analysing language is a promising avenue using both human judgments and automated machine learning judgments. For both forms of credibility assessment, automated adversarial attacks that rewrite deceptive statements to appear truthful pose a serious threat. Methods: We used a dataset of 243 truthful and 262 fabricated autobiographical stories in a deception detection task for humans and machine learning models. A large language model was tasked to rewrite deceptive statements so that they appear truthful. In Study 1, humans who made a deception judgment or used the detailedness heuristic and two machine learning models (a fine-tuned language model and a simple n-gram model) judged original or adversarial modifications of deceptive statements. In Study 2, we manipulated the target alignment of the modifications, i.e. tailoring the attack to whether the statements would be assessed by humans or computer models. Results: When adversarial modifications were aligned with their target, human (d=-0.07 and d=-0.04) and machine judgments (51% accuracy) dropped to the chance level. When the attack was not aligned with the target, both human heuristics judgments (d=0.30 and d=0.36) and machine learning predictions (63-78%) were significantly better than chance. Conclusions: Easily accessible language models can effectively help anyone fake deception detection efforts both by humans and machine learning models. Robustness against adversarial modifications for humans and machines depends on that target alignment. We close with suggestions on advancing deception research with adversarial attack designs and techniques.

### 摘要
背景：通过分析语言进行欺骗检测是一种具有前景的研究方向，既涉及人类判断也包含自动化机器学习判断。对于这两种可信度评估方式，通过自动对抗性攻击重写欺骗性陈述以显得真实的做法构成了严重威胁。方法：我们采用包含243篇真实和262篇虚构自传故事的数据集，分别对人类和机器学习模型进行欺骗检测实验。研究使用大型语言模型对欺骗性陈述进行重写使其呈现真实特征。在研究1中，进行欺骗判断的人类被试（采用详细度启发式策略）与两种机器学习模型（微调语言模型和简单n-gram模型）分别评估原始陈述及其对抗性修改版本。在研究2中，我们操控了修改的目标对齐方式，即根据评估主体（人类或计算机模型）定制攻击策略。结果：当对抗性修改与目标评估主体对齐时，人类判断效果（d=-0.07和d=-0.04）与机器学习准确率（51%）均降至随机水平；当攻击未与目标对齐时，人类启发式判断（d=0.30和d=0.36）与机器学习预测准确率（63-78%）均显著优于随机水平。结论：易获取的语言模型能有效帮助任何人伪造欺骗检测结果，这对人类和机器学习模型均构成挑战。人类与机器对抗对抗性修改的鲁棒性取决于目标对齐程度。最后我们提出采用对抗性攻击设计和技术推进欺骗检测研究的建议。

---

## [LLM Safety Alignment is Divergence Estimation in Disguise](https://arxiv.org/abs/2502.00657)

### Abstract
arXiv:2502.00657v2 Announce Type: replace-cross 
Abstract: We present a theoretical framework showing that popular LLM alignment methods, including RLHF and its variants, can be understood as divergence estimators between aligned (safe or preferred) and unaligned (harmful or less preferred) distributions. This perspective explains the emergence of separation in the latent space between safe and harmful prompts after alignment. As an application of our general divergence framework, we propose KLDO, a novel KL divergence-based alignment method, and empirically validate its effectiveness. We further show that using compliance-refusal datasets, rather than standard preference-based datasets, leads to stronger separation and improved safety alignment. Finally, to quantify the separation effect, we propose a distance-based metric in the prompt representation space, which also acts as a statistically significant indicator for model safety.

### 摘要
我们提出一个理论框架，表明包括RLHF及其变体在内的主流LLM对齐方法均可被理解为对齐（安全或偏好）分布与未对齐（有害或低偏好）分布之间的散度估计器。该视角解释了对齐后潜在空间中安全提示与有害提示出现分离现象的成因。作为通用散度框架的应用，我们提出KLDO这一基于KL散度的新型对齐方法，并通过实验验证其有效性。进一步研究表明，采用合规-拒绝型数据集（而非基于标准偏好的数据集）能产生更强的分离效果并提升安全对齐性能。最后，为量化分离效应，我们在提示表征空间中提出一种基于距离的度量指标，该指标同时具备统计显著性的模型安全性指示功能。

---

## [TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference](https://arxiv.org/abs/2501.16007)

### Abstract
arXiv:2501.16007v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have proven to be very capable, but access to frontier models currently relies on inference providers. This introduces trust challenges: how can we be sure that the provider is using the model configuration they claim? We propose TOPLOC, a novel method for verifiable inference that addresses this problem. TOPLOC leverages a compact locality-sensitive hashing mechanism for intermediate activations, which can detect unauthorized modifications to models, prompts, or precision with 100% accuracy, achieving no false positives or negatives in our empirical evaluations. Our approach is robust across diverse hardware configurations, GPU types, and algebraic reorderings, which allows for validation speeds significantly faster than the original inference. By introducing a polynomial encoding scheme, TOPLOC minimizes the memory overhead of the generated proofs by $1000\times$, requiring only 258 bytes of storage per 32 new tokens, compared to the 262 KB requirement of storing the token embeddings directly for Llama 3.1-8B-Instruct. Our method empowers users to verify LLM inference computations efficiently, fostering greater trust and transparency in open ecosystems and laying a foundation for decentralized, verifiable and trustless AI services.

### 摘要
大型语言模型（LLMs）已被证明具备强大能力，但目前前沿模型的访问依赖于推理服务提供商。这引发了信任挑战：如何确保提供商实际使用其宣称的模型配置？我们提出TOPLOC——一种可验证推理的新方法以解决该问题。TOPLOC采用紧凑的局部敏感哈希机制处理中间激活值，能100%准确检测对模型、提示词或精度的未授权修改，实证评估中实现零误报与漏报。该方法在多样化硬件配置、GPU类型及代数重排序场景下均保持鲁棒性，其验证速度显著快于原始推理过程。通过引入多项式编码方案，TOPLOC将生成证明的内存开销降低1000倍，Llama 3.1-8B-Instruct模型每生成32个新标记仅需258字节存储空间，相较直接存储标记嵌入向量的262KB需求大幅优化。本方案使用户能高效验证LLM推理计算，为开放生态系统增强信任与透明度，并为去中心化、可验证且无需信任的AI服务奠定基础。

---

## [AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model for Atmospheric Science](https://arxiv.org/abs/2502.01159)

### Abstract
arXiv:2502.01159v2 Announce Type: replace-cross 
Abstract: The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. Toward this end, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. AtmosSci-Bench features a dual-format design comprising both multiple-choice questions (MCQs) and open-ended questions (OEQs), enabling scalable automated evaluation alongside deeper analysis of conceptual understanding. We employ a template-based MCQ generation framework to create diverse, graduate-level problems with symbolic perturbation, while OEQs are used to probe open-ended reasoning. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at Our source codes are currently available at https://github.com/Relaxed-System-Lab/AtmosSci-Bench.

### 摘要
大型语言模型（LLMs）的快速发展，尤其是其推理能力的进步，为解决大气科学领域的复杂挑战带来了变革性潜力。然而，要有效利用LLMs，需要建立稳健且全面的评估基准。为此，我们提出了AtmosSci-Bench，这是一个新颖的基准测试，旨在系统评估LLMs在五大类大气科学问题上的表现：水文学、大气动力学、大气物理学、地球物理学和物理海洋学。AtmosSci-Bench采用双格式设计，包含选择题（MCQs）和开放式问题（OEQs），既能实现可扩展的自动化评估，又能深入分析概念理解。我们采用基于模板的MCQ生成框架，通过符号扰动创建多样化的研究生水平问题，而OEQs则用于探究开放式推理能力。我们对代表性LLMs进行了全面评估，将其分为四类：指令调优模型、高级推理模型、数学增强模型和领域特定气候模型。我们的分析揭示了LLMs在大气科学中的推理和问题解决能力的一些有趣见解。我们相信，AtmosSci-Bench通过提供一个标准且严格的评估框架，能够为推动LLMs在气候服务中的应用迈出关键一步。我们的源代码目前可在https://github.com/Relaxed-System-Lab/AtmosSci-Bench获取。

---

## [Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search](https://arxiv.org/abs/2502.02508)

### Abstract
arXiv:2502.02508v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.

### 摘要
大型语言模型（LLMs）在多个领域展现出卓越的推理能力。近期研究表明，增加测试时计算量可显著提升LLMs的推理性能，这通常需要在外置LLM验证器的引导下进行大规模推理采样，从而形成双主体系统。尽管依赖外部引导，该系统的有效性仍证明单一LLM具备处理复杂任务的潜力。由此我们提出新的研究问题：能否通过内化搜索能力来根本性增强单一LLM的推理性能？本研究探索了一个正交方向，专注于对LLMs进行自回归搜索的后训练（即通过自我反思和新策略的自主探索实现扩展推理过程）。为此，我们提出动作-思维链（COAT）推理框架及两阶段训练范式：1）小规模格式调优阶段以内化COAT推理格式；2）利用强化学习的大规模自我提升阶段。基于该方法，我们在开源模型和数据上训练出70亿参数的Satori模型。大量实证评估表明，Satori在数学推理基准测试中达到最先进水平，同时展现出对领域外任务的强泛化能力。代码、数据及模型已全面开源。

---

## [Survey on Vision-Language-Action Models](https://arxiv.org/abs/2502.06851)

### Abstract
arXiv:2502.06851v3 Announce Type: replace-cross 
Abstract: This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.

### 摘要
本文呈现了一份关于视觉-语言-动作（VLA）模型的AI生成综述，总结了关键方法、研究发现及未来方向。该内容由大语言模型（LLMs）生成，仅用于演示目的。本工作并非原创研究，而是旨在展示AI如何协助自动化文献综述。随着AI生成内容日益普及，确保其准确性、可靠性与合理整合仍面临挑战。未来研究将致力于构建AI辅助文献综述的结构化框架，探索提升引证准确性、来源可信度及上下文理解的技术。通过剖析LLMs在学术写作中的潜力与局限，本研究旨在为推动AI融入科研工作流程的广泛讨论作出贡献。本工作作为利用AI生成文献综述系统化方法的初步探索，旨在提升学术知识整合的效率与可扩展性。

---

## [Reflection-Window Decoding: Text Generation with Selective Refinement](https://arxiv.org/abs/2502.03678)

### Abstract
arXiv:2502.03678v3 Announce Type: replace-cross 
Abstract: The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.

### 摘要
大型语言模型（LLMs）中用于文本生成的自回归解码方法虽然被广泛使用，但由于缺乏对生成内容进行 refinement 和/或校正的内置机制，本质上存在次优性。本文从联合概率的角度考虑生成响应的最优性，即同时考虑所有 token 的全局最优解。我们通过理论分析刻画了自回归生成响应与同长度全局最优响应之间的潜在偏差。分析表明，当文本生成过程中出现显著不确定性时需保持警惕，这可能预示着生成历史的次优性。为克服自回归解码在文本生成中的缺陷，我们提出一种结合滑动反射窗口和暂停准则的方法，使得 refinement 与生成能在解码过程中交替进行。这种选择性 refinement 框架在效率与最优性之间取得平衡，大量实验结果验证了该方法的有效性。

---

## [Safety at Scale: A Comprehensive Survey of Large Model Safety](https://arxiv.org/abs/2502.05206)

### Abstract
arXiv:2502.05206v4 Announce Type: replace-cross 
Abstract: The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.

### 摘要
大型模型通过大规模预训练展现出卓越的学习与泛化能力，其快速发展已重塑人工智能（AI）领域格局。这些模型现已成为对话式AI、推荐系统、自动驾驶、内容生成、医疗诊断和科学发现等广泛应用的基础。然而，其广泛部署也暴露出重大安全风险，引发对鲁棒性、可靠性和伦理影响的担忧。本文系统综述了当前大型模型安全研究进展，涵盖视觉基础模型（VFMs）、大语言模型（LLMs）、视觉语言预训练模型（VLP）、视觉语言模型（VLMs）、扩散模型（DMs）以及基于大模型的智能体。我们的贡献可归纳为：（1）提出针对这些模型安全威胁的完整分类体系，包括对抗攻击、数据投毒、后门攻击、越狱与提示注入攻击、能效-延迟攻击、数据与模型提取攻击，以及新兴的智能体专属威胁；（2）综述各类攻击的现有防御策略，并总结安全研究常用数据集与基准；（3）在此基础上指出大模型安全领域的开放挑战，强调需要建立全面安全评估体系、可扩展的有效防御机制及可持续数据实践。更重要的是，我们强调研究界集体努力与国际合作的必要性。本工作可为研究人员和实践者提供参考，促进持续开发保障AI模型的综合防御系统与平台。

---

## [Iterative Deepening Sampling as Efficient Test-Time Scaling](https://arxiv.org/abs/2502.05449)

### Abstract
arXiv:2502.05449v2 Announce Type: replace-cross 
Abstract: Recent reasoning models, such as OpenAI's O1 series, have demonstrated exceptional performance on complex reasoning tasks and revealed new test-time scaling laws. Inspired by this, many people have been studying how to train models to achieve effective self-evaluation and self-correction to further enable the scaling paradigm. However, less studied is how to efficiently scale test-time compute from a fixed model, and this remains a challenge. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving at test time, which can also subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how systematically triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.

### 摘要
近期推理模型（如OpenAI的O1系列）在复杂推理任务中展现出卓越性能，并揭示了新的测试时缩放规律。受此启发，众多研究者开始探索如何训练模型实现有效的自我评估与自我校正，以进一步推动缩放范式的发展。然而，如何从固定模型高效扩展测试时计算仍研究不足且存在挑战。本文通过提升测试时复杂问题求解的自我反思数据生成质量来应对这一挑战，该改进同时能促进下一代大语言模型（LLM）的训练。具体而言，我们探究了系统性触发模型的自我校正机制如何提升复杂推理任务的表现。为此，我们提出一种新颖的迭代深化采样算法框架，旨在增强自我校正能力并生成更高质量的样本。通过在Math500和AIME基准上的大量实验，我们证明该方法在困难任务上实现了更高的成功率，并通过详尽的消融研究分析了其在多样化场景中的有效性。

---

## [KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference](https://arxiv.org/abs/2502.04420)

### Abstract
arXiv:2502.04420v4 Announce Type: replace-cross 
Abstract: KV cache quantization can improve Large Language Models (LLMs) inference throughput and latency in long contexts and large batch-size scenarios while preserving LLMs effectiveness. However, current methods have three unsolved issues: overlooking layer-wise sensitivity to KV cache quantization, high overhead of online fine-grained decision-making, and low flexibility to different LLMs and constraints. Therefore, we theoretically analyze the inherent correlation of layer-wise transformer attention patterns to KV cache quantization errors and study why key cache is generally more important than value cache for quantization error reduction. We further propose a simple yet effective framework KVTuner to adaptively search for the optimal hardware-friendly layer-wise KV quantization precision pairs for coarse-grained KV cache with multi-objective optimization and directly utilize the offline searched configurations during online inference. To reduce the computational cost of offline calibration, we utilize the intra-layer KV precision pair pruning and inter-layer clustering to reduce the search space. Experimental results show that we can achieve nearly lossless 3.25-bit mixed precision KV cache quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum inference throughput can be improved by 21.25\% compared with KIVI-KV8 quantization over various context lengths. Our code and searched configurations are available at https://github.com/cmd2001/KVTuner.

### 摘要
KV缓存量化可在长上下文和大批量场景下提升大语言模型（LLM）的推理吞吐量和延迟，同时保持模型效能。然而现有方法存在三个未解决问题：忽略层级对KV缓存量化的敏感性、在线细粒度决策的高开销，以及针对不同LLM和约束条件的低适应性。为此，我们通过理论分析揭示了层级Transformer注意力模式与KV缓存量化误差的内在关联，并阐明为何降低量化误差时键缓存通常比值缓存更重要。我们进一步提出高效框架KVTuner，通过多目标优化自适应搜索粗粒度KV缓存的最优硬件友好型层级量化精度组合，并在在线推理时直接应用离线搜索的配置。为降低离线校准的计算成本，采用层内KV精度组合剪枝和层间聚类来压缩搜索空间。实验结果表明，对于Llama-3.1-8B-Instruct等模型可实现近乎无损的3.25位混合精度KV缓存量化，在数学推理任务中对Qwen2.5-7B-Instruct等敏感模型则达到4.0位。相比KIVI-KV8量化方案，在不同上下文长度下最高可提升21.25%的推理吞吐量。代码及搜索配置详见https://github.com/cmd2001/KVTuner。

---

## [Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2502.08826)

### Abstract
arXiv:2502.08826v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) suffer from hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information for improved factual grounding. With advances in multimodal learning, Multimodal RAG extends this approach by incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges beyond those in unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, benchmarks, metrics, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We review training strategies, robustness enhancements, loss functions, and agent-based approaches, while also exploring the diverse Multimodal RAG scenarios. In addition, we outline open challenges and future directions to guide research in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. All resources are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.

### 摘要
大型语言模型（LLMs）因依赖静态训练数据而存在幻觉与知识陈旧问题。检索增强生成（RAG）通过整合外部动态信息改善事实基础，有效缓解了这些缺陷。随着多模态学习的进展，多模态RAG进一步引入文本、图像、音频和视频等多种模态以增强生成质量。然而跨模态对齐与推理带来的挑战远超单模态RAG范畴。本综述对多模态RAG系统进行结构化全面分析，涵盖数据集、基准测试、评估指标、方法论以及检索-融合-增强-生成环节的创新技术。我们系统梳理了训练策略、鲁棒性增强、损失函数和基于智能体的方法，同时探讨了多样化多模态RAG应用场景。此外，本文指明开放挑战与未来方向，以引导这一演进领域的研究。本综述为开发更具能力、可有效利用多模态动态外部知识库的可靠AI系统奠定基础。所有资源公开于https://github.com/llm-lab-org/Multimodal-RAG-Survey。

---

## [Language in the Flow of Time: Time-Series-Paired Texts Weaved into a Unified Temporal Narrative](https://arxiv.org/abs/2502.08942)

### Abstract
arXiv:2502.08942v2 Announce Type: replace-cross 
Abstract: While many advances in time series models focus exclusively on numerical data, research on multimodal time series, particularly those involving contextual textual information commonly encountered in real-world scenarios, remains in its infancy. With recent progress in large language models and time series learning, we revisit the integration of paired texts with time series through the Platonic Representation Hypothesis, which posits that representations of different modalities converge to shared spaces. In this context, we identify that time-series-paired texts may naturally exhibit periodic properties that closely mirror those of the original time series. Building on this insight, we propose a novel framework, Texts as Time Series (TaTS), which considers the time-series-paired texts to be auxiliary variables of the time series. TaTS can be plugged into any existing numerical-only time series models and enable them to handle time series data with paired texts effectively. Through extensive experiments on both multimodal time series forecasting and imputation tasks across benchmark datasets with various existing time series models, we demonstrate that TaTS can enhance predictive performance without modifying model architectures. Code available at https://github.com/iDEA-iSAIL-Lab-UIUC/TaTS.

### 摘要
尽管时间序列模型的许多进展仅关注数值数据，但针对多模态时间序列（尤其是现实场景中常见的包含上下文文本信息的时间序列）的研究仍处于起步阶段。随着大语言模型和时间序列学习的最新发展，我们通过柏拉图表征假说重新审视文本与时间序列的配对整合，该假说认为不同模态的表征会收敛到共享空间。在此背景下，我们发现与时间序列配对的文本可能天然呈现出与原始时间序列高度相似的周期性特征。基于这一发现，我们提出了一种新颖框架——文本即时间序列（TaTS），将时间序列配对文本视为时间序列的辅助变量。TaTS可无缝嵌入任何现有纯数值时间序列模型，使其能有效处理带配对文本的时间序列数据。通过在多个基准数据集上对多模态时间序列预测和填补任务进行广泛实验（涵盖各类现有时间序列模型），我们证明TaTS无需修改模型架构即可提升预测性能。代码详见https://github.com/iDEA-iSAIL-Lab-UIUC/TaTS。

---

## [RoToR: Towards More Reliable Responses for Order-Invariant Inputs](https://arxiv.org/abs/2502.08662)

### Abstract
arXiv:2502.08662v3 Announce Type: replace-cross 
Abstract: Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to mixture of order-invariant and sensitive inputs in practical listwise problems. Then, to overcome these issues we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA), and MMLU benchmarks, we show that RoToR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner (https://github.com/soyoung97/RoToR)

### 摘要
减轻语言模型（LMs）对列表式输入的位置偏差（如“中间丢失”问题）是一个重要且广为人知的挑战。尽管已有研究提出零样本顺序不变的语言模型来解决这一问题，但其在实际列表式问题中的应用效果仍有限。本研究首先通过识别并克服两个关键限制，使零样本不变语言模型更具实用性：（1）因修改位置ID分配以强制不变性而导致的训练与推理分布不匹配；（2）实际列表式任务中未能适应顺序不变与顺序敏感输入的混合情况。针对这些问题，我们提出：（1）RoToR模型——一种通过最小化位置ID修改实现真正顺序不变性的零样本不变语言模型；（2）选择性路由框架——可自适应处理列表任务中顺序不变与顺序敏感输入的动态架构。在“中间丢失”（LitM）、知识图谱问答（KGQA）和MMLU基准测试中，实验表明配备选择性路由的RoToR能以零样本方式有效处理实际列表式输入任务（https://github.com/soyoung97/RoToR）。

---

## [Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training](https://arxiv.org/abs/2502.11191)

### Abstract
arXiv:2502.11191v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a 15.88% improvement in the aggregate score, while reasoning distillation leads to a 10% gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community. For access to all datasets and model weights, please refer to https://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243.

### 摘要
大型语言模型（LLMs）在金融、法律和医学等专业领域已展现出显著进展。然而在网络安全领域，我们注意到开源数据集的匮乏，尤其是高质量网络安全预训练语料的缺失——尽管大量研究表明LLMs的知识获取主要源于预训练阶段。为此，我们提出了一套覆盖全训练阶段的综合数据集，包括预训练数据、指令微调数据以及结合网络安全领域自反思数据的推理蒸馏数据。大量消融实验证明其在公共网络安全基准测试中的有效性：使用本数据集进行持续预训练可使综合得分提升15.88%，而推理蒸馏则带来安全认证（CISSP）10%的性能增益。我们将依据ODC-BY和MIT许可证开放全部数据集及训练好的网络安全LLMs，以促进学界研究。所有数据集与模型权重可通过https://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243获取。

---

## [LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing](https://arxiv.org/abs/2502.11368)

### Abstract
arXiv:2502.11368v2 Announce Type: replace-cross 
Abstract: The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus and code for reproducibility.

### 摘要
本文探讨了大型语言模型（LLMs）在多维度分析性写作评估中的表现，即其基于多项评估标准提供分数和评语的能力。我们采用由二语研究生撰写的文献综述语料库（该语料库已由人类专家根据9项分析标准进行评估），并促使多个主流LLM在不同条件下执行相同任务。为评估反馈评语的质量，我们应用了一种新型反馈评语质量评估框架。与依赖人工判断的现有方法相比，该框架具有可解释性、成本效益、可扩展性和可复现性。研究发现，LLMs能够生成质量较好且总体可靠的多维度分析性评估。我们公开了语料库和代码以确保研究可复现性。

---

## [Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation](https://arxiv.org/abs/2502.10762)

### Abstract
arXiv:2502.10762v2 Announce Type: replace-cross 
Abstract: User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.

### 摘要
用户信息需求通常具有高度多样性和差异性。当前研究的一个关键挑战是如何实现可控的多目标生成，同时能够快速适应测试阶段多样化的用户需求。现有解决方案（如Rewarded Soup）侧重于合并针对单一目标单独调优的语言模型。尽管这类方法易于实现且被广泛采用，但由于忽视了竞争目标对模型调优的影响，其在实现最优性能方面存在局限。为解决这一问题，我们提出Bone Soup——一种新颖的模型融合方法，该方法首先通过考虑多目标影响寻求系列骨干模型，再进行模型融合。具体而言，Bone Soup首先使用多目标强化学习针对不同目标训练多个骨干模型，每个骨干模型由组合的骨干奖励信号引导。为确保这些模型对帕累托前沿保持最优性，骨干奖励通过将标准奖励函数组合成基向量来构建，并可通过基于规则的构造方法进行修改。Bone Soup采用对称循环矩阵映射生成融合系数，根据用户偏好合并骨干模型。大量实验结果表明，Bone Soup在可控多目标生成中展现出强大的可控性和帕累托最优性，为测试阶段满足多样化用户需求提供了更高效有效的解决方案。

---

## [How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training](https://arxiv.org/abs/2502.11196)

### Abstract
arXiv:2502.11196v2 Announce Type: replace-cross 
Abstract: Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.

### 摘要
尽管大型语言模型（LLMs）在知识密集型任务中展现出卓越能力，但其在新知识内化机制——尤其是如何将习得知识结构化嵌入神经计算过程——方面仍存在关键认知缺口。本研究通过知识电路演化的视角，识别出支持知识存储与处理的计算子图，对该问题展开系统性探究。通过持续预训练过程中电路演化的全面分析，我们获得以下核心发现：（1）新知识获取受其与既有知识相关性的显著影响；（2）知识电路演化呈现从形成期到优化期的明显相位转变；（3）电路演化遵循由深及浅的层级发展模式。这些发现不仅深化了我们对LLMs新知识获取机制的理论认识，还为改进持续预训练策略以提升模型性能提供了潜在启示。代码与数据详见https://github.com/zjunlp/DynamicKnowledgeCircuits。

---

## [HumT DumT: Measuring and controlling human-like language in LLMs](https://arxiv.org/abs/2502.13259)

### Abstract
arXiv:2502.13259v2 Announce Type: replace-cross 
Abstract: Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to deception, overreliance, and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HumT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs in many contexts. HumT also offers insights into the perceptions and impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DumT, a method using HumT to systematically control and reduce the degree of human-like tone while preserving model performance. DumT offers a practical approach for mitigating risks associated with anthropomorphic language generation.

### 摘要
大型语言模型是否应该生成使其显得像人类的语言？拟人化语言可能提升用户体验，但也可能导致欺骗、过度依赖和刻板印象。评估这些潜在影响需要系统化测量模型输出中拟人化语调的方法。我们提出HumT和SocioT两种度量指标，基于语言模型的相对概率来量化文本数据中的拟人化语调及其他社会认知维度。通过对偏好和使用数据集的HumT测量，发现多数情境下用户更倾向非拟人化的模型输出。HumT还揭示了拟人化的认知影响：拟人化输出与温暖感、社会亲近度、女性化特质及低社会地位高度相关，这些因素与前述风险密切相关。我们开发了DumT方法，利用HumT在保持模型性能的同时系统控制并降低拟人化语调程度，为缓解拟人化语言生成风险提供了实践方案。

---

## [Diversity-oriented Data Augmentation with Large Language Models](https://arxiv.org/abs/2502.11671)

### Abstract
arXiv:2502.11671v2 Announce Type: replace-cross 
Abstract: Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \textit&#123;Insufficient Attention to Sample Distribution Diversity&#125;. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \textbf&#123;\underline&#123;D&#125;&#125;iversity-\textbf&#123;\underline&#123;o&#125;&#125;riented data \textbf&#123;\underline&#123;Aug&#125;&#125;mentation framework (\textbf&#123;DoAug&#125;). % \(\mathscr&#123;DoAug&#125;\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \(10.52\%\), surpassing the runner-up baseline with more than three percentage points.

### 摘要
数据增强是自然语言处理（NLP）中通过生成多样化样本来丰富训练数据集的关键技术，对于提升模型鲁棒性和泛化能力至关重要。然而，当前仍存在一个显著挑战：对样本分布多样性的关注不足。现有方法大多侧重于增加样本数量，却忽视了样本分布多样性，这可能导致模型过拟合。针对此问题，我们探究了数据增强对数据集多样性的影响，并提出一种面向多样性的数据增强框架（DoAug）。具体而言，我们采用多样性导向的微调方法训练大语言模型（LLM）作为多样化复述生成器，能够通过生成多样化复述实现文本数据集增强。随后，我们将该LLM复述器应用于精选的高信息量核心样本集，并将生成的复述与原始数据整合以构建更具多样性的增强数据集。最后，我们在12个真实文本数据集上进行了大量实验。结果表明，经微调的LLM增强器在保持标签一致性的同时提升了多样性，从而增强了下游任务的鲁棒性和性能。具体而言，其平均性能提升达10.52%，较次优基线高出超过三个百分点。

---

## [VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare](https://arxiv.org/abs/2502.13775)

### Abstract
arXiv:2502.13775v2 Announce Type: replace-cross 
Abstract: Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.

### 摘要
对齐技术已成为确保大语言模型（LLM）输出符合人类价值观的核心手段。然而现有对齐范式通常建模单一或平均化的偏好，未能考虑跨文化、人口统计和社群视角的多样性。这一局限在健康相关场景中尤为关键——由于文化、宗教、个人价值观及观点冲突的影响，多元性在此领域至关重要。尽管多元化对齐研究已取得进展，但尚无工作聚焦健康领域，这很可能源于公开数据集的缺失。为填补该空白，我们推出VITAL基准数据集，包含13.1K个涉及价值观的健康情境和5.4K道多选题，专为评估多元化对齐方法而设计。通过对八个不同规模LLM的广泛测试，我们发现现有多元化对齐技术难以有效适应多样化的医疗保健观念，这凸显了特定领域定制化AI对齐的必要性。本研究揭示了当前方法的局限性，并为开发健康领域专用对齐解决方案奠定了基础。

---

## [Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information](https://arxiv.org/abs/2502.14258)

### Abstract
arXiv:2502.14258v2 Announce Type: replace-cross 
Abstract: While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads that primarily handle temporal knowledge, through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions ("In 2004") but also textual aliases ("In the year ..."), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.

### 摘要
虽然语言模型提取事实的能力已被广泛研究，但其如何处理随时间变化的事实仍缺乏深入探索。我们通过电路分析发现了"时间头"——专门处理时序知识的特定注意力头。研究表明这些注意力头普遍存在于多个模型中，尽管具体位置可能不同，且其响应会随知识类型及对应年份而变化。禁用这些注意力头会削弱模型回忆特定时间知识的能力，同时保持其在不损害时间无关知识和问答性能情况下的通用能力。值得注意的是，这些注意力头不仅能被数字条件（"2004年"）激活，还能响应文本别名（"在那年..."），表明其编码了超越简单数字表征的时间维度。此外，我们通过展示如何通过调整这些注意力头的数值来编辑时序知识，拓展了本研究的潜在应用价值。

---

## [Harnessing PDF Data for Improving Japanese Large Multimodal Models](https://arxiv.org/abs/2502.14778)

### Abstract
arXiv:2502.14778v2 Announce Type: replace-cross 
Abstract: Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data. Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge. To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized. We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation. Additionally, we construct instruction data from extracted image-text pairs to enrich the training data. To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark. Our results demonstrate substantial improvements, with performance gains ranging from 2.1% to 13.8% on Heron-Bench. Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs.

### 摘要
大型多模态模型（LMMs）在英语任务中表现出色，但由于缺乏高质量训练数据，其在日语场景的应用效果仍受限。当前日语LMMs多依赖翻译的英语数据集，难以捕捉日本特有的文化知识。为此，我们探索了日语PDF数据作为训练资源的潜力——这一领域目前尚未得到充分开发。本文提出全自动处理流程，通过预训练模型实现PDF文档的布局分析、OCR识别和视觉-语言配对，无需人工标注即可提取图文对。此外，我们从提取的图文对构建指令数据以扩充训练集。为评估PDF数据的有效性，我们训练了日语LMMs并在日本LMM基准测试中评估性能。实验结果显示模型在Heron-Bench上的性能提升达2.1%至13.8%。进一步分析揭示了PDF数据对模型规模、语言模型等因素的影响，证实其作为日语LMMs多模态资源的重要价值。

---

## [Data-Constrained Synthesis of Training Data for De-Identification](https://arxiv.org/abs/2502.14677)

### Abstract
arXiv:2502.14677v3 Announce Type: replace-cross 
Abstract: Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.

### 摘要
许多敏感领域（如临床领域）由于隐私风险而缺乏广泛可用的数据集。大型语言模型（LLM）生成能力的提升使得合成数据集成为一种可行的解决方案。本研究将LLM领域适应至临床领域，生成合成临床文本，并通过基于编码器的命名实体识别（NER）模型自动标注个人身份信息标签。随后利用合成语料库训练合成NER模型。结果表明，使用合成语料库训练NER模型仅会导致预测性能的小幅下降。我们通过系统性消融实验（采用瑞典语和西班牙语数据）探究了该过程的局限性。分析显示，较小规模的数据集即可满足LLM领域适应以进行数据合成的需求。该过程的有效性几乎完全取决于基于原始数据训练的机器标注NER模型的性能。

---

## [RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation](https://arxiv.org/abs/2502.13957)

### Abstract
arXiv:2502.13957v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.

### 摘要
检索增强生成（RAG）在知识密集型任务中展现出巨大潜力，近期发展的智能代理RAG通过语言代理与外部知识源的多轮交互实现了自适应信息检索。然而现有智能代理RAG方法多依赖临时提示工程，缺乏统一优化框架。我们提出RAG-Gym平台，系统探索三大优化维度：（1）提示工程，（2）执行器调优，（3）评判器训练。在提示工程方面，我们开发了Re$^2$Search代理，其融合推理反思机制，性能显著优于标准提示方案。执行器调优中评估了三种主流训练后算法，通过细粒度过程监督确定直接偏好优化最为有效。实验表明经训练的评判器可通过筛选更优质中间推理步骤来提升推理质量。综合这些发现，我们最终构建的优化版Re$^2$Search++代理在平均F1值上以3.2%至11.6%的相对优势超越Search-R1等最新方法。此外，我们探究了不同奖励源的影响，分析了训练与推理的扩展特性，为智能代理RAG优化提供了实用见解。项目主页详见https://rag-gym.github.io。

---

## [SEA-HELM: Southeast Asian Holistic Evaluation of Language Models](https://arxiv.org/abs/2502.14301)

### Abstract
arXiv:2502.14301v2 Announce Type: replace-cross 
Abstract: With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and culturally representative evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner. We make the SEA-HELM evaluation code publicly available.

### 摘要
随着大型语言模型(LLM)新型能力的快速涌现，对综合性多语言与多元文化基准测试的需求愈发凸显。尽管现有LLM基准能够评估模型在英语及东南亚(SEA)地区中低资源语言中的特定能力，但迄今为止尚未开发出全面且具有文化代表性的SEA语言评估套件。本文提出SEA-HELM——一个强调东南亚语言的综合性语言文化LLM评估体系，包含五大核心支柱：(1)自然语言处理经典任务，(2)LLM专项测试，(3)东南亚语言学特性，(4)东南亚文化特征，(5)安全性。该套件目前支持菲律宾语、印尼语、泰米尔语、泰语和越南语。我们同步推出SEA-HELM排行榜，使用户能够以系统化且用户友好的方式理解模型的多语言与多元文化表现。SEA-HELM评估代码已公开发布。

---

## [R-LoRA: Randomized Multi-Head LoRA for Efficient Multi-Task Learning](https://arxiv.org/abs/2502.15455)

### Abstract
arXiv:2502.15455v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) is computationally expensive, and Low-Rank Adaptation (LoRA) provides a cost-effective solution by approximating weight updates through low-rank matrices. In real-world scenarios, LLMs are fine-tuned on data from multiple domains to perform tasks across various fields, embodying multi-task learning (MTL). LoRA often underperforms in such complex scenarios. To enhance LoRA's capability in multi-task learning, we propose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head Randomization diversifies the head matrices through Multi-Head Dropout and Multi-Head Random Initialization, enabling more efficient learning of task-specific features while maintaining shared knowledge representation. Our approach not only improves performance in MTL but also reduces GPU memory usage and training time. Experiments show that R-LoRA's gains stem from increased diversity in the head matrices, demonstrating its effectiveness for multi-task learning. The code is available at https://github.com/jinda-liu/R-LoRA

### 摘要
微调大型语言模型（LLM）的计算成本高昂，而低秩适应（LoRA）通过低秩矩阵近似权重更新，提供了一种经济高效的解决方案。在实际场景中，大型语言模型需基于多领域数据进行微调以执行跨领域任务，这体现了多任务学习（MTL）的特性。然而，LoRA在此类复杂场景中往往表现不佳。为增强LoRA在多任务学习中的能力，我们提出R-LoRA方法，该方法引入多头随机化机制。通过多头丢弃（Multi-Head Dropout）和多头随机初始化（Multi-Head Random Initialization）技术，多头随机化使头部矩阵多样化，从而在保持共享知识表征的同时，更高效地学习任务特定特征。我们的方法不仅提升了多任务学习性能，还降低了GPU内存占用和训练时间。实验表明，R-LoRA的性能提升源于头部矩阵多样性的增加，验证了其对于多任务学习的有效性。代码已开源：https://github.com/jinda-liu/R-LoRA

---

## [Prediction hubs are context-informed frequent tokens in LLMs](https://arxiv.org/abs/2502.10201)

### Abstract
arXiv:2502.10201v2 Announce Type: replace-cross 
Abstract: Hubness, the tendency for a few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first prove that the only large-scale representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appearance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. However, when other distances are used to compare LLM representations, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. There are two main takeaways. First, hubness, while omnipresent in high-dimensional spaces, is not a negative property that needs to be mitigated when LLMs are being used for next token prediction. Second, when comparing representations from LLMs using Euclidean or cosine distance, there is a high risk of nuisance hubs and practitioners should use mitigation techniques if relevant.

### 摘要
枢纽性（hubness）指少数点成为大量其他点的最近邻的倾向性，当对高维数据应用标准距离度量时普遍存在，通常会对基于距离的分析产生负面影响。由于自回归大语言模型（LLMs）在高维表征上运行，我们探讨其是否同样受枢纽性影响。首先通过理论证明，LLMs执行的大规模表征比较操作（即上下文向量与非嵌入向量之间用于确定延续概率的比较）并不具备通常导致干扰性枢纽现象的距离集中特性。随后通过实证研究表明，这种比较仍会导致高度枢纽性，但此时的枢纽点并非干扰因素，而是由上下文调制的频繁标记（tokens）在下一标记预测候选池中高频出现所致。然而，当使用其他距离度量比较LLM表征时，由于缺乏相同理论保证，我们确实观察到干扰性枢纽点的出现。研究得出两个主要结论：第一，虽然枢纽性普遍存在于高维空间，但在LLMs用于下一标记预测时，其并非需要缓解的负面属性；第二，当使用欧氏距离或余弦距离比较LLM表征时，存在较高干扰性枢纽风险，实践者应根据情况采用缓解技术。

---

## [Standard Benchmarks Fail - Auditing LLM Agents in Finance Must Prioritize Risk](https://arxiv.org/abs/2502.15865)

### Abstract
arXiv:2502.15865v2 Announce Type: replace-cross 
Abstract: Standard benchmarks fixate on how well large language model (LLM) agents perform in finance, yet say little about whether they are safe to deploy. We argue that accuracy metrics and return-based scores provide an illusion of reliability, overlooking vulnerabilities such as hallucinated facts, stale data, and adversarial prompt manipulation. We take a firm position: financial LLM agents should be evaluated first and foremost on their risk profile, not on their point-estimate performance. Drawing on risk-engineering principles, we outline a three-level agenda: model, workflow, and system, for stress-testing LLM agents under realistic failure modes. To illustrate why this shift is urgent, we audit six API-based and open-weights LLM agents on three high-impact tasks and uncover hidden weaknesses that conventional benchmarks miss. We conclude with actionable recommendations for researchers, practitioners, and regulators: audit risk-aware metrics in future studies, publish stress scenarios alongside datasets, and treat ``safety budget'' as a primary success criterion. Only by redefining what ``good'' looks like can the community responsibly advance AI-driven finance.

### 摘要
标准基准测试固守于衡量大语言模型（LLM）智能体在金融领域的性能表现，却鲜少关注其部署安全性。我们认为准确性指标与收益导向的评分制造了可靠性的假象，忽视了诸如事实幻觉、数据陈旧和对抗性提示操控等脆弱性。本文明确主张：金融领域LLM智能体的评估应首要关注其风险特征，而非点估计表现。借鉴风险工程原理，我们提出三级压力测试框架——模型层、工作流层和系统层，用于在现实故障模式下检验LLM智能体。为阐明这一转变的紧迫性，我们对六个基于API和开源权重的LLM智能体开展三项高影响力任务审计，发现传统基准测试未能捕捉的潜在缺陷。最后向研究者、从业者和监管方提出可执行建议：在后续研究中采用风险感知的审计指标，随数据集同步发布压力测试场景，并将"安全预算"作为核心成功标准。唯有重新定义"优秀"的内涵，学界才能负责任地推进人工智能驱动的金融发展。

---

## [A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models](https://arxiv.org/abs/2503.01854)

### Abstract
arXiv:2503.01854v2 Announce Type: replace-cross 
Abstract: This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \textit&#123;LLM unlearning&#125;. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.

### 摘要
本研究探讨大型语言模型（LLMs）中的机器遗忘技术，即LLM遗忘。该技术提供了一种原则性方法，可在无需完全重新训练的情况下，消除不良数据（如敏感或非法信息）对LLMs的影响，同时保持模型整体性能。尽管研究兴趣日益增长，但目前缺乏对现有工作进行系统梳理与关键洞见提炼的综合性综述，本文旨在填补这一空白。我们首先阐述LLM遗忘的定义与范式，进而对现有研究进行完整分类。随后系统归纳当前主流遗忘方法，总结其优势与局限。此外，我们梳理了评估指标与基准测试，为当前评估方法提供结构化概览。最后，我们指出未来研究的潜在方向，强调该领域的关键挑战与机遇。

---

## [TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency](https://arxiv.org/abs/2502.19163)

### Abstract
arXiv:2502.19163v2 Announce Type: replace-cross 
Abstract: Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at https://github.com/HenryPengZou/TestNUC.

### 摘要
测试时计算方法通过利用推理过程中的额外计算资源，已被证明能有效提升大语言模型性能。本研究提出了一种新颖的线性扩展方法TestNUC，该方法通过利用相邻未标注数据的局部一致性来改进测试时预测——它不仅考虑模型对当前实例的预测结果，还结合对邻近未标注实例的预测结果进行分类。我们在意图分类、主题挖掘、领域发现和情感检测等八个多样化数据集上评估TestNUC，结果表明其性能始终优于标准提示法和自一致性等基线方法。此外，TestNUC可与现有测试时计算方法无缝集成，显著提升其性能。分析表明，TestNUC能有效随未标注数据量增加而扩展，并在不同嵌入模型中表现稳健，具有实际应用价值。代码已开源：https://github.com/HenryPengZou/TestNUC。

---

## [DIS-CO: Discovering Copyrighted Content in VLMs Training Data](https://arxiv.org/abs/2502.17358)

### Abstract
arXiv:2502.17358v3 Announce Type: replace-cross 
Abstract: How can we verify whether copyrighted content was used to train a large vision-language model (VLM) without direct access to its training data? Motivated by the hypothesis that a VLM is able to recognize images from its training corpus, we propose DIS-CO, a novel approach to infer the inclusion of copyrighted content during the model's development. By repeatedly querying a VLM with specific frames from targeted copyrighted material, DIS-CO extracts the content's identity through free-form text completions. To assess its effectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames paired with detailed captions, drawn from films released both before and after a model's training cutoff. Our results show that DIS-CO significantly improves detection performance, nearly doubling the average AUC of the best prior method on models with logits available. Our findings also highlight a broader concern: all tested models appear to have been exposed to some extent to copyrighted content. Our code and data are available at https://github.com/avduarte333/DIS-CO

### 摘要
我们如何在不直接访问训练数据的情况下，验证大型视觉语言模型（VLM）是否使用了受版权保护的内容进行训练？基于VLM能够识别其训练语料库中图像的假设，我们提出了DIS-CO这一创新方法，用于推断模型开发过程中是否包含受版权保护的内容。该方法通过向VLM重复输入目标版权材料的特定帧，利用自由文本补全技术提取内容身份信息。为评估其有效性，我们构建了MovieTection基准测试集，包含14,000帧图像及对应详细描述文本，这些素材均选自模型训练截止时间前后发布的电影。实验结果表明，DIS-CO显著提升了检测性能，在具有logits输出的模型上，其平均AUC值较现有最佳方法提升近一倍。研究还揭示了一个更广泛的问题：所有测试模型都显示出不同程度地接触过受版权保护内容。代码与数据详见https://github.com/avduarte333/DIS-CO。

---

## [Optimizing Multi-Hop Document Retrieval Through Intermediate Representations](https://arxiv.org/abs/2503.04796)

### Abstract
arXiv:2503.04796v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) encounters challenges when addressing complex queries, particularly multi-hop questions. While several methods tackle multi-hop queries by iteratively generating internal queries and retrieving external documents, these approaches are computationally expensive. In this paper, we identify a three-stage information processing pattern in LLMs during layer-by-layer reasoning, consisting of extraction, processing, and subsequent extraction steps. This observation suggests that the representations in intermediate layers contain richer information compared to those in other layers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike prior methods that focus on generating new internal queries, L-RAG leverages intermediate representations from the middle layers, which capture next-hop information, to retrieve external knowledge. L-RAG achieves performance comparable to multi-step approaches while maintaining inference overhead similar to that of standard RAG. Experimental results show that L-RAG outperforms existing RAG methods on open-domain multi-hop question-answering datasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is available in https://anonymous.4open.science/r/L-RAG-ADD5/

### 摘要
检索增强生成（RAG）在处理复杂查询（尤其是多跳问题）时面临挑战。现有方法通常通过迭代生成内部查询并检索外部文档来解决多跳查询，但这些方法计算成本高昂。本文发现大语言模型（LLM）在逐层推理过程中存在三阶段信息处理模式，包括提取、处理和再次提取步骤。这一现象表明中间层的表征相比其他层包含更丰富的信息。基于此发现，我们提出分层检索增强生成（L-RAG）。与先前专注于生成新内部查询的方法不同，L-RAG利用中间层捕获下一跳信息的表征来检索外部知识。L-RAG在保持与标准RAG相近推理开销的同时，实现了与多步方法相当的性能。实验结果表明，在开放域多跳问答数据集（包括MuSiQue、HotpotQA和2WikiMultiHopQA）上，L-RAG优于现有RAG方法。代码发布于https://anonymous.4open.science/r/L-RAG-ADD5/

---

## [Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models](https://arxiv.org/abs/2503.01461)

### Abstract
arXiv:2503.01461v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the constructed data. We conduct evaluation on various benchmarks such as math (GSM8K, MATH, AIME). instruction-following (Multi-IF) and planning (Blocksworld), results demonstrate our approaches substantially improve the reasoning performance of distilled models compared to standard distilled models via reducing the hallucinations in long-time thinking. The project homepage is https://github.com/AIDC-AI/Marco-o1.

### 摘要
诸如OpenAI o1和DeepSeek-R1等大型推理模型（LRMs）通过扩展测试时计算和生成长链思维（CoT），展现了卓越的推理能力。蒸馏——在LRMs生成的数据上进行后训练——是一种直接而有效的方法，用于增强较小模型的推理能力，但面临一个关键瓶颈：我们发现，蒸馏的长链思维数据对小型模型的学习造成困难，并在使用监督微调（SFT）和强化学习（RL）方法时导致偏见的继承（即过度思考）。为缓解这一瓶颈，我们提出通过蒙特卡洛树搜索（MCTS）从头构建基于树的链式思维数据。随后，我们利用一系列链式思维感知方法，包括思维长度平衡、细粒度DPO和联合后训练目标，以增强在构建数据上的SFT和RL。我们在多个基准测试（如数学（GSM8K、MATH、AIME）、指令跟随（Multi-IF）和规划（Blocksworld）上进行了评估，结果表明，与通过标准蒸馏得到的模型相比，我们的方法通过减少长时间思考中的幻觉，显著提升了蒸馏模型的推理性能。项目主页为https://github.com/AIDC-AI/Marco-o1。

---

## [ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning](https://arxiv.org/abs/2503.13089)

### Abstract
arXiv:2503.13089v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) scale, model compression is crucial for edge deployment and accessibility. Weight-only quantization reduces model size but suffers from performance degradation at lower bit widths. Moreover, standard finetuning is incompatible with quantized models, and alternative methods often fall short of full finetuning. In this paper, we propose ClusComp, a simple yet effective compression paradigm that clusters weight matrices into codebooks and finetunes them block-by-block. ClusComp (1) achieves superior performance in 2-4 bit quantization, (2) pushes compression to 1-bit while outperforming ultra-low-bit methods with minimal finetuning, and (3) enables efficient finetuning, even surpassing existing quantization-based approaches and rivaling full FP16 finetuning. Notably, ClusComp supports compression and finetuning of 70B LLMs on a single A6000-48GB GPU.

### 摘要
随着大语言模型（LLMs）规模的扩大，模型压缩对于边缘部署和可访问性至关重要。仅权重量化虽能减小模型体积，但在较低比特宽度下会出现性能下降。此外，标准微调方法与量化模型不兼容，而替代方案往往难以达到全精度微调的效果。本文提出ClusComp——一种简单高效的压缩范式，通过将权重矩阵聚类为码本并逐块微调，实现以下突破：（1）在2-4比特量化中取得卓越性能；（2）将压缩推进至1比特的同时，以极少量微调超越超低位方法；（3）支持高效微调，其效果不仅优于现有基于量化的方法，更能媲美FP16全精度微调。值得注意的是，ClusComp可在单块A6000-48GB GPU上完成700亿参数大模型的压缩与微调。

---

## [MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance](https://arxiv.org/abs/2503.13509)

### Abstract
arXiv:2503.13509v2 Announce Type: replace-cross 
Abstract: We introduce MentalChat16K, an English benchmark dataset combining a synthetic mental health counseling dataset and a dataset of anonymized transcripts from interventions between Behavioral Health Coaches and Caregivers of patients in palliative or hospice care. Covering a diverse range of conditions like depression, anxiety, and grief, this curated dataset is designed to facilitate the development and evaluation of large language models for conversational mental health assistance. By providing a high-quality resource tailored to this critical domain, MentalChat16K aims to advance research on empathetic, personalized AI solutions to improve access to mental health support services. The dataset prioritizes patient privacy, ethical considerations, and responsible data usage. MentalChat16K presents a valuable opportunity for the research community to innovate AI technologies that can positively impact mental well-being. The dataset is available at https://huggingface.co/datasets/ShenLab/MentalChat16K and the code and documentation are hosted on GitHub at https://github.com/ChiaPatricia/MentalChat16K.

### 摘要
我们推出MentalChat16K——一个结合合成心理健康咨询数据集与行为健康教练同姑息治疗/临终关怀患者护理者间匿名干预转录本的英文基准数据集。该精选数据集涵盖抑郁、焦虑和哀伤等多种症状，旨在促进对话式心理健康辅助大语言模型的开发与评估。通过提供这一关键领域的高质量定制资源，MentalChat16K致力于推动具有共情力的个性化AI解决方案研究，以改善心理健康支持服务的可及性。数据集严格遵循患者隐私保护、伦理考量及负责任数据使用原则，为研究界创新AI技术以提升心理健康福祉提供了宝贵机遇。数据集可通过https://huggingface.co/datasets/ShenLab/MentalChat16K获取，相关代码及文档存放于GitHub平台https://github.com/ChiaPatricia/MentalChat16K。

---

## [HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation](https://arxiv.org/abs/2503.04800)

### Abstract
arXiv:2503.04800v2 Announce Type: replace-cross 
Abstract: While Retrieval-Augmented Generation (RAG) has emerged as an effective approach for addressing the knowledge outdating problem in Large Language Models (LLMs), it still faces a critical challenge: the prevalence of outdated information in knowledge bases. Current research primarily focuses on incorporating up-to-date information, yet the impact of outdated information coexisting in retrieval sources remains inadequately addressed. To bridge this gap, we introduce HoH, the first benchmark specifically designed to evaluate the impact of outdated information on RAG. Our benchmark leverages token-level diff algorithms combined with LLM pipelines to efficiently create a large-scale QA dataset that accurately captures the evolution of temporal knowledge in real-world facts. Through comprehensive experiments, we reveal that outdated information significantly degrades RAG performance in two critical ways: (1) it substantially reduces response accuracy by distracting models from correct information, and (2) it can mislead models into generating potentially harmful outputs, even when current information is available. Current RAG approaches struggle with both retrieval and generation aspects when handling outdated information. These findings highlight the urgent need for innovative solutions to address the temporal challenges in RAG. Our code and data are available at: https://github.com/0russwest0/HoH.

### 摘要
尽管检索增强生成（RAG）已成为解决大语言模型（LLM）知识过时问题的有效方法，但其仍面临一个关键挑战：知识库中过时信息的普遍存在。当前研究主要聚焦于整合最新信息，然而检索源中过时信息的共存影响仍未得到充分解决。为填补这一空白，我们提出了首个专门评估过时信息对RAG影响的基准测试HoH。该基准通过结合令牌级差异算法与LLM流程，高效构建了大规模问答数据集，精准捕捉现实世界事实中时序知识的演变规律。综合实验表明，过时信息会通过两种关键方式显著降低RAG性能：（1）分散模型对正确信息的注意力，大幅降低回答准确率；（2）即便当前信息可用，仍可能误导模型生成具有潜在危害的输出。现有RAG方法在处理过时信息时，其检索与生成环节均存在明显不足。这些发现凸显了针对RAG时序挑战开发创新解决方案的迫切需求。代码与数据详见：https://github.com/0russwest0/HoH。

---

## [REALM: A Dataset of Real-World LLM Use Cases](https://arxiv.org/abs/2503.18792)

### Abstract
arXiv:2503.18792v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), such as the GPT series, have driven significant industrial applications, leading to economic and societal transformations. However, a comprehensive understanding of their real-world applications remains limited. To address this, we introduce REALM, a dataset of over 94,000 LLM use cases collected from Reddit and news articles. REALM captures two key dimensions: the diverse applications of LLMs and the demographics of their users. It categorizes LLM applications and explores how users' occupations relate to the types of applications they use. By integrating real-world data, REALM offers insights into LLM adoption across different domains, providing a foundation for future research on their evolving societal roles.

### 摘要
以GPT系列为代表的大语言模型（LLMs）已推动重大工业应用，引发经济与社会变革。然而，学界对其现实应用场景仍缺乏系统性认知。为此，我们构建了REALM数据集——通过采集Reddit论坛及新闻报道整理的94,000余条LLM应用案例。该数据集涵盖两大核心维度：LLM的多元化应用场景及其用户人口统计特征，既对模型应用进行分类研究，又探究用户职业属性与使用场景的关联。通过整合现实世界数据，REALM揭示了不同领域LLM的采纳现状，为后续研究模型社会角色演进提供了基础支撑。

---

## [GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification](https://arxiv.org/abs/2503.05763)

### Abstract
arXiv:2503.05763v3 Announce Type: replace-cross 
Abstract: Integrating structured graph data with rich textual information from nodes poses a significant challenge, particularly for heterophilic node classification. Current approaches often struggle with computational costs or effective fusion of disparate modalities. We propose \textbf&#123;Graph Masked Language Model (GMLM)&#125;, a novel architecture efficiently combining Graph Neural Networks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three key innovations: (i) a \textbf&#123;dynamic active node selection&#125; strategy for scalable PLM text processing; (ii) a GNN-specific \textbf&#123;contrastive pretraining stage&#125; using soft masking with a learnable graph \texttt&#123;[MASK]&#125; token for robust structural representations; and (iii) a \textbf&#123;dedicated fusion module&#125; integrating RGCN-based GNN embeddings with PLM (GTE-Small \&amp; DistilBERT) embeddings. Extensive experiments on heterophilic benchmarks (Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably, GMLM(DistilBERT) achieves significant performance gains, improving accuracy by over \textbf&#123;4.7\%&#125; on Cornell and over \textbf&#123;2.0\%&#125; on Texas compared to the previous best-performing baselines. This work underscores the benefits of targeted PLM engagement and modality-specific pretraining for improved, efficient learning on text-rich graphs.

### 摘要
将结构化图数据与节点丰富的文本信息相融合是一个重大挑战，尤其在异配性节点分类任务中。现有方法常受限于计算成本或多模态有效融合问题。本文提出图掩码语言模型(GMLM)，该新型架构高效结合了图神经网络(GNN)与预训练语言模型(PLM)，包含三大创新：(i) 面向可扩展PLM文本处理的动态活跃节点选择策略；(ii) 采用带可学习图[MASK]标记的软掩码技术，通过GNN专用对比预训练阶段获取鲁棒的结构表征；(iii) 整合基于RGCN的GNN嵌入与PLM(GTE-Small &amp; DistilBERT)嵌入的专用融合模块。在异配性基准数据集(Cornell、Wisconsin、Texas)上的大量实验验证了GMLM的优越性，其中GMLM(DistilBERT)相较最优基线模型取得显著提升：Cornell数据集准确率提高4.7%以上，Texas数据集提升2.0%以上。本研究表明，针对性的PLM参与机制与面向模态的预训练策略可有效提升文本丰富图数据的学习效率与性能。

---

## [Position: Beyond Assistance - Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care](https://arxiv.org/abs/2503.16456)

### Abstract
arXiv:2503.16456v2 Announce Type: replace-cross 
Abstract: This position paper argues for a fundamental shift in how Large Language Models (LLMs) are integrated into the mental health care domain. We advocate for their role as co-creators rather than mere assistive tools. While LLMs have the potential to enhance accessibility, personalization, and crisis intervention, their adoption remains limited due to concerns about bias, evaluation, over-reliance, dehumanization, and regulatory uncertainties. To address these challenges, we propose two structured pathways: SAFE-i (Supportive, Adaptive, Fair, and Ethical Implementation) Guidelines for ethical and responsible deployment, and HAAS-e (Human-AI Alignment and Safety Evaluation) Framework for multidimensional, human-centered assessment. SAFE-i provides a blueprint for data governance, adaptive model engineering, and real-world integration, ensuring LLMs align with clinical and ethical standards. HAAS-e introduces evaluation metrics that go beyond technical accuracy to measure trustworthiness, empathy, cultural sensitivity, and actionability. We call for the adoption of these structured approaches to establish a responsible and scalable model for LLM-driven mental health support, ensuring that AI complements, rather than replaces, human expertise.

### 摘要
本立场文件主张对大型语言模型（LLMs）在心理健康护理领域的整合方式进行根本性变革。我们倡导将其定位为共同创造者而非单纯辅助工具。尽管LLMs在提升可及性、个性化服务和危机干预方面具有潜力，但由于对偏见、评估、过度依赖、去人性化及监管不确定性的担忧，其应用仍受限制。为解决这些挑战，我们提出两条结构化路径：确保伦理与负责任部署的SAFE-i（支持性、适应性、公平性与伦理实施）指南，以及进行多维度、以人为本评估的HAAS-e（人机对齐与安全评估）框架。SAFE-i为数据治理、自适应模型工程和现实场景整合提供蓝图，确保LLMs符合临床与伦理标准。HAAS-e引入超越技术准确性的评估指标，用以衡量可信度、共情能力、文化敏感性与可操作性。我们呼吁采用这些结构化方法，以建立负责任且可扩展的LLM驱动心理健康支持模式，确保人工智能是对人类专业知识的补充而非替代。

---

## [ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems](https://arxiv.org/abs/2503.20756)

### Abstract
arXiv:2503.20756v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.

### 摘要
近年来，大型多模态模型（LMMs）在自动驾驶系统（ADS）中展现出应用潜力。然而，其对交通知识的误解、复杂道路条件以及车辆多样化状态等挑战阻碍了该模型在ADS中的直接应用。为解决这些问题，我们提出采用知识编辑技术，该方法无需完整模型重训练即可实现针对性行为修正。同时，我们推出了ADS-Edit——一个专为自动驾驶设计的多元知识编辑数据集，包含多样化真实场景、多模态数据类型及综合性评估指标。通过系统实验，我们得出了若干重要结论。本研究有望推动知识编辑技术在自动驾驶领域的深入应用。代码与数据详见https://github.com/zjunlp/EasyEdit。

---

## [VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted Code Transformations](https://arxiv.org/abs/2503.19449)

### Abstract
arXiv:2503.19449v2 Announce Type: replace-cross 
Abstract: Auto-vectorization is a fundamental optimization for modern compilers to exploit SIMD parallelism. However, state-of-the-art approaches still struggle to handle intricate code patterns, often requiring manual hints or domain-specific expertise. Large language models (LLMs), with their ability to capture intricate patterns, provide a promising solution, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. In this paper, we present VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilers auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. experimental results show that among all TSVC functions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans achieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test cases. This marks a significant advancement over state-of-the-art approaches while maintaining a cost efficiency of $0.012 per function optimization for LLM API usage.

### 摘要
自动向量化是现代编译器利用SIMD并行性的基础优化技术。然而，现有先进方法仍难以处理复杂代码模式，往往需要人工提示或领域专业知识。大型语言模型（LLMs）凭借其捕捉复杂模式的能力提供了潜在解决方案，但由于幻觉问题和缺乏领域特定推理，其在编译器优化中的有效应用仍面临挑战。本文提出VecTrans框架，该框架利用LLMs增强基于编译器的代码向量化。VecTrans首先通过编译器分析识别可向量化代码区域，随后调用LLM将这些区域重构为更易于编译器自动向量化的模式。为确保语义正确性，VecTrans进一步在中间表示（IR）层面集成混合验证机制。通过上述方法，VecTrans将LLMs的适应性与编译器向量化的精确性相结合，从而有效开辟向量化机会。实验结果表明，在GCC、ICC、Clang和毕昇编译器均无法向量化的所有TSVC函数中，VecTrans实现了1.77倍的几何平均加速比，并在51个测试用例中成功向量化24个。这标志着对现有先进方法的显著突破，同时保持每函数优化0.012美元的LLM API使用成本效率。

---

## [LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration](https://arxiv.org/abs/2504.00010)

### Abstract
arXiv:2504.00010v2 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) generation has made remarkable progress, yet existing systems still lack intuitive control over spatial composition, object consistency, and multi-step editing. We present $\textbf&#123;LayerCraft&#125;$, a modular framework that uses large language models (LLMs) as autonomous agents to orchestrate structured, layered image generation and editing. LayerCraft supports two key capabilities: (1) $\textit&#123;structured generation&#125;$ from simple prompts via chain-of-thought (CoT) reasoning, enabling it to decompose scenes, reason about object placement, and guide composition in a controllable, interpretable manner; and (2) $\textit&#123;layered object integration&#125;$, allowing users to insert and customize objects -- such as characters or props -- across diverse images or scenes while preserving identity, context, and style. The system comprises a coordinator agent, the $\textbf&#123;ChainArchitect&#125;$ for CoT-driven layout planning, and the $\textbf&#123;Object Integration Network (OIN)&#125;$ for seamless image editing using off-the-shelf T2I models without retraining. Through applications like batch collage editing and narrative scene generation, LayerCraft empowers non-experts to iteratively design, customize, and refine visual content with minimal manual effort. Code will be released at https://github.com/PeterYYZhang/LayerCraft.

### 摘要
文本到图像（T2I）生成技术已取得显著进展，但现有系统仍缺乏对空间构图、对象一致性和多步编辑的直观控制。我们提出$\textbf&#123;LayerCraft&#125;$，一种模块化框架，利用大语言模型（LLMs）作为自主代理来协调结构化、分层的图像生成与编辑。LayerCraft支持两大核心功能：（1）通过思维链（CoT）推理实现$\textit&#123;结构化生成&#125;$，能够从简单提示分解场景、推理对象布局，并以可控且可解释的方式指导构图；（2）$\textit&#123;分层对象集成&#125;$，允许用户在不同图像或场景中插入并自定义对象（如角色或道具），同时保持身份、上下文和风格的统一。该系统由协调代理$\textbf&#123;ChainArchitect&#125;$（负责CoT驱动的布局规划）和$\textbf&#123;对象集成网络（OIN）&#125;$（利用现成T2I模型实现无需重新训练的无缝图像编辑）组成。通过批量拼贴编辑和叙事场景生成等应用，LayerCraft使非专业人士能够以最少的手动操作迭代设计、定制和优化视觉内容。代码将在https://github.com/PeterYYZhang/LayerCraft发布。

---

## [Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation](https://arxiv.org/abs/2504.01919)

### Abstract
arXiv:2504.01919v3 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) has significantly reshaped the landscape of machine translation (MT), particularly for low-resource languages and domains that lack sufficient parallel corpora, linguistic tools, and computational infrastructure. This survey presents a comprehensive overview of recent progress in leveraging LLMs for MT. We analyze techniques such as few-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning (e.g., LoRA, adapters) that enable effective adaptation to under-resourced settings. The paper also explores synthetic data generation strategies using LLMs, including back-translation and lexical augmentation. Additionally, we compare LLM-based translation with traditional encoder-decoder models across diverse language pairs, highlighting the strengths and limitations of each. We discuss persistent challenges - such as hallucinations, evaluation inconsistencies, and inherited biases, while also evaluating emerging LLM-driven metrics for translation quality. This survey offers practical insights and outlines future directions for building robust, inclusive, and scalable MT systems in the era of large-scale generative models.

### 摘要
大型语言模型（LLM）的出现显著重塑了机器翻译（MT）的格局，尤其对于缺乏充足平行语料库、语言学工具和计算基础设施的低资源语言及领域。本综述全面概述了利用LLM进行机器翻译的最新进展，分析了包括少样本提示、跨语言迁移以及参数高效微调（如LoRA、适配器）等关键技术，这些技术能有效适应资源匮乏场景。论文还探讨了基于LLM的合成数据生成策略，包括反向翻译与词汇增强方法。此外，我们通过多语言对比实验，系统比较了LLM与传统编码器-解码器模型的翻译性能，阐明各自优势与局限。研究深入讨论了持续性挑战——如幻觉现象、评估标准不一致性及模型固有偏见等问题，同时评估了新兴的LLM驱动翻译质量评估指标。本综述为在大规模生成模型时代构建鲁棒、包容且可扩展的机器翻译系统提供了实践洞见，并指明了未来研究方向。

---

## [The Structural Safety Generalization Problem](https://arxiv.org/abs/2504.09712)

### Abstract
arXiv:2504.09712v2 Announce Type: replace-cross 
Abstract: LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research.

### 摘要
LLM越狱是一个普遍存在的安全挑战。鉴于该问题目前尚未得到有效解决，我们建议针对一个关键失效机制：安全防护在语义等价输入间泛化能力的缺失。我们进一步聚焦研究目标，要求所研究的攻击方法具备可解释性、模型间可迁移性及目标间可迁移性等理想特性。在此框架下，我们通过发现多轮对话、多图像及基于翻译的新型攻击漏洞进行红队测试。这些攻击通过设计与其单轮、单图像或未翻译版本保持语义等价，从而实现系统性对比；实验表明不同结构会导致不同的安全防护结果。随后，我们通过提出"结构重写护栏"证明该框架支持新型防御方案的潜力，该护栏将输入转换为更有利于安全评估的结构形式。该防护机制能显著提升对有害输入的拒绝率，同时避免过度拒绝良性输入。因此，通过构建这个比全局防御更易处理但对长期安全至关重要的中间挑战框架，我们为AI安全研究标定了一个关键里程碑。

---

## [The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs](https://arxiv.org/abs/2504.11711)

### Abstract
arXiv:2504.11711v3 Announce Type: replace-cross 
Abstract: Static analysis plays a crucial role in software vulnerability detection, yet faces a persistent precision-scalability tradeoff. In large codebases like the Linux kernel, traditional static analysis tools often generate excessive false positives due to simplified vulnerability modeling and overapproximation of path and data constraints. While large language models (LLMs) demonstrate promising code understanding capabilities, their direct application to program analysis remains unreliable due to inherent reasoning limitations.
  We introduce BugLens, a post-refinement framework that significantly enhances static analysis precision for bug detection. BugLens guides LLMs through structured reasoning steps to assess security impact and validate constraints from the source code. When evaluated on Linux kernel taint-style bugs detected by static analysis tools, BugLens improves precision approximately 7-fold (from 0.10 to 0.72), substantially reducing false positives while uncovering four previously unreported vulnerabilities. Our results demonstrate that a well-structured, fully automated LLM-based workflow can effectively complement and enhance traditional static analysis techniques.

### 摘要
静态分析在软件漏洞检测中起着关键作用，但始终面临精度与可扩展性之间的权衡。在Linux内核等大型代码库中，传统静态分析工具由于采用简化的漏洞建模以及对路径和数据约束的过度近似，往往会产生大量误报。尽管大语言模型（LLMs）展现出优异的代码理解能力，但其固有的推理限制使得直接应用于程序分析仍不可靠。

我们提出BugLens框架，该后置精炼系统能显著提升静态分析的漏洞检测精度。BugLens通过结构化推理步骤引导大语言模型评估安全影响并验证源代码中的约束条件。在针对静态分析工具检测出的Linux内核污点类漏洞进行评估时，BugLens将检测精度提升约7倍（从0.10提高至0.72），在发现四个未报告漏洞的同时大幅降低误报率。研究结果表明，基于大语言模型的结构化全自动工作流能有效补充并增强传统静态分析技术。

---

## [Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models](https://arxiv.org/abs/2503.22877)

### Abstract
arXiv:2503.22877v2 Announce Type: replace-cross 
Abstract: Fact-checking is a potentially useful application of Large Language Models (LLMs) to combat the growing dissemination of disinformation. However, the performance of LLMs varies across geographic regions. In this paper, we evaluate the factual accuracy of open and private models across a diverse set of regions and scenarios.
  Using a dataset containing 600 fact-checked statements balanced across six global regions we examine three experimental setups of fact-checking a statement: (1) when just the statement is available, (2) when an LLM-based agent with Wikipedia access is utilized, and (3) as a best case scenario when a Retrieval-Augmented Generation (RAG) system provided with the official fact check is employed. Our findings reveal that regardless of the scenario and LLM used, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global North perform substantially better than those from the Global South. Furthermore, this gap is broadened for the more realistic case of a Wikipedia agent-based system, highlighting that overly general knowledge bases have a limited ability to address region-specific nuances. These results underscore the urgent need for better dataset balancing and robust retrieval strategies to enhance LLM fact-checking capabilities, particularly in geographically diverse contexts.

### 摘要
事实核查是大型语言模型（LLMs）对抗日益增长的虚假信息传播的潜在应用。然而，LLMs在不同地理区域的性能存在差异。本文评估了开放和私有模型在不同区域和场景中的事实准确性。我们使用包含600条事实核查声明的数据集（均衡覆盖六个全球区域），研究了三种事实核查实验设置：（1）仅提供声明；（2）使用具备维基百科访问功能的LLM代理；（3）最佳情境下采用检索增强生成（RAG）系统并获取官方事实核查结果。研究发现，无论使用何种场景和LLM（包括GPT-4、Claude Sonnet和LLaMA），全球北方国家的声明表现显著优于全球南方国家。此外，在更贴近现实的维基百科代理系统中，这种差距进一步扩大，表明过度通用的知识库难以应对区域特异性差异。这些结果强调亟需改进数据集平衡和增强检索策略，以提升LLMs的事实核查能力，特别是在地理多样性语境中。

---

## [SD$^2$: Self-Distilled Sparse Drafters](https://arxiv.org/abs/2504.08838)

### Abstract
arXiv:2504.08838v2 Announce Type: replace-cross 
Abstract: Speculative decoding is a powerful technique for reducing the latency of Large Language Models (LLMs), offering a fault-tolerant framework that enables the use of highly compressed draft models. In this work, we introduce Self-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages self-data distillation and fine-grained weight sparsity to produce highly efficient and well-aligned draft models. SD$^2$ systematically enhances draft token acceptance rates while significantly reducing Multiply-Accumulate operations (MACs), even in the Universal Assisted Generation (UAG) setting, where draft and target models originate from different model families. On a Llama-3.1-70B target model, SD$^2$ provides a 1.59$\times$ higher Mean Accepted Length (MAL) compared to layer-pruned draft models and reduces MACs by over 43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our 1.5B and 3B unstructured sparse drafters outperform both dense and layer-pruned models in terms of end-to-end latency improvements; highlighting the potential of sparsity-aware fine-tuning and compression strategies to improve LLM inference efficiency while maintaining alignment with target models.

### 摘要
推测解码是一种有效降低大型语言模型（LLMs）延迟的技术，其通过容错框架支持使用高度压缩的草稿模型。本研究提出自蒸馏稀疏草稿模型（SD$^2$），该方法利用自数据蒸馏和细粒度权重稀疏化技术，生成高效且与目标模型高度对齐的草稿模型。SD$^2$在通用辅助生成（UAG）场景下（即草稿模型与目标模型来自不同模型家族时），能系统性提升草稿标记接受率，同时显著减少乘累加运算（MACs）。以Llama-3.1-70B为目标模型时，SD$^2$相较于层剪枝草稿模型将平均接受长度（MAL）提高1.59倍，相比稠密草稿模型减少43.87%的MACs且MAL仅下降8.36%。我们提出的1.5B和3B非结构化稀疏草稿模型在端到端延迟优化方面均优于稠密模型和层剪枝模型，这凸显了稀疏感知微调与压缩策略在提升LLM推理效率的同时保持目标模型对齐方面的潜力。

---

## [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)

### Abstract
arXiv:2504.13861v2 Announce Type: replace-cross 
Abstract: Though Large Vision-Language Models (LVLMs) are being actively explored in medicine, their ability to conduct telemedicine consultations combining accurate diagnosis with professional dialogue remains underexplored. In this paper, we present 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source framework for simulating and evaluating LVLM-driven telemedical consultations. 3MDBench simulates patient variability through four temperament-based Patient Agents and an Assessor Agent that jointly evaluate diagnostic accuracy and dialogue quality. It includes 3013 cases across 34 diagnoses drawn from real-world telemedicine interactions, combining textual and image-based data. The experimental study compares diagnostic strategies for popular LVLMs, including GPT-4o-mini, LLaVA-3.2-11B-Vision-Instruct, and Qwen2-VL-7B-Instruct. We demonstrate that multimodal dialogue with internal reasoning improves F1 score by 6.5% over non-dialogue settings, highlighting the importance of context-aware, information-seeking questioning. Moreover, injecting predictions from a diagnostic convolutional network into the LVLM's context boosts F1 by up to 20%. Source code is available at https://anonymous.4open.science/r/3mdbench_acl-0511.

### 摘要
尽管大型视觉语言模型（LVLMs）在医学领域正被积极探索，但其结合准确诊断与专业对话的远程医疗咨询能力仍未得到充分研究。本文提出3MDBench（医疗多模态多智能体对话基准），这是一个用于模拟和评估LVLM驱动远程医疗咨询的开源框架。3MDBench通过四个基于气质的患者智能体和一个评估智能体来模拟患者差异性，共同评估诊断准确性和对话质量。该框架包含来自真实远程医疗交互的34种诊断共3013个案例，整合了文本与图像数据。实验研究比较了主流LVLMs（包括GPT-4o-mini、LLaVA-3.2-11B-Vision-Instruct和Qwen2-VL-7B-Instruct）的诊断策略。研究表明，带有内部推理的多模态对话相较于非对话设置可将F1分数提升6.5%，凸显了情境感知的信息寻求提问的重要性。此外，将诊断卷积网络的预测结果注入LVLM上下文可使F1分数最高提升20%。源代码详见https://anonymous.4open.science/r/3mdbench_acl-0511。

---

## [Unveiling the Lack of LVLM Robustness to Fundamental Visual Variations: Why and Path Forward](https://arxiv.org/abs/2504.16727)

### Abstract
arXiv:2504.16727v3 Announce Type: replace-cross 
Abstract: Large Vision Language Models (LVLMs) excel in various vision-language tasks. Yet, their robustness to visual variations in position, scale, orientation, and context that objects in natural scenes inevitably exhibit due to changes in viewpoint and environment remains largely underexplored. To bridge this gap, we introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating Visual Variation Robustness of LVLMs, which encompasses automated evaluation dataset generation and principled metrics for thorough robustness assessment. Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability to visual variations, in which even advanced models that excel at complex vision-language tasks significantly underperform on simple tasks such as object recognition. Interestingly, these models exhibit a distinct visual position bias that contradicts theories of effective receptive fields, and demonstrate a human-like visual acuity threshold. To identify the source of these vulnerabilities, we present a systematic framework for component-level analysis, featuring a novel visualization approach for aligned visual features. Results show that these vulnerabilities stem from error accumulation in the pipeline architecture and inadequate multimodal alignment. Complementary experiments with synthetic data further demonstrate that these limitations are fundamentally architectural deficiencies, scoring the need for architectural innovations in future LVLM designs.

### 摘要
大型视觉语言模型（LVLMs）在各类视觉语言任务中表现卓越。然而，这些模型对自然场景中物体因视角和环境变化必然产生的位置、尺度、方位及上下文等视觉变动的鲁棒性，目前仍缺乏深入研究。为填补这一空白，我们提出V²R-Bench——一个用于评估LVLMs视觉变异鲁棒性的综合基准框架，包含自动化评估数据集生成和系统性度量指标以实现全面鲁棒性评估。通过对21个LVLMs的广泛测试，我们揭示了模型对视觉变异的惊人脆弱性：即便在复杂视觉语言任务中表现优异的先进模型，在物体识别等基础任务上也会显著失效。有趣的是，这些模型表现出与有效感受野理论相悖的视觉位置偏差，同时呈现出类人类的视觉敏锐度阈值。为探究脆弱性根源，我们提出组件级分析的系统框架，采用创新的对齐视觉特征可视化方法。结果表明，这些缺陷源于流水线架构中的误差累积及多模态对齐不足。合成数据补充实验进一步证明，这些局限性本质上是架构缺陷，凸显未来LVLM设计需要进行架构创新。

---

## [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)

### Abstract
arXiv:2504.17004v2 Announce Type: replace-cross 
Abstract: Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations.
  First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.

### 摘要
自动检测幻觉是否可行？本研究提出一个理论框架来分析大型语言模型（LLM）产生幻觉的自动检测可行性。受Gold-Angluin经典语言识别框架及其近期被Kleinberg和Mullainathan应用于语言生成的启发，我们探究算法在从未知目标语言$K$（选自可数集合）的示例中训练并访问LLM时，能否可靠判断LLM输出正确或构成幻觉。

首先，我们建立幻觉检测与经典语言识别任务的等价性。证明任何幻觉检测方法均可转换为语言识别方法，反之亦然。鉴于语言识别固有的困难性，这表明若检测器仅使用目标语言的正例训练，则对大多数语言集合而言幻觉检测从根本上不可行。

其次，我们发现采用专家标注反馈（即同时使用正例和显式标注错误陈述的负例训练检测器）会彻底改变这一结论。在此增强训练机制下，自动幻觉检测对所有可数语言集合成为可能。

这些结果凸显了专家标注样本在训练幻觉检测器中的核心作用，并为基于反馈的方法（如人类反馈强化学习RLHF）提供了理论支撑——这类方法已被证明对LLM可靠部署至关重要。

---

