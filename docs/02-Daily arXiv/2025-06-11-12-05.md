# 2025-06-11-12-05

## [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/abs/2506.08119)

### Abstract
arXiv:2506.08119v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate impressive general-purpose reasoning and problem-solving abilities. However, they struggle with executing complex, long-horizon workflows that demand strict adherence to Standard Operating Procedures (SOPs), a critical requirement for real-world industrial automation. Despite this need, there is a lack of public benchmarks that reflect the complexity, structure, and domain-specific nuances of SOPs. To address this, we present three main contributions. First, we introduce a synthetic data generation framework to create realistic, industry-grade SOPs that rigorously test the planning, reasoning, and tool-use capabilities of LLM-based agents. Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800 tasks across 10 industrial domains, each with APIs, tool interfaces, and human-validated test cases. Third, we evaluate two prominent agent architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing average success rates of only 27% and 48%, respectively. Remarkably, when the tool registry is much larger than necessary, agents invoke incorrect tools nearly 100% of the time. These findings underscore a substantial gap between current agentic capabilities of LLMs and the demands of automating real-world SOPs. Performance varies significantly by task and domain, highlighting the need for domain-specific benchmarking and architectural choices before deployment. SOP-Bench is publicly available at http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the prompts underpinning the data generation framework to support new domain-specific SOP benchmarks. We invite the community to extend SOP-Bench with SOPs from their industrial domains.

---

## [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)

### Abstract
arXiv:2506.08249v1 Announce Type: new 
Abstract: Language models (LMs) are increasingly being deployed to perform autonomous data analyses. However, their data awareness -- the ability to recognize, reason over, and appropriately handle data artifacts such as missing values, outliers, and logical inconsistencies -- remains underexplored. These artifacts are especially common in real-world tabular data and, if mishandled, can significantly compromise the validity of analytical conclusions. To address this gap, we present RADAR, a benchmark for systematically evaluating data-aware reasoning on tabular data. We develop a framework to simulate data artifacts via programmatic perturbations to enable targeted evaluation of model behavior. RADAR comprises 2980 table query pairs, grounded in real-world data spanning 9 domains and 5 data artifact types. In addition to evaluating artifact handling, RADAR systematically varies table size to study how reasoning performance holds when increasing table size. Our evaluation reveals that, despite decent performance on tables without data artifacts, frontier models degrade significantly when data artifacts are introduced, exposing critical gaps in their capacity for robust, data-aware analysis. Designed to be flexible and extensible, RADAR supports diverse perturbation types and controllable table sizes, offering a valuable resource for advancing tabular reasoning.

### 摘要
语言模型（LMs）正越来越多地被部署用于执行自主数据分析。然而，其数据意识——即识别、推理并恰当处理缺失值、异常值和逻辑不一致等数据伪影的能力——仍未得到充分探索。这些伪影在现实世界表格数据中尤为常见，若处理不当将严重损害分析结论的有效性。为解决这一不足，我们提出RADAR基准测试，用于系统评估表格数据的数据感知推理能力。我们开发了一个通过程序化扰动模拟数据伪影的框架，以实现对模型行为的针对性评估。RADAR包含2980个基于真实世界数据的表格-查询对，涵盖9个领域和5种数据伪影类型。除评估伪影处理能力外，RADAR还系统性地调整表格尺寸以研究推理性能随表格扩大的变化规律。实验结果表明：前沿模型在无数据伪影的表格上表现尚可，但一旦引入数据伪影，其性能显著下降，暴露出其在稳健数据感知分析能力上的重大缺陷。RADAR设计具有灵活性和可扩展性，支持多种扰动类型和可控表格尺寸，为推进表格推理研究提供了宝贵资源。

---

## [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/abs/2506.08332)

### Abstract
arXiv:2506.08332v1 Announce Type: new 
Abstract: Machine learning has been widely used to optimize complex engineering workflows across numerous domains. In the context of integrated circuit design, modern flows (e.g., going from a register-transfer level netlist to physical layouts) involve extensive configuration via thousands of parameters, and small changes to these parameters can have large downstream impacts on desired outcomes - namely design performance, power, and area. Recent advances in Large Language Models (LLMs) offer new opportunities for learning and reasoning within such high-dimensional optimization tasks. In this work, we introduce ORFS-agent, an LLM-based iterative optimization agent that automates parameter tuning in an open-source hardware design flow. ORFS-agent adaptively explores parameter configurations, demonstrating clear improvements over standard Bayesian optimization approaches in terms of resource efficiency and final design metrics. Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Moreover, by following natural language objectives to trade off certain metrics for others, ORFS-agent demonstrates a flexible and interpretable framework for multi-objective optimization. Crucially, RFS-agent is modular and model-agnostic, and can be plugged in to any frontier LLM without any further fine-tuning.

### 摘要
机器学习已被广泛应用于优化众多领域的复杂工程流程。在集成电路设计领域，现代设计流程（如从寄存器传输级网表到物理版图）涉及通过数千个参数进行广泛配置，这些参数的微小变化可能对设计性能、功耗和面积等关键指标产生重大下游影响。大型语言模型（LLM）的最新进展为这类高维优化任务提供了新的学习和推理机遇。本研究提出ORFS-agent，这是一种基于LLM的迭代优化代理，可自动化实现开源硬件设计流程中的参数调优。该代理能自适应探索参数配置，在资源效率和最终设计指标方面均显著优于标准贝叶斯优化方法。我们在两种不同工艺节点和一系列电路基准上的实证评估表明，ORFS-agent可将布线长度和有效时钟周期改善13%以上，同时减少40%的优化迭代次数。此外，通过遵循自然语言目标在不同指标间进行权衡，ORFS-agent展现出灵活且可解释的多目标优化框架。值得注意的是，RFS-agent采用模块化设计且与模型无关，无需微调即可适配任何前沿LLM。

---

## [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/abs/2506.08134)

### Abstract
arXiv:2506.08134v1 Announce Type: new 
Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML), is strained by a crisis of scale. Exponential growth in manuscript submissions to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite capacity of qualified reviewers, leading to concerns about review quality, consistency, and reviewer fatigue. This position paper argues that AI-assisted peer review must become an urgent research and infrastructure priority. We advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language Models (LLMs) not as replacements for human judgment, but as sophisticated collaborators for authors, reviewers, and Area Chairs (ACs). We propose specific roles for AI in enhancing factual verification, guiding reviewer performance, assisting authors in quality improvement, and supporting ACs in decision-making. Crucially, we contend that the development of such systems hinges on access to more granular, structured, and ethically-sourced peer review process data. We outline a research agenda, including illustrative experiments, to develop and validate these AI assistants, and discuss significant technical and ethical challenges. We call upon the ML community to proactively build this AI-assisted future, ensuring the continued integrity and scalability of scientific validation, while maintaining high standards of peer review.

### 摘要
同行评审作为机器学习（ML）领域科学进步的基石，正面临规模危机带来的严峻挑战。NeurIPS、ICML和ICLR等顶级ML会议稿件提交量的指数级增长，已远超合格评审人员的有限承载能力，引发了对评审质量、一致性和审稿疲劳的普遍担忧。本立场文件主张，AI辅助评审必须成为紧迫的研究与基础设施优先事项。我们倡导构建一个全面的AI增强生态系统，其核心在于将大语言模型（LLMs）定位于人类判断的协同者而非替代者，为作者、审稿人和领域主席（ACs）提供智能化协作支持。我们具体提出AI可在四个方面发挥作用：增强事实核查、指导审稿表现、辅助作者提升稿件质量，以及支持ACs决策。关键的是，我们认为此类系统的开发依赖于获取更细粒度、结构化且符合伦理的评审过程数据。本文规划了包含示范性实验的研究议程以开发验证这些AI辅助工具，并探讨了重大技术伦理挑战。我们呼吁ML学界主动共建这个AI辅助的未来，在维持高标准同行评审的同时，确保科学验证的持续完整性与可扩展性。

---

## [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/abs/2506.08098)

### Abstract
arXiv:2506.08098v1 Announce Type: new 
Abstract: The emergence of capable large language model (LLM) based agents necessitates memory architectures that transcend mere data storage, enabling continuous learning, nuanced reasoning, and dynamic adaptation. Current memory systems often grapple with fundamental limitations in structural flexibility, temporal awareness, and the ability to synthesize higher-level insights from raw interaction data. This paper introduces Cognitive Weave, a novel memory framework centered around a multi-layered spatio-temporal resonance graph (STRG). This graph manages information as semantically rich insight particles (IPs), which are dynamically enriched with resonance keys, signifiers, and situational imprints via a dedicated semantic oracle interface (SOI). These IPs are interconnected through typed relational strands, forming an evolving knowledge tapestry. A key component of Cognitive Weave is the cognitive refinement process, an autonomous mechanism that includes the synthesis of insight aggregates (IAs) condensed, higher-level knowledge structures derived from identified clusters of related IPs. We present comprehensive experimental results demonstrating Cognitive Weave's marked enhancement over existing approaches in long-horizon planning tasks, evolving question-answering scenarios, and multi-session dialogue coherence. The system achieves a notable 34% average improvement in task completion rates and a 42% reduction in mean query latency when compared to state-of-the-art baselines. Furthermore, this paper explores the ethical considerations inherent in such advanced memory systems, discusses the implications for long-term memory in LLMs, and outlines promising future research trajectories.

### 摘要
基于大型语言模型（LLM）的智能代理的崛起，要求内存架构超越单纯的数据存储，实现持续学习、细致推理和动态适应。当前内存系统常受限于结构灵活性、时间感知能力以及从原始交互数据中综合高层次洞见的根本性缺陷。本文提出'认知编织'——一种以多层时空共振图（STRG）为核心的新型记忆框架。该框架将信息管理为语义丰富的洞见粒子（IPs），通过专用语义预言接口（SOI）动态注入共振密钥、指征符和情境印记。这些IPs通过类型化关系链相互连接，形成动态演进的知识图谱。关键组件'认知精炼流程'作为自主机制，能综合从相关IPs聚类中衍生的浓缩高阶知识结构——洞见聚合体（IAs）。实验结果表明，在长周期规划任务、演进式问答场景及多会话对话连贯性方面，认知编织较现有方法有显著提升：任务完成率平均提高34%，查询延迟均值降低42%。此外，本文探讨了此类先进内存系统固有的伦理考量，分析了其对LLM长期记忆的影响，并展望了未来研究方向。

---

## [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)

### Abstract
arXiv:2506.08321v1 Announce Type: new 
Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for math proofs. LeanTutor interacts with the student in natural language, formally verifies student-written math proofs in Lean, generates correct next steps, and provides the appropriate instructional guidance. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. The first module faithfully autoformalizes student proofs into Lean and verifies proof accuracy via successful code compilation. If the proof has an error, the incorrect step is identified. The next-step generator module outputs a valid next Lean tactic for incorrect proofs via LLM-based candidate generation and proof search. The feedback generator module leverages Lean data to produce a pedagogically-motivated natural language hint for the student user. To evaluate our system, we introduce PeanoBench, a human-written dataset derived from the Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each natural language proof step is paired with the corresponding logically equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of tactics in correct proofs and accurately identifies the incorrect step in 30% of incorrect proofs. In generating natural language hints for erroneous proofs, LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

### 摘要
我们推出LeanTutor——一个基于大型语言模型(LLM)的数学证明辅导系统。该系统通过自然语言与学生互动，能够对以Lean语言编写的数学证明进行形式化验证，生成正确的后续步骤，并提供恰当的教学指导。LeanTutor由三个模块组成：(1)自动形式化/证明检查器；(2)后续步骤生成器；(3)自然语言反馈生成器。首个模块能准确地将学生证明自动形式化为Lean代码，并通过成功编译来验证证明的正确性。若证明存在错误，该模块可识别错误步骤。后续步骤生成模块通过基于LLM的候选生成和证明搜索，为错误证明输出有效的后续Lean策略。反馈生成模块则利用Lean数据，为学生用户生成具有教学意义的自然语言提示。为评估系统性能，我们提出了PeanoBench数据集——源自"自然数游戏"的人工编写数据集，包含371个皮亚诺算术证明，其中每个自然语言证明步骤都与Lean中逻辑等效的策略相对应。实验表明，自动形式化模块能正确形式化57%的正确证明策略，并在30%的错误证明中准确定位错误步骤。在针对错误证明生成自然语言提示方面，LeanTutor在准确性和相关性指标上均优于简单基线模型。

---

## [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)

### Abstract
arXiv:2506.08446v1 Announce Type: new 
Abstract: Mathematical reasoning has long represented one of the most fundamental and challenging frontiers in artificial intelligence research. In recent years, large language models (LLMs) have achieved significant advances in this area. This survey examines the development of mathematical reasoning abilities in LLMs through two high-level cognitive phases: comprehension, where models gain mathematical understanding via diverse pretraining strategies, and answer generation, which has progressed from direct prediction to step-by-step Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical reasoning, ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discuss recent work on extended CoT and "test-time scaling". Despite notable progress, fundamental challenges remain in terms of capacity, efficiency, and generalization. To address these issues, we highlight promising research directions, including advanced pretraining and knowledge augmentation techniques, formal reasoning frameworks, and meta-generalization through principled learning paradigms. This survey tries to provide some insights for researchers interested in enhancing reasoning capabilities of LLMs and for those seeking to apply these techniques to other domains.

### 摘要
数学推理长期以来一直是人工智能研究中最基础且最具挑战性的前沿领域之一。近年来，大型语言模型（LLMs）在该领域取得了显著进展。本综述通过两个高层认知阶段考察LLMs数学推理能力的发展：理解阶段（模型通过多样化预训练策略获得数学认知）和答案生成阶段（从直接预测逐步发展到分步思维链推理）。我们系统回顾了增强数学推理的方法，涵盖从零样本提示到监督微调、强化学习等微调方法，并探讨了扩展思维链与"测试时缩放"的最新研究。尽管进展显著，但在模型容量、效率和泛化性方面仍存在根本性挑战。针对这些问题，我们重点指出了若干具有前景的研究方向，包括先进预训练与知识增强技术、形式化推理框架，以及通过原则性学习范式实现元泛化。本综述旨在为关注LLMs推理能力提升的研究者，以及寻求将这些技术应用于其他领域的研究人员提供参考。

---

## [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/abs/2506.08422)

### Abstract
arXiv:2506.08422v1 Announce Type: new 
Abstract: Having a unified, coherent taxonomy is essential for effective knowledge representation in domain-specific applications as diverse terminologies need to be mapped to underlying concepts. Traditional manual approaches to taxonomy alignment rely on expert review of concept pairs, but this becomes prohibitively expensive and time-consuming at scale, while subjective interpretations often lead to expert disagreements. Existing automated methods for taxonomy alignment have shown promise but face limitations in handling nuanced semantic relationships and maintaining consistency across different domains. These approaches often struggle with context-dependent concept mappings and lack transparent reasoning processes. We propose a novel framework that combines large language models (LLMs) with expert calibration and iterative prompt optimization to automate taxonomy alignment. Our method integrates expert-labeled examples, multi-stage prompt engineering, and human validation to guide LLMs in generating both taxonomy linkages and supporting rationales. In evaluating our framework on a domain-specific mapping task of concept essentiality, we achieved an F1-score of 0.97, substantially exceeding the human benchmark of 0.68. These results demonstrate the effectiveness of our approach in scaling taxonomy alignment while maintaining high-quality mappings and preserving expert oversight for ambiguous cases.

### 摘要
在领域特定应用中，统一的、连贯的分类体系对于有效的知识表征至关重要，因为需要将多样化的术语映射到基础概念上。传统的分类体系对齐方法依赖于专家对概念对的评审，但在大规模应用时成本高昂且耗时，同时主观解释常导致专家意见分歧。现有的自动分类体系对齐方法虽展现出潜力，但在处理细微语义关系和保持跨领域一致性方面存在局限。这些方法往往难以应对依赖上下文的概念映射，且缺乏透明的推理过程。我们提出了一种新颖框架，将大语言模型（LLMs）与专家校准及迭代式提示优化相结合，以实现分类体系自动对齐。该方法整合了专家标注示例、多阶段提示工程和人工验证，以引导LLMs生成分类体系关联及支持性论证。在对概念必要性这一特定领域映射任务的评估中，我们的框架取得了0.97的F1分数，显著超过人工基准的0.68。这些结果表明，我们的方法在扩展分类体系对齐规模的同时，能保持高质量的映射，并为模糊案例保留专家监督，具有显著有效性。

---

## [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)

### Abstract
arXiv:2506.08462v1 Announce Type: new 
Abstract: Industrial processes must be robust and adaptable, as environments and tasks are often unpredictable, while operational errors remain costly and difficult to detect. AI-based control systems offer a path forward, yet typically depend on supervised learning with extensive labelled datasets, which limits their ability to generalize across variable and data-scarce industrial settings. Foundation models could enable broader reasoning and knowledge integration, but rarely deliver the quantitative precision demanded by engineering applications. Here, we introduceControl and Interpretation of Production via Hybrid Expertise and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming to replicate human-like reasoning for industrial control, instantiated in a commercial-grade 3D printer. It integrates a process expert, a regression model enabling quantitative characterization of system states required for engineering tasks. CIPHER also incorporates retrieval-augmented generation to access external expert knowledge and support physics-informed, chain-of-thought reasoning. This hybrid architecture exhibits strong generalization to out-of-distribution tasks. It interprets visual or textual inputs from process monitoring, explains its decisions, and autonomously generates precise machine instructions, without requiring explicit annotations. CIPHER thus lays the foundations for autonomous systems that act with precision, reason with context, and communicate decisions transparently, supporting safe and trusted deployment in industrial settings.

### 摘要
工业过程必须具备鲁棒性和适应性，因为工作环境与任务往往难以预测，而操作错误仍然成本高昂且难以检测。基于人工智能的控制系统提供了一条可行路径，但这类系统通常依赖于需要大量标注数据的监督学习，这限制了其在多变且数据稀缺的工业场景中的泛化能力。基础模型虽能实现更广泛的推理与知识整合，却很少能满足工程应用所需的定量精度要求。本文提出'基于混合专家与推理的生产控制与解释系统'(CIPHER)：一种旨在模拟人类工业控制推理能力的视觉-语言-动作(VLA)模型框架，并在商用级3D打印机上实现。该系统集成了过程专家模块——一个能为工程任务提供所需系统状态定量表征的回归模型，同时采用检索增强生成技术来获取外部专家知识并支持基于物理原理的思维链推理。这种混合架构展现出对分布外任务的强泛化能力，能够解释来自过程监控的视觉或文本输入，说明其决策依据，并自动生成精确的机器指令，且无需显式标注。CIPHER因此为自主系统奠定了技术基础：这些系统能以精确性执行操作、基于上下文进行推理，并透明地传达决策，从而支持工业场景中安全可信的部署。

---

## [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/abs/2506.08532)

### Abstract
arXiv:2506.08532v1 Announce Type: new 
Abstract: The rapid growth of the low-altitude economy has driven the widespread adoption of unmanned aerial vehicles (UAVs). This growing deployment presents new challenges for UAV trajectory planning in complex urban environments. However, existing studies often overlook key factors, such as urban airspace constraints and economic efficiency, which are essential in low-altitude economy contexts. Deep reinforcement learning (DRL) is regarded as a promising solution to these issues, while its practical adoption remains limited by low learning efficiency. To overcome this limitation, we propose a novel UAV trajectory planning framework that combines DRL with large language model (LLM) reasoning to enable safe, compliant, and economically viable path planning. Experimental results demonstrate that our method significantly outperforms existing baselines across multiple metrics, including data collection rate, collision avoidance, successful landing, regulatory compliance, and energy efficiency. These results validate the effectiveness of our approach in addressing UAV trajectory planning key challenges under constraints of the low-altitude economy networking.

### 摘要
低空经济的快速发展推动了无人飞行器（UAV）的广泛应用。这种日益增长的部署为复杂城市环境中的无人机轨迹规划带来了新的挑战。然而现有研究往往忽略了低空经济场景中的关键因素，如城市空域约束和经济效率。深度强化学习（DRL）被视为解决这些问题的潜在方案，但其实际应用仍受限于较低的学习效率。为突破这一局限，我们提出了一种新型无人机轨迹规划框架，通过将DRL与大语言模型（LLM）推理相结合，实现安全、合规且经济可行的路径规划。实验结果表明，我们的方法在数据采集率、碰撞规避、成功着陆、法规遵从性和能源效率等多重指标上显著优于现有基线。这些结果验证了本方法在低空经济网络约束条件下解决无人机轨迹规划关键挑战的有效性。

---

## [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)

### Abstract
arXiv:2506.08507v1 Announce Type: new 
Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality.

### 摘要
由大语言模型（LLM）驱动的多智能体系统（MAS）近期已成为解决复杂现实任务的重要范式。然而，现有MAS构建方法通常依赖人工设计的交互机制或启发式规则，这不仅引入了人为偏见，还限制了系统自主能力。尽管近期自适应MAS构建取得进展，现有系统仍主要局限于半自主模式。本研究提出MasHost——一个基于强化学习（RL）的自主且查询自适应的MAS设计框架。通过将MAS构建建模为图搜索问题，所提出的MasHost通过统一概率采样机制联合采样智能体角色及其交互关系。除前人工作中追求的准确性与效率目标外，我们引入组件合理性作为MAS设计中新颖的附加原则。为实现这一多目标优化，提出分层相对策略优化（HRPO），这种新型RL策略通过协同整合组间相对优势与动作级奖励来实现优化。据我们所知，MasHost是首个基于RL的自主MAS图构建框架。在六个基准测试上的大量实验表明，MasHost持续优于多数竞争基线，验证了其有效性、效率及结构合理性。

---

## [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/abs/2506.08390)

### Abstract
arXiv:2506.08390v1 Announce Type: new 
Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can automatically allocate more reasoning strengths (i.e., the number of reasoning tokens) for harder problems, exhibiting difficulty-awareness for better task performance. While this automatic reasoning strength allocation phenomenon has been widely observed, its underlying mechanism remains largely unexplored. To this end, we provide explanations for this phenomenon from the perspective of model activations. We find evidence that LRMs pre-plan the reasoning strengths in their activations even before generation, with this reasoning strength causally controlled by the magnitude of a pre-allocated directional vector. Specifically, we show that the number of reasoning tokens is predictable solely based on the question activations using linear probes, indicating that LRMs estimate the required reasoning strength in advance. We then uncover that LRMs encode this reasoning strength through a pre-allocated directional vector embedded in the activations of the model, where the vector's magnitude modulates the reasoning strength. Subtracting this vector can lead to reduced reasoning token number and performance, while adding this vector can lead to increased reasoning token number and even improved performance. We further reveal that this direction vector consistently yields positive reasoning length prediction, and it modifies the logits of end-of-reasoning token &lt;/think&gt; to affect the reasoning length. Finally, we demonstrate two potential applications of our findings: overthinking behavior detection and enabling efficient reasoning on simple problems. Our work provides new insights into the internal mechanisms of reasoning in LRMs and offers practical tools for controlling their reasoning behaviors. Our code is available at https://github.com/AlphaLab-USTC/LRM-plans-CoT.

### 摘要
近期研究表明，大型推理模型（LRMs）能够针对难度更高的问题自动分配更多推理强度（即推理标记数量），这种难度感知特性可提升任务表现。尽管这种自动推理强度分配现象已被广泛观测到，但其内在机制仍未得到充分探索。为此，我们从模型激活的角度对这一现象进行解释。研究发现，LRMs在生成前便通过激活状态预先规划推理强度，这种推理强度由预分配方向向量的模长进行因果控制。具体而言，我们证明仅通过线性探测器基于问题激活状态即可预测推理标记数量，表明LRMs会预先估算所需推理强度。进一步发现，LRMs通过嵌入模型激活状态中的预分配方向向量来编码推理强度，该向量的模长调控着推理强度：减去该向量会导致推理标记数量与性能下降，而增加该向量则能提升推理标记数量甚至改善性能。我们揭示该方向向量始终产生正向推理长度预测，并通过修改终止推理标记的logits来影响推理长度。最后，我们展示了研究发现的两种潜在应用：过度推理行为检测与简单问题高效推理实现。本研究为LRMs的推理内部机制提供了新见解，并为控制其推理行为提供了实用工具。

---

## [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/abs/2506.08486)

### Abstract
arXiv:2506.08486v1 Announce Type: new 
Abstract: The rise of large language models (LLMs) has created new possibilities for digital twins in healthcare. However, the deployment of such systems in consumer health contexts raises significant concerns related to hallucination, bias, lack of transparency, and ethical misuse. In response to recommendations from health authorities such as the World Health Organization (WHO), we propose Responsible Health Twin (RHealthTwin), a principled framework for building and governing AI-powered digital twins for well-being assistance. RHealthTwin processes multimodal inputs that guide a health-focused LLM to produce safe, relevant, and explainable responses. At the core of RHealthTwin is the Responsible Prompt Engine (RPE), which addresses the limitations of traditional LLM configuration. Conventionally, users input unstructured prompt and the system instruction to configure the LLM, which increases the risk of hallucination. In contrast, RPE extracts predefined slots dynamically to structure both inputs. This guides the language model to generate responses that are context aware, personalized, fair, reliable, and explainable for well-being assistance. The framework further adapts over time through a feedback loop that updates the prompt structure based on user satisfaction. We evaluate RHealthTwin across four consumer health domains including mental support, symptom triage, nutrition planning, and activity coaching. RPE achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical compliance and instruction-following metrics using LLM-as-judge evaluation, outperforming baseline strategies. We envision RHealthTwin as a forward-looking foundation for responsible LLM-based applications in health and well-being.

### 摘要
大型语言模型（LLM）的兴起为医疗保健领域的数字孪生创造了新的可能性。然而，此类系统在消费者健康场景中的部署引发了关于幻觉、偏见、缺乏透明度和伦理滥用等方面的重大担忧。为响应世界卫生组织（WHO）等卫生机构的建议，我们提出'负责任健康孪生'（RHealthTwin）框架——一个用于构建和管理人工智能驱动的健康辅助数字孪生的原则性框架。RHealthTwin通过处理多模态输入，引导健康导向的LLM生成安全、相关且可解释的响应。其核心是'负责任提示引擎'（RPE），该引擎解决了传统LLM配置的局限性。传统方法中，用户需输入非结构化提示和系统指令来配置LLM，这会增加幻觉风险。相比之下，RPE通过动态提取预定义槽位来结构化输入内容，从而引导语言模型生成具有情境感知、个性化、公平、可靠且可解释的健康辅助响应。该框架还通过基于用户满意度更新提示结构的反馈循环实现持续优化。我们在心理健康支持、症状分诊、营养规划和活动指导四个消费者健康领域评估RHealthTwin。RPE在基准数据集上取得了BLEU=0.41、ROUGE-L=0.63和BERTScore=0.89的最先进结果。通过LLM作为评估者的方法，我们在伦理合规性和指令遵循指标上超过90%，优于基线策略。我们期待RHealthTwin成为基于LLM的健康与福祉应用中具有前瞻性的负责任基础框架。

---

## [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)

### Abstract
arXiv:2506.08528v1 Announce Type: new 
Abstract: Troubleshooting performance problems of large model training (LMT) is immensely challenging, due to unprecedented scales of modern GPU clusters, the complexity of software-hardware interactions, and the data intensity of the training process. Existing troubleshooting approaches designed for traditional distributed systems or datacenter networks fall short and can hardly apply to real-world training systems. In this paper, we present PerfTracker, the first online troubleshooting system utilizing fine-grained profiling, to diagnose performance issues of large-scale model training in production. PerfTracker can diagnose performance issues rooted in both hardware (e.g., GPUs and their interconnects) and software (e.g., Python functions and GPU operations). It scales to LMT on modern GPU clusters. PerfTracker effectively summarizes runtime behavior patterns of fine-grained LMT functions via online profiling, and leverages differential observability to localize the root cause with minimal production impact. PerfTracker has been deployed as a production service for large-scale GPU clusters of O(10, 000) GPUs (product homepage https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool). It has been used to diagnose a variety of difficult performance issues.

### 摘要
由于现代GPU集群的庞大规模、软件-硬件交互的复杂性以及训练过程的数据密集性，大型模型训练(LMT)的性能问题排查面临巨大挑战。现有针对传统分布式系统或数据中心网络的故障诊断方法存在不足，难以应用于实际训练系统。本文提出PerfTracker，首个利用细粒度性能分析的在线诊断系统，用于生产环境中大规模模型训练的性能问题定位。该系统能够诊断硬件(如GPU及其互连)和软件(如Python函数与GPU操作)层面的性能问题，并支持现代GPU集群上的LMT规模扩展。PerfTracker通过在线性能分析有效归纳细粒度LMT函数的运行时行为模式，并利用差分可观测性以最小化生产影响实现根因定位。该系统已作为生产服务部署于规模达O(10,000)GPU的大型集群(产品主页https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool)，成功诊断了多种复杂性能问题。

---

## [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)

### Abstract
arXiv:2506.08800v1 Announce Type: new 
Abstract: Data science aims to extract insights from data to support decision-making processes. Recently, Large Language Models (LLMs) are increasingly used as assistants for data science, by suggesting ideas, techniques and small code snippets, or for the interpretation of results and reporting. Proper automation of some data-science activities is now promised by the rise of LLM agents, i.e., AI systems powered by an LLM equipped with additional affordances--such as code execution and knowledge bases--that can perform self-directed actions and interact with digital environments. In this paper, we survey the evaluation of LLM assistants and agents for data science. We find (1) a dominant focus on a small subset of goal-oriented activities, largely ignoring data management and exploratory activities; (2) a concentration on pure assistance or fully autonomous agents, without considering intermediate levels of human-AI collaboration; and (3) an emphasis on human substitution, therefore neglecting the possibility of higher levels of automation thanks to task transformation.

### 摘要
数据科学旨在从数据中提取洞见以支持决策过程。近年来，大型语言模型（LLMs）越来越多地被用作数据科学的辅助工具，通过提供创意、技术建议和小型代码片段，或协助结果解释与报告撰写。随着LLM智能体的兴起，部分数据科学活动的自动化现已成为可能——这类由LLM驱动的AI系统配备额外功能（如代码执行和知识库），能够执行自主行动并与数字环境交互。本文系统考察了数据科学领域LLM助手与智能体的评估现状，发现：（1）研究主要集中于小部分目标导向活动，严重忽视数据管理与探索性活动；（2）重点关注纯辅助型或完全自主型智能体，未充分考虑人机协作的中间形态；（3）过度强调人类替代，而忽视了通过任务转型实现更高层次自动化的可能性。

---

## [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)

### Abstract
arXiv:2506.08771v1 Announce Type: new 
Abstract: Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -- which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context) -- offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath-based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub: https://github.com/susantiyuni/path-to-causality

### 摘要
推断变量对之间的因果关系对于理解复杂系统中的多元交互至关重要。基于知识的因果发现——通过推理变量的元数据（如名称或文本语境）来推断因果关系——为依赖观测数据的传统方法提供了引人注目的替代方案。然而，现有使用大语言模型（LLMs）的方法常产生不稳定且不一致的结果，影响了因果推断的可靠性。为此，我们提出一种将知识图谱（KGs）与LLMs相结合的新方法，以增强基于知识的因果发现。该方法首先识别KGs中基于元路径的信息子图，继而通过基于学习排序的模型进一步优化子图选择。排名最高的子图随后被整合到零样本提示中，从而提升LLMs推断因果关系的有效性。在生物医学和开放领域数据集上的大量实验表明，我们的方法在不同LLMs和KGs的评估中，F1分数最高可超越基线方法44.4分。代码和数据集已发布于GitHub：https://github.com/susantiyuni/path-to-causality

---

## [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)

### Abstract
arXiv:2506.08745v1 Announce Type: new 
Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential in complex reasoning tasks, yet effective training often relies on external supervision, which limits the broader applicability. In this work, we propose a novel self-rewarding reinforcement learning framework to enhance Large Language Model (LLM) reasoning by leveraging the consistency of intermediate reasoning states across different reasoning trajectories. Our key insight is that correct responses often exhibit consistent trajectory patterns in terms of model likelihood: their intermediate reasoning states tend to converge toward their own final answers (high consistency) with minimal deviation toward other candidates (low volatility). Inspired by this observation, we introduce CoVo, an intrinsic reward mechanism that integrates Consistency and Volatility via a robust vector-space aggregation strategy, complemented by a curiosity bonus to promote diverse exploration. CoVo enables LLMs to perform RL in a self-rewarding manner, offering a scalable pathway for learning to reason without external supervision. Extensive experiments on diverse reasoning benchmarks show that CoVo achieves performance comparable to or even surpassing supervised RL. Our code is available at https://github.com/sastpg/CoVo.

### 摘要
强化学习（RL）的最新进展凸显了其在复杂推理任务中的潜力，但有效的训练通常依赖外部监督，这限制了其更广泛的应用。本研究提出了一种新颖的自奖励强化学习框架，通过利用不同推理轨迹间中间推理状态的一致性来增强大语言模型（LLM）的推理能力。我们的核心发现是：正确回答往往在模型似然度方面表现出一致的轨迹模式——其中间推理状态会高度趋近于自身最终答案（高一致性），同时极少偏离至其他候选答案（低波动性）。受此启发，我们提出了CoVo机制，该内在奖励机制通过鲁棒的向量空间聚合策略整合一致性与波动性指标，并辅以好奇心奖励以促进多样化探索。CoVo使LLM能够以自奖励方式进行强化学习，为无监督推理学习提供了可扩展的途径。在多样化推理基准上的大量实验表明，CoVo实现了与监督强化学习相当甚至更优的性能。代码已开源：https://github.com/sastpg/CoVo。

---

## [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)

### Abstract
arXiv:2506.00160v1 Announce Type: cross 
Abstract: The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more engaging experience for human players in LLM-agent-based social deduction games like Werewolf. Previous works either fine-tuning, advanced prompting engineering, or additional experience pool to achieve engaging text-format Werewolf game experience. We propose a novel yet straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS) models designed for enhanced compatibility with various LLM models, and improved user engagement. We argue with ever enhancing LLM reasoning, extra components will be unnecessary in the case of Werewolf.

### 摘要
随着大语言模型（LLM）在推理和说服能力上的显著进步，基于社交演绎游戏系统的商业应用与人工智能研究正日益普及。特别是DeepSeek R1和V3模型的崛起，使得LLM在基于智能体的狼人杀等社交演绎游戏中能为人类玩家提供更具吸引力的体验。先前的研究或通过微调模型，或采用高级提示工程，或引入额外经验池来实现沉浸式文本版狼人杀游戏。我们提出了一种新颖而简洁的基于LLM的狼人杀系统，该系统通过优化的文本转语音（TTS）模型设计，可增强与各类LLM模型的兼容性并提升用户参与度。我们认为随着LLM推理能力的持续强化，在狼人杀游戏场景中附加组件将不再必要。

---

## [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)

### Abstract
arXiv:2506.08027v1 Announce Type: cross 
Abstract: Precision scaling - using fewer bits to represent model parameters and related tensors during pre-training - has emerged as a compelling technique for improving GPU efficiency without sacrificing accuracy. Microscaling (MX) formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling this precision scaling aspect. These formats combine narrow floating-point data types with per-block scaling factors, offering a fine-grained approach to quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared to other reduced-precision representations, in practice they must be used carefully in order to successfully converge an LLM on a multi-trillion token dataset. In this paper, we show that the rounding mode suggested in OCP specification can lead to divergence when pre-training an LLM. We show an improved rounding mode, which uses round-to-infinity to compute scaling factors, enables successful pre-training in MXFP8 for an 8B model on 15T tokens.

### 摘要
精确缩放——在预训练过程中使用更少的比特来表示模型参数及相关张量——已成为一种在不牺牲准确性的前提下提升GPU效率的有效技术。英伟达最新Blackwell GPU中的微缩放（MX）格式在这一领域实现了重大突破。这些格式将窄位浮点数据类型与逐块缩放因子相结合，为张量量化提供了细粒度解决方案。尽管MX格式相比其他低精度表示法有望提升数值稳定性，但在实际应用中需谨慎使用，以确保大型语言模型（LLM）能在数万亿token数据集上成功收敛。本文研究表明：遵循OCP规范建议的舍入模式可能导致LLM预训练发散。我们提出一种改进的舍入模式，该模式采用"无限舍入"法计算缩放因子，从而成功实现了80亿参数模型在15万亿token数据集上采用MXFP8格式的预训练。

---

## [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/abs/2506.09038)

### Abstract
arXiv:2506.09038v1 Announce Type: new 
Abstract: For Large Language Models (LLMs) to be reliably deployed in both everyday and high-stakes domains, knowing when not to answer is equally critical as answering correctly. Real-world user queries, which can be underspecified, ill-posed, or fundamentally unanswerable, require LLMs to reason about uncertainty and selectively abstain -- i.e., refuse to answer definitively. However, abstention remains understudied, without a systematic evaluation framework for modern LLMs. In this work, we introduce AbstentionBench, a large-scale benchmark for holistically evaluating abstention across 20 diverse datasets, including questions with unknown answers, underspecification, false premises, subjective interpretations, and outdated information. Evaluating 20 frontier LLMs reveals abstention is an unsolved problem, and one where scaling models is of little use. While recent reasoning LLMs have shown impressive results in complex problem solving, surprisingly, we find that reasoning fine-tuning degrades abstention (by $24\%$ on average), even for math and science domains on which reasoning models are explicitly trained. We find that while a carefully crafted system prompt can boost abstention in practice, it does not resolve models' fundamental inability to reason about uncertainty. We release AbstentionBench to foster research into advancing LLM reliability.

### 摘要
要使大语言模型（LLMs）在日常及高风险领域可靠部署，明确何时不应回答与正确回答问题同等重要。现实中的用户查询可能存在未充分说明、表述不当或本质上无法回答的情况，这要求LLMs能够推理不确定性并选择性弃权——即拒绝给出确定性答案。然而，弃权机制研究仍显不足，缺乏针对现代LLMs的系统性评估框架。本研究提出AbstentionBench，一个涵盖20个多样化数据集的大规模基准测试，用于全面评估模型在面对未知答案、未充分说明、错误前提、主观解读及过时信息等问题时的弃权表现。通过对20个前沿LLMs的评估发现，弃权仍是一个未解决的难题，且模型规模扩大对其改善作用有限。尽管近期具备推理能力的LLMs在复杂问题求解中表现突出，但令人惊讶的是，我们发现推理微调反而会降低弃权能力（平均下降24%），即使在数学和科学等明确训练过的领域也是如此。研究表明，虽然精心设计的系统提示在实践中能提升弃权表现，但无法从根本上解决模型在不确定性推理方面的缺陷。我们公开AbstentionBench以促进提升LLM可靠性的相关研究。

---

## [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/abs/2506.08872)

### Abstract
arXiv:2506.08872v1 Announce Type: new 
Abstract: This study explores the neural and behavioral consequences of LLM-assisted essay writing. Participants were divided into three groups: LLM, Search Engine, and Brain-only (no tools). Each completed three sessions under the same condition. In a fourth session, LLM users were reassigned to Brain-only group (LLM-to-Brain), and Brain-only users were reassigned to LLM condition (Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18 completing session 4. We used electroencephalography (EEG) to assess cognitive load during essay writing, and analyzed essays using NLP, as well as scoring essays with the help from human teachers and an AI judge. Across groups, NERs, n-gram patterns, and topic ontology showed within-group homogeneity. EEG revealed significant differences in brain connectivity: Brain-only participants exhibited the strongest, most distributed networks; Search Engine users showed moderate engagement; and LLM users displayed the weakest connectivity. Cognitive activity scaled down in relation to external tool use. In session 4, LLM-to-Brain participants showed reduced alpha and beta connectivity, indicating under-engagement. Brain-to-LLM users exhibited higher memory recall and activation of occipito-parietal and prefrontal areas, similar to Search Engine users. Self-reported ownership of essays was the lowest in the LLM group and the highest in the Brain-only group. LLM users also struggled to accurately quote their own work. While LLMs offer immediate convenience, our findings highlight potential cognitive costs. Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels. These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI's role in learning.

### 摘要
本研究探讨了大型语言模型（LLM）辅助写作对神经活动与行为表现的影响。参与者被分为三组：LLM组、搜索引擎组及纯大脑组（不使用工具）。每组在相同条件下完成三次写作任务。在第四次任务中，LLM组被重新分配至纯大脑条件（LLM-纯大脑组），纯大脑组则转为使用LLM（纯大脑-LLM组）。共有54名参与者完成前三次任务，其中18人参与第四次任务。我们采用脑电图（EEG）评估写作过程中的认知负荷，并通过自然语言处理技术分析文本特征，同时结合教师评分与AI评分进行作文评估。结果显示：各组在命名实体识别、n元语法模式及主题本体方面呈现组内同质性。EEG数据表明脑网络连接存在显著差异：纯大脑组展现出最强且分布最广的神经网络连接；搜索引擎组呈现中等强度激活；而LLM组则显示最弱的神经连接。认知活动强度与外部工具使用呈负相关。在第四次任务中，LLM-纯大脑组表现出α波与β波连接减弱，提示认知投入不足；纯大脑-LLM组则显示出与搜索引擎组相似的记忆提取增强及枕顶叶-前额叶区域激活。自我报告显示，LLM组的文本所有权感知最低，纯大脑组最高。LLM使用者还表现出自我引用准确性下降的现象。尽管LLM能提供即时便利，但研究发现其可能带来认知代价：四个月追踪显示LLM使用者在神经、语言及行为层面持续表现欠佳。这些发现对LLM依赖的长期教育影响提出警示，并强调需深入探究AI在学习中的作用。

---

## [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)

### Abstract
arXiv:2506.08022v1 Announce Type: cross 
Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations.

### 摘要
大型多模态模型（LMMs）的任务适应与对齐通过指令微调取得显著进展，并经由近期偏好优化得到进一步加强。然而，大多数LMM在推理过程中仍存在严重的模态失衡问题，即语言先验偏差凌驾于视觉输入之上，这限制了其在下游任务中的泛化能力并导致幻觉现象。现有LMM偏好优化方法在构建训练数据时，未能有效抑制其大型语言模型（LLM）骨干的内部偏差，且严重依赖离线数据，缺乏适应训练过程中动态分布变化的多样化响应探索能力。而近期提出的基于在线生成数据与验证奖励来提升推理能力的群组相对策略优化（GRPO）方法，在多模态模型对齐领域仍研究不足。本文提出新型偏好学习框架——模态平衡偏好优化（MBPO），通过对抗性扰动输入图像生成困难负样本（即因视觉信息利用不足而被LLM偏差误导的拒绝响应），构建更有效的离线偏好数据集；同时利用封闭式任务易于验证的特性，生成带有验证奖励的在线响应。随后采用GRPO方法进行离线-在线混合数据训练。大量实验表明，MBPO能显著提升LMM在复杂视觉语言任务上的性能，并有效减少幻觉现象。

---

## [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)

### Abstract
arXiv:2506.04760v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical documents for query expansion, thereby enhancing information retrieval performance. However, the efficacy of this method is highly dependent on the quality of the generated documents, which often requires complex prompt strategies and the integration of advanced dense retrieval techniques. This can be both costly and computationally intensive. To mitigate these limitations, we explore the use of zero-shot LLM-based query expansion to improve sparse retrieval, particularly for learned sparse retrievers. We introduce a novel fusion ranking framework, Exp4Fuse, which enhances the performance of sparse retrievers through an indirect application of zero-shot LLM-based query expansion. Exp4Fuse operates by simultaneously considering two retrieval routes-one based on the original query and the other on the LLM-augmented query. It then generates two ranked lists using a sparse retriever and fuses them using a modified reciprocal rank fusion method. We conduct extensive evaluations of Exp4Fuse against leading LLM-based query expansion methods and advanced retrieval techniques on three MS MARCO-related datasets and seven low-resource datasets. Experimental results reveal that Exp4Fuse not only surpasses existing LLM-based query expansion methods in enhancing sparse retrievers but also, when combined with advanced sparse retrievers, achieves SOTA results on several benchmarks. This highlights the superior performance and effectiveness of Exp4Fuse in improving query expansion for sparse retrieval.

### 摘要
大型语言模型（LLMs）在生成假设性文档以进行查询扩展方面展现出潜力，从而提升信息检索性能。然而，该方法的有效性高度依赖于生成文档的质量，这通常需要复杂的提示策略与先进稠密检索技术的结合，导致成本高昂且计算密集。为缓解这些局限性，我们探索了基于零样本LLM的查询扩展在改进稀疏检索（尤其是学习型稀疏检索器）中的应用。我们提出了一种新颖的融合排序框架Exp4Fuse，该框架通过间接应用基于零样本LLM的查询扩展来增强稀疏检索器的性能。Exp4Fuse的运作机制是同时考虑两条检索路径——基于原始查询的路径和基于LLM增强查询的路径，随后使用稀疏检索器生成两个排序列表，并采用改进的互逆排序融合方法进行合并。我们在三个MS MARCO相关数据集和七个低资源数据集上，对Exp4Fuse与主流基于LLM的查询扩展方法及先进检索技术进行了全面评估。实验结果表明，Exp4Fuse不仅在增强稀疏检索器方面超越了现有基于LLM的查询扩展方法，当与先进稀疏检索器结合时，还在多个基准测试中取得了最先进的结果。这凸显了Exp4Fuse在改进稀疏检索查询扩展方面的卓越性能和有效性。

---

## [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)

### Abstract
arXiv:2506.05695v1 Announce Type: cross 
Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by transferring the teacher model's capabilities to a smaller student model, reducing inference cost and memory usage while maintaining performance. However, existing KD methods for LLMs often fail to prevent significant shifts in the student model's distribution during training, leading to issues such as catastrophic forgetting, mode collapse, and training-inference mismatch. To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of "progressive overload" (POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures. By starting with the easiest samples and progressively increasing the difficulty, the approach enhances both the stability and efficiency of learning. Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families. Our findings highlight the effectiveness of sorted training samples in KD for LLMs. More generally, our work demonstrates how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.

### 摘要
知识蒸馏（KD）通过将教师模型的能力迁移至更小的学生模型来压缩大型语言模型（LLMs），在保持性能的同时降低推理成本与内存占用。然而，现有LLM知识蒸馏方法往往无法避免学生模型在训练过程中发生显著分布偏移，从而导致灾难性遗忘、模式崩溃及训练-推理不匹配等问题。为解决这些挑战，我们受力量训练中"渐进超负荷"原则（POCL）启发，提出一种可无缝集成到现有白盒知识蒸馏方法的新型插件式课程学习框架，其计算开销极低。该框架包含两个核心组件：（1）难度评估器：对训练样本按从易到难排序并划分；（2）训练调度器：以固定间隔逐步将这些样本子集引入蒸馏过程，同时应用温度值持续上升的损失函数。通过从最简单样本开始并渐进提升难度，该方法显著增强了学习的稳定性与效率。在指令跟随场景下的广泛实验表明，POCL能持续提升不同白盒知识蒸馏方法和模型族所蒸馏学生模型的性能。我们的研究结果凸显了排序训练样本在LLM知识蒸馏中的有效性。更广泛而言，本工作展示了如何在知识蒸馏过程中结构化训练数据，以增强蒸馏后LLMs的稳定性与性能。

---

## [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)

### Abstract
arXiv:2506.06363v1 Announce Type: cross 
Abstract: Atomistic simulations are essential tools in chemistry and materials science, accelerating the discovery of novel catalysts, energy storage materials, and pharmaceuticals. However, running these simulations remains challenging due to the wide range of computational methods, diverse software ecosystems, and the need for expert knowledge and manual effort for the setup, execution, and validation stages. In this work, we present ChemGraph, an agentic framework powered by artificial intelligence and state-of-the-art simulation tools to streamline and automate computational chemistry and materials science workflows. ChemGraph leverages graph neural network-based foundation models for accurate yet computationally efficient calculations and large language models (LLMs) for natural language understanding, task planning, and scientific reasoning to provide an intuitive and interactive interface. Users can perform tasks such as molecular structure generation, single-point energy, geometry optimization, vibrational analysis, and thermochemistry calculations with methods ranging from tight-binding and machine learning interatomic potentials to density functional theory or wave function theory-based methods. We evaluate ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs (GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows, while more complex tasks benefit from using larger models like GPT-4o. Importantly, we show that decomposing complex tasks into smaller subtasks through a multi-agent framework enables smaller LLM models to match or exceed GPT-4o's performance in specific scenarios.

### 摘要
原子尺度模拟是化学和材料科学领域的重要工具，可加速新型催化剂、储能材料和药物的发现。然而，由于计算方法多样、软件生态复杂，且在设置、执行和验证阶段需要专业知识和人工操作，这类模拟的实施仍面临挑战。本研究提出ChemGraph框架，该智能代理系统整合人工智能与尖端模拟工具，旨在实现计算化学与材料科学工作流的自动化和高效化。ChemGraph采用基于图神经网络的基础模型进行高精度、低计算成本的运算，并利用大语言模型（LLMs）实现自然语言理解、任务规划和科学推理，从而提供直观的交互界面。用户可执行分子结构生成、单点能量计算、几何优化、振动分析和热化学计算等任务，方法涵盖紧束缚近似、机器学习原子间势、密度泛函理论及波函数理论等多种计算层级。通过13项基准测试表明：较小规模LLMs（GPT-4o-mini、Claude-3.5-haiku、Qwen2.5-14B）在简单工作流中表现良好，而复杂任务则需GPT-4o等更大模型。关键发现是：通过多代理框架将复杂任务分解为子任务，可使小型LLMs在特定场景下达到或超越GPT-4o的性能。

---

## [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)

### Abstract
arXiv:2506.08074v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in external evidence, yet it still falters when answers must be pieced together across semantically distant documents. We close this gap with the Hierarchical Lexical Graph (HLG), a three-tier index that (i) traces every atomic proposition to its source, (ii) clusters propositions into latent topics, and (iii) links entities and relations to expose cross-document paths. On top of HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG, which performs fine-grained entity-aware beam search over propositions for high-precision factoid questions, and TopicGraphRAG, which selects coarse topics before expanding along entity links to supply broad yet relevant context for exploratory queries. Additionally, existing benchmarks lack the complexity required to rigorously evaluate multi-hop summarization systems, often focusing on single-document queries or limited datasets. To address this, we introduce a synthetic dataset generation pipeline that curates realistic, multi-document question-answer pairs, enabling robust evaluation of multi-hop retrieval systems. Extensive experiments across five datasets demonstrate that our methods outperform naive chunk-based RAG achieving an average relative improvement of 23.1% in retrieval recall and correctness. Open-source Python library is available at https://github.com/awslabs/graphrag-toolkit.

### 摘要
检索增强生成（RAG）技术通过外部证据为大型语言模型提供支持，但在需要跨语义关联较远的文档拼接答案时仍存在不足。我们通过分层词汇图（HLG）填补了这一空白，该三级索引能够：（i）追踪每个原子命题的来源；（ii）将命题聚类为潜在主题；（iii）通过实体和关系链接揭示跨文档路径。基于HLG，我们构建了两个互补的即插即用检索器：StatementGraphRAG通过细粒度实体感知的束搜索处理命题，适用于高精度事实型问题；TopicGraphRAG则先筛选粗粒度主题，再沿实体链接扩展，为探索性查询提供广泛而相关的上下文。此外，现有基准测试缺乏严格评估多跳摘要系统所需的复杂性，通常仅关注单文档查询或有限数据集。为此，我们提出了一种合成数据集生成流程，可构建真实的多文档问答对，从而实现对多跳检索系统的稳健评估。在五个数据集上的大量实验表明，我们的方法优于基于简单文本块的RAG，检索召回率和正确率平均相对提升23.1%。开源Python库详见https://github.com/awslabs/graphrag-toolkit。

---

## [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)

### Abstract
arXiv:2506.08173v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown strong capabilities in code generation and comprehension, yet their application to complex software engineering tasks often suffers from low precision and limited interpretability. We present Repeton, a fully open-source framework that leverages LLMs for precise and automated code manipulation in real-world Git repositories. Rather than generating holistic fixes, Repeton operates through a structured patch-and-test pipeline: it iteratively diagnoses issues, proposes code changes, and validates each patch through automated testing. This stepwise process is guided by lightweight heuristics and development tools, avoiding reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite benchmark, our method shows good performance compared to RAG-based methods in both patch validity and interpretability. By decomposing software engineering tasks into modular, verifiable stages, Repeton provides a practical path toward scalable and transparent autonomous debugging.

### 摘要
大语言模型（LLMs）在代码生成与理解方面展现出强大能力，但在复杂软件工程任务中的应用往往存在精度不足和可解释性有限的问题。本文提出Repeton——一个完全开源的框架，通过LLMs实现真实Git仓库中精准、自动化的代码操作。与生成整体修复方案不同，Repeton采用结构化"补丁-测试"流程运作：迭代执行问题诊断、代码变更提议，并通过自动化测试验证每个补丁。该逐步过程由轻量级启发式规则和开发工具引导，避免依赖基于嵌入的检索系统。在SWE-bench Lite基准测试中，本方法在补丁有效性和可解释性方面均展现出优于基于RAG方法的性能。通过将软件工程任务分解为模块化、可验证的阶段，Repeton为可扩展且透明的自主调试提供了实用路径。

---

## [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)

### Abstract
arXiv:2506.08171v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been successfully applied to a variety of coding tasks, including code generation, completion, and repair. However, more complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper investigates the capacity of LLMs to reason about worst-case executions in programs through symbolic constraints analysis, aiming to connect LLMs and symbolic reasoning approaches. Specifically, we define and address the problem of worst-case symbolic constraints analysis as a measure to assess the comprehension of LLMs. We evaluate the performance of existing LLMs on this novel task and further improve their capabilities through symbolic reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories) constraint solving and supported by a specially designed dataset of symbolic constraints. Experimental results show that our solver-aligned model, WARP-1.0-3B, consistently surpasses size-matched and even much larger baselines, demonstrating that a 3B LLM can recover the very constraints that pin down an algorithm's worst-case behaviour through reinforcement learning methods. These findings suggest that LLMs are capable of engaging in deeper symbolic reasoning, supporting a closer integration between neural network-based learning and formal methods for rigorous program analysis.

### 摘要
大型语言模型（LLMs）已成功应用于多种编码任务，包括代码生成、补全和修复。然而，LLMs在更复杂的符号推理任务上的应用仍鲜有研究。本文通过符号约束分析，探究LLMs对程序最坏情况执行的推理能力，旨在建立LLMs与符号推理方法之间的联系。具体而言，我们将最坏情况符号约束分析问题定义为评估LLMs理解能力的指标，并评估现有LLMs在这一新任务上的表现。基于SMT（可满足性模理论）约束求解，并通过专门设计的符号约束数据集支持，我们进一步采用符号推理引导的微调方法提升模型性能。实验结果表明，我们提出的求解器对齐模型WARP-1.0-3B consistently outperforms size-matched and even much larger baselines，证明3B规模的LLM能通过强化学习方法还原锁定算法最坏情况行为的关键约束。这些发现表明，LLMs具备进行更深层次符号推理的潜力，为神经网络学习与形式化方法在严格程序分析中的紧密结合提供了支持。

---

## [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)

### Abstract
arXiv:2506.08184v1 Announce Type: cross 
Abstract: Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen models' ability to suppress irrelevant content during retrieval.

### 摘要
大型语言模型（LLMs）中的信息检索日益被认为与生成能力紧密交织，而非简单的查找功能。尽管较长的上下文通常被认为有助于提升检索效果，但上下文内部干扰的影响仍未得到充分研究。为解决这一问题，我们借鉴认知科学中的主动干扰（PI）范式——即先前的信息会干扰对新近更新的回忆。在人类中，对此类干扰的敏感性与工作记忆容量呈负相关。我们提出PI-LLM评估方法，通过顺序输入语义相关的键值更新并仅查询最终值进行测试。尽管这些最终值明确位于查询位置之前，但随着干扰累积，LLM的检索准确率呈对数线性下降直至趋零；错误主要源于检索到先前被覆盖的值。尝试通过提示工程（如指示模型忽略早期输入）缓解干扰的效果有限。这些发现揭示了LLMs在解耦干扰和灵活处理信息能力上的根本性限制，表明其存在超越上下文访问的工作记忆瓶颈。这要求开发新方法以增强模型在检索过程中抑制无关内容的能力。

---

## [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)

### Abstract
arXiv:2506.08228v1 Announce Type: cross 
Abstract: We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training.

### 摘要
我们研究了一类编码器-解码器自回归变压器模型在自动驾驶领域联合运动预测与规划任务中的实证缩放规律。基于50万小时驾驶数据集，我们发现与语言建模类似，模型性能随计算总预算呈幂律函数提升，且模型训练损失与评估指标间存在强相关性。最有趣的是，闭环指标同样随规模扩大而改善，这对开环指标在模型开发与性能优化中的适用性具有重要启示。我们还研究了训练计算最优模型中变压器参数量与训练数据规模的最佳缩放比例，发现随着训练计算预算增加，模型规模需以1.5倍于数据集增速进行扩展。在推理计算缩放方面，小模型通过输出采样与聚类可媲美大模型性能，直至达到交叉点后大模型才显现更高推理计算效率。实验结果表明，优化运动预测与规划模型的训练及推理时缩放特性是提升其应对多样化驾驶场景性能的关键。最后，我们初步探索了利用其他智能体常规日志驾驶数据训练来提升主智能体性能的效用，这对解决大容量模型训练中机器人数据稀缺问题具有重要意义。

---

## [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)

### Abstract
arXiv:2506.08018v1 Announce Type: cross 
Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput.

### 摘要
大型语言模型（LLM）推理过程中键值（KV）缓存的高内存需求严重限制了其在资源受限平台上的部署。量化技术能有效缓解KV缓存带来的内存压力，但现有方法要么依赖静态的统一精度分配，要么无法在长上下文任务中动态区分关键KV，迫使在内存-精度-吞吐量之间做出权衡。本文提出一种名为KVmix的新型KV缓存混合精度量化方法。该方法通过基于梯度的重要性分析，评估各键/值投影矩阵对模型损失的影响，实现分层级的混合精度位宽分配：对重要层级动态分配高精度，同时对低影响层级进行激进量化，从而达成精度与效率的可调平衡。KVmix还提出动态长上下文优化策略，自适应地为近期关键令牌保留全精度KV对并压缩历史令牌，在低内存占用下实现高质量序列生成。此外，KVmix提供高效低位量化方案及CUDA内核以优化计算开销。在Llama、Mistral等LLM上的实验表明，KVmix在极低量化配置（键2.19位/值2.38位）下实现接近无损的推理性能，同时获得4.9倍内存压缩和5.3倍推理吞吐量提升。

---

## [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)

### Abstract
arXiv:2506.08231v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to extract clinical data from electronic health records (EHRs), offering significant improvements in scalability and efficiency for real-world data (RWD) curation in oncology. However, the adoption of LLMs introduces new challenges in ensuring the reliability, accuracy, and fairness of extracted data, which are essential for research, regulatory, and clinical applications. Existing quality assurance frameworks for RWD and artificial intelligence do not fully address the unique error modes and complexities associated with LLM-extracted data. In this paper, we propose a comprehensive framework for evaluating the quality of clinical data extracted by LLMs. The framework integrates variable-level performance benchmarking against expert human abstraction, automated verification checks for internal consistency and plausibility, and replication analyses comparing LLM-extracted data to human-abstracted datasets or external standards. This multidimensional approach enables the identification of variables most in need of improvement, systematic detection of latent errors, and confirmation of dataset fitness-for-purpose in real-world research. Additionally, the framework supports bias assessment by stratifying metrics across demographic subgroups. By providing a rigorous and transparent method for assessing LLM-extracted RWD, this framework advances industry standards and supports the trustworthy use of AI-powered evidence generation in oncology research and practice.

### 摘要
大型语言模型（LLMs）在从电子健康记录（EHRs）中提取临床数据方面的应用日益广泛，为肿瘤学真实世界数据（RWD）的规模化高效整理提供了显著改进。然而，LLMs的采用也带来了确保数据可靠性、准确性和公平性的新挑战，这些特性对研究、监管和临床应用至关重要。现有的RWD和人工智能质量保证框架未能完全解决与LLM提取数据相关的独特错误模式和复杂性。本文提出一个用于评估LLMs提取临床数据质量的综合框架。该框架整合了三个维度：基于专家人工抽取的变量级性能基准测试、针对内部一致性与合理性的自动化验证检查，以及通过对比LLM提取数据与人工抽取数据集或外部标准进行的复现分析。这种多维方法能够识别最需改进的变量、系统检测潜在错误，并确认数据集在真实世界研究中的适用性。此外，该框架还支持通过人口统计学亚组分层指标进行偏倚评估。通过提供严格透明的LLM提取RWD评估方法，本框架推动了行业标准发展，并支持肿瘤学研究和实践中人工智能生成证据的可信使用。

---

## [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)

### Abstract
arXiv:2506.08147v1 Announce Type: cross 
Abstract: Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide.

### 摘要
社交媒体平台作为公共话语的关键场域，深刻影响着舆论走向与社群动态，但其广泛使用也加剧了仇恨言论等有害内容的传播，威胁网络安全与包容性。尽管英语和西班牙语等语言的仇恨言论检测已得到充分研究，乌尔都语领域仍存在探索不足，特别是基于翻译方法的研究。为填补这一空白，我们构建了一个包含10,193条推文的三语数据集（英语3,834条、乌尔都语3,197条、西班牙语3,162条），通过关键词过滤采集，并均衡标注为4,849条仇恨言论和5,344条非仇恨言论。研究方法采用注意力层作为Transformer架构与大语言模型（LLMs）的前置处理模块，以增强多语言仇恨言论检测的特征提取能力；针对非Transformer模型则使用TF-IDF进行特征提取。数据集在GPT-3.5 Turbo、Qwen 2.5 72B等前沿模型及SVM等传统机器学习模型（如BERT、RoBERTa等Transformer架构）上进行基准测试。三名标注员遵循严格准则确保数据质量，Fleiss' Kappa系数达0.821。结合注意力层与GPT-3.5 Turbo、Qwen 2.5 72B的混合方案表现优异：英语（GPT-3.5 Turbo）宏观F1值0.87、西班牙语（GPT-3.5 Turbo）0.85、乌尔都语（Qwen 2.5 72B）0.81，多语言联合模型（Qwen 2.5 72B）达0.88。相较SVM基线模型，性能提升分别为英语8.75%（基线0.80）、西班牙语8.97%（基线0.78）、乌尔都语5.19%（基线0.77）及多语言模型7.32%（基线0.82）。该框架为多语言仇恨言论检测提供了可靠解决方案，有助于构建更安全的全球数字社区。

---

## [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)

### Abstract
arXiv:2506.08234v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

### 摘要
大型语言模型（LLMs）与人工智能系统的最新进展，引发了复杂AI工作流设计与优化的范式转变。通过整合多元组件，复合AI系统在执行复杂任务方面展现出日益增强的能力。然而随着系统复杂性提升，优化目标不仅涉及单个组件性能，更延伸至组件间交互机制，这带来了新的挑战。尽管监督微调（SFT）和强化学习（RL）等传统优化方法仍具基础性地位，但自然语言反馈机制的兴起为优化不可微分系统开辟了新途径。本文系统性地综述了复合AI系统优化的最新进展，涵盖数值化与基于语言的两类技术。我们正式定义了复合AI系统优化的概念框架，沿多个关键维度对现有方法进行分类，并在这个快速发展的领域中指明开放的研究挑战与未来方向。

---

## [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)

### Abstract
arXiv:2506.08260v1 Announce Type: cross 
Abstract: Inference making is an essential but complex skill in reading comprehension (RC). Some inferences require resolving references across sentences, and some rely on using prior knowledge to fill in the detail that is not explicitly written in the text. Diagnostic RC questions can help educators provide more effective and targeted reading instruction and interventions for school-age students. We introduce a taxonomy of inference types for RC and use it to analyze the distribution of items within a diagnostic RC item bank. Next, we present experiments using GPT-4o to generate bridging-inference RC items for given reading passages via few-shot prompting, comparing conditions with and without chain-of-thought prompts. Generated items were evaluated on three aspects: overall item quality, appropriate inference type, and LLM reasoning, achieving high inter-rater agreements above 0.90. Our results show that GPT-4o produced 93.8% good-quality questions suitable for operational use in grade 3-12 contexts; however, only 42.6% of the generated questions accurately matched the targeted inference type. We conclude that combining automatic item generation with human judgment offers a promising path toward scalable, high-quality diagnostic RC assessments.

### 摘要
推理能力是阅读理解（RC）中一项重要而复杂的技能。部分推理需要解决跨句子的指代关系，另一些则需利用先验知识填补文本未明确表述的细节。诊断性阅读理解题目能帮助教育者为学龄学生提供更具针对性的阅读教学与干预措施。本文提出了一种阅读理解推理类型分类法，并运用该方法分析了某诊断性阅读理解题库中的题目分布。随后，我们通过少量示例提示，利用GPT-4o为指定阅读段落生成桥接推理类题目，比较了采用思维链提示与未采用提示的生成效果。生成题目从整体质量、推理类型准确性和大语言模型推理能力三个维度进行评估，评分者间一致性系数均高于0.90。实验结果表明：GPT-4o生成的题目中93.8%达到3-12年级操作使用标准，但仅42.6%的题目精确匹配目标推理类型。研究表明，将自动题目生成与人工判断相结合，可为规模化高质量诊断性阅读理解评估提供可行路径。

---

## [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)

### Abstract
arXiv:2506.08210v1 Announce Type: cross 
Abstract: Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.

### 摘要
文本到图像生成和大语言模型（LLM）均已取得显著进展。然而，许多文本到图像模型仍采用相对过时的T5和CLIP作为文本编码器。本研究探讨了将现代仅解码器架构的大语言模型作为文本到图像扩散模型文本编码器的有效性。我们构建了标准化的训练与评估流程，能够分离并评估不同文本嵌入的效果。通过训练总计27个采用12种不同文本编码器的文本到图像模型，系统分析了影响文本到图像生成的关键LLM特性，包括嵌入提取方法、不同LLM变体及模型规模。实验表明，现行采用末层嵌入作为条件的方法会导致性能下降。相反，我们探索了来自各层的嵌入，发现采用全层层归一化平均嵌入能显著提升复杂提示语的对齐效果。多数采用此条件方法的LLM均超越基线T5模型，在高级视觉语言推理能力上展现出更优性能。

---

## [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)

### Abstract
arXiv:2506.08235v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers.

### 摘要
大型语言模型（LLMs）正日益被用于文献综述、观点生成和科学论文分析等复杂研究任务，然而它们是否真正理解并处理复杂研究论文中的 intricate relationships（如主张与支撑证据间的逻辑关联），这一能力仍 largely unexplored（尚未被充分探索）。本研究提出CLAIM-BENCH——一个用于评估LLMs在科学主张-证据提取与验证任务中表现的综合基准，该任务反映了对科学论证的深层理解。我们系统比较了三种受分治法启发的策略在六种不同LLMs上的表现，揭示了各模型在科学理解方面的特定优势与局限。通过对多个研究领域300余组主张-证据对的评估，我们发现LLMs在处理复杂科学内容时存在显著缺陷。结果表明，在主张-证据识别任务中，GPT-4和Claude等闭源模型在精确率与召回率上 consistently outperform（持续优于）开源模型。此外，经策略性设计的三轮递进式与逐条提示方法虽增加计算成本，但显著提升了LLMs准确关联分散证据与主张的能力。CLAIM-BENCH为评估LLMs的科学理解能力设立了新标准，既可作为诊断工具，也为构建具备全文深度可靠推理能力的系统指明了发展方向。

---

## [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)

### Abstract
arXiv:2506.08266v1 Announce Type: cross 
Abstract: Existing approaches to language model alignment often treat safety as a tradeoff against helpfulness, which can lead to unacceptable responses in sensitive domains. To ensure reliable performance in such settings, we propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human preferences into helpfulness and harmlessness (safety), which are learned by training a reward model and a cost model, respectively. It then employs a two-step process to find safe solutions. In the first step, it optimizes the reward function under an intentionally pessimistic version of the cost constraint. In the second step, the trained model undergoes a safety test to verify whether its performance stays within an upper-confidence bound of the actual cost constraint. We provide a theoretical analysis of HC-RLHF, including proof that it will not return an unsafe solution with a probability greater than a user-specified threshold. For our empirical analysis, we apply HC-RLHF to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF produces safe models with high probability and can improve harmlessness and helpfulness compared to previous methods.

### 摘要
现有语言模型对齐方法常将安全性视为与有用性的权衡，这可能导致在敏感领域产生不可接受的响应。为确保此类场景下的可靠性能，我们提出高置信度安全人类反馈强化学习（HC-RLHF），该方法在最大化有用性的同时提供高置信度的安全保证。与先前方法类似，HC-RLHF将人类偏好显式解耦为有用性和无害性（安全性），分别通过训练奖励模型和成本模型进行学习。随后采用两步流程寻找安全解：第一步在故意保守的成本约束条件下优化奖励函数；第二步对训练模型进行安全性测试，验证其性能是否保持在真实成本约束的置信上界内。我们提供了HC-RLHF的理论分析，包括证明其返回不安全解的概率不超过用户指定阈值。实证分析中，我们将HC-RLHF应用于对齐三种不同语言模型（Qwen2-1.5B、Qwen2.5-3B和LLaMa3.2-3B）与人类偏好。结果表明，HC-RLHF能以高概率生成安全模型，且在无害性和有用性方面较现有方法有所提升。

---

## [How Good LLM-Generated Password Policies Are?](https://arxiv.org/abs/2506.08320)

### Abstract
arXiv:2506.08320v1 Announce Type: cross 
Abstract: Generative AI technologies, particularly Large Language Models (LLMs), are rapidly being adopted across industry, academia, and government sectors, owing to their remarkable capabilities in natural language processing. However, despite their strengths, the inconsistency and unpredictability of LLM outputs present substantial challenges, especially in security-critical domains such as access control. One critical issue that emerges prominently is the consistency of LLM-generated responses, which is paramount for ensuring secure and reliable operations.
  In this paper, we study the application of LLMs within the context of Cybersecurity Access Control Systems. Specifically, we investigate the consistency and accuracy of LLM-generated password policies, translating natural language prompts into executable pwquality.conf configuration files. Our experimental methodology adopts two distinct approaches: firstly, we utilize pre-trained LLMs to generate configuration files purely from natural language prompts without additional guidance. Secondly, we provide these models with official pwquality.conf documentation to serve as an informative baseline. We systematically assess the soundness, accuracy, and consistency of these AI-generated configurations. Our findings underscore significant challenges in the current generation of LLMs and contribute valuable insights into refining the deployment of LLMs in Access Control Systems.

### 摘要
生成式人工智能技术，尤其是大语言模型（LLMs），凭借其在自然语言处理方面的卓越能力，正迅速被工业界、学术界和政府机构采用。然而，尽管LLMs具有强大功能，其输出的不一致性和不可预测性带来了重大挑战，特别是在访问控制等安全关键领域。一个尤为突出的关键问题是LLM生成响应的稳定性，这对于确保安全可靠的操作至关重要。

本文研究了LLMs在网络安全访问控制系统中的应用。具体而言，我们探究了LLM生成的密码策略的一致性与准确性，将自然语言提示转化为可执行的pwquality.conf配置文件。我们的实验方法采用两种不同策略：首先，我们利用预训练的LLMs仅根据自然语言提示生成配置文件，不提供额外指导；其次，我们为这些模型提供官方的pwquality.conf文档作为参考基准。我们系统评估了这些AI生成配置的合理性、准确性和一致性。研究结果揭示了当前一代LLMs存在的显著挑战，并为优化LLMs在访问控制系统中的部署提供了重要见解。

---

## [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)

### Abstract
arXiv:2506.08311v1 Announce Type: cross 
Abstract: With the advent of large language models (LLMs), software engineering agents (SWE agents) have emerged as a powerful paradigm for automating a range of software tasks -- from code generation and repair to test case synthesis. These agents operate autonomously by interpreting user input and responding to environmental feedback. While various agent architectures have demonstrated strong empirical performance, the internal decision-making worfklows that drive their behavior remain poorly understood. Deeper insight into these workflows hold promise for improving both agent reliability and efficiency. In this work, we present the first systematic study of SWE agent behavior through the lens of execution traces. Our contributions are as follows: (1) we propose the first taxonomy of decision-making pathways across five representative agents; (2) using this taxonomy, we identify three core components essential to agent success -- bug localization, patch generation, and reproduction test generation -- and study each in depth; (3) we study the impact of test generation on successful patch production; and analyze strategies that can lead to successful test generation; (4) we further conduct the first large-scale code clone analysis comparing agent-generated and developer-written patches and provide a qualitative study revealing structural and stylistic differences in patch content. Together, these findings offer novel insights into agent design and open avenues for building agents that are both more effective and more aligned with human development practices.

### 摘要
随着大语言模型（LLMs）的出现，软件工程智能体（SWE智能体）已成为自动化执行各类软件任务的强大范式——从代码生成与修复到测试用例合成。这些智能体通过解析用户输入并响应环境反馈实现自主运行。尽管多种智能体架构已展现出卓越的实证性能，但其驱动行为的内在决策机制仍缺乏深入理解。对这些工作流程的深入洞察有望提升智能体的可靠性和效率。本研究首次通过执行轨迹对SWE智能体行为进行系统研究，主要贡献包括：（1）提出首个涵盖五种代表性智能体的决策路径分类体系；（2）基于该分类体系，识别出决定智能体成功率的三大核心组件——缺陷定位、补丁生成和复现测试生成，并分别进行深度研究；（3）探究测试生成对成功补丁产出的影响，分析有效测试生成的实现策略；（4）首次开展大规模代码克隆分析，对比智能体生成补丁与开发者编写补丁的差异，并通过定性研究揭示两者在补丁内容结构与风格上的特征差异。这些发现为智能体设计提供了新见解，并为构建更高效且更符合人类开发实践的智能体开辟了新路径。

---

## [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)

### Abstract
arXiv:2506.08346v1 Announce Type: cross 
Abstract: Deep speech classification tasks, including keyword spotting and speaker verification, are vital in speech-based human-computer interaction. Recently, the security of these technologies has been revealed to be susceptible to backdoor attacks. Specifically, attackers use noisy disruption triggers and speech element triggers to produce poisoned speech samples that train models to become vulnerable. However, these methods typically create only a limited number of backdoors due to the inherent constraints of the trigger function. In this paper, we propose that speech backdoor attacks can strategically focus on speech elements such as timbre and emotion, leveraging the Speech Large Language Model (SLLM) to generate diverse triggers. Increasing the number of triggers may disproportionately elevate the poisoning rate, resulting in higher attack costs and a lower success rate per trigger. We introduce the Multiple Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this challenge. The proposed attack is called the Speech Prompt Backdoor Attack (SPBA). Building on this foundation, we conducted attack experiments on two speech classification tasks, demonstrating that SPBA shows significant trigger effectiveness and achieves exceptional performance in attack metrics.

### 摘要
深度语音分类任务（包括关键词检测和说话人验证）在基于语音的人机交互中至关重要。近期研究发现这些技术的安全性易受后门攻击威胁。具体而言，攻击者通过噪声干扰触发器和语音元素触发器生成毒化语音样本，使训练模型产生漏洞。然而，由于触发器函数的内在限制，这些方法通常只能创建有限数量的后门。本文提出语音后门攻击可策略性地聚焦于音色、情感等语音元素，并利用语音大语言模型（SLLM）生成多样化触发器。增加触发器数量可能导致投毒率不成比例地升高，致使攻击成本增加且单触发器成功率下降。为此，我们引入多重梯度下降算法（MGDA）作为缓解策略。所提出的攻击方法称为语音提示后门攻击（SPBA）。基于此，我们在两个语音分类任务上进行了攻击实验，结果表明SPBA具有显著的触发器有效性，并在攻击指标上表现出卓越性能。

---

## [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)

### Abstract
arXiv:2506.08277v1 Announce Type: cross 
Abstract: Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models in both unimodal and multimodal stimulus settings. More recently, instruction-tuned multimodal models have shown to generate task-specific representations that align strongly with brain activity. However, prior work evaluating the brain alignment of MLLMs has primarily focused on unimodal settings or relied on non-instruction-tuned multimodal models for multimodal stimuli. To address this gap, we investigated brain alignment, that is, measuring the degree of predictivity of neural activity recorded while participants were watching naturalistic movies (video along with audio) with representations derived from MLLMs. We utilized instruction-specific embeddings from six video and two audio instruction-tuned MLLMs. Experiments with 13 video task-specific instructions show that instruction-tuned video MLLMs significantly outperform non-instruction-tuned multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for both video and audio tasks using language-guided instructions shows clear disentanglement in task-specific representations from MLLMs, leading to precise differentiation of multimodal functional processing in the brain. We also find that MLLM layers align hierarchically with the brain, with early sensory areas showing strong alignment with early layers, while higher-level visual and language regions align more with middle to late layers. These findings provide clear evidence for the role of task-specific instructions in improving the alignment between brain activity and MLLMs, and open new avenues for mapping joint information processing in both the systems. We make the code publicly available [https://github.com/subbareddy248/mllm_videos].

### 摘要
近期基于体素的多模态脑编码研究表明，在单模态和多模态刺激情境下，多模态大语言模型（MLLMs）相比单模态模型展现出更高程度的脑对齐特性。最新研究发现，经过指令调优的多模态模型能生成与脑活动高度吻合的任务特异性表征。然而，现有评估MLLMs脑对齐的研究主要集中于单模态情境，或依赖未经指令调优的多模态模型处理多模态刺激。为填补这一空白，我们通过测量被试观看自然主义影片（视频与音频）时记录的神经活动与MLLMs表征之间的预测程度，探究了脑对齐特性。我们采用了六个视频和两个音频指令调优MLLMs的任务特异性嵌入。基于13种视频任务指令的实验表明，指令调优视频MLLMs显著优于未经指令调优的多模态模型（提升15%）和单模态模型（提升20%）。我们对视频和音频任务MLLMs的语言引导指令评估显示，模型能清晰解耦任务特异性表征，从而精确区分大脑中的多模态功能处理。同时发现MLLMs各层级与大脑存在层次化对齐：早期感觉皮层与模型浅层强相关，而高级视觉和语言区域则更多与中深层对齐。这些发现为任务特异性指令在提升脑活动与MLLMs对齐中的作用提供了明确证据，并为探索两个系统的联合信息处理开辟了新途径。代码已公开[https://github.com/subbareddy248/mllm_videos]。

---

## [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)

### Abstract
arXiv:2506.08336v1 Announce Type: cross 
Abstract: Despite their growing adoption across domains, large language model (LLM)-powered agents face significant security risks from backdoor attacks during training and fine-tuning. These compromised agents can subsequently be manipulated to execute malicious operations when presented with specific triggers in their inputs or environments. To address this pressing risk, we present ReAgent, a novel defense against a range of backdoor attacks on LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies among the user's instruction, the agent's planning, and its execution. Drawing on this insight, ReAgent employs a two-level approach to detect potential backdoors. At the execution level, ReAgent verifies consistency between the agent's thoughts and actions; at the planning level, ReAgent leverages the agent's capability to reconstruct the instruction based on its thought trajectory, checking for consistency between the reconstructed instruction and the user's instruction. Extensive evaluation demonstrates ReAgent's effectiveness against various backdoor attacks across tasks. For instance, ReAgent reduces the attack success rate by up to 90\% in database operation tasks, outperforming existing defenses by large margins. This work reveals the potential of utilizing compromised agents themselves to mitigate backdoor risks.

### 摘要
尽管大语言模型（LLM）驱动的智能体在各领域应用日益广泛，但其在训练与微调阶段面临后门攻击带来的重大安全风险。这些被攻陷的智能体在输入或环境中出现特定触发器时，可能被操纵执行恶意操作。为应对这一紧迫威胁，我们提出ReAgent——一种针对基于LLM智能体后门攻击的新型防御方案。后门攻击通常会导致用户指令、智能体规划与执行行为之间的不一致性。基于此洞见，ReAgent采用两级检测机制：在执行层面验证智能体思考与行动的一致性；在规划层面利用智能体根据思维轨迹重构指令的能力，检测重构指令与用户原始指令的匹配度。大量实验表明，ReAgent能有效抵御跨任务的后门攻击。例如在数据库操作任务中，ReAgent将攻击成功率降低达90%，显著优于现有防御方案。本研究揭示了利用被攻陷智能体自身特性来缓解后门风险的潜在可能性。

---

## [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)

### Abstract
arXiv:2506.08349v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on various medical benchmarks, but their capabilities across different cognitive levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a multi-cognitive-level evaluation framework for assessing LLMs in the medical domain in this study. The framework integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Using this framework, we systematically evaluate state-of-the-art general and medical LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek. Our findings reveal a significant performance decline as cognitive complexity increases across evaluated models, with model size playing a more critical role in performance at higher cognitive levels. Our study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.

### 摘要
大语言模型（LLMs）在各种医学基准测试中展现出卓越性能，但其在不同认知层级的能力仍待深入探索。受布鲁姆分类法启发，本研究提出一个多认知层级的评估框架，用于系统评估医学领域的大语言模型。该框架整合现有医学数据集，并针对三个认知层级设计评估任务：基础知识掌握、综合知识应用及基于场景的问题解决。基于此框架，我们对六大主流模型家族（Llama、Qwen、Gemma、Phi、GPT和DeepSeek）中最先进的通用及医学大语言模型进行了系统评估。研究发现，随着认知复杂度提升，所有被评估模型均呈现显著性能下降，且模型规模在高层级认知任务中对性能的影响更为关键。本研究揭示了提升大语言模型高层级医学认知能力的必要性，并为开发适用于真实医疗场景的大语言模型提供了重要启示。

---

## [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)

### Abstract
arXiv:2506.08379v1 Announce Type: cross 
Abstract: Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization.

### 摘要
利用更多测试时计算已被证明是提升大语言模型（LLM）推理能力的有效方法。在各种方法中，验证-改进范式因其支持动态解决方案探索和反馈整合而脱颖而出。然而，现有方法常受限于反馈空间受限及各方缺乏协同训练，导致性能欠佳。为解决这一问题，我们将这一多轮优化过程建模为马尔可夫决策过程，并提出DPSDP（动态规划直接策略搜索）——一种通过自生成数据的直接偏好学习来训练演员-评论家LLM系统迭代优化答案的强化学习算法。理论上，DPSDP能匹配训练分布内任何策略的性能。实证中，我们基于不同基础模型实例化DPSDP，并在分布内外基准测试中均观察到性能提升。例如，在MATH 500基准测试中，基于Ministral模型的五步优化多数投票将首轮准确率从58.2%提升至63.2%。消融实验进一步验证了多智能体协作与分布外泛化的优势。

---

## [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)

### Abstract
arXiv:2506.08433v1 Announce Type: cross 
Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware, and annotated data, often resulting in a positionality rooted in predominant cultures and values (Santy et al., 2023). Domain adaptation has emerged as a promising strategy to better align models with diverse cultural and value contexts (Hershcovich et al., 2022), but its computational cost remains a significant barrier, particularly for research groups lacking access to large-scale infrastructure. In this paper, we evaluate how the use of different numerical precisions and data parallelization strategies impacts both training speed (as a proxy to energy and hardware consumption) and model accuracy, with the goal of facilitating domain adaptation in low-resource environments. Our findings are relevant to any setting where energy efficiency, accessibility, or limited hardware availability are key concerns.

### 摘要
训练大型语言模型(LLM)在能源、硬件和标注数据方面成本高昂，往往导致模型植根于主流文化和价值观(Santy等人，2023)。领域适应已成为使模型更好适应多元文化和价值背景的有前景策略(Hershcovich等人，2022)，但其计算成本仍是重大障碍，尤其对缺乏大规模基础设施的研究团队而言。本文评估了不同数值精度和数据并行策略如何影响训练速度(作为能源和硬件消耗的代理指标)与模型准确性，旨在促进资源受限环境下的领域适应。我们的研究结果适用于任何以能效、可及性或有限硬件可用性为关键考量的场景。

---

## [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)

### Abstract
arXiv:2506.08373v1 Announce Type: cross 
Abstract: Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.

### 摘要
由于Transformer模型具有二次计算复杂度和线性内存复杂度的特性，优化长上下文大语言模型（LLM）的推理过程变得愈发重要。现有的近似方法（如键值（KV）缓存丢弃、稀疏注意力和提示压缩）通常依赖于对令牌或KV对重要性的粗略预测。我们提出了一种新颖的近似LLM推理框架，该框架利用小型草稿模型更准确地预测令牌和KV对的重要性。具体而言，我们引入了该框架的两种实现方式：（i）SpecKV，通过草稿输出来精确评估每个KV对的重要性，从而实现更有效的KV缓存丢弃；（ii）SpecPC，利用草稿模型的注意力激活来识别并丢弃不重要的提示令牌。据我们所知，这是首次将草稿模型应用于近似LLM推理加速，扩展了其在传统无损推测解码之外的用途。我们通过理论和实证分析验证了方法的合理性，并证明了草稿模型与目标模型的注意力模式之间存在强相关性。在长上下文基准测试上的大量实验表明，我们的方法在保持内存使用、延迟和吞吐量同等改进的同时，始终能够获得比现有基线更高的准确率。代码已开源：https://github.com/furiosa-ai/draft-based-approx-llm。

---

## [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)

### Abstract
arXiv:2506.08430v1 Announce Type: cross 
Abstract: Large language model (LLM) have become mainstream methods in the field of sarcasm detection. However, existing LLM methods face challenges in irony detection, including: 1. single-perspective limitations, 2. insufficient comprehensive understanding, and 3. lack of interpretability. This paper introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven multi-agent system designed to overcome these issues. CAF-I employs specialized agents for Context, Semantics, and Rhetoric, which perform multidimensional analysis and engage in interactive collaborative optimization. A Decision Agent then consolidates these perspectives, with a Refinement Evaluator Agent providing conditional feedback for optimization. Experiments on benchmark datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of 76.31, a 4.98 absolute improvement over the strongest prior baseline. This success is attained by its effective simulation of human-like multi-perspective analysis, enhancing detection accuracy and interpretability.

### 摘要
大语言模型（LLM）已成为讽刺检测领域的主流方法。然而，现有LLM方法在讽刺检测中面临以下挑战：1. 单一视角局限性；2. 综合理解不足；3. 缺乏可解释性。本文提出协作式讽刺分析框架（CAF-I），这是一个基于LLM的多智能体系统，旨在解决上述问题。CAF-I通过语境、语义和修辞三个专用智能体进行多维分析，并实施交互式协同优化。决策智能体整合多方视角，优化评估智能体则提供条件反馈以实现迭代改进。在基准数据集上的实验表明，CAF-I实现了最先进的零样本性能：在绝大多数指标上达到SOTA水平，平均Macro-F1值达76.31，较现有最强基线绝对提升4.98。该成果源于其有效模拟人类多视角分析机制，从而同时提升了检测准确性与可解释性。

---

## [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)

### Abstract
arXiv:2506.08488v1 Announce Type: cross 
Abstract: In recent years, researchers have started analyzing the cultural sensitivity of LLMs. In this respect, Etiquettes have been an active area of research. Etiquettes are region-specific and are an essential part of the culture of a region; hence, it is imperative to make LLMs sensitive to etiquettes. However, there needs to be more resources in evaluating LLMs for their understanding and bias with regard to etiquettes. In this resource paper, we introduce EtiCor++, a corpus of etiquettes worldwide. We introduce different tasks for evaluating LLMs for knowledge about etiquettes across various regions. Further, we introduce various metrics for measuring bias in LLMs. Extensive experimentation with LLMs shows inherent bias towards certain regions.

### 摘要
近年来，研究者开始关注大语言模型的文化敏感性研究。礼仪规范作为区域文化的重要组成部分，因其地域特异性已成为该领域的热点研究方向。为确保大语言模型具备礼仪敏感性，当前亟需建立相关评估体系，但针对模型礼仪认知与偏见的评估资源仍显不足。本资源论文提出全球礼仪语料库EtiCor++，设计多维度任务用于评估模型对不同区域礼仪知识的掌握程度，并提出多种量化模型偏见的指标。通过大量实验验证，发现现有模型对特定区域存在固有偏见。

---

## [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)

### Abstract
arXiv:2506.08388v1 Announce Type: cross 
Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply "connect-the-dots" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework.

### 摘要
通过强化学习（RL）训练推理语言模型（LMs）以追求单次正确性，本质上依赖于模型在初始化时能够通过探索偶然解决任务的能力。此外，推理LMs的一个关键应用场景是作为教师模型，用于蒸馏新学生模型以及冷启动后续RL迭代，而非直接部署。基于这些考量，我们提出了一种新框架，通过训练一类专注于生成最有效下游蒸馏结果的强化学习教师模型（RLTs），避免了RL的探索难题。RLTs的输入同时包含问题及其解答，其任务是为学生模型生成针对性的详细解释以"连接关键点"。我们通过将每条解释输入学生模型并测试其对问题解答的理解程度，从而获得密集奖励来训练RLTs。实验表明，7B规模RLT的原始输出在竞赛级和研究生级任务上的最终性能，优于现有基于数量级更大LMs的推理轨迹收集与后处理的蒸馏及冷启动流程。此外，RLTs在训练更大规模学生模型时仍保持有效性，并能零样本迁移到分布外任务，为RL推理框架开启了效率与可复用性的新境界。

---

## [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)

### Abstract
arXiv:2506.08403v1 Announce Type: cross 
Abstract: Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at https://github.com/weiyali126/TACTIC.

### 摘要
机器翻译长期以来是自然语言处理的核心任务。随着大语言模型（LLMs）的快速发展，翻译质量取得了显著提升。然而，如何充分发挥LLMs的翻译潜力仍是一个开放性问题。近期研究尝试通过多智能体系统将复杂翻译任务分解为协作子任务，初步证明通过智能体协作与专业化可提升翻译质量。但现有多智能体翻译框架大多忽视了认知翻译研究的基础性见解，这些见解强调人类译者如何运用不同认知策略，例如平衡直译与意译、基于上下文优化表达以及迭代评估输出。为弥补这一不足，我们提出名为TACTIC（基于认知理论的交互协作翻译智能体）的认知启发多智能体框架。该框架包含六个功能各异的智能体，分别对应人类翻译行为中观察到的关键认知过程：起草、润色、评估、评分、上下文推理和外部知识收集。通过模拟基于理论的交互式翻译工作流，TACTIC能有效发挥LLMs的全部翻译潜力。在FLORES-200和WMT24基准测试的多种语言对上，实验结果表明我们的方法持续取得最先进性能。以DeepSeek-V3为基础模型时，TACTIC平均超越GPT-4.1达+0.6 XCOMET和+1.18 COMETKIWI-23；相较于DeepSeek-R1，其性能进一步提升+0.84 XCOMET和+2.99 COMETKIWI-23。代码已开源：https://github.com/weiyali126/TACTIC。

---

## [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)

### Abstract
arXiv:2506.08500v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.

### 摘要
检索增强生成（RAG）是一种通过相关且最新信息增强大语言模型（LLM）的常用方法。然而，检索到的来源往往包含相互冲突的信息，而模型应如何处理此类差异仍不明确。在本研究中，我们首先提出了一种新颖的RAG知识冲突类型分类法，并定义了每种类型下模型的理想行为。随后，我们引入了CONFLICTS基准测试集，该数据集在真实RAG场景中提供了专家标注的冲突类型，是首个能够追踪模型处理广泛知识冲突进展的基准。通过在该基准上的大量实验，我们发现LLM在合理解决不同来源间的冲突时常常表现欠佳。尽管通过提示LLM显式推理检索文档中的潜在冲突能显著提升其回答的质量与合理性，但未来研究仍存在较大改进空间。

---

## [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)

### Abstract
arXiv:2506.08479v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs) both address context limitations of LLMs in open-domain question answering (QA). However, optimal external context to retrieve remains an open problem: fixing the retrieval size risks either wasting tokens or omitting key evidence. Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM prompting and perform well on factoid QA, but struggle with aggregation QA, where the optimal context size is both unknown and variable. We present Adaptive-$k$ retrieval, a simple and effective single-pass method that adaptively selects the number of passages based on the distribution of the similarity scores between the query and the candidate passages. It does not require model fine-tuning, extra LLM inferences or changes to existing retriever-reader pipelines. On both factoid and aggregation QA benchmarks, Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x fewer tokens than full-context input, yet still retrieves 70% of relevant passages. It improves accuracy across five LCLMs and two embedding models, highlighting that dynamically adjusting context size leads to more efficient and accurate QA.

### 摘要
检索增强生成（RAG）和长上下文语言模型（LCLM）均致力于解决开放域问答（QA）中大型语言模型（LLM）的上下文限制问题。然而，如何确定最优的外部检索上下文仍是一个悬而未决的问题：固定检索规模可能导致令牌浪费或关键证据遗漏。现有自适应方法（如Self-RAG和Self-Route）依赖于迭代式LLM提示，在事实型QA中表现良好，但在聚合型QA中表现欠佳——此类任务的最优上下文规模既未知又可变。本文提出自适应$k$检索法，这是一种基于查询与候选段落相似度分数分布、自适应选择段落数量的单次高效方法。该方法无需模型微调、额外LLM推理或修改现有检索-阅读器流程。在事实型和聚合型QA基准测试中，自适应$k$方法在令牌使用量比全上下文输入减少10倍的情况下，性能匹配或优于固定$k$基线，同时仍能检索70%的相关段落。该方法在五种LCLM和两种嵌入模型上均提升了准确率，证明动态调整上下文规模可实现更高效、更精准的问答。

---

## [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)

### Abstract
arXiv:2506.08512v1 Announce Type: cross 
Abstract: Video Temporal Grounding (VTG), which aims to localize video clips corresponding to natural language queries, is a fundamental yet challenging task in video understanding. Existing Transformer-based methods often suffer from redundant attention and suboptimal multi-modal alignment. To address these limitations, we propose MLVTG, a novel framework that integrates two key modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba blocks as a backbone instead of Transformers to model temporal dependencies and extract robust video representations for multi-modal alignment. LLMRefiner leverages the specific frozen layer of a pre-trained Large Language Model (LLM) to implicitly transfer semantic priors, enhancing multi-modal alignment without fine-tuning. This dual alignment strategy, temporal modeling via structured state-space dynamics and semantic purification via textual priors, enables more precise localization. Extensive experiments on QVHighlights, Charades-STA, and TVSum demonstrate that MLVTG achieves state-of-the-art performance and significantly outperforms existing baselines.

### 摘要
视频时序定位（VTG）旨在根据自然语言查询定位对应的视频片段，是视频理解领域中基础但具有挑战性的任务。现有基于Transformer的方法常面临注意力冗余和多模态对齐欠优的问题。为克服这些局限，我们提出MLVTG框架，其整合了两个核心模块：MambaAligner和LLMRefiner。MambaAligner采用堆叠的Vision Mamba块作为主干网络替代Transformer，通过结构化状态空间动态建模时序依赖关系，并提取鲁棒的视频表征以实现多模态对齐；LLMRefiner则利用预训练大语言模型（LLM）的特定冻结层隐式迁移语义先验，无需微调即可增强多模态对齐。这种双重对齐策略——通过状态空间动态进行时序建模与通过文本先验进行语义净化——实现了更精准的定位。在QVHighlights、Charades-STA和TVSum数据集上的大量实验表明，MLVTG取得了最先进的性能，显著优于现有基线方法。

---

## [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)

### Abstract
arXiv:2506.08524v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and multimodal processing, yet they fundamentally lack physical awareness--understanding of real-world physical phenomena. In this work, we present ACORN, a framework that teaches LLMs physical awareness through sound, focusing on fundamental physical phenomena like the Doppler effect, multipath effect, and spatial relationships. To overcome data scarcity, ACORN introduce a physics-based simulator combining real-world sound sources with controlled physical channels to generate diverse training data. Using this simulator, we build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an audio encoder that processes both magnitude and phase information. By connecting our audio encoder to state-of-the-art LLMs, we demonstrate reasonable results in both simulated and real-world tasks, such as line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation, paving the way for enabling LLMs to understand physical world.

### 摘要
大语言模型（LLMs）在文本和多模态处理方面展现出卓越能力，但其本质上缺乏物理意识——即对现实世界物理现象的理解。本研究提出ACORN框架，通过声音教授LLMs物理意识，重点关注多普勒效应、多径效应和空间关系等基础物理现象。为解决数据稀缺问题，ACORN引入基于物理的模拟器，将真实声源与可控物理通道结合以生成多样化训练数据。利用该模拟器，我们构建了AQA-PHY综合音频问答数据集，并提出同时处理幅度和相位信息的音频编码器。通过将该编码器与最先进的大语言模型连接，我们在模拟和真实场景任务（如视距检测、多普勒效应估计和波达方向估计）中均取得合理结果，为LLMs理解物理世界开辟了新路径。

---

## [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)

### Abstract
arXiv:2506.08487v1 Announce Type: cross 
Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and resource-constrained deployments has outpaced our understanding of their ethical risks. To the best of our knowledge, we present the first large-scale audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we analyze both utility and fairness across ambiguous and disambiguated contexts. This evaluation reveals three key insights. First, competence and fairness need not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while exhibiting minimal bias, showing that efficient and ethical NLP is attainable. Second, social bias varies significantly by architecture: Qwen 2.5 models may appear fair, but this often reflects vacuous neutrality, random guessing, or evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2 models exhibit stronger stereotypical bias, suggesting overconfidence rather than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but increases disability-related bias in Phi-4-Mini by over 7 percentage points. These insights provide practical guidance for the responsible deployment of SLMs in applications demanding fairness and efficiency, particularly benefiting small enterprises and resource-constrained environments.

### 摘要
小型语言模型（SLMs）在终端设备和资源受限环境中的快速普及已超出我们对其伦理风险的认知范围。本研究首次对参数量在0.5至50亿之间的指令微调SLMs进行了大规模审计——这一介于BERT类编码器与旗舰大模型之间的"中间层"长期被忽视。我们评估了Qwen 2.5、LLaMA 3.2、Gemma 3和Phi系列的九个开源模型，通过BBQ基准测试在零样本提示下分析模糊与明确语境中的效用与公平性。研究揭示三个关键发现：首先，能力与公平性并非必然对立——Phi系列在保持F1分数超过90%的同时展现出极低偏差，证明高效且符合伦理的自然语言处理是可实现的；其次，社会偏见因架构差异显著：Qwen 2.5模型可能看似公平，但这常反映为空洞中立、随机猜测或回避行为，而非真正的伦理对齐，而LLaMA 3.2模型则表现出更强的刻板偏见，暗示其存在过度自信而非中立性；第三，量化压缩带来微妙权衡：4位AWQ量化使LLaMA 3.2-3B在模糊场景下F1分数提升，但导致Phi-4-Mini的残障相关偏见增加超过7个百分点。这些发现为需要兼顾公平性与效率的SLMs应用部署提供了实践指导，特别有利于中小企业和资源受限环境。

---

## [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)

### Abstract
arXiv:2506.08552v1 Announce Type: cross 
Abstract: Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA without additional training.

### 摘要
推理是大型语言模型实现语言理解的关键组件。尽管思维链提示通过显式中间步骤提升了性能，但其存在显著的token开销和固定推理轨迹的缺陷，无法实现逐步优化。近期潜在推理技术的进展通过直接在模型隐空间优化内部推理过程（无需生成显式输出）解决了这些局限。然而核心挑战依然存在：如何在训练后阶段有效更新推理嵌入以引导模型获得更精确解。为攻克这一难题，我们提出一种轻量级训练后框架，采用两种创新策略优化潜在推理轨迹：1）对比推理反馈——通过将推理嵌入与强弱基线对比，借助嵌入增强技术推断有效更新方向；2）残差嵌入优化——通过渐进融合当前与历史梯度来稳定更新过程，实现快速且受控的收敛。我们在五个推理基准上进行了大量实验与案例研究，验证了所提框架的有效性。特别值得注意的是，该方法在MathQA上实现了5%的准确率提升且无需额外训练。

---

## [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)

### Abstract
arXiv:2506.08647v1 Announce Type: cross 
Abstract: We explore a generative relation extraction (RE) pipeline tailored to the study of interactions in the intestinal microbiome, a complex and low-resource biomedical domain. Our method leverages summarization with large language models (LLMs) to refine context before extracting relations via instruction-tuned generation. Preliminary results on a dedicated corpus show that summarization improves generative RE performance by reducing noise and guiding the model. However, BERT-based RE approaches still outperform generative models. This ongoing work demonstrates the potential of generative methods to support the study of specialized domains in low-resources setting.

### 摘要
我们研究了一种专为肠道微生物组（一个复杂且资源匮乏的生物医学领域）相互作用分析设计的生成式关系抽取（RE）流程。该方法利用大语言模型（LLMs）的摘要生成能力优化上下文信息，继而通过指令调优的生成方式进行关系抽取。在专用语料库上的初步实验表明，摘要生成能通过降低噪声和引导模型来提升生成式RE性能。然而基于BERT的RE方法仍优于生成模型。这项持续性的研究工作证明了生成式方法在资源匮乏环境下支持专业领域研究的潜力。

---

## [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)

### Abstract
arXiv:2506.08572v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive generalization capabilities across various tasks, but their claim to practical relevance is still mired by concerns on their reliability. Recent works have proposed examining the activations produced by an LLM at inference time to assess whether its answer to a question is correct. Some works claim that a "geometry of truth" can be learned from examples, in the sense that the activations that generate correct answers can be distinguished from those leading to mistakes with a linear classifier. In this work, we underline a limitation of these approaches: we observe that these "geometries of truth" are intrinsically task-dependent and fail to transfer across tasks. More precisely, we show that linear classifiers trained across distinct tasks share little similarity and, when trained with sparsity-enforcing regularizers, have almost disjoint supports. We show that more sophisticated approaches (e.g., using mixtures of probes and tasks) fail to overcome this limitation, likely because activation vectors commonly used to classify answers form clearly separated clusters when examined across tasks.

### 摘要
大语言模型（LLMs）在各种任务中展现出卓越的泛化能力，但其实际应用仍受可靠性问题的困扰。近期研究提出通过分析LLM推理时产生的激活状态来评估其回答的正确性。部分研究声称可以从样本中学习"真理的几何结构"，即通过线性分类器区分生成正确答案与错误答案的激活模式。本研究揭示了此类方法的局限性：我们发现这些"真理的几何结构"本质上具有任务依赖性，无法跨任务迁移。具体而言，我们证明针对不同任务训练的线性分类器相似度极低，且当采用稀疏化正则器训练时，其支持集几乎完全不相交。进一步研究表明，更复杂的方法（如使用混合探针和任务）仍无法克服这一局限，这可能因为常用于答案分类的激活向量在跨任务分析时会形成明显分离的簇。

---

## [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)

### Abstract
arXiv:2506.08646v1 Announce Type: cross 
Abstract: Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at https://github.com/SpursGoZmy/TableDreamer

### 摘要
尽管近期基于大语言模型（LLM）的数据合成方法取得了显著进展，但这些方法在生成表格指令调优数据时仍面临两个局限：其一，无法充分探索表格理解任务庞大的输入空间，导致数据多样性不足；其二，忽视目标LLM在表格理解能力上的弱点，盲目追求数据量增长，造成数据效率低下。本文提出名为TableDreamer的渐进式弱点导向数据合成框架以解决上述问题。具体而言，我们首先生成多样化表格及相关指令作为种子数据，随后在新识别的弱点数据引导下对输入空间进行迭代探索，最终将这些数据作为微调目标LLM的训练集。在10个表格基准测试上的大量实验表明，该框架仅用27K条GPT-4o合成数据就将Llama3.1-8B-instruct的平均准确率提升11.62%（从49.07%至60.69%），其性能优于使用更多训练数据的当前最优数据合成基线。代码与数据详见https://github.com/SpursGoZmy/TableDreamer。

---

## [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)

### Abstract
arXiv:2506.08712v1 Announce Type: cross 
Abstract: We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead.

### 摘要
我们提出ConfPO方法，这是一种基于训练策略置信度来识别并优化偏好关键标记的大语言模型偏好学习技术，无需任何辅助模型或额外计算。与直接偏好优化（DPO）等现有直接对齐算法（DAAs）对所有标记概率进行均匀调整不同，ConfPO专注于对偏好影响最大的标记进行优化。这种针对性方法通过更有效地利用KL散度预算，在提高对齐质量的同时缓解过度优化（即奖励破解）问题。相较于近期依赖信用分配模型或AI标注器的标记级方法（其可扩展性和可靠性存在争议），ConfPO具有简单、轻量且无需模型的特点。在AlpacaEval 2和Arena-Hard等具有挑战性的对齐基准测试中，实验结果表明ConfPO在不同大语言模型上均优于均匀DAAs，在零额外计算开销的情况下实现了更好的对齐效果。

---

## [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)

### Abstract
arXiv:2506.08726v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.

### 摘要
大型语言模型（LLMs）在众多自然语言处理任务中展现出卓越能力，但在处理包含表格与文本数据的金融文档数值问答时仍存在困难。近期研究表明，在给定真实标签的条件下，批评代理（即自我修正机制）对此类任务具有显著效果。本文基于该框架，探究了传统批评代理在缺乏真实标签时的有效性，并通过实验证明其性能在此场景下会显著下降。为此，我们提出一种改进型批评代理及计算器代理，其表现优于当前最先进的思维链编程方法且更具安全性。此外，我们还深入研究了不同代理间的交互机制及其对整体性能的影响。

---

## [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon &amp; Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)

### Abstract
arXiv:2506.08727v1 Announce Type: cross 
Abstract: While Generative AI stands to be one of the fastest adopted technologies ever, studies have made evident that the usage of Large Language Models (LLMs) puts significant burden on energy grids and our environment. It may prove a hindrance to the Sustainability goals of any organization. A crucial step in any Sustainability strategy is monitoring or estimating the energy consumption of various components. While there exist multiple tools for monitoring energy consumption, there is a dearth of tools/frameworks for estimating the consumption or carbon emissions. Current drawbacks of both monitoring and estimation tools include high input data points, intrusive nature, high error margin, etc. We posit that leveraging emerging LLM benchmarks and related data points can help overcome aforementioned challenges while balancing accuracy of the emission estimations. To that extent, we discuss the challenges of current approaches and present our evolving framework, R-ICE, which estimates prompt level inference carbon emissions by leveraging existing state-of-the-art(SOTA) benchmark. This direction provides a more practical and non-intrusive way to enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our promising validation results suggest that benchmark-based modelling holds great potential for inference emission estimation and warrants further exploration from the scientific community.

### 摘要
尽管生成式人工智能有望成为有史以来普及速度最快的技术之一，但研究已表明大型语言模型（LLM）的使用会对电网和环境造成重大负担，这可能阻碍各类组织的可持续发展目标。在可持续发展战略中，监测或估算各组件能耗是关键步骤。虽然现有多种能耗监测工具，但估算能耗与碳排放的工具/框架仍显不足。当前监测与估算工具存在输入数据要求高、侵入性强、误差幅度大等缺陷。我们认为，利用新兴的LLM基准测试及相关数据点可在平衡排放估算精度的同时解决上述挑战。为此，本文探讨了现有方法的局限性，并提出动态演进框架R-ICE——该框架通过利用现有最先进（SOTA）基准测试来估算提示级推理碳排放。这一方向为动态LLM路由、碳核算等新兴应用场景提供了更实用且非侵入性的实现路径。验证结果表明，基于基准测试的建模方法在推理排放估算领域具有显著潜力，值得科学界进一步探索。

---

## [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)

### Abstract
arXiv:2506.08669v1 Announce Type: cross 
Abstract: Small language models (SLMs) offer promising and efficient alternatives to large language models (LLMs). However, SLMs' limited capacity restricts their reasoning capabilities and makes them sensitive to prompt variations. To address these challenges, we propose a novel framework that enhances SLM reasoning capabilities through LLM generated blueprints. The blueprints provide structured, high-level reasoning guides that help SLMs systematically tackle related problems. Furthermore, our framework integrates a prompt template search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our framework demonstrates improved SLM performance across various tasks, including math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves the reasoning capabilities of SLMs without increasing model size or requiring additional training, offering a lightweight and deployment-friendly solution for on-device or resource-constrained environments.

### 摘要
小语言模型（SLMs）为大型语言模型（LLMs）提供了高效且具有前景的替代方案。然而，SLMs的有限能力限制了其推理能力，并使其对提示变化敏感。为解决这些问题，我们提出了一种新颖框架，通过LLM生成的蓝图增强SLMs的推理能力。这些蓝图提供了结构化的高层次推理指导，帮助SLMs系统性地解决相关问题。此外，我们的框架集成了提示模板搜索机制，以减轻SLMs对提示变化的敏感性。实验表明，该框架在数学（GSM8K）、编程（MBPP）和逻辑推理（BBH）等多种任务中显著提升了SLMs的性能。我们的方法在不增加模型规模或需要额外训练的情况下改进了SLMs的推理能力，为设备端或资源受限环境提供了一种轻量级且易于部署的解决方案。

---

## [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)

### Abstract
arXiv:2506.08753v1 Announce Type: cross 
Abstract: This study explores the application of in-context learning (ICL) to the dialogue state tracking (DST) problem and investigates the factors that influence its effectiveness. We use a sentence embedding based k-nearest neighbour method to retrieve the suitable demonstrations for ICL. The selected demonstrations, along with the test samples, are structured within a template as input to the LLM. We then conduct a systematic study to analyse the impact of factors related to demonstration selection and prompt context on DST performance. This work is conducted using the MultiWoZ2.4 dataset and focuses primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and Llama3.2-3B-Instruct models. Our findings provide several useful insights on in-context learning abilities of LLMs for dialogue state tracking.

### 摘要
本研究探讨了上下文学习（ICL）在对话状态跟踪（DST）问题中的应用，并分析了影响其效果的关键因素。我们采用基于句子嵌入的k近邻方法检索适合ICL的示例样本，将筛选出的示例与测试样本通过模板结构化后输入大语言模型（LLM）。通过系统实验，我们深入研究了示例选择策略和提示上下文相关因素对DST性能的影响。本工作基于MultiWoZ2.4数据集，主要针对OLMo-7B-instruct、Mistral-7B-Instruct-v0.3和Llama3.2-3B-Instruct模型展开。研究结果为理解大语言模型在对话状态跟踪任务中的上下文学习能力提供了重要启示。

---

## [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)

### Abstract
arXiv:2506.08790v1 Announce Type: cross 
Abstract: Software sustainability is emerging as a primary concern, aiming to optimize resource utilization, minimize environmental impact, and promote a greener, more resilient digital ecosystem. The sustainability or "greenness" of software is typically determined by the adoption of sustainable coding practices. With a maturing ecosystem around generative AI, many software developers now rely on these tools to generate code using natural language prompts. Despite their potential advantages, there is a significant lack of studies on the sustainability aspects of AI-generated code. Specifically, how environmentally friendly is the AI-generated code based upon its adoption of sustainable coding practices? In this paper, we present the results of an early investigation into the sustainability aspects of AI-generated code across three popular generative AI tools - ChatGPT, BARD, and Copilot. The results highlight the default non-green behavior of tools for generating code, across multiple rules and scenarios. It underscores the need for further in-depth investigations and effective remediation strategies.

### 摘要
软件可持续性正逐渐成为核心议题，其目标在于优化资源利用、减少环境影响，并推动构建更绿色、更具韧性的数字生态系统。软件可持续性或"绿色程度"通常取决于可持续编码实践的采用程度。随着生成式人工智能生态系统的日益成熟，许多软件开发人员开始依赖此类工具通过自然语言提示生成代码。尽管存在潜在优势，但针对AI生成代码可持续性方面的研究仍严重不足。具体而言，基于可持续编码实践的采用情况，AI生成的代码在环保性方面表现如何？本文呈现了对三种主流生成式AI工具（ChatGPT、BARD和Copilot）所生成代码可持续性特征的初步研究结果。研究结果揭示了这些代码生成工具在多种规则和场景下默认存在的非绿色行为，强调了开展深入研究和制定有效改进策略的必要性。

---

## [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)

### Abstract
arXiv:2506.08762v1 Announce Type: cross 
Abstract: Financial analysis presents complex challenges that could leverage large language model (LLM) capabilities. However, the scarcity of challenging financial datasets, particularly for Japanese financial data, impedes academic innovation in financial analytics. As LLMs advance, this lack of accessible research resources increasingly hinders their development and evaluation in this specialized domain. To address this gap, we introduce EDINET-Bench, an open-source Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction. EDINET-Bench is constructed by downloading annual reports from the past 10 years from Japan's Electronic Disclosure for Investors' NETwork (EDINET) and automatically assigning labels corresponding to each evaluation task. Our experiments reveal that even state-of-the-art LLMs struggle, performing only slightly better than logistic regression in binary classification for fraud detection and earnings forecasting. These results highlight significant challenges in applying LLMs to real-world financial applications and underscore the need for domain-specific adaptation. Our dataset, benchmark construction code, and evaluation code is publicly available to facilitate future research in finance with LLMs.

### 摘要
金融分析领域存在诸多复杂挑战，这些挑战可能利用大型语言模型（LLM）的能力。然而，具有挑战性的金融数据集（尤其是日本金融数据）的稀缺性阻碍了金融分析领域的学术创新。随着LLM技术的发展，这种可获取研究资源的缺乏日益制约着该专业领域中LLM的开发与评估。为填补这一空白，我们推出了EDINET-Bench——一个开源的日本金融基准测试集，旨在评估LLM在会计欺诈检测、盈利预测和行业预测等具有挑战性的金融任务中的表现。该基准通过从日本投资者电子披露网络（EDINET）下载过去10年的年度报告，并自动分配与每个评估任务相对应的标签构建而成。实验结果表明，即使是最先进的LLM也表现欠佳：在欺诈检测和盈利预测的二元分类任务中，其性能仅略优于逻辑回归。这些发现凸显了LLM在实际金融应用中的重大挑战，并强调了领域适应性调整的必要性。我们公开提供数据集、基准构建代码和评估代码，以促进未来基于LLM的金融研究。

---

## [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)

### Abstract
arXiv:2506.08889v1 Announce Type: cross 
Abstract: We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.

### 摘要
我们提出SeerAttention-R，一个专为推理模型长序列解码设计的稀疏注意力框架。该框架在SeerAttention基础上进行扩展，保留了通过自蒸馏门控机制学习注意力稀疏性的设计，同时取消了查询池化以适配自回归解码。凭借轻量级的插件式门控结构，SeerAttention-R具有高度灵活性，无需修改预训练模型原有参数即可轻松集成。实验表明，在仅训练0.4B令牌的情况下，SeerAttention-R在AIME基准测试中能以4K令牌预算保持近乎无损的推理准确率（大稀疏注意力块尺寸为64/128时）。通过TileLang开发的优化稀疏解码内核，在H100 GPU上90%稀疏度下实现了较FlashAttention-3最高达9倍的理论加速比。代码已开源：https://github.com/microsoft/SeerAttention。

---

## [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)

### Abstract
arXiv:2506.08899v1 Announce Type: cross 
Abstract: We present a novel approach to the automated semantic analysis of legal texts using large language models (LLMs), targeting their transformation into formal representations in Defeasible Deontic Logic (DDL). We propose a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. Our methodology is evaluated across various LLM configurations, including prompt engineering strategies, fine-tuned models, and multi-stage pipelines, focusing on legal norms from the Australian Telecommunications Consumer Protections Code. Empirical results demonstrate promising alignment between machine-generated and expert-crafted formalizations, showing that LLMs - particularly when prompted effectively - can significantly contribute to scalable legal informatics.

### 摘要
我们提出了一种利用大型语言模型（LLMs）对法律文本进行自动化语义分析的新方法，旨在将其转化为可废止道义逻辑（DDL）的形式化表示。该研究设计了一个结构化流程：首先将复杂的规范性语言分割为原子片段，随后提取道义规则，并评估其句法与语义连贯性。我们在不同LLM配置下对该方法进行了评估，包括提示工程策略、微调模型和多阶段处理流程，并以《澳大利亚电信消费者保护准则》中的法律规范为研究对象。实验结果表明，机器生成的形式化表示与专家构建的结果具有显著一致性，证明LLMs——尤其是当采用有效提示策略时——能够为可扩展的法律信息学做出重要贡献。

---

## [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)

### Abstract
arXiv:2506.08952v1 Announce Type: cross 
Abstract: Communication among humans relies on conversational grounding, allowing interlocutors to reach mutual understanding even when they do not have perfect knowledge and must resolve discrepancies in each other's beliefs. This paper investigates how large language models (LLMs) manage common ground in cases where they (don't) possess knowledge, focusing on facts in the political domain where the risk of misinformation and grounding failure is high. We examine the ability of LLMs to answer direct knowledge questions and loaded questions that presuppose misinformation. We evaluate whether loaded questions lead LLMs to engage in active grounding and correct false user beliefs, in connection to their level of knowledge and their political bias. Our findings highlight significant challenges in LLMs' ability to engage in grounding and reject false user beliefs, raising concerns about their role in mitigating misinformation in political discourse.

### 摘要
人类交流依赖于会话基础，使对话者即使在知识不完备且必须解决彼此信念差异时仍能达成相互理解。本文研究大型语言模型（LLMs）在（不）具备知识情况下如何管理共同基础，重点关注存在高度错误信息与基础失败风险的政治领域事实。我们检验LLMs回答直接知识性问题及预设错误信息的诱导性问题的能力，评估诱导性问题是否促使LLMs参与主动基础构建并纠正用户的错误信念，同时分析该行为与其知识水平及政治偏见的关联。研究结果突显LLMs在参与基础构建和拒绝用户错误信念方面存在显著挑战，这对其在政治话语中缓解错误信息的作用提出了重要关切。

---

## [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)

### Abstract
arXiv:2506.08920v1 Announce Type: cross 
Abstract: Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations.

### 摘要
针对大语言模型（LLM）的知识编辑技术能够注入后续可逐字复现的知识，但这些技术在知识传播方面存在不足：模型无法回答需要基于注入知识进行推理的问题。我们提出了一种基于超网络的知识传播方法PropMEND，该方法通过元学习来调整语言建模损失的梯度更新，从而促进注入信息的传播。我们的方法扩展了MEND[29]的元目标，使得针对知识的梯度更新能够转化为支持涉及该知识的多跳问题解答。在RippleEdit数据集上的实验表明，该方法在具有挑战性的多跳问题上实现了近2倍的准确率提升，这些问题的答案并未明确包含在注入事实中。我们进一步引入新数据集Controlled RippleEdit，用于评估超网络的泛化能力，测试其在超网络训练阶段未见的实体和关系上的知识传播效果。PropMEND在未见实体-关系对上仍优于现有方法，但性能差距显著缩小，这表明未来需要在更广泛关系中进行知识传播的研究。

---

## [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)

### Abstract
arXiv:2506.08927v1 Announce Type: cross 
Abstract: Recent research in vision-language models (VLMs) has centered around the possibility of equipping them with implicit long-form chain-of-thought reasoning -- akin to the success observed in language models -- via distillation and reinforcement learning. But what about the non-reasoning models already trained and deployed across the internet? Should we simply abandon them, or is there hope for a search mechanism that can elicit hidden knowledge and induce long reasoning traces -- without any additional training or supervision? In this paper, we explore this possibility using a Monte Carlo Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer pairs into the model's output stream. We show that framing reasoning as a search process -- where subquestions act as latent decisions within a broader inference trajectory -- helps the model "connect the dots" between fragmented knowledge and produce extended reasoning traces in non-reasoning models. We evaluate our method across three benchmarks and observe consistent improvements. Notably, our approach yields a 2% overall improvement on MMMU-PRO, including a significant 9% gain in Liberal Arts.

### 摘要
近期视觉语言模型（VLM）的研究主要聚焦于通过蒸馏学习和强化学习，为其配备隐式长链思维推理能力——类似于语言模型中已取得的成功。但那些已在互联网上训练并部署的非推理模型又当如何？我们是否应该直接弃用它们，抑或存在某种搜索机制能够在不进行额外训练或监督的情况下，激发其隐藏知识并诱导长推理轨迹？本文通过蒙特卡洛树搜索（MCTS）启发的算法探索这种可能性，该算法将子问题-子答案对注入模型的输出流。我们证明，将推理构建为搜索过程——其中子问题作为更广泛推理轨迹中的潜在决策点——能帮助模型在非推理模型中"串联零散知识"并生成扩展推理轨迹。通过在三个基准测试上的评估，我们观察到一致的性能提升。值得注意的是，本方法在MMMU-PRO基准上实现了2%的整体提升，其中人文学科领域显著提高了9%。

---

## [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)

### Abstract
arXiv:2506.08935v1 Announce Type: cross 
Abstract: While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. https://github.com/shinandrew/YouronMath.

### 摘要
尽管大语言模型（LLMs）在包括数学推理在内的多项任务中表现卓越，但其开发通常需要极高的计算资源。近期进展虽降低了训练高效模型的成本，但这些方法仍依赖于高端硬件集群。本文证明，通过整合强化学习与内存优化技术，仅需单个普通游戏GPU即可训练出稳健的数学推理模型。具体而言，我们在显存16GB的RTX 3080 Ti上训练了一个15亿参数的数学推理模型，在资源受限环境下，其于数学推理基准测试中的表现达到或超越了数倍规模更大的模型。本研究结果挑战了'尖端数学推理必须依赖大规模基础设施'的范式，为高性能AI研究提供了普惠化路径。https://github.com/shinandrew/YouronMath。

---

## [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)

### Abstract
arXiv:2506.08827v1 Announce Type: cross 
Abstract: The extraction of information about traffic accidents from legal documents is crucial for quantifying insurance company costs. Extracting entities such as percentages of physical and/or psychological disability and the involved compensation amounts is a challenging process, even for experts, due to the subtle arguments and reasoning in the court decision. A two-step procedure is proposed: first, segmenting the document identifying the most relevant segments, and then extracting the entities. For text segmentation, two methodologies are compared: a classic method based on regular expressions and a second approach that divides the document into blocks of n-tokens, which are then vectorized using multilingual models for semantic searches (text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models (LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to the selected segments for entity extraction. For the LLaMA models, fine-tuning is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a significant number of hallucinations in extractions which are an important contention point for named entity extraction. This work shows that these hallucinations are substantially reduced after finetuning the model. The performance of the methodology based on segment vectorization and subsequent use of LLMs significantly surpasses the classic method which achieves an accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning achieves the highest accuracy 79.4%, surpassing its base version 61.7%. Notably, the base LLaMA-3 8B model already performs comparably to the finetuned LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

### 摘要
从法律文书中提取交通事故信息对量化保险公司成本至关重要。由于法院判决中微妙的论证和推理，即使对专家而言，提取身体和/或心理残疾百分比及相应赔偿金额等实体仍具挑战性。本研究提出两步流程：首先对文档进行分段以识别最相关段落，随后进行实体提取。在文本分段阶段，比较了两种方法：基于正则表达式的经典方法，以及将文档划分为n个令牌块后通过多语言模型（text-embedding-ada-002/MiniLM-L12-v2）进行向量化以实现语义搜索的方法。接着对选定段落采用大型语言模型（LLaMA-2 7b/70b、LLaMA-3 8b和GPT-4 Turbo）进行提示式实体提取。针对LLaMA模型采用LoRA进行微调。实验表明，即使温度参数为零，LLaMA-2 7b在提取过程中仍存在大量幻觉现象，这对命名实体提取构成关键争议点。经微调后该模型的幻觉现象显著减少。基于段落向量化和LLMs的方法性能显著优于经典方法（准确率39.5%）。在开源模型中，经微调的LLaMA-2 70B以79.4%的准确率位居首位，超越其基础版本（61.7%）。值得注意的是，基础版LLaMA-3 8B已达到76.6%的准确率，与微调后的LLaMA-2 70B相当，体现了模型发展的快速进步。而GPT-4 Turbo则以86.1%的准确率创下最佳表现。

---

## [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)

### Abstract
arXiv:2506.09046v1 Announce Type: cross 
Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for addressing complex, high-dimensional tasks, but current approaches often rely on static, manually engineered multi-agent configurations. To overcome these constraints, we present the Agentic Neural Network(ANN), a framework that conceptualizes multi-agent collaboration as a layered neural network architecture. In this design, each agent operates as a node, and each layer forms a cooperative "team" focused on a specific subtask. Agentic Neural Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing inspiration from neural network forward passes, tasks are dynamically decomposed into subtasks, and cooperative agent teams with suitable aggregation methods are constructed layer by layer. (2) Backward Phase-Mirroring backpropagation, we refine both global and local collaboration through iterative feedback, allowing agents to self-evolve their roles, prompts, and coordination. This neuro-symbolic approach enables ANN to create new or specialized agent teams post-training, delivering notable gains in accuracy and adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent baselines under the same configurations, showing consistent performance improvements. Our findings indicate that ANN provides a scalable, data-driven framework for multi-agent systems, combining the collaborative capabilities of LLMs with the efficiency and flexibility of neural network principles. We plan to open-source the entire framework.

### 摘要
利用多个大型语言模型（LLMs）已被证明能有效解决复杂的高维任务，但现有方法通常依赖于静态、人工设计的多智能体配置。为突破这些限制，我们提出了代理神经网络（ANN）框架，该框架将多智能体协作概念化为分层的神经网络架构。在此设计中，每个智能体作为节点运行，每层形成一个专注于特定子任务的协作"团队"。代理神经网络采用两阶段优化策略：（1）前向阶段——受神经网络前向传播启发，动态将任务分解为子任务，并逐层构建具有合适聚合方法的协作智能体团队；（2）反向阶段——类比反向传播，通过迭代反馈优化全局和局部协作，使智能体能自我进化其角色、提示和协调机制。这种神经符号方法使ANN能在训练后创建新的或专业化的智能体团队，显著提升准确性和适应性。在四个基准数据集上的实验表明，相同配置下ANN优于主流多智能体基线方法，展现出稳定的性能提升。研究结果表明，ANN为多智能体系统提供了可扩展、数据驱动的框架，将LLMs的协作能力与神经网络原理的高效性和灵活性相结合。我们计划开源整个框架。

---

## [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)

### Abstract
arXiv:2506.09033v1 Announce Type: cross 
Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit&#123;i.e.&#125;, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf&#123;Router-R1&#125;, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1.

### 摘要
多样化的海量语言模型（LLM）快速涌现，推动了能够将用户查询分配给最合适模型的LLM路由器的研发。然而，现有LLM路由器通常执行单轮次、一对一的映射（即独立地将每个查询分配给单一模型），这限制了其处理需要多个LLM互补优势的复杂任务的能力。本文提出**Router-R1**，一个基于强化学习（RL）的框架，将多LLM路由与聚合建模为序列决策过程。Router-R1将路由器本身实例化为一个具备推理能力的LLM，通过交替执行“思考”动作（内部推演）与“路由”动作（动态模型调用），并将各响应整合至其演化的上下文中。为引导学习，我们采用轻量级基于规则的奖励机制，包含格式奖励、最终结果奖励以及用于性能与成本权衡优化的新型成本奖励，从而开辟了通过RL优化性能-成本权衡的路径。Router-R1仅需依赖定价、延迟和示例性能等简单模型描述符即可运行，展现出对未见模型选择的强泛化能力。在七个通用及多跳QA基准测试上的实验表明，Router-R1优于多个强基线，在保持鲁棒泛化性与成本管理的同时实现了更优性能。代码发布于https://github.com/ulab-uiuc/Router-R1。

---

## [Human-like object concept representations emerge naturally in multimodal large language models](https://arxiv.org/abs/2407.01067)

### Abstract
arXiv:2407.01067v2 Announce Type: replace 
Abstract: Understanding how humans conceptualize and categorize natural objects offers critical insights into perception and cognition. With the advent of Large Language Models (LLMs), a key question arises: can these models develop human-like object representations from linguistic and multimodal data? In this study, we combined behavioral and neuroimaging analyses to explore the relationship between object concept representations in LLMs and human cognition. We collected 4.7 million triplet judgments from LLMs and Multimodal LLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity structure of 1,854 natural objects. The resulting 66-dimensional embeddings were stable, predictive, and exhibited semantic clustering similar to human mental representations. Remarkably, the dimensions underlying these embeddings were interpretable, suggesting that LLMs and MLLMs develop human-like conceptual representations of objects. Further analysis showed strong alignment between model embeddings and neural activity patterns in brain regions such as EBA, PPA, RSC, and FFA. This provides compelling evidence that the object representations in LLMs, while not identical to human ones, share fundamental similarities that reflect key aspects of human conceptual knowledge. Our findings advance the understanding of machine intelligence and inform the development of more human-like artificial cognitive systems.

### 摘要
理解人类如何概念化和分类自然对象，为感知与认知研究提供了关键洞见。随着大语言模型（LLMs）的出现，一个核心问题随之产生：这些模型能否从语言和多模态数据中形成类人的对象表征？本研究结合行为实验与神经影像分析，探究LLMs中的对象概念表征与人类认知之间的关系。我们收集了470万组来自LLMs和多模态大语言模型（MLLMs）的三元组判断数据，通过降维处理获得了刻画1,854个自然对象相似性结构的低维嵌入表征。最终生成的66维嵌入具有稳定性、预测性，并展现出与人类心理表征相似的语义聚类特征。值得注意的是，这些嵌入的底层维度具有可解释性，表明LLMs和MLLMs能够形成类人的对象概念表征。进一步分析显示，模型嵌入与大脑特定区域（包括EBA、PPA、RSC和FFA）的神经活动模式高度吻合。这有力证明：尽管LLMs的对象表征与人类并非完全相同，但二者存在反映人类概念知识核心特征的根本相似性。本研究推动了对机器智能的理解，并为开发更具人类特质的人工认知系统提供了理论依据。

---

## [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)

### Abstract
arXiv:2506.09034v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDA's parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO's formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training.

### 摘要
微调大型语言模型（LLM）时常面临GPU内存瓶颈：如Adam等一阶优化器的反向传播会使内存使用量增至推理阶段的10倍以上（例如OPT-30B模型需633GB）。零阶（ZO）优化器通过仅依赖前向传播估计梯度来规避这一成本，但现有方法如MeZO通常需要更多步骤才能收敛。零阶优化中速度与内存的权衡能否被根本性改善？归一化SGD在保持比Adam更高内存效率的同时展现出强劲的实证性能。基于此，我们提出FZOO——一种达到Adam级速度的快速零阶优化器。FZOO通过采用基于批次损失标准差的自适应步长批量单边估计，显著减少收敛所需的前向传播次数，并利用Rademacher随机向量扰动结合CUDA并行计算加速单批次处理。在包括RoBERTa-large、OPT（350M-66B）、Phi-2和Llama3等多样化模型上的11项任务实验验证了FZOO的有效性。平均而言，FZOO在准确率上比MeZO提升3%，同时减少3倍前向传播次数。对于RoBERTa-large模型，FZOO相较MeZO实现了5.6%的平均准确率提升和18倍前向传播次数缩减，达到与Adam相当的收敛速度。我们还提供了理论分析，证明FZOO与归一化SGD更新规则的形式等价性及其收敛保证。FZOO可无缝集成至参数高效微调（PEFT）技术，实现更大内存节省。总体而言，我们的研究成果使得单GPU高速全参数微调成为可能，并为内存高效预训练的未来研究指明了方向。

---

## [EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations](https://arxiv.org/abs/2502.14760)

### Abstract
arXiv:2502.14760v2 Announce Type: replace 
Abstract: A fundamental problem in combinatorial optimization is identifying equivalent formulations. Despite the growing need for automated equivalence checks -- driven, for example, by optimization copilots, which generate problem formulations from natural language descriptions -- current approaches rely on simple heuristics that fail to reliably check formulation equivalence. Inspired by Karp reductions, in this work we introduce Quasi-Karp equivalence, a formal criterion for determining when two optimization formulations are equivalent based on the existence of a mapping between their decision variables. We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings for scalable, reliable equivalence checking, with a verification stage that ensures mapped solutions preserve feasibility and optimality without additional solver calls. To evaluate our approach, we construct EquivaFormulation, the first open-source dataset of equivalent optimization formulations, generated by applying transformations such as adding slack variables or valid inequalities to existing formulations. Empirically, EquivaMap significantly outperforms existing methods, achieving substantial improvements in correctly identifying formulation equivalence.

### 摘要
组合优化中的一个基础性问题是识别等价表述形式。尽管自动化等价性检查的需求日益增长（例如由优化协导器驱动，这类工具能够根据自然语言描述生成问题表述），但现有方法仍依赖于简单的启发式规则，无法可靠地验证表述等价性。受卡普归约启发，本研究提出准卡普等价性——一种基于决策变量间映射关系存在的形式化判定准则，用于确定两种优化表述的等价性。我们开发了EquivaMap框架，该框架利用大语言模型自动发现此类映射，实现可扩展且可靠的等价性检查，并通过验证阶段确保映射解保持可行性与最优性，而无需额外调用求解器。为评估该方法，我们构建了首个开源等价优化表述数据集EquivaFormulation，该数据集通过对现有表述施加添加松弛变量或有效不等式等变换生成。实验表明，EquivaMap显著优于现有方法，在正确识别表述等价性方面实现重大提升。

---

## [Textual Unlearning Gives a False Sense of Unlearning](https://arxiv.org/abs/2406.13348)

### Abstract
arXiv:2406.13348v3 Announce Type: replace-cross 
Abstract: Language Models (LMs) are prone to ''memorizing'' training data, including substantial sensitive user information. To mitigate privacy risks and safeguard the right to be forgotten, machine unlearning has emerged as a promising approach for enabling LMs to efficiently ''forget'' specific texts. However, despite the good intentions, is textual unlearning really as effective and reliable as expected? To address the concern, we first propose Unlearning Likelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing method, and find that unlearned texts can still be detected with very high confidence after unlearning. Further, we conduct an in-depth investigation on the privacy risks of textual unlearning mechanisms in deployment and present the Textual Unlearning Leakage Attack (TULA), along with its variants in both black- and white-box scenarios. We show that textual unlearning mechanisms could instead reveal more about the unlearned texts, exposing them to significant membership inference and data reconstruction risks. Our findings highlight that existing textual unlearning actually gives a false sense of unlearning, underscoring the need for more robust and secure unlearning mechanisms.

### 摘要
语言模型（LMs）容易"记忆"训练数据，其中包含大量敏感用户信息。为降低隐私风险并保障被遗忘权，机器遗忘已成为使LMs高效"遗忘"特定文本的有效方法。然而，尽管初衷良好，文本遗忘机制是否真如预期般可靠有效？针对该问题，我们首先提出严格文本遗忘审计方法U-LiRA+（遗忘似然比攻击增强版），发现被遗忘文本在遗忘后仍能以极高置信度被检测到。进一步地，我们对部署中文本遗忘机制的隐私风险展开深入研究，提出文本遗忘泄露攻击（TULA）及其黑白盒场景变体。研究表明，文本遗忘机制反而会暴露更多关于被遗忘文本的信息，使其面临严重的成员推断与数据重建风险。我们的发现揭示：现有文本遗忘机制实际营造了虚假的遗忘安全感，这凸显了对更鲁棒安全遗忘机制的迫切需求。

---

## [Query Rewriting via LLMs](https://arxiv.org/abs/2502.12918)

### Abstract
arXiv:2502.12918v3 Announce Type: replace 
Abstract: When complex SQL queries suffer slow executions despite query optimization, DBAs typically invoke automated query rewriting tools to recommend ``lean'' equivalents that are conducive to faster execution. The rewritings are usually achieved via transformation rules, but these rules are limited in scope and difficult to update in a production system. Recently, LLM-based techniques have also been suggested, but they are prone to semantic and syntactic errors.
  We investigate here how the remarkable cognitive capabilities of LLMs can be leveraged for performant query rewriting while incorporating safeguards and optimizations to ensure correctness and efficiency. Our study shows that these goals can be progressively achieved through incorporation of (a) an ensemble suite of basic prompts, (b) database-sensitive prompts via redundancy removal and selectivity-based rewriting rules, and (c) LLM token probability-guided rewrite paths. Further, a suite of logic-based and statistical tools can be used to check for semantic violations in the rewrites prior to DBA consideration.
  We have implemented the above LLM-infused techniques in the LITHE system, and evaluated complex analytic queries from standard benchmarks on contemporary database platforms. The results show significant performance improvements for slow queries, with regard to both abstract costing and actual execution, over both SOTA techniques and the native query optimizer. For instance, with TPC-DS on PostgreSQL, the geometric mean of the runtime speedups for slow queries was as high as 13.2 over the native optimizer, whereas SOTA delivered 4.9 in comparison.
  Overall, LITHE is a promising step toward viable LLM-based advisory tools for ameliorating enterprise query performance.

### 摘要
当复杂SQL查询在优化后仍执行缓慢时，数据库管理员通常调用自动化查询重写工具来推荐利于快速执行的"精简"等效查询。这类重写通常通过转换规则实现，但这些规则适用范围有限且难以在生产系统中更新。近期也有基于大语言模型的技术被提出，但它们容易产生语义和句法错误。

本文研究如何利用大语言模型的卓越认知能力进行高性能查询重写，同时引入保障措施和优化策略以确保正确性和效率。研究表明，通过整合以下要素可逐步实现这些目标：(a)基础提示的集成套件，(b)通过冗余消除和基于选择性的重写规则实现的数据库敏感提示，以及(c)大语言模型词元概率引导的重写路径。此外，可采用基于逻辑和统计的工具套件在重写提交给数据库管理员前检查语义违规。

我们已在LITHE系统中实现了上述融合大语言模型的技术，并在现代数据库平台上评估了标准基准测试中的复杂分析查询。结果表明，相较于现有最优技术和原生查询优化器，该系统对慢查询在抽象成本估算和实际执行方面均实现了显著性能提升。例如在PostgreSQL上运行TPC-DS时，慢查询运行时间的几何平均加速比达到原生优化器的13.2倍，而现有最优技术仅为4.9倍。

总体而言，LITHE为开发可行的大语言模型咨询工具以改善企业查询性能迈出了重要一步。

---

## [BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing](https://arxiv.org/abs/2505.01343)

### Abstract
arXiv:2505.01343v2 Announce Type: replace 
Abstract: Large multi-modal models inevitably decay over time as facts update and previously learned information becomes outdated. Traditional approaches such as fine-tuning are often impractical for updating these models due to their size and complexity. Instead, direct knowledge editing within the models presents a more viable solution. Current model editing techniques, however, typically overlook the unique influence ranges of different facts, leading to compromised model performance in terms of both generality and locality. To address this issue, we introduce the concept of the generality-locality trade-off in multi-modal model editing. We develop a new model editing dataset named OKEDIT, specifically designed to effectively evaluate this trade-off. Building on this foundation, we propose \textbf&#123;BalancEdit&#125;, a novel method for balanced model editing that dynamically achieves an optimal balance between generality and locality. BalancEdit utilizes a unique mechanism that generates both positive and negative samples for each fact to accurately determine its influence scope and incorporates these insights into the model's latent space using a discrete, localized codebook of edits, without modifying the underlying model weights. To our knowledge, this is the first approach explicitly addressing the generality-locality trade-off in multi-modal model editing. Our comprehensive results confirm the effectiveness of BalancEdit, demonstrating minimal trade-offs while maintaining robust editing capabilities. Our code and dataset are available at https://github.com/donglgcn/BalancEdit/tree/MMOKVQA.

### 摘要
大型多模态模型会随着事实更新和先前学习信息过时而不可避免地出现性能衰退。由于模型规模和复杂性，传统的微调等方法通常难以用于更新这些模型。相比之下，直接在模型内部进行知识编辑提供了更可行的解决方案。然而，当前模型编辑技术普遍忽视了不同事实的独特影响范围，导致模型在通用性和局部性方面的性能受损。为解决这一问题，我们提出了多模态模型编辑中通用性与局部性的权衡概念，并开发了名为OKEDIT的新型模型编辑数据集，专门用于有效评估这一权衡关系。在此基础上，我们提出BalancEdit——一种平衡的模型编辑方法，能动态实现通用性与局部性的最优平衡。该方法通过为每个事实生成正负样本的独特机制精确确定其影响范围，并将这些信息融入模型的潜在空间，使用离散化、局部化的编辑码本而不修改底层模型权重。据我们所知，这是首个明确解决多模态模型编辑中通用性-局部性权衡问题的方法。综合实验结果表明，BalancEdit在保持强大编辑能力的同时实现了最小的性能权衡。代码和数据集详见：https://github.com/donglgcn/BalancEdit/tree/MMOKVQA。

---

## [AI as Decision-Maker: Ethics and Risk Preferences of LLMs](https://arxiv.org/abs/2406.01168)

### Abstract
arXiv:2406.01168v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit surprisingly diverse risk preferences when acting as AI decision makers, a crucial characteristic whose origins remain poorly understood despite their expanding economic roles. We analyze 50 LLMs using behavioral tasks, finding stable but diverse risk profiles. Alignment tuning for harmlessness, helpfulness, and honesty significantly increases risk aversion, causally increasing risk aversion confirmed via comparative difference analysis: a ten percent ethics increase cuts risk appetite two to eight percent. This induced caution persists against prompts and affects economic forecasts. Alignment enhances safety but may also suppress valuable risk taking, revealing a tradeoff risking suboptimal economic outcomes. With AI models becoming more powerful and influential in economic decisions while alignment grows increasingly critical, our empirical framework serves as an adaptable and enduring benchmark to track risk preferences and monitor this crucial tension between ethical alignment and economically valuable risk-taking.

### 摘要
大型语言模型（LLMs）作为人工智能决策者时表现出惊人的多样化风险偏好，这一关键特性虽在经济角色中日益重要，但其成因仍鲜为人知。我们通过行为任务分析了50个LLMs，发现其具有稳定但差异化的风险特征。针对无害性、助益性和诚实性的对齐调优会显著增强风险规避倾向，比较差异分析证实了这一因果关系：伦理标准每提升10%，风险偏好会降低2%至8%。这种诱导的谨慎态度不受提示词影响，并会作用于经济预测。对齐虽能提升安全性，但也可能抑制有价值的风险承担行为，揭示出伦理对齐与经济收益性风险承担之间的权衡困境。随着AI模型在经济决策中的影响力增强且对齐要求日益关键，我们的实证框架可作为适应性长效基准，用于追踪风险偏好并监测伦理对齐与经济性风险承担之间的重要张力。

---

## [High-Throughput Phenotyping of Clinical Text Using Large Language Models](https://arxiv.org/abs/2408.01214)

### Abstract
arXiv:2408.01214v2 Announce Type: replace-cross 
Abstract: High-throughput phenotyping automates the mapping of patient signs to standardized ontology concepts and is essential for precision medicine. This study evaluates the automation of phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using large language models. Due to their rich phenotype data, these summaries can be surrogates for physician notes. We conduct a performance comparison of GPT-4 and GPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to inter-rater agreement. Despite some limitations in sign normalization, the extensive pre-training of GPT-4 results in high performance and generalizability across several phenotyping tasks while obviating the need for manually annotated training data. Large language models are expected to be the dominant method for automating high-throughput phenotyping of clinical text.

### 摘要
高通量表型分析通过将患者体征自动映射至标准化本体概念，对精准医疗至关重要。本研究评估了利用大语言模型对在线《人类孟德尔遗传》(OMIM)数据库临床摘要进行表型分析的自动化效果。由于这些摘要包含丰富的表型数据，可作为医师记录的替代样本。我们对GPT-4与GPT-3.5-Turbo进行了性能比较，结果表明：GPT-4在体征识别、分类和标准化方面优于GPT-3.5-Turbo，其与人工标注者的一致性达到评估者间一致性的可比水平。尽管在体征标准化方面存在局限，但GPT-4通过大规模预训练实现了高性能，并在多项表型分析任务中展现出良好泛化能力，同时无需人工标注训练数据。大语言模型预计将成为临床文本高通量表型分析自动化的主导方法。

---

## [Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop](https://arxiv.org/abs/2405.17998)

### Abstract
arXiv:2405.17998v2 Announce Type: replace-cross 
Abstract: Recommender systems are essential for information access, allowing users to present their content for recommendation. With the rise of large language models (LLMs), AI-generated content (AIGC), primarily in the form of text, has become a central part of the content ecosystem. As AIGC becomes increasingly prevalent, it is important to understand how it affects the performance and dynamics of recommender systems. To this end, we construct an environment that incorporates AIGC to explore its short-term impact. The results from popular sequential recommendation models reveal that AIGC are ranked higher in the recommender system, reflecting the phenomenon of source bias. To further explore the long-term impact of AIGC, we introduce a feedback loop with realistic simulators. The results show that the model's preference for AIGC increases as the user clicks on AIGC rises and the model trains on simulated click data. This leads to two issues: In the short term, bias toward AIGC encourages LLM-based content creation, increasing AIGC content, and causing unfair traffic distribution. From a long-term perspective, our experiments also show that when AIGC dominates the content ecosystem after a feedback loop, it can lead to a decline in recommendation performance. To address these issues, we propose a debiasing method based on L1-loss optimization to maintain long-term content ecosystem balance. In a real-world environment with AIGC generated by mainstream LLMs, our method ensures a balance between AIGC and human-generated content in the ecosystem. The code and dataset are available at https://github.com/Yuqi-Zhou/Rec_SourceBias.

### 摘要
推荐系统作为信息获取的重要工具，使用户能够展示其内容以获得推荐。随着大语言模型（LLMs）的兴起，以文本形式为主的人工智能生成内容（AIGC）已成为内容生态系统的核心组成部分。鉴于AIGC日益普及，理解其如何影响推荐系统的性能与动态机制至关重要。为此，我们构建了一个包含AIGC的实验环境以探究其短期影响。基于主流序列推荐模型的实验结果表明，AIGC在推荐系统中排名更高，这反映了来源偏差现象。为深入探究AIGC的长期影响，我们引入具有现实模拟能力的反馈循环机制。结果显示，随着用户对AIGC点击量的增加以及模型基于模拟点击数据的训练，系统对AIGC的偏好持续增强。这引发两个问题：短期而言，对AIGC的偏差会激励基于LLM的内容创作，增加AIGC内容占比并导致流量分配不公；长期来看，实验表明当AIGC通过反馈循环主导内容生态系统后，将导致推荐性能下降。针对这些问题，我们提出基于L1损失优化的去偏方法以维持长期内容生态平衡。在采用主流LLMs生成AIGC的真实场景中，该方法能有效保持AIGC与人类创作内容在生态系统中的平衡。

---

## [xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/abs/2410.16267)

### Abstract
arXiv:2410.16267v2 Announce Type: replace-cross 
Abstract: We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html

### 摘要
我们提出xGen-MM-Vid（BLIP-3-Video）：一种面向视频的多模态语言模型，专门设计用于高效捕捉多帧时序信息。该模型在传统视觉标记器基础上创新性地引入"时序编码器"，可将多帧标记序列映射为紧凑的视觉标记集合。这使得BLIP-3-Video所需视觉标记数量远少于同类模型（如32 vs. 4608个标记）。我们探索了多种时序编码器架构，包括可学习的时空池化模块及Token Turing Machines等序列模型。实验证实，BLIP-3-Video在视频问答任务中能达到与更大规模前沿模型（如340亿参数）相当的准确率，而模型尺寸更小（仅40亿参数）且通过减少视觉标记使用显著提升效率。项目网站详见https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html

---

## [From Language Models over Tokens to Language Models over Characters](https://arxiv.org/abs/2412.03719)

### Abstract
arXiv:2412.03719v2 Announce Type: replace-cross 
Abstract: Modern language models are internally -- and mathematically -- distributions over $\it&#123;token&#125;$ strings rather than $\it&#123;character&#125;$ strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model. Thus, the tokenizer and consequent processing are very sensitive to the specification of the prompt (e.g., whether the prompt ends with a space or not). This paper presents algorithms for converting token-level language models to character-level ones. We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality. Across four publicly available language models, we find that -- even with a small computation budget -- our method is able to accurately approximate the character-level distribution at reasonably fast speeds, and that a significant improvement in the language model's compression rate (bits/byte) is achieved.

### 摘要
现代语言模型在内部及数学本质上是对标记字符串而非字符字符串的概率分布，这为在其基础上构建用户应用程序的程序员带来了诸多挑战。例如，若提示词以字符串形式指定，则须先进行标记化处理才能输入标记级语言模型。因此，分词器及后续处理对提示词的规范极为敏感（如是否以空格结尾）。本文提出了将标记级语言模型转换为字符级模型的算法，包括精确算法与近似算法。在实证部分，我们测试了实际运行时间与近似质量。通过对四个公开语言模型的测试发现：即使在有限计算资源下，本方法仍能以较快速度精确逼近字符级分布，并显著提升语言模型的压缩率（比特/字节）。

---

## [JuStRank: Benchmarking LLM Judges for System Ranking](https://arxiv.org/abs/2412.09569)

### Abstract
arXiv:2412.09569v2 Announce Type: replace-cross 
Abstract: Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.

### 摘要
鉴于生成式AI的快速发展，亟需对现有众多模型及配置进行系统化比较与选择。此类评估的规模与多样性使得基于大语言模型（LLM）的评判器成为解决该挑战的理想方案。关键在于，该方法需首先验证LLM评判器自身的质量。既往研究主要关注LLM评判器的实例级评估，即通过一组响应或响应配对来评判模型，而不考虑其来源系统。我们认为这种设置忽视了影响系统级排序的关键因素，例如评判器对特定系统的正向或负向偏好。为填补这一空白，我们首次开展LLM评判器作为系统排序器的大规模研究：通过聚合多个系统输出的评判分数生成系统得分，并通过将所得系统排序与人工排序对比来评估评判器质量。除整体评判器评估外，我们的分析还提供了评判器行为的细粒度特征描述，包括其决断力与偏好倾向。

---

## [The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games](https://arxiv.org/abs/2411.15129)

### Abstract
arXiv:2411.15129v2 Announce Type: replace-cross 
Abstract: What can we learn about language from studying how it is used by ChatGPT and other large language model (LLM)-based chatbots? In this paper, we analyse the distinctive character of language generated by ChatGPT, in relation to questions raised by natural language processing pioneer, and student of Wittgenstein, Margaret Masterman. Following frequent complaints that LLM-based chatbots produce "slop," or even "bullshit," in the sense of Frankfurt's popular monograph On Bullshit, we conduct an empirical study to contrast the language of 1,000 scientific publications with typical text generated by ChatGPT. We then explore whether the same language features can be detected in two well-known contexts of social dysfunction: George Orwell's critique of political speech, and David Graeber's characterisation of bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a statistical model of sloppy bullshit can reliably relate the Frankfurtian artificial bullshit of ChatGPT to the political and workplace functions of bullshit as observed in natural human language.

### 摘要
通过研究ChatGPT及其他基于大语言模型（LLM）的聊天机器人如何使用语言，我们能从中获得哪些关于语言的认知？本文分析了ChatGPT生成语言的独特性，并关联自然语言处理先驱、维特根斯坦学者玛格丽特·马斯特曼提出的问题。针对频繁出现的关于LLM聊天机器人产生"敷衍废话"（slop）甚至"胡言乱语"（bullshit，源自法兰克福的流行专著《论胡扯》）的批评，我们开展了一项实证研究，对比了1000篇科学出版物与ChatGPT典型生成文本的语言特征。进而，我们探究了这些语言特征是否能在两种著名的社会功能障碍情境中被检测到：乔治·奥威尔对政治话语的批判，以及大卫·格雷伯对"狗屁工作"（bullshit jobs）的论述。通过简单的假设检验方法，我们证明了一个关于敷衍性胡扯的统计模型能够可靠地将ChatGPT生成的法兰克福式人工胡扯，与自然人类语言中观察到的政治及职场场景下的胡扯功能联系起来。

---

## [RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation](https://arxiv.org/abs/2501.08617)

### Abstract
arXiv:2501.08617v3 Announce Type: replace-cross 
Abstract: While Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning generative AI, we present empirical evidence that it can also cause severe, systematic misalignment. We hypothesize that this stems from evaluator feedback depending on downstream outcome predictions (foresight) that can be influenced by the AI's output, inducing Goodhart's law dynamics. We present a theoretical analysis showing that conditioning evaluator feedback on downstream observations (hindsight) inhibits this effect by decoupling the alignment signal from potentially compromised predictions--crucially, the result holds even if the observed outcomes are sampled from the AI's own world model. Building on this insight, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which presents plausible simulated outcomes to evaluators before eliciting feedback. We validate RLHS across three consultancy settings--marketplace interactions, restaurant recommendations, and online course advising--using both online (PPO) and offline (DPO) fine-tuning methods, and show that it substantially improves alignment over RLHF in experiments and human evaluations. We perform post-hoc benchmark evaluations on TruthfulQA, HaluEval, and TrustLLM, finding that even after single-task fine-tuning, RLHF misalignment persists, whereas RLHS consistently outperforms baselines and demonstrates robust alignment generalization. The project webpage and code are available at https://rl-hindsight.github.io.

### 摘要
尽管基于人类反馈的强化学习（RLHF）在生成式人工智能对齐方面展现出潜力，但我们提供的实证证据表明，该方法也可能导致严重的系统性失准现象。我们假设其根源在于评估者反馈依赖于可能受AI输出影响的下游结果预测（前瞻），从而诱发古德哈特定律的动态效应。通过理论分析表明，将评估者反馈建立在下游观测结果（后验）基础上可抑制这种效应——关键在于，即使观测结果是从AI自身世界模型中采样，该结论依然成立。基于这一发现，我们提出后验模拟强化学习（RLHS），该方法在获取反馈前向评估者呈现可信的模拟结果。我们在三种咨询场景（市场交易、餐厅推荐和在线课程指导）中验证RLHS，采用在线（PPO）和离线（DPO）两种微调方法，实验和人类评估均显示其对齐效果显著优于RLHF。对TruthfulQA、HaluEval和TrustLLM的后续基准测试表明，即使经过单任务微调，RLHF的失准问题仍然存在，而RLHS始终优于基线方法并展现出强大的对齐泛化能力。项目网页及代码详见https://rl-hindsight.github.io。

---

## [DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models](https://arxiv.org/abs/2411.03250)

### Abstract
arXiv:2411.03250v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code, and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2%-7% in certain cases. Data and code are available at https://github.com/bytedance/DiffLM.

### 摘要
近年来，大型语言模型（LLM）在知识和生成能力上的显著提升，引发了利用LLM进行高质量数据合成的广泛关注。然而，由于LLM对目标数据分布的理解有限以及提示工程的复杂性（尤其是结构化格式数据），通过提示LLM生成合成数据仍面临挑战。为解决这些问题，我们提出DiffLM——一个基于变分自编码器（VAE）的可控数据合成框架，该框架进一步：（1）利用扩散模型在学习的潜在分布中保留更多原始分布和格式结构信息；（2）通过即插即用的潜在特征注入模块，将目标分布知识的学习与LLM生成目标解耦。由于我们观察到VAE潜在表示与真实数据分布之间存在显著差异，框架中引入了潜在扩散模块以学习完全表达性的潜在分布。在七种结构化格式数据（即表格数据、代码数据和工具数据）的真实数据集上的评估表明，DiffLM能生成高质量数据，在某些情况下其下游任务性能比真实数据高出2%-7%。数据和代码详见https://github.com/bytedance/DiffLM。

---

## [LLM Alignment as Retriever Optimization: An Information Retrieval Perspective](https://arxiv.org/abs/2502.03699)

### Abstract
arXiv:2502.03699v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative. In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.

### 摘要
大型语言模型（LLMs）通过其在推理、编码和沟通方面的能力彻底改变了人工智能领域，推动着各行各业的创新。其真正潜力取决于有效的对齐机制，以确保正确、可信且符合伦理的行为，从而应对错误信息、幻觉、偏见和滥用等挑战。尽管现有的基于强化学习（RL）的对齐方法以复杂著称，直接优化方法提供了一种更简洁的替代方案。本研究借鉴成熟的信息检索（IR）原理，提出了一种新颖的直接优化方法用于LLM对齐。我们构建了一个系统化框架，将LLM生成与奖励模型映射至IR的检索器-重排序范式，从而 bridging LLM对齐与IR方法论。基于此，我们提出了检索器偏好优化对齐法（LarPO），这种新方法能显著提升整体对齐质量。大量实验验证了LarPO的有效性，在AlpacaEval2和MixEval-Hard数据集上分别实现了38.9%和13.7%的平均性能提升。本研究通过融合IR理论基础为LLM对齐开辟了新途径，为未来研究提供了具有前景的方向。

---

## [In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs](https://arxiv.org/abs/2502.04390)

### Abstract
arXiv:2502.04390v2 Announce Type: replace-cross 
Abstract: Through systematic empirical investigation, we uncover a fundamental and concerning property of Large Language Models: while they can safely learn facts that don't contradict their knowledge, attempting to update facts with contradictory information triggers catastrophic corruption of unrelated knowledge. Unlike humans, who naturally resist contradictory information, these models indiscriminately accept contradictions, leading to devastating interference, destroying up to 80% of unrelated knowledge even when learning as few as 10-100 contradicting facts. To understand whether this interference could be mitigated through selective plasticity, we experiment with targeted network updates, distinguishing between previously used (stubborn) and rarely used (plastic) neurons. We uncover another asymmetry: while sparing frequently-used neurons significantly improves retention of existing knowledge for non-contradictory updates (98% vs 93% with standard updates), contradictory updates trigger catastrophic interference regardless of targeting strategy. This effect which persists across tested model scales (GPT-2 to GPT-J-6B), suggests a fundamental limitation in how neural networks handle contradictions. Finally, we demonstrate that contradictory information can be reliably detected (95%+ accuracy) using simple model features, offering a potential protective mechanism. These findings motivate new architectures that can, like humans, naturally resist contradictions rather than allowing destructive overwrites.

### 摘要
通过系统性实证研究，我们发现大型语言模型存在一个根本性问题：虽然它们能安全学习与既有知识不冲突的事实，但试图用矛盾信息更新知识时，会引发对无关知识的灾难性破坏。与人类天然抗拒矛盾信息不同，这些模型会不加区分地接受矛盾，导致破坏性干扰——即使仅学习10-100个矛盾事实，也会损毁高达80%的无关知识。为探究能否通过选择性可塑性缓解这种干扰，我们尝试针对性更新网络参数，区分高频使用（顽固）和低频使用（可塑）神经元。研究发现另一不对称性：对于非矛盾更新，保留高频神经元能显著提升既有知识保持率（98% vs 标准更新的93%）；但矛盾更新无论采用何种定位策略都会引发灾难性干扰。该现象在测试模型规模（GPT-2至GPT-J-6B）中普遍存在，表明神经网络处理矛盾存在根本性局限。最后我们证明，利用简单模型特征可可靠检测矛盾信息（准确率95%+），这为构建保护机制提供了可能。这些发现启示我们需要开发能像人类一样天然抗拒矛盾、而非允许破坏性覆盖的新架构。

---

## [Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies](https://arxiv.org/abs/2502.05202)

### Abstract
arXiv:2502.05202v2 Announce Type: replace-cross 
Abstract: Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms demonstrate significant speedups of up to 2.8x over standard autoregressive decoding. By enabling any off-the-shelf model to serve as a drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.

### 摘要
加速大语言模型（LLM）的推理是生成式人工智能面临的关键挑战。推测解码（SD）方法通过单次目标前向传递生成多个令牌，实现了显著的效率提升。然而，现有SD方法要求草稿模型与目标模型共享相同词汇表，这限制了潜在草稿模型的选择范围，通常需要从头训练草稿模型。我们提出了三种新的SD方法，消除了这种共享词汇表的限制。这三种方法均能保持目标分布（即无损），且可直接应用于现成模型，无需额外训练或修改。实证研究表明，在摘要生成、编程和长上下文任务中，我们的算法相比标准自回归解码实现了最高达2.8倍的显著加速。通过允许任何现成模型作为草稿模型且无需重新训练，本工作极大拓展了SD框架在实际应用中的适用范围。

---

## [Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense](https://arxiv.org/abs/2502.00840)

### Abstract
arXiv:2502.00840v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear. In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven state-of-the-art techniques across three popular categories (activation polynomialization, activation sparsification, and activation quantization), revealing consistent safety degradation across ten safety-aligned LLMs. To overcome the hurdle of devising a unified defense accounting for diverse activation approximation methods, we perform an in-depth analysis of their shared error patterns and uncover three key findings. We propose QuadA, a novel safety enhancement method tailored to mitigate the safety compromises introduced by activation approximations. Extensive experiments and ablation studies corroborate QuadA's effectiveness in enhancing the safety capabilities of LLMs after activation approximations.

### 摘要
大型语言模型（LLMs）已在多个领域展现出卓越能力。随着LLMs功能演进与部署场景的扩展，其部署挑战因模型规模庞大及Llama、Gemma、Mistral等知名模型系列采用的先进却复杂的激活设计而加剧，这在资源受限的部署场景中尤为突出——缓解推理瓶颈已成为当务之急。在近期众多研究中，激活近似技术作为提升推理效率的有效途径崭露头角，在私有推理等应用场景中甚至被视为不可或缺。尽管该技术能以极小的效用代价实现显著加速，且在实际部署中表现出良好的可行性，但其安全性影响仍不明确。本研究通过首次系统性地评估激活近似技术填补了LLM安全领域的这一关键空白。我们对三类主流方法（激活多项式化、激活稀疏化、激活量化）中的七种前沿技术进行安全审查，发现十种安全对齐LLM均存在持续性安全性能下降。为克服针对不同激活近似方法设计统一防御方案的难题，我们深入分析其共有误差模式并揭示三项关键发现，进而提出专用于缓解激活近似安全风险的创新方法QuadA。大量实验与消融研究证实，QuadA能有效提升激活近似后LLM的安全性能。

---

## [TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation](https://arxiv.org/abs/2502.07306)

### Abstract
arXiv:2502.07306v2 Announce Type: replace-cross 
Abstract: In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps on the complex R2R-Habitat instruction dataset and quantify in detail the effect of visual grounding on navigation performance.

### 摘要
在本研究中，我们提出了一种模块化的视觉语言导航（VLN）任务解决方案，通过将该问题分解为四个子模块，在零样本设置下利用最先进的大型语言模型（LLM）和视觉语言模型（VLM）。给定自然语言导航指令，我们首先提示LLM提取地标及其访问顺序。在已知环境模型的前提下，我们检索最后一个地标的top-k候选位置，并利用环境拓扑图的最短路径算法，从起始位置到最后一个地标生成k条路径假设。每条路径假设由全景图像序列表示。随后，我们采用动态规划算法计算全景序列与地标名称序列的对齐分数，该分数与VLM获得的匹配分数一致。最终，我们通过计算具有最高对齐分数的假设路径与真实路径之间的nDTW指标来评估路径保真度。在复杂的R2R-Habitat指令数据集上，我们的方法展现出优于VLMaps等联合语义地图方案的性能，并详细量化了视觉 grounding 对导航性能的影响。

---

## [Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews](https://arxiv.org/abs/2502.15226)

### Abstract
arXiv:2502.15226v2 Announce Type: replace-cross 
Abstract: Which large language model (LLM) is better? Every evaluation tells a story, but what do users really think about current LLMs? This paper presents CLUE, an LLM-powered interviewer that conducts in-the-moment user experience interviews, right after users interact with LLMs, and automatically gathers insights about user opinions from massive interview logs. We conduct a study with thousands of users to understand user opinions on mainstream LLMs, recruiting users to first chat with a target LLM and then be interviewed by CLUE. Our experiments demonstrate that CLUE captures interesting user opinions, e.g., the bipolar views on the displayed reasoning process of DeepSeek-R1 and demands for information freshness and multi-modality. Our code and data are at https://github.com/cxcscmu/LLM-Interviewer.

### 摘要
哪种大语言模型（LLM）更优秀？每项评估都讲述了一个故事，但用户对当前LLM的真实看法是什么？本文提出CLUE，一个由LLM驱动的访谈系统，能够在用户与LLM交互后即时进行用户体验访谈，并自动从海量访谈日志中提取用户意见。我们通过数千名用户的研究来理解用户对主流LLM的评价，招募用户先与目标LLM对话，随后接受CLUE的访谈。实验表明，CLUE能够捕捉到有趣的用户观点，例如对DeepSeek-R1展示推理过程的两极评价，以及对信息时效性和多模态功能的需求。代码与数据详见https://github.com/cxcscmu/LLM-Interviewer。

---

## [Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment](https://arxiv.org/abs/2503.09081)

### Abstract
arXiv:2503.09081v2 Announce Type: replace-cross 
Abstract: While multi-modal learning has advanced significantly, current approaches often create inconsistencies in representation and reasoning of different modalities. We propose UMaT, a theoretically-grounded framework that unifies visual and auditory inputs as structured text for large language models, addressing semantic alignment, temporal synchronization, and efficient sparse information retrieval. It significantly improves state-of-the-art Long Video Question Answering accuracy (up to 13.7%, and 16.9% on long videos) via redundancy minimization and structured textual representation for unified multi-modal reasoning

### 摘要
尽管多模态学习已取得显著进展，但现有方法常导致不同模态在表征与推理上的不一致。我们提出理论基础的UMaT框架，将视觉与听觉输入统一为大型语言模型的结构化文本，解决语义对齐、时序同步及高效稀疏信息检索问题。通过冗余最小化和结构化文本表征实现统一多模态推理，该框架显著提升了长视频问答任务的最优准确率（最高提升13.7%，长视频场景达16.9%）。

---

## [Self-Training Elicits Concise Reasoning in Large Language Models](https://arxiv.org/abs/2502.20122)

### Abstract
arXiv:2502.20122v3 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at https://github.com/TergelMunkhbat/concise-reasoning

### 摘要
思维链（CoT）推理使大型语言模型（LLMs）能够通过中间标记的额外计算来解决复杂任务。然而，我们认为典型的推理轨迹包含许多冗余标记，导致额外的推理成本。通过考察当前LLMs的输出分布，我们发现了它们相对于默认行为具备更简洁推理的潜在能力。为激发这种能力，我们提出了简单的微调方法，在特定任务设置中利用通过最佳N采样和少样本条件获取的自生成简洁推理路径。我们的组合方法在GSM8K和MATH五个模型系列上平均减少了30%的输出标记，同时保持平均准确率。通过利用LLMs的基本随机性和上下文学习能力，我们的自训练方法在包括经过广泛训练后模型在内的广泛模型上稳健地实现了简洁推理。

---

## [Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?](https://arxiv.org/abs/2504.06006)

### Abstract
arXiv:2504.06006v3 Announce Type: replace-cross 
Abstract: Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of leveraging large language models (LLMs) for hyperparameter optimization by fine-tuning a parameter-efficient version of Code Llama using LoRA. The adapted LLM is capable of generating accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional approaches such as Optuna, which rely on computationally intensive trial-and-error procedures, our method achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our findings demonstrate that LLM-based optimization not only matches the performance of state-of-the-art techniques like Tree-structured Parzen Estimators (TPE) but also substantially accelerates the tuning process. This positions LLMs as a promising alternative for rapid experimentation, particularly in resource-constrained environments such as edge devices and mobile platforms, where computational efficiency is essential. In addition to improved efficiency, the method offers time savings and consistent performance across various tasks, highlighting its robustness and generalizability. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research.

### 摘要
最优超参数选择对于最大化神经网络性能至关重要，尤其在模型复杂度日益增长的背景下。本研究探讨了利用大型语言模型（LLM）进行超参数优化的可行性，通过LoRA技术对参数高效的Code Llama进行微调。改进后的LLM能够针对不同神经网络架构生成精准高效的超参数推荐方案。与传统方法（如Optuna）依赖计算密集的试错过程不同，我们的方法在均方根误差（RMSE）指标上达到相当或更优结果的同时，显著降低了计算开销。实验表明，基于LLM的优化不仅匹配了树结构Parzen估计器（TPE）等先进技术的性能，还大幅加速了调参过程。这使LLM成为快速实验的有力替代方案，特别适用于边缘设备和移动平台等计算资源受限的场景。除效率提升外，该方法还具有跨任务的时间节省和稳定性能优势，体现了其鲁棒性和泛化能力。所有生成超参数均收录于公开可用的LEMUR神经网络数据集，该数据集可作为超参数优化研究的开源基准。

---

## [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)

### Abstract
arXiv:2505.03452v2 Announce Type: replace-cross 
Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a given use case can be complex and expensive. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To address this gap, we present a comprehensive study involving 5 HPO algorithms over 5 datasets from diverse domains, including a new one collected for this work on real-world product documentation. Our study explores the largest HPO search space considered to date, with three evaluation metrics as optimization targets. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing model selection first is preferable to the prevalent practice of optimizing according to RAG pipeline order.

### 摘要
寻找适用于特定用例的最佳检索增强生成（RAG）配置可能既复杂又成本高昂。针对这一挑战，近期出现了RAG超参数优化（HPO）框架，但其有效性尚未经过严格基准测试。为填补这一空白，我们开展了一项综合性研究，在涵盖多个领域的5个数据集（包括为本研究新收集的真实世界产品文档数据集）上对比了5种HPO算法。本研究探索了迄今为止最大规模的HPO搜索空间，并以三项评估指标作为优化目标。结果分析表明：RAG HPO可通过贪婪算法或随机搜索高效完成，且能显著提升所有数据集的RAG性能；对于贪婪HPO方法，我们发现优先优化模型选择的策略优于当前按RAG流程顺序优化的主流实践。

---

## [Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items](https://arxiv.org/abs/2505.01015)

### Abstract
arXiv:2505.01015v2 Announce Type: replace-cross 
Abstract: The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs' value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects' actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 44 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data.

### 摘要
随着对更真实、更符合人类价值观的语言模型响应需求日益增长，评估模型价值的基准测试重要性日益凸显。然而现有基准依赖于人工或机器标注，这些标注易受价值观相关偏见影响。此外，测试场景往往偏离实际应用环境——模型通常被用于生成文本并表达价值观。为解决这些问题，我们提出"价值观画像"基准测试框架，该可靠评估体系具备两大特征：首先，基准项目源自真实用户与语言模型的交互记录，增强了评估结果与实际应用的相关性；其次，每个项目由人类受试者根据与自身想法的相似度进行评分，并计算这些评分与受试者实际价值观得分的相关性。这种经过心理测量学验证的方法确保与特定价值观强相关的项目能可靠评估对应价值观。通过对44个语言模型的评估，我们发现这些模型普遍重视仁爱、安全与自主价值观，而相对忽视传统、权力与成就价值观。分析还揭示了语言模型对不同人口群体的认知偏见，这些偏差与真实人类数据存在显著差异。

---

## [Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models](https://arxiv.org/abs/2503.22879)

### Abstract
arXiv:2503.22879v3 Announce Type: replace-cross 
Abstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms two state-of-the-art SSM quantization methods and delivers 1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\times$ memory reduction with only a $1.6\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.

### 摘要
状态空间模型（SSMs）正逐渐成为Transformer的有力替代方案，因其稳定的内存占用和卓越性能而备受关注。然而，由于存储需求和计算能力的限制，在云服务或资源受限设备上扩展SSM仍面临挑战。为此，采用低位宽数据格式对SSM进行量化可有效缩减模型规模并利用硬件加速优势。鉴于SSM易受量化误差影响，近期研究主要聚焦于针对特定模型或位宽进行优化，以兼顾效率与性能。不同应用场景需要差异化位宽配置，例如W4A8适用于提升大批量解码速度，而W4A16则能优化单用户短提示应用的生成速度。为此，我们提出Quamba2框架，兼容Mamba1和Mamba2架构的W8A8、W4A8及W4A16配置，满足多平台部署SSM的迫切需求。基于SSM的通道顺序保持特性和激活持续性，我们提出离线量化方法：通过排序聚类对线性递归输入$x$进行8位量化，同时对输入相关参数$B$和$C$实施按状态组分组的量化。为确保SSM输出的计算不变性，我们依据聚类序列离线重排权重。实验表明，Quamba2-8B在预填充和生成阶段分别实现1.3倍和3倍加速，内存占用降低4倍，平均准确率仅下降1.6%，性能优于两种最先进的SSM量化方法。MMLU评估验证了本框架的泛化性与鲁棒性。代码及量化模型将发布于：https://github.com/enyac-group/Quamba。

---

