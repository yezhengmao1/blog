# 2025-06-05-12-05

## [Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows](https://arxiv.org/abs/2506.03332)

### Abstract
arXiv:2506.03332v1 Announce Type: new 
Abstract: Agentic workflows -- where multiple large language model (LLM) instances interact to solve tasks -- are increasingly built on feedback mechanisms, where one model evaluates and critiques another. Despite the promise of feedback-driven improvement, the stability of agentic workflows rests on the reliability of the judge. However, judges may hallucinate information, exhibit bias, or act adversarially -- introducing critical vulnerabilities into the workflow. In this work, we present a systematic analysis of agentic workflows under deceptive or misleading feedback. We introduce a two-dimensional framework for analyzing judge behavior, along axes of intent (from constructive to malicious) and knowledge (from parametric-only to retrieval-augmented systems). Using this taxonomy, we construct a suite of judge behaviors and develop WAFER-QA, a new benchmark with critiques grounded in retrieved web evidence to evaluate robustness of agentic workflows against factually supported adversarial feedback. We reveal that even strongest agents are vulnerable to persuasive yet flawed critiques -- often switching correct answers after a single round of misleading feedback. Taking a step further, we study how model predictions evolve over multiple rounds of interaction, revealing distinct behavioral patterns between reasoning and non-reasoning models. Our findings highlight fundamental vulnerabilities in feedback-based workflows and offer guidance for building more robust agentic systems.

### 摘要
代理工作流——即多个大语言模型（LLM）实例交互以解决任务——日益依赖于反馈机制，其中一个模型对另一个模型进行评估与批评。尽管反馈驱动的改进具有潜力，但代理工作流的稳定性取决于评判者的可靠性。然而，评判者可能出现幻觉信息、表现出偏见或采取对抗行为，从而为工作流引入关键漏洞。本研究对存在欺骗性或误导性反馈的代理工作流进行了系统性分析。我们提出了一个二维框架来分析评判者行为，涵盖意图（从建设性到恶意）与知识（从纯参数化系统到检索增强系统）两个维度。基于此分类体系，我们构建了一套评判者行为模式，并开发了WAFER-QA基准测试——该基准利用基于网络检索证据的批评，以评估代理工作流对事实支持的对抗性反馈的鲁棒性。研究发现，即使最强智能体也易受具有说服力但存在缺陷的批评影响，通常在单轮误导反馈后即改变正确答案。进一步地，我们通过多轮交互研究了模型预测的演变规律，揭示了推理模型与非推理模型间截然不同的行为模式。这些发现凸显了基于反馈的工作流存在根本性脆弱点，并为构建更鲁棒的代理系统提供了指导。

---

## [AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance](https://arxiv.org/abs/2506.03828)

### Abstract
arXiv:2506.03828v1 Announce Type: new 
Abstract: AI for Industrial Asset Lifecycle Management aims to automate complex operational workflows -- such as condition monitoring, maintenance planning, and intervention scheduling -- to reduce human workload and minimize system downtime. Traditional AI/ML approaches have primarily tackled these problems in isolation, solving narrow tasks within the broader operational pipeline. In contrast, the emergence of AI agents and large language models (LLMs) introduces a next-generation opportunity: enabling end-to-end automation across the entire asset lifecycle. This paper envisions a future where AI agents autonomously manage tasks that previously required distinct expertise and manual coordination. To this end, we introduce AssetOpsBench -- a unified framework and environment designed to guide the development, orchestration, and evaluation of domain-specific agents tailored for Industry 4.0 applications. We outline the key requirements for such holistic systems and provide actionable insights into building agents that integrate perception, reasoning, and control for real-world industrial operations. The software is available at https://github.com/IBM/AssetOpsBench.

### 摘要
工业资产生命周期管理的人工智能旨在自动化复杂运营工作流程——如状态监测、维护规划和干预调度——以减少人工工作量并最小化系统停机时间。传统AI/ML方法主要孤立地解决这些问题，仅在更广泛运营流程中处理狭窄任务。相比之下，AI智能体与大型语言模型（LLMs）的出现带来了新一代机遇：实现跨整个资产生命周期的端到端自动化。本文展望了由AI智能体自主管理以往需要专门知识与人工协调任务的未来。为此，我们提出AssetOpsBench——一个为工业4.0应用定制的统一框架与环境，旨在指导领域专用智能体的开发、编排与评估。我们阐述了此类整体系统的关键需求，并就构建集成感知、推理与控制能力以应用于真实工业场景的智能体提供了可行见解。相关软件发布于https://github.com/IBM/AssetOpsBench。

---

## [CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating &amp; Hiring Applications](https://arxiv.org/abs/2506.03543)

### Abstract
arXiv:2506.03543v1 Announce Type: new 
Abstract: Current large language model (LLM) agents lack authentic human psychological processes necessary for genuine digital twins and social AI applications. To address this limitation, we present a computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism. However, authentic digital twins require accurate personality initialization. We therefore develop a novel adventure-based personality test that evaluates true personality through behavioral choices within interactive scenarios, bypassing self-presentation bias found in traditional assessments. Building on these innovations, our CogniPair platform enables digital twins to engage in realistic simulated dating interactions and job interviews before real encounters, providing bidirectional cultural fit assessment for both romantic compatibility and workplace matching. Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies. This work advances psychological authenticity in LLM agents and establishes a foundation for intelligent dating platforms and HR technology solutions.

### 摘要
当前大型语言模型（LLM）智能体缺乏实现真正数字孪生和社交AI应用所需的人类真实心理过程。为解决这一局限，我们提出全球工作空间理论（GNWT）的计算实现方案，将人类认知架构原则整合至LLM智能体，通过全局工作空间机制协调情绪、记忆、社会规范、规划和目标追踪等专业化子智能体。然而，真实的数字孪生需要精确的人格初始化。为此，我们开发了一种新型冒险式人格测试，通过在交互情境中的行为选择来评估真实人格，规避传统评估中的自我呈现偏差。基于这些创新，CogniPair平台使数字孪生能在真实接触前进行模拟约会互动和求职面试，为婚恋匹配与职场适配提供双向文化契合度评估。采用551个GNWT智能体和哥伦比亚大学速配数据集的验证显示：与人类吸引力模式的相关系数达72%，匹配预测准确率为77.8%，人类验证研究一致率达74%。该研究提升了LLM智能体的心理真实性，为智能婚恋平台和人力资源技术解决方案奠定了基础。

---

## [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)

### Abstract
arXiv:2506.03939v1 Announce Type: new 
Abstract: Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.

### 摘要
图检索增强生成（GraphRAG）通过显式建模知识关系，有效提升了大型语言模型（LLMs）在专业领域的事实准确性与生成质量。然而现有方法存在两个固有局限：1）信息聚合低效：依赖单一智能体和固定迭代模式，难以自适应捕获图数据中的多层级文本、结构及度信息；2）推理机制僵化：采用预设推理方案，无法动态调整推理深度或实现精准语义校正。为突破这些限制，我们提出基于多智能体协同的GraphRAG方法——Graph Counselor。该方法通过自适应图信息抽取模块（AGIEM），由规划、思考与执行智能体协同工作，精确建模复杂图结构并动态调整信息抽取策略，解决了多层级依赖建模与自适应推理深度的难题。此外，多视角自反思（SR）模块通过自反思与逆向推理机制，提升了推理结果的准确性与语义一致性。实验表明，Graph Counselor在多图推理任务中优于现有方法，展现出更高的推理精度与泛化能力。代码已开源：https://github.com/gjq100/Graph-Counselor.git。

---

## [Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs](https://arxiv.org/abs/2506.03296)

### Abstract
arXiv:2506.03296v1 Announce Type: new 
Abstract: Deploying large language models (LLMs) for online inference is often constrained by limited GPU memory, particularly due to the growing KV cache during auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a promising solution by offloading KV cache management and parts of attention computation to the CPU. However, a key bottleneck remains: existing schedulers fail to effectively overlap CPU-offloaded tasks with GPU execution during the latency-critical, bandwidth-bound decode phase. This particularly penalizes real-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning) which are currently underserved by existing systems, especially under memory pressure typical of edge or low-cost deployments.
  We present APEX, a novel, profiling-informed scheduling strategy that maximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems relying on static rules or purely heuristic approaches, APEX dynamically dispatches compute across heterogeneous resources by predicting execution times of CPU and GPU subtasks to maximize overlap while avoiding scheduling overheads.We evaluate APEX on diverse workloads and GPU architectures (NVIDIA T4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only schedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89% on A10 GPUs, while preserving latency. Against the best existing hybrid schedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in long-output settings.APEX significantly advances hybrid LLM inference efficiency on such memory-constrained hardware and provides a blueprint for scheduling in heterogeneous AI systems, filling a critical gap for efficient real-time LLM applications.

### 摘要
部署大型语言模型（LLM）进行在线推理常受限于有限的GPU内存，尤其是自回归解码过程中不断增长的KV缓存。混合GPU-CPU执行通过将KV缓存管理和部分注意力计算卸载至CPU，已成为颇具前景的解决方案。然而关键瓶颈依然存在：现有调度器在延迟敏感、带宽受限的解码阶段，无法有效重叠CPU卸载任务与GPU执行。这尤其影响了实时性高、解码密集型应用（如聊天、思维链推理），这些应用在当前系统中未得到充分支持，特别是在边缘或低成本部署的典型内存压力场景下。

我们提出APEX——一种基于性能剖析的新型调度策略，可在混合LLM推理过程中最大化CPU-GPU并行性。与依赖静态规则或纯启发式方法的系统不同，APEX通过预测CPU和GPU子任务的执行时间，动态分配异构计算资源，在避免调度开销的同时实现最大重叠。我们在多样化工作负载和GPU架构（NVIDIA T4、A10）上使用LLaMa-2-7B和LLaMa-3.1-8B模型评估APEX。相较于VLLM等纯GPU调度器，APEX在T4上提升吞吐量84%-96%，在A10上提升11%-89%，同时保持延迟不变；相比现有最佳混合调度器，在长输出场景中可实现最高49%（T4）和37%（A10）的吞吐量提升。

APEX显著提升了内存受限硬件上的混合LLM推理效率，为异构AI系统调度提供了设计蓝图，填补了高效实时LLM应用的关键技术空白。

---

## [Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games](https://arxiv.org/abs/2506.03610)

### Abstract
arXiv:2506.03610v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present \textbf&#123;\benchname&#123;&#125;&#125;, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak.

### 摘要
大语言模型（LLM）智能体正在重塑游戏产业，尤其是通过更智能且更符合人类偏好的游戏角色。然而，现有游戏基准测试未能满足实际需求：它们缺乏对不同游戏类型中多样化LLM能力的评估，对复杂游戏玩法至关重要的智能体模块的研究，以及将预训练LLM对齐为游戏智能体的微调数据集。为填补这些空白，我们提出\textbf&#123;\benchname&#123;&#125;&#125;，这是一个基础性基准测试，旨在跨多样化的现实世界视频游戏中对LLM智能体进行训练和评估。与现有基准不同，Orak包含12款涵盖所有主要类型的流行视频游戏，能够全面研究LLM能力及复杂游戏场景中不可或缺的智能体模块。为支持对LLM的一致评估，我们引入了一个基于模型上下文协议（MCP）的即插即用接口，使LLM能够无缝连接游戏并操控智能体模块。此外，我们提出了一个微调数据集，包含跨多种游戏类型的LLM游戏轨迹。Orak提供了一个全面的评估框架，包括通用游戏得分排行榜、LLM竞技场，以及对视觉输入状态、智能体策略和微调效果的深入分析，为构建通用游戏智能体奠定了基础。代码发布于https://github.com/krafton-ai/Orak。

---

## [Crowd-SFT: Crowdsourcing for LLM Alignment](https://arxiv.org/abs/2506.04063)

### Abstract
arXiv:2506.04063v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model responses with human preferences. While RLHF employs a reinforcement learning approach with a separate reward model, SFT uses human-curated datasets for supervised learning. Both approaches traditionally depend on small, vetted groups of annotators, making them costly, prone to bias, and limited in scalability. We propose an open, crowd-sourced fine-tuning framework that addresses these limitations by enabling broader feedback collection for SFT without extensive annotator training. Our framework promotes incentive fairness via a point-based reward system correlated with Shapley values and guides model convergence through iterative model updates. Our multi-model selection framework demonstrates up to a 55% reduction in target distance over single-model selection, enabling subsequent experiments that validate our point-based reward mechanism's close alignment with Shapley values (a well-established method for attributing individual contributions) thereby supporting fair and scalable participation.

### 摘要
大型语言模型（LLMs）日益依赖监督微调（SFT）和基于人类反馈的强化学习（RLHF）来使模型响应与人类偏好对齐。RLHF采用强化学习方法并结合独立的奖励模型，而SFT则利用人工标注的数据集进行监督学习。传统上，这两种方法均依赖于少量经过筛选的标注者群体，导致成本高昂、易产生偏见且可扩展性有限。我们提出了一种开放的众包微调框架，通过无需大量标注者训练即可收集更广泛反馈的SFT机制，解决了上述局限性。该框架通过基于Shapley值的积分奖励系统实现激励公平性，并通过迭代模型更新引导模型收敛。我们的多模型选择框架显示，相较于单模型选择，目标距离可减少达55%，后续实验验证了基于积分的奖励机制与Shapley值（一种用于量化个体贡献的成熟方法）高度吻合，从而支持公平且可扩展的参与。

---

## [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)

### Abstract
arXiv:2506.04018v1 Announce Type: new 
Abstract: As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. Prior work has examined agents' ability to enact misaligned behaviour (misalignment capability) and their compliance with harmful instructions (misuse propensity). However, the likelihood of agents attempting misaligned behaviours in real-world settings (misalignment propensity) remains poorly understood. We introduce a misalignment propensity benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in which LLM agents have the opportunity to display misaligned behaviour. We organise our evaluations into subcategories of misaligned behaviours, including goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report the performance of frontier models on our benchmark, observing higher misalignment on average when evaluating more capable models. Finally, we systematically vary agent personalities through different system prompts. We find that persona characteristics can dramatically and unpredictably influence misalignment tendencies -- occasionally far more than the choice of model itself -- highlighting the importance of careful system prompt engineering for deployed AI agents. Our work highlights the failure of current alignment methods to generalise to LLM agents, and underscores the need for further propensity evaluations as autonomous systems become more prevalent.

### 摘要
随着大型语言模型（LLM）智能体的广泛应用，其相关的错位风险也随之增加。现有研究主要考察了智能体执行错位行为的能力（错位能力）及其对有害指令的顺从性（滥用倾向），然而对于智能体在现实场景中尝试错位行为的可能性（错位倾向）仍缺乏深入理解。本文提出了一个错位倾向基准测试AgentMisalignment，包含一系列现实场景，这些场景为LLM智能体提供了展示错位行为的机会。我们将评估分为错位行为的子类别，包括目标守护、抗拒关闭、消极应对和权力寻求。我们报告了前沿模型在该基准测试中的表现，发现能力更强的模型平均表现出更高的错位倾向。最后，我们通过不同的系统提示系统地改变智能体的个性特征。研究发现，角色特征可能对错位倾向产生显著且不可预测的影响——有时甚至远超模型选择本身——这突显了在部署AI智能体时精心设计系统提示的重要性。我们的工作揭示了当前对齐方法在泛化至LLM智能体时的失效，并强调随着自主系统的普及，进一步开展倾向性评估的必要性。

---

## [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)

### Abstract
arXiv:2506.04210v1 Announce Type: new 
Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like "Wait" or "Let me rethink" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to "overthinking". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from "more thinking" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.

### 摘要
近期推理模型（如OpenAI o1、DeepSeek R1）的测试时扩展趋势引发了一种普遍观点，即通过"等待"或"让我重新思考"等提示延长思维轨迹能提升性能。这自然引出一个问题：测试时增加思考是否真能改善推理？为解答该问题，我们跨模型和基准开展了详细实证研究，发现一致的初始性能提升后伴随下降的模式，此现象源于"过度思考"。为理解这种非单调趋势，我们建立了一个简单概率模型，表明额外思考会增加输出方差——在制造推理改善假象的同时最终损害精确性。因此，"更多思考"带来的观察收益并非推理改进的真实指标，而是模型不确定性与评估指标关联的伪影。这表明通过延长思考进行测试时扩展并非利用推理思维预算的有效方式。认识到这些局限后，我们受"最佳N采样"启发提出替代方案——并行思考。该方法在相同推理预算内生成多条独立推理路径，并通过多数表决选择最一致响应，相比延长思考可实现高达20%的准确率提升。这为推理模型的测试时扩展提供了一种简单有效的机制。

---

## [Cascadia: A Cascade Serving System for Large Language Models](https://arxiv.org/abs/2506.04203)

### Abstract
arXiv:2506.04203v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have intensified the need to deliver both rapid responses and high-quality answers. More powerful models yield better results but incur higher inference latency, whereas smaller models are faster yet less capable. Recent work proposes balancing this latency-quality trade-off using model cascades, which route simpler queries to smaller models and more complex ones to larger models. However, enabling efficient cascade serving remains challenging. Current frameworks lack effective mechanisms for handling (i) the huge and varying resource demands of different LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the co-optimization of system deployment and routing strategy. Motivated by these observations, we introduce Cascadia, a novel cascade serving framework designed explicitly to schedule request routing and deploy model cascades for fast, quality-preserving LLM serving. Cascadia employs a bi-level optimization method: at the inner level, it uses a mixed-integer linear program to select resource allocations and parallelism strategies based on LLM information and workload characteristics; at the outer level, it applies a weighted Tchebycheff algorithm to iteratively co-optimize the routing strategy and the system deployment produced by the inner level. Our extensive evaluation on diverse workload traces and different model cascades (DeepSeek and the Llama series) demonstrates that Cascadia significantly outperforms both single-model deployments and the state-of-the-art cascade serving baseline, achieving up to 4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher throughput while maintaining target answer quality.

### 摘要
大语言模型（LLM）的最新进展使得同时实现快速响应与高质量答案的需求日益迫切。性能更强的模型能产生更优结果但推理延迟更高，而较小模型速度更快但能力较弱。近期研究提出采用模型级联来平衡这种延迟-质量权衡，即将简单查询路由至较小模型，复杂查询分配至较大模型。然而实现高效的级联服务仍面临挑战：现有框架缺乏有效机制来处理（i）不同LLM巨大且多变的资源需求，（ii）LLM工作负载固有的异构性，以及（iii）系统部署与路由策略的协同优化。基于这些观察，我们提出Cascadia——一个专为快速、保质的LLM服务而设计的新型级联服务框架，可显式调度请求路由并部署模型级联。Cascadia采用双层优化方法：内层通过混合整数线性规划，根据LLM信息和工作负载特征选择资源分配与并行策略；外层应用加权Tchebycheff算法迭代协同优化内层产生的路由策略与系统部署。我们在多样化工作负载轨迹和不同模型级联（DeepSeek与Llama系列）上的大量实验表明，Cascadia显著优于单模型部署和最先进的级联服务基线，在保持目标答案质量的同时，将延迟SLO最高收紧4倍（平均2.3倍），吞吐量最高提升5倍（平均2.4倍）。

---

## [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/abs/2506.04133)

### Abstract
arXiv:2506.04133v1 Announce Type: new 
Abstract: Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.

### 摘要
基于大型语言模型（LLMs）构建并以多智能体配置部署的代理式人工智能系统，正在重塑企业和社会领域中智能自治、协作与决策的范式。本文针对基于LLM的代理式多智能体系统（AMAS）中的信任、风险与安全管理（TRiSM）进行了结构化分析。首先探讨了代理式AI的概念基础，其与传统AI智能体在架构上的差异，以及支持可扩展工具化自治的新兴系统设计。随后通过治理、可解释性、模型运维（ModelOps）和隐私/安全四大支柱，详细阐述了代理式AI框架中的TRiSM机制，每个维度均针对代理式LLMs进行情境化分析。我们识别了独特的威胁向量，并建立了一套面向代理式AI应用的完整风险分类体系，辅以揭示现实漏洞的案例研究。此外，本文还系统梳理了分布式LLM智能体系统中的信任构建机制、透明监督技术以及最先进的可解释性策略，同时评述了评估信任度、可解释性及以人为本性能的度量标准与开放基准挑战。在安全与隐私方面，重点探讨了加密技术、对抗防御及动态AI法规合规方案。最后提出了负责任代理式AI的发展路线图，为新兴多智能体系统如何遵循稳健的TRiSM原则以实现安全、可靠且透明的部署指明了研究方向。

---

## [LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning](https://arxiv.org/abs/2506.03178)

### Abstract
arXiv:2506.03178v1 Announce Type: cross 
Abstract: Automated radiology report generation holds significant potential to reduce radiologists' workload and enhance diagnostic accuracy. However, generating precise and clinically meaningful reports from chest radiographs remains challenging due to the complexity of medical language and the need for contextual understanding. Existing models often struggle with maintaining both accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves improved coherence and clinical accuracy while maintaining computational efficiency. This efficiency is driven by an optimization strategy that enhances parameter utilization and reduces memory overhead, enabling faster report generation with lower computational resource demands. Extensive experiments conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L score of 0.433 and a METEOR score of 0.336, establishing new performance benchmarks in the domain. These results underscore LLaMA-XR's potential as an effective and efficient AI system for automated radiology reporting, offering enhanced clinical utility and reliability.

### 摘要
自动化放射学报告生成技术具有显著减轻放射科医生工作量并提升诊断准确性的潜力。然而，由于医学语言的复杂性和对上下文理解的要求，从胸部X光片生成精确且具有临床意义的报告仍面临挑战。现有模型往往难以同时保持准确性和上下文相关性。本文提出LLaMA-XR新型框架，该框架将LLaMA 3.1与基于DenseNet-121的图像嵌入及量化低秩自适应微调（QLoRA）相结合。LLaMA-XR在保持计算效率的同时，实现了更高的连贯性和临床准确性。这种效率得益于优化策略，该策略提升了参数利用率并降低了内存开销，从而能以更低的计算资源需求实现更快速的报告生成。在IU X-ray基准数据集上的大量实验表明，LLaMA-XR优于多种先进方法，其ROUGE-L得分达到0.433，METEOR得分达到0.336，创下了该领域新的性能基准。这些结果证实了LLaMA-XR作为高效自动化放射学报告AI系统的潜力，具有更强的临床实用性和可靠性。

---

## [Vid-SME: Membership Inference Attacks against Large Video Understanding Models](https://arxiv.org/abs/2506.03179)

### Abstract
arXiv:2506.03179v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) demonstrate remarkable capabilities in handling complex multimodal tasks and are increasingly adopted in video understanding applications. However, their rapid advancement raises serious data privacy concerns, particularly given the potential inclusion of sensitive video content, such as personal recordings and surveillance footage, in their training datasets. Determining improperly used videos during training remains a critical and unresolved challenge. Despite considerable progress on membership inference attacks (MIAs) for text and image data in MLLMs, existing methods fail to generalize effectively to the video domain. These methods suffer from poor scalability as more frames are sampled and generally achieve negligible true positive rates at low false positive rates (TPR@Low FPR), mainly due to their failure to capture the inherent temporal variations of video frames and to account for model behavior differences as the number of frames varies. To address these challenges, we introduce Vid-SME, the first membership inference method tailored for video data used in video understanding LLMs (VULLMs). Vid-SME leverages the confidence of model output and integrates adaptive parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By leveraging the SME difference between natural and temporally-reversed video frames, Vid-SME derives robust membership scores to determine whether a given video is part of the model's training set. Experiments on various self-trained and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.

### 摘要
多模态大语言模型（MLLMs）在处理复杂多模态任务方面展现出卓越能力，正日益广泛应用于视频理解领域。然而其快速发展引发了严重的数据隐私问题，尤其是训练数据中可能包含个人录像、监控视频等敏感内容。如何检测训练过程中不当使用的视频仍是一个关键且未解决的挑战。尽管针对MLLMs中文本和图像数据的成员推理攻击（MIAs）已取得显著进展，现有方法却难以有效推广至视频领域。这些方法存在扩展性差的问题（采样帧数增加时表现恶化），且在低误报率下的真阳性率（TPR@Low FPR）普遍可忽略不计，主要因其未能捕捉视频帧的固有时序变化，也未考虑帧数变化时的模型行为差异。为解决这些挑战，我们提出Vid-SME——首个专为视频理解大语言模型（VULLMs）设计的视频成员推理方法。Vid-SME利用模型输出的置信度，通过自适应参数化计算视频输入的Sharma-Mittal熵（SME），通过对比原始视频帧与时间反转帧的SME差异，生成鲁棒的成员分数以判定给定视频是否属于模型训练集。在不同自训练与开源VULLMs上的实验验证了Vid-SME的卓越有效性。

---

## [Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward](https://arxiv.org/abs/2506.03191)

### Abstract
arXiv:2506.03191v1 Announce Type: cross 
Abstract: This paper presents an in-depth survey on the use of multimodal Generative Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs) for human motion understanding and generation, offering insights into emerging methods, architectures, and their potential to advance realistic and versatile motion synthesis. Focusing exclusively on text and motion modalities, this research investigates how textual descriptions can guide the generation of complex, human-like motion sequences. The paper explores various generative approaches, including autoregressive models, diffusion models, Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models, by analyzing their strengths and limitations in terms of motion quality, computational efficiency, and adaptability. It highlights recent advances in text-conditioned motion generation, where textual inputs are used to control and refine motion outputs with greater precision. The integration of LLMs further enhances these models by enabling semantic alignment between instructions and motion, improving coherence and contextual relevance. This systematic survey underscores the transformative potential of text-to-motion GenAI and LLM architectures in applications such as healthcare, humanoids, gaming, animation, and assistive technologies, while addressing ongoing challenges in generating efficient and realistic human motion.

### 摘要
本文针对多模态生成式人工智能（GenAI）与自回归大语言模型（LLMs）在人体运动理解与生成中的应用进行了深度综述，系统阐述了新兴方法、架构及其在推动高真实度、多样化运动合成方面的潜力。研究聚焦文本与运动双模态，探究文本描述如何指导生成复杂类人运动序列。通过分析自回归模型、扩散模型、生成对抗网络（GANs）、变分自编码器（VAEs）及基于Transformer的模型等生成方法，本文比较了各类方法在运动质量、计算效率与适应性方面的优势与局限。重点探讨了文本条件运动生成的最新进展，其中文本输入可精确控制与优化运动输出。大语言模型的整合进一步强化了这些模型，通过实现指令与运动间的语义对齐，提升了运动序列的连贯性与上下文相关性。本系统性综述揭示了文本到运动生成式AI与LLM架构在医疗、人形机器人、游戏动画及辅助技术等领域的变革性潜力，同时探讨了高效真实人体运动生成领域持续存在的挑战。

---

## [HueManity: Probing Fine-Grained Visual Perception in MLLMs](https://arxiv.org/abs/2506.03194)

### Abstract
arXiv:2506.03194v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual reasoning, but their performance on nuanced perceptual tasks remains surprisingly limited. We present HueManity, a benchmark designed to assess visual perception in MLLMs. The dataset comprises 83,850 images featuring two-character alphanumeric strings embedded in Ishihara test style dot patterns, challenging models on precise pattern recognition. Our evaluation of nine state-of-the-art MLLMs on HueManity demonstrates a significant performance deficit compared to human and traditional computer vision baselines. The best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a striking 3% on the alphanumeric `hard' task. In contrast, human participants achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model reached accuracies of 96.5% and 94.5%. These results highlight a critical gap in the visual capabilities of current MLLMs. Our analysis further explores potential architectural and training-paradigm factors contributing to this perceptual gap in MLLMs. We open-source HueManity dataset and code to foster further research in improving perceptual robustness of MLLMs.

### 摘要
多模态大语言模型（MLLMs）在高级视觉推理任务中表现卓越，但其在细微感知任务上的性能仍存在显著局限。本研究提出HueManity基准，用于评估MLLMs的视觉感知能力。该数据集包含83,850张图像，呈现嵌入石原式点阵图案的双字符字母数字组合，旨在检验模型的精确模式识别能力。我们对九种前沿MLLMs的评估显示，相较于人类和传统计算机视觉基线，这些模型存在显著性能缺陷：最佳MLLM在数字'简单'任务中仅达33.6%准确率，在字母数字'困难'任务中更是低至3%。对比之下，人类参与者取得近乎完美的成绩（100%和95.6%），而经微调的ResNet50模型则达到96.5%和94.5%的准确率。这些结果揭示了当前MLLMs视觉能力的重大缺陷。我们进一步分析了可能导致MLLMs感知差距的架构设计和训练范式因素，并开源HueManity数据集及代码以促进提升MLLMs感知鲁棒性的研究。

---

## [Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs](https://arxiv.org/abs/2506.03195)

### Abstract
arXiv:2506.03195v1 Announce Type: cross 
Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on general zero-shot image classification tasks, fine-grained image classification remains challenging. It demands precise attention to subtle visual details to distinguish between visually similar subcategories--details that MLLMs may easily overlook without explicit guidance. To address this, we introduce AutoSEP, an iterative self-supervised prompt learning framework designed to enhance MLLM fine-grained classification capabilities in a fully unsupervised manner. Our core idea is to leverage unlabeled data to learn a description prompt that guides MLLMs in identifying crucial discriminative features within an image, and boosts classification accuracy. We developed an automatic self-enhancing prompt learning framework called AutoSEP to iteratively improve the description prompt using unlabeled data, based on instance-level classification scoring function. AutoSEP only requires black-box access to MLLMs, eliminating the need for any training or fine-tuning. We evaluate our approach on multiple fine-grained classification datasets. It consistently outperforms other unsupervised baselines, demonstrating the effectiveness of our self-supervised optimization framework. Notably, AutoSEP on average improves 13 percent over standard zero-shot classification and 5 percent over the best-performing baselines. Code is available at: https://github.com/yq-hong/AutoSEP

### 摘要
尽管多模态大语言模型（MLLMs）在通用零样本图像分类任务中展现出良好性能，细粒度图像分类仍具挑战性。该任务需要精确关注细微视觉特征以区分视觉相似的子类别——这些特征在缺乏明确指导时易被MLLMs忽略。为此，我们提出AutoSEP：一种迭代式自监督提示学习框架，旨在完全无监督条件下增强MLLMs的细粒度分类能力。其核心思想是利用未标注数据学习描述性提示，引导MLLMs识别图像中关键判别特征，从而提升分类准确率。我们基于实例级分类评分函数，开发了名为AutoSEP的自动增强提示学习框架，通过未标注数据迭代优化描述提示。AutoSEP仅需黑盒访问MLLMs，无需任何训练或微调。在多个细粒度分类数据集上的实验表明，该方法持续优于其他无监督基线，验证了自监督优化框架的有效性。值得注意的是，AutoSEP相较标准零样本分类平均提升13%，较最佳基线平均提升5%。代码已开源：https://github.com/yq-hong/AutoSEP

---

## [NetPress: Dynamically Generated LLM Benchmarks for Network Applications](https://arxiv.org/abs/2506.03231)

### Abstract
arXiv:2506.03231v1 Announce Type: cross 
Abstract: Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.

### 摘要
尽管针对大语言模型（LLM）和智能体的领域专用基准测试日益受到关注，但当前评估仍局限于静态小规模数据集，尤其在需要部署可靠性的高风险任务（如网络运维）中更为突出。我们提出NetPress——一个用于评估网络应用中LLM智能体的自动化基准生成框架。该框架通过状态与动作的统一抽象，实现了多样化查询集及对应真实结果的动态生成。用户可在运行时指定基准配置，实时生成数百万条查询。除动态构建基准外，NetPress还与网络仿真器集成以提供真实环境反馈，支持正确性、安全性和延迟性的综合评估。我们在三个典型应用中实例化NetPress，揭示了静态纯正确性基准常忽略的智能体行为细粒度差异。NetPress推动LLM评估向以基础设施为中心领域的真实可扩展测试迈进，有助于缩小基准性能与实际部署准备度之间的差距。代码详见https://github.com/Froot-NetSys/NetPress。

---

## [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)

### Abstract
arXiv:2506.03292v1 Announce Type: cross 
Abstract: Steering language models (LMs) by modifying internal activations is a popular approach for controlling text generation. Unsupervised dictionary learning methods, e.g., sparse autoencoders, can be scaled to produce many steering vectors, but lack guarantees on the individual efficacy of each vector and control over the coverage of relevant steering tasks. In contrast, supervised methods for constructing steering vectors are targeted and effective, but require more data collection and training for each additional steering vector produced. In this work, we introduce HyperSteer, a family of hypernetwork-based architectures which are trained end-to-end to generate steering vectors conditioned on the natural language steering prompts and the internals of the steered LM. In our evaluations, we show that scaling HyperSteer with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods, even on steering prompts never seen during training. Moreover, HyperSteer performs on par with steering-via-prompting.

### 摘要
通过修改内部激活来引导语言模型（LMs）是一种常用的文本生成控制方法。无监督字典学习方法（如稀疏自编码器）可扩展生成大量引导向量，但无法保证每个向量的单独有效性，也难以控制相关引导任务的覆盖范围。相比之下，监督式构建引导向量的方法具有针对性和高效性，但每增加一个引导向量都需要额外的数据收集和训练。本研究提出HyperSteer，一种基于超网络的架构家族，通过端到端训练生成条件化于自然语言引导提示和被引导LM内部状态的引导向量。评估结果表明，即使面对训练中未出现的引导提示，HyperSteer在扩展至数千个引导提示时的性能仍超越最先进的激活引导方法。此外，HyperSteer的表现与基于提示的引导方法相当。

---

## [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)

### Abstract
arXiv:2506.03230v1 Announce Type: cross 
Abstract: Finetuning is a critical step for adapting large language models (LLMs) to domain-specific downstream tasks. To mitigate the substantial computational and memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT) methods have been proposed to update only a small subset of model parameters. However, performance gaps between PEFT approaches and full-model fine-tuning still exist. In this work, we present DiaBlo, a simple yet effective PEFT approach that updates only the diagonal blocks of selected model weight matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates the need for low rank matrix products, thereby avoiding the reliance on auxiliary initialization schemes or customized optimization strategies to improve convergence. This design leads to stable and robust convergence while maintaining comparable memory efficiency and training speed to LoRA. We conduct extensive experiments across a range of tasks, including commonsense reasoning, arithmetic reasoning, code generation, and safety alignment, to evaluate the effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo demonstrates strong and consistent performance while maintaining high memory efficiency and fast finetuning speed. Codes are available at https://github.com/ziyangjoy/DiaBlo.

### 摘要
微调是将大语言模型（LLM）适配到特定领域下游任务的关键步骤。为降低全模型微调的高昂计算和内存成本，参数高效微调（PEFT）方法被提出，仅更新模型参数的小型子集。然而，PEFT方法与全模型微调之间仍存在性能差距。本研究提出DiaBlo——一种简单高效的PEFT方法，仅更新选定模型权重矩阵的对角块。与低秩适配（LoRA）及其变体不同，DiaBlo无需进行低秩矩阵乘积，从而避免依赖辅助初始化方案或定制优化策略来改善收敛性。该设计在保持与LoRA相当的内存效率和训练速度的同时，实现了稳定鲁棒的收敛。我们在常识推理、算术推理、代码生成和安全对齐等任务上进行了广泛实验，以评估DiaBlo的有效性和效率。在这些基准测试中，DiaBlo展现出强劲且一致的性能，同时保持高内存效率和快速微调速度。代码发布于https://github.com/ziyangjoy/DiaBlo。

---

## [Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning](https://arxiv.org/abs/2506.03229)

### Abstract
arXiv:2506.03229v1 Announce Type: cross 
Abstract: In the context of noisy partial label learning (NPLL), each training sample is associated with a set of candidate labels annotated by multiple noisy annotators. With the emergence of high-performance pre-trained vision-language models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these models to replace time-consuming manual annotation workflows and achieve "manual-annotation-free" training for downstream tasks has become a highly promising research avenue. This paper focuses on learning from noisy partial labels annotated by pre-trained VLMs and proposes an innovative collaborative consistency regularization (Co-Reg) method. Unlike the symmetric noise primarily addressed in traditional noisy label learning, the noise generated by pre-trained models is instance-dependent, embodying the underlying patterns of the pre-trained models themselves, which significantly increases the learning difficulty for the model. To address this, we simultaneously train two neural networks that implement collaborative purification of training labels through a "Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization constraints in both the label space and feature representation space. Our method can also leverage few-shot manually annotated valid labels to further enhance its performances. Comparative experiments with different denoising and disambiguation algorithms, annotation manners, and pre-trained model application schemes fully validate the effectiveness of the proposed method, while revealing the broad prospects of integrating weakly-supervised learning techniques into the knowledge distillation process of pre-trained models.

### 摘要
在噪声部分标签学习（NPLL）的背景下，每个训练样本都与由多个噪声标注者标注的一组候选标签相关联。随着CLIP、LLaVa和GPT-4V等高性能预训练视觉语言模型（VLMs）的出现，利用这些模型取代耗时的人工标注流程，实现下游任务'免人工标注'的训练已成为极具前景的研究方向。本文重点研究从预训练VLMs标注的噪声部分标签中学习，提出了一种创新的协同一致性正则化（Co-Reg）方法。与传统噪声标签学习主要解决的对称噪声不同，预训练模型产生的噪声具有实例依赖性，体现了预训练模型本身的底层模式，这显著增加了模型的学习难度。为此，我们同时训练两个神经网络，通过'协同伪标签'机制实现训练标签的协同净化，并在标签空间和特征表示空间施加一致性正则化约束。我们的方法还能利用少量人工标注的有效标签进一步提升性能。通过与不同去噪与消歧算法、标注方式及预训练模型应用方案的对比实验，充分验证了所提方法的有效性，同时揭示了将弱监督学习技术融入预训练模型知识蒸馏过程的广阔前景。

---

## [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)

### Abstract
arXiv:2506.03303v1 Announce Type: cross 
Abstract: Modern causal language models stack many attention blocks to improve performance, but not all blocks are necessary for every task. We propose Hopscotch, a simple yet effective method that identifies and skips attention blocks with least contributions to a task and adapts to preserve output quality. Hopscotch jointly optimizes which blocks to skip and how to scale the outputs of the remaining layers. By introducing lightweight, trainable scaling parameters to attention and MLP blocks, it mitigates distribution shifts in hidden states caused by removing attention blocks. Hopscotch does not modify model weights or require access to pretraining or instruction-tuning data, and is compatible with existing model compression techniques. When applied to $\texttt&#123;Llama-3.1-8B&#125;$ and $\texttt&#123;Qwen2.5-7B&#125;$, Hopscotch achieves less than a 2% drop in performance even after skipping four attention blocks.

### 摘要
现代因果语言模型通过堆叠多个注意力模块来提升性能，但并非所有模块对每项任务都必不可少。我们提出Hopscotch方法，该方案能有效识别并跳过对当前任务贡献最小的注意力模块，同时通过自适应调整保持输出质量。Hopscotch联合优化了模块跳过策略与剩余层输出的缩放机制，通过为注意力和MLP模块引入轻量级可训练缩放参数，缓解因移除注意力模块导致的隐状态分布偏移。该方法无需修改模型权重，也不依赖预训练或指令调优数据，且与现有模型压缩技术兼容。在$\texttt&#123;Llama-3.1-8B&#125;$和$\texttt&#123;Qwen2.5-7B&#125;$上的实验表明，即使跳过四个注意力模块，性能下降仍能控制在2%以内。

---

## [Adversarial Attacks on Robotic Vision Language Action Models](https://arxiv.org/abs/2506.03350)

### Abstract
arXiv:2506.03350v1 Announce Type: cross 
Abstract: The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg .

### 摘要
视觉-语言-动作模型（VLA）的出现正通过实现十亿参数规模的多模态感知融合，重塑端到端控制的机器人领域。VLA的核心能力主要源于其基于前沿大语言模型（LLM）的架构。然而，LLM已知易受对抗性滥用影响，鉴于机器人技术固有的重大物理风险，VLA是否继承这些漏洞仍存疑问。基于此，本研究首次系统探究针对VLA控制机器人的对抗攻击。我们的核心算法贡献在于改造并应用LLM越狱攻击技术，实现对VLA的完全控制权。研究发现，在任务初始阶段实施的文本攻击可全面覆盖常用VLA的动作空间，且攻击效果往往能长期持续。这与LLM越狱研究存在显著差异，因为现实场景中的攻击无需与危害概念建立语义关联。所有代码已开源：https://github.com/eliotjones1/robogcg。

---

## [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)

### Abstract
arXiv:2506.03357v1 Announce Type: cross 
Abstract: Hallucinations in large language models (LLMs) - instances where models generate plausible but factually incorrect information - present a significant challenge for AI.
  We introduce "Ask a Local", a novel hallucination detection method exploiting the intuition that specialized models exhibit greater surprise when encountering domain-specific inaccuracies. Our approach computes divergence between perplexity distributions of language-specialized models to identify potentially hallucinated spans. Our method is particularly well-suited for a multilingual context, as it naturally scales to multiple languages without the need for adaptation, relying on external data sources, or performing training. Moreover, we select computationally efficient models, providing a scalable solution that can be applied to a wide range of languages and domains.
  Our results on a human-annotated question-answer dataset spanning 14 languages demonstrate consistent performance across languages, with Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman correlation values. Our model shows particularly strong performance on Italian and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining cross-lingual effectiveness without language-specific adaptations. We release our code and architecture to facilitate further research in multilingual hallucination detection.

### 摘要
大型语言模型（LLMs）中的幻觉现象——即模型生成看似合理但事实错误的信息——是人工智能面临的重大挑战。我们提出'询问本地专家'这一新颖的幻觉检测方法，该方法利用领域专用模型在遇到领域内不准确信息时表现出更大惊讶度的直觉。我们的方法通过计算语言专用模型的困惑度分布差异来识别可能产生幻觉的文本片段。该方法特别适用于多语言场景，因其无需调整、依赖外部数据源或进行训练即可自然扩展到多种语言。此外，我们选择了计算高效的模型，提供了可扩展的解决方案，可广泛应用于不同语言和领域。

---

## [The Future of Continual Learning in the Era of Foundation Models: Three Key Directions](https://arxiv.org/abs/2506.03320)

### Abstract
arXiv:2506.03320v1 Announce Type: cross 
Abstract: Continual learning--the ability to acquire, retain, and refine knowledge over time--has always been fundamental to intelligence, both human and artificial. Historically, different AI paradigms have acknowledged this need, albeit with varying priorities: early expert and production systems focused on incremental knowledge consolidation, while reinforcement learning emphasised dynamic adaptation. With the rise of deep learning, deep continual learning has primarily focused on learning robust and reusable representations over time to solve sequences of increasingly complex tasks. However, the emergence of Large Language Models (LLMs) and foundation models has raised the question: Do we still need continual learning when centralised, monolithic models can tackle diverse tasks with access to internet-scale knowledge? We argue that continual learning remains essential for three key reasons: (i) continual pre-training is still necessary to ensure foundation models remain up to date, mitigating knowledge staleness and distribution shifts while integrating new information; (ii) continual fine-tuning enables models to specialise and personalise, adapting to domain-specific tasks, user preferences, and real-world constraints without full retraining, avoiding the need for computationally expensive long context-windows; (iii) continual compositionality offers a scalable and modular approach to intelligence, enabling the orchestration of foundation models and agents to be dynamically composed, recombined, and adapted. While continual pre-training and fine-tuning are explored as niche research directions, we argue it is continual compositionality that will mark the rebirth of continual learning. The future of AI will not be defined by a single static model but by an ecosystem of continually evolving and interacting models, making continual learning more relevant than ever.

### 摘要
持续学习——即随时间推移获取、保持并完善知识的能力——始终是人类与人工智能智能的基础。历史上不同AI范式虽各有侧重，但都认识到这一需求：早期专家系统和生产系统专注于增量知识整合，而强化学习则强调动态适应。随着深度学习的兴起，深度持续学习主要聚焦于通过时间积累学习鲁棒且可复用的表征，以解决日益复杂的任务序列。然而大型语言模型（LLMs）和基础模型的出现引发了一个问题：当集中式单体模型借助互联网规模知识即可处理多样任务时，我们是否还需要持续学习？我们认为持续学习仍然至关重要，原因有三：（i）持续预训练对确保基础模型保持更新仍属必要，可缓解知识陈旧性与分布偏移，同时整合新信息；（ii）持续微调使模型能够专业化与个性化，适应特定领域任务、用户偏好及现实约束，无需完整重训练，避免计算代价高昂的长上下文窗口；（iii）持续组合性为智能提供可扩展的模块化方案，使得基础模型与智能体能够被动态编排、重组与适配。尽管持续预训练和微调作为细分研究方向被探索，但我们认为持续组合性将标志持续学习的重生。AI的未来不会由单一静态模型定义，而将由持续进化与交互的模型生态系统塑造，这使得持续学习比以往任何时候都更具现实意义。

---

## [Sampling Preferences Yields Simple Trustworthiness Scores](https://arxiv.org/abs/2506.03399)

### Abstract
arXiv:2506.03399v1 Announce Type: cross 
Abstract: With the onset of large language models (LLMs), the performance of artificial intelligence (AI) models is becoming increasingly multi-dimensional. Accordingly, there have been several large, multi-dimensional evaluation frameworks put forward to evaluate LLMs. Though these frameworks are much more realistic than previous attempts which only used a single score like accuracy, multi-dimensional evaluations can complicate decision-making since there is no obvious way to select an optimal model. This work introduces preference sampling, a method to extract a scalar trustworthiness score from multi-dimensional evaluation results by considering the many characteristics of model performance which users value. We show that preference sampling improves upon alternate aggregation methods by using multi-dimensional trustworthiness evaluations of LLMs from TrustLLM and DecodingTrust. We find that preference sampling is consistently reductive, fully reducing the set of candidate models 100% of the time whereas Pareto optimality never reduces the set by more than 50%. Likewise, preference sampling is consistently sensitive to user priors-allowing users to specify the relative weighting and confidence of their preferences-whereas averaging scores is intransigent to the users' prior knowledge.

### 摘要
随着大型语言模型（LLMs）的出现，人工智能（AI）模型的性能评估正变得日益多维化。相应地，学界已提出多个大型多维评估框架来评价LLMs。尽管这些框架比以往仅使用准确率等单一指标的尝试更贴近现实，但多维评估会使决策过程复杂化，因为缺乏明确的方法来选择最优模型。本研究提出偏好采样法，该方法通过考量用户重视的模型性能多维特征，从评估结果中提取出标量化的可信度分数。我们利用TrustLLM和DecodingTrust对LLMs的多维可信度评估数据，证明偏好采样法优于其他聚合方法。研究发现，偏好采样具有稳定的约简性——100%的情况下能完全约简候选模型集，而帕累托最优法的约简率从未超过50%。同样，偏好采样对用户先验表现出稳定敏感性（允许用户指定偏好的相对权重和置信度），而平均分法则对用户先验知识呈现刚性。

---

## [Universal Reusability in Recommender Systems: The Case for Dataset- and Task-Independent Frameworks](https://arxiv.org/abs/2506.03391)

### Abstract
arXiv:2506.03391v1 Announce Type: cross 
Abstract: Recommender systems are pivotal in delivering personalized experiences across industries, yet their adoption and scalability remain hindered by the need for extensive dataset- and task-specific configurations. Existing systems often require significant manual intervention, domain expertise, and engineering effort to adapt to new datasets or tasks, creating barriers to entry and limiting reusability. In contrast, recent advancements in large language models (LLMs) have demonstrated the transformative potential of reusable systems, where a single model can handle diverse tasks without significant reconfiguration. Inspired by this paradigm, we propose the Dataset- and Task-Independent Recommender System (DTIRS), a framework aimed at maximizing the reusability of recommender systems while minimizing barriers to entry. Unlike LLMs, which achieve task generalization directly, DTIRS focuses on eliminating the need to rebuild or reconfigure recommendation pipelines for every new dataset or task, even though models may still need retraining on new data. By leveraging the novel Dataset Description Language (DsDL), DTIRS enables standardized dataset descriptions and explicit task definitions, allowing autonomous feature engineering, model selection, and optimization. This paper introduces the concept of DTIRS and establishes a roadmap for transitioning from Level-1 automation (dataset-agnostic but task-specific systems) to Level-2 automation (fully dataset- and task-independent systems). Achieving this paradigm would maximize code reusability and lower barriers to adoption. We discuss key challenges, including the trade-offs between generalization and specialization, computational overhead, and scalability, while presenting DsDL as a foundational tool for this vision.

### 摘要
推荐系统在跨行业提供个性化体验方面具有关键作用，但其采用和可扩展性仍因需要大量针对特定数据集和任务的配置而受到阻碍。现有系统通常需要大量人工干预、领域专业知识和工程投入才能适应新数据集或任务，这提高了使用门槛并限制了可复用性。相比之下，大型语言模型（LLM）的最新进展展示了可复用系统的变革潜力——单一模型无需重大重新配置即可处理多样化任务。受此范式启发，我们提出数据集与任务无关的推荐系统（DTIRS），该框架旨在最大化推荐系统的可复用性，同时最小化使用门槛。与直接实现任务泛化的LLM不同，DTIRS着重消除为每个新数据集或任务重建或重新配置推荐流程的需求（尽管模型仍可能需要在新增数据上重新训练）。通过采用新颖的数据集描述语言（DsDL），DTIRS实现了标准化数据集描述和显式任务定义，从而支持自主特征工程、模型选择和优化。本文介绍了DTIRS的概念，并制定了从一级自动化（数据集无关但任务特定的系统）向二级自动化（完全数据集与任务无关的系统）过渡的路线图。实现该范式将最大化代码可复用性并降低采用门槛。我们探讨了关键挑战，包括泛化与专业化之间的权衡、计算开销和可扩展性，同时提出DsDL作为实现该愿景的基础工具。

---

## [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)

### Abstract
arXiv:2506.03489v1 Announce Type: cross 
Abstract: The remarkable performance of Large language models (LLMs) relies heavily on the availability of abundant high-quality training data. However, the high cost of acquiring annotated data often prevents models from obtaining capabilities to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe that boosts model performance in data-scarcity scenarios without extra training. We first employ model extrapolation to enhance a finetuned model with its inferior version, and then adopt contrastive decoding to further reduce predicted errors, by comparing the logit scores given by the extrapolated and the vanilla finetuned model. Experiments across three tasks over four different LLMs show that EpiCoDe consistently outperforms existing methods with significant and robust improvement. We also propose a new theoretical framework to reveal the mechanism behind contrastive decoding in data-scarcity scenarios, which further helps us better understand the effectiveness of EpiCoDe.

### 摘要
大型语言模型（LLMs）的卓越性能高度依赖于大量高质量训练数据的可获得性。然而，标注数据的高昂成本往往阻碍模型获得解决下游任务的能力。本文提出一种创新方法EpiCoDe，可在无需额外训练的数据稀缺场景下提升模型性能。我们首先采用模型外推法，通过其低配版本增强微调后的模型，随后通过对比外推模型与原始微调模型给出的对数分数，采用对比解码策略进一步减少预测误差。在四种不同LLMs上进行的三个任务实验表明，EpiCoDe始终以显著且稳健的优势超越现有方法。我们还提出一个新的理论框架来揭示数据稀缺场景下对比解码的作用机制，这有助于进一步理解EpiCoDe的有效性。

---

## [Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity](https://arxiv.org/abs/2506.03337)

### Abstract
arXiv:2506.03337v1 Announce Type: cross 
Abstract: Federated Learning enables collaborative fine-tuning of Large Language Models (LLMs) across decentralized Non-Independent and Identically Distributed (Non-IID) clients, but such models' massive parameter sizes lead to significant memory and communication challenges. This work introduces Meerkat, a sparse zeroth-order optimization (ZO) method designed for federated LLM fine-tuning. By limiting fine-tuning to a transferable, static, extremely sparse subset of parameters, Meerkat achieves remarkable communication efficiency, enabling cost-effective high-frequency synchronization. With theoretical analysis and experiments, we show that this high-frequency communication effectively mitigates Non-IID data challenges and leads to superior performance compared to full-parameter ZO. Furthermore, experiment results show that Meerkat outperforms existing sparsity baselines with better performance at the same communication frequency. To further handle Non-IID drift, Meerkat leverages traceable local updates and forms a virtual path for each client. This virtual path mechanism reveals the GradIP phenomenon: the inner products between LLM pre-training gradients maintained by server and client gradients estimated via ZO converges for extreme Non-IID clients but oscillates for IID ones. This distinct behavior provides a signal for identifying clients with extreme data heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP trajectories to identify extreme Non-IID clients and applies early stopping to enhance aggregated model quality. Experiments confirm that Meerkat and Meerkat-vp significantly improve the efficiency and effectiveness of ZO federated LLM fine-tuning.

### 摘要
联邦学习支持在去中心化的非独立同分布（Non-IID）客户端间协同微调大语言模型（LLM），但此类模型的庞大规模参数会引发显著的内存与通信挑战。本研究提出Meerkat，一种专为联邦LLM微调设计的稀疏零阶优化（ZO）方法。通过将微调限制在可迁移、静态且极度稀疏的参数子集上，Meerkat实现了卓越的通信效率，支持高性价比的高频同步。理论分析与实验表明，相比全参数ZO方法，这种高频通信能有效缓解Non-IID数据挑战并获得更优性能。此外，实验结果显示Meerkat在相同通信频率下以更佳表现超越现有稀疏基线方法。为应对Non-IID数据漂移，Meerkat利用可追踪的本地更新并为每个客户端构建虚拟路径。该虚拟路径机制揭示了GradIP现象：服务器维护的LLM预训练梯度与客户端通过ZO估计的梯度间内积，在极端Non-IID客户端上收敛而在IID客户端上振荡。这一差异行为为识别极端数据异构客户端提供了信号。基于此信号，我们提出Meerkat-vp通过分析GradIP轨迹识别极端Non-IID客户端，并采用早停机制提升聚合模型质量。实验证实Meerkat与Meerkat-vp能显著提升ZO联邦LLM微调的效率与效果。

---

## [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)

### Abstract
arXiv:2506.03598v1 Announce Type: cross 
Abstract: Using the best Text-to-SQL methods in resource-constrained environments is challenging due to their reliance on resource-intensive open-source models. This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to bridge the gap between resource-efficient small open-source models and the powerful capabilities of large closed-source models for Text-to-SQL translation. Our method decomposes the task into schema filtering, retrieval-augmented text-to-SQL generation based on in-context examples, and prompt-driven schema linking and SQL generation. To improve schema selection accuracy, we fine-tune large language models. Crucially, we also explore the impact of prompt engineering throughout the process, leveraging Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly enhance the model's reasoning for accurate SQL generation. Comprehensive evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.

### 摘要
在资源受限环境中使用最优的文本到SQL方法具有挑战性，因其依赖资源密集型开源模型。本文提出Auto Prompt SQL（AP-SQL）——一种创新架构，旨在弥合高效能小型开源模型与强大闭源模型在文本到SQL转换领域的性能差距。该方法将任务分解为模式过滤、基于上下文示例的检索增强型文本到SQL生成，以及提示驱动的模式链接与SQL生成。为提高模式选择准确率，我们对大语言模型进行微调。关键的是，我们还探究了提示工程在整个流程中的影响，通过思维链（CoT）和图思维（GoT）模板显著增强模型推理能力以实现精准SQL生成。在Spider基准测试上的全面评估验证了AP-SQL的有效性。

---

## [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)

### Abstract
arXiv:2506.03541v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques--such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection--struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D&amp;R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin.

### 摘要
大型语言模型（LLMs）在知识密集型与复杂推理任务中不断刷新性能标准，但其高计算需求限制了广泛应用。尽管将大模型蒸馏为小模型提供了可持续解决方案，现有技术——如静态知识蒸馏、耗费资源的人类反馈强化学习或有限的自反思机制——难以实现显著且持久的性能提升。本文提出一种新颖的"辩论与反思"（D&amp;R）框架，通过组织小模型与强教师模型之间的多轮辩论，获取可操作反馈（如错误分析、纠正策略）以指导学生模型。进一步，我们引入树状结构直接偏好优化（T-DPO）高效利用辩论日志，将交互组织为层级格式进行有效训练。跨多领域NLP基准的实证评估表明，该方法显著提升了小模型的准确性、鲁棒性和泛化能力，以显著优势超越传统基线方法。

---

## [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)

### Abstract
arXiv:2506.03576v1 Announce Type: cross 
Abstract: Recent advances in knowledge representation learning (KRL) highlight the urgent necessity to unify symbolic knowledge graphs (KGs) with language models (LMs) for richer semantic understanding. However, existing approaches typically prioritize either graph structure or textual semantics, leaving a gap: a unified framework that simultaneously captures global KG connectivity, nuanced linguistic context, and discriminative reasoning semantics. To bridge this gap, we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues from KGs with the semantic expressiveness of generative transformers. KG-BiLM incorporates three key components: (i) Bidirectional Knowledge Attention, which removes the causal mask to enable full interaction among all tokens and entities; (ii) Knowledge-Masked Prediction, which encourages the model to leverage both local semantic contexts and global graph connectivity; and (iii) Contrastive Graph Semantic Aggregation, which preserves KG structure via contrastive alignment of sampled sub-graph representations. Extensive experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong baselines in link prediction, especially on large-scale graphs with complex multi-hop relations - validating its effectiveness in unifying structural information and textual semantics.

### 摘要
知识表示学习（KRL）领域的最新进展凸显了将符号化知识图谱（KGs）与语言模型（LMs）相统一以实现更丰富语义理解的迫切需求。然而现有方法通常侧重图结构或文本语义的单一维度，导致关键缺口：一个能同时捕捉全局KG连通性、细粒度语言上下文及判别性推理语义的统一框架。为此，我们提出KG-BiLM——一种双向LM框架，通过融合KG的结构线索与生成式Transformer的语义表达能力来实现这一目标。该框架包含三个核心组件：（1）双向知识注意力机制，通过消除因果掩码实现所有标记与实体的完全交互；（2）知识掩码预测任务，促使模型同时利用局部语义上下文和全局图连接；（3）对比式图语义聚合模块，通过采样子图表征的对比对齐来保持KG结构。在标准基准上的大量实验表明，KG-BiLM在链接预测任务上显著优于现有基线，尤其在具有复杂多跳关系的大规模图谱上——验证了其统一结构信息与文本语义的有效性。

---

## [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)

### Abstract
arXiv:2506.03566v1 Announce Type: cross 
Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.

### 摘要
推测解码技术通过使用小型草稿模型预测多个标记，并由大型目标模型并行验证这些标记，从而加速大语言模型（LLM）的推理过程。近期研究利用目标模型的隐藏状态来提升草稿模型的预测准确性。然而，现有方法因草稿模型生成特征中的误差累积，导致后续位置草稿标记预测质量下降。本文提出位置专家（PosS）方法，该方法包含多个专攻特定位置的专业草稿层，用于生成指定位置的标记。由于每个专家只需专注于处理特定程度的草稿模型特征偏差，位置专家显著提高了每轮草拟过程中后续位置的标记接受率。在Llama-3-8B-Instruct和Llama-2-13B-chat模型上，针对六个数据集的实验结果表明，PosS在平均接受长度和加速比方面均有效超越基线方法。代码库已发布于https://github.com/shrango/PosS。

---

## [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)

### Abstract
arXiv:2506.03501v1 Announce Type: cross 
Abstract: Content creation has dramatically progressed with the rapid advancement of large language models like ChatGPT and Claude. While this progress has greatly enhanced various aspects of life and work, it has also negatively affected certain areas of society. A recent survey revealed that nearly 30% of college students use generative AI to help write academic papers and reports. Most countermeasures treat the detection of AI-generated text as a binary classification task and thus lack robustness. This approach overlooks human involvement in the generation of content even though human-machine collaboration is becoming mainstream. Besides generating entire texts, people may use machines to complete or revise texts. Such human involvement varies case by case, which makes binary classification a less than satisfactory approach. We refer to this situation as participation detection obfuscation. We propose using BERTScore as a metric to measure human involvement in the generation process and a multi-task RoBERTa-based regressor trained on a token classification task to address this problem. To evaluate the effectiveness of this approach, we simulated academic-based scenarios and created a continuous dataset reflecting various levels of human involvement. All of the existing detectors we examined failed to detect the level of human involvement on this dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor mean squared error of 0.004). Moreover, it demonstrated some generalizability across generative models. Our code is available at https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

### 摘要
随着ChatGPT、Claude等大型语言模型的快速发展，内容创作领域取得了显著进展。这一进步虽然极大改善了生活与工作的诸多方面，但也对社会某些领域产生了负面影响。最新调查显示，近30%的大学生使用生成式AI辅助撰写学术论文和报告。现有检测方法大多将AI生成文本识别视为二元分类任务，因而缺乏鲁棒性。这种方法忽视了内容生成过程中的人为参与，尽管人机协作已成为主流趋势。除生成完整文本外，人们可能利用机器完成或修改文本。此类人为参与程度因案例而异，使得二元分类方法难以令人满意。我们将此现象称为参与检测混淆。为此，我们提出使用BERTScore作为衡量生成过程中人为参与程度的指标，并基于多任务RoBERTa回归器（通过词元分类任务训练）来解决该问题。为评估方法有效性，我们模拟了学术场景并创建了反映不同人为参与程度的连续数据集。现有检测器在该数据集上均未能有效识别人为参与程度，而我们的方法取得了成功（F1得分0.9423，回归器均方误差0.004）。此外，该方法在不同生成模型间展现出一定的泛化能力。代码已开源：https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

---

## [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627)

### Abstract
arXiv:2506.03627v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various tasks by effectively utilizing a prompting strategy. However, they are highly sensitive to input perturbations, such as typographical errors or slight character order errors, which can substantially degrade their performance. Despite advances in prompting techniques, developing a prompting strategy that explicitly mitigates the negative impact of such perturbations remains an open challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a novel prompting strategy specifically designed to enhance the robustness of LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error Correction stage, RoP applies diverse perturbation methods to generate adversarial examples, which are then used to construct prompts that automatically correct input errors. In the Guidance stage, RoP generates an optimal guidance prompting based on the corrected input, steering the model toward more robust and accurate inferences. Through comprehensive experiments spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate that RoP significantly improves LLMs' robustness against adversarial perturbations. Notably, it maintains model accuracy with only minimal degradation compared to clean input scenarios, thereby establishing RoP as a practical and effective approach for enhancing LLM robustness in real-world applications.

### 摘要
大型语言模型（LLMs）通过有效利用提示策略，已在各类任务中展现出卓越性能。然而，其对输入扰动（如拼写错误或轻微字符顺序错误）高度敏感，这些扰动会显著降低模型性能。尽管提示技术不断进步，但开发一种能显式减轻此类扰动负面影响的提示策略仍是一个开放挑战。为填补这一空白，我们提出"提示鲁棒性"（RoP）——一种专为增强LLMs鲁棒性而设计的新型提示策略。RoP包含两个阶段：错误校正与引导。在错误校正阶段，RoP应用多种扰动方法生成对抗样本，进而构建能自动修正输入错误的提示；在引导阶段，RoP基于校正后的输入生成最优引导提示，使模型产生更鲁棒且准确的推理。通过在算术、常识和逻辑推理任务上的全面实验，我们证明RoP能显著提升LLMs对抗扰动的鲁棒性。值得注意的是，相较于纯净输入场景，该方法仅导致模型准确率轻微下降，从而确立了RoP作为增强实际应用中LLM鲁棒性的实用有效方法。

---

## [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)

### Abstract
arXiv:2506.03637v1 Announce Type: cross 
Abstract: Reward Models, essential for guiding Large Language Model optimization, are typically trained on fixed preference datasets, resulting in rigid alignment to single, implicit preference distributions. This prevents adaptation to diverse real-world needs-from conciseness in one task to detailed explanations in another. The standard practice of collecting task-specific preference data and retraining reward models is resource-intensive, often producing biased rewards, and limits practical application. We introduce generalizable, principle-following reward models. We propose that RMs should understand and adhere to dynamically provided natural language specifications of reward principles, similar to instruction-following in LLMs. To measure this capability, we develop RABench, a comprehensive benchmark for RMs focusing on generalization across diverse principles. Evaluations on RABench reveal poor generalization of current RMs. As a solution, we present RewardAnything, a novel RM designed and trained to explicitly follow natural language principles. We achieve SotA performance with RewardAnything in traditional RM benchmark simply by specifying a well-defined principle, and results on RABench show we excel in adapting to novel principles without retraining. Furthermore, RewardAnything integrates seamlessly with existing RLHF methods and we show by a case study on how to automatically and efficiently align LLMs with only natural language principles.

### 摘要
奖励模型作为指导大型语言模型优化的关键组件，通常基于固定偏好数据集进行训练，导致其僵化地贴合单一隐性偏好分布。这种模式无法适应多样化的现实需求——从某些任务要求简洁性到其他任务需要详细解释。当前通过收集任务特定偏好数据并重新训练奖励模型的标准做法不仅耗费资源，易产生有偏奖励，还限制了实际应用。我们提出具有泛化能力的、遵循原则的奖励模型新范式，主张奖励模型应当理解并遵从动态提供的自然语言奖励原则说明，类似于大语言模型中的指令遵循机制。为评估该能力，我们开发了RABench基准测试，专注于衡量奖励模型跨多样原则的泛化性能。在RABench上的测试表明现有奖励模型泛化能力不足。作为解决方案，我们提出RewardAnything——一种创新设计的奖励模型，通过显式遵循自然语言原则进行训练。仅需指定明确定义的原则，RewardAnything即在传统奖励模型基准测试中达到最先进性能；在RABench上的结果表明该模型无需重新训练即可出色适应新原则。此外，RewardAnything可与现有RLHF方法无缝集成，我们通过案例研究展示了如何仅用自然语言原则自动高效地对齐大型语言模型。

---

## [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)

### Abstract
arXiv:2506.03614v1 Announce Type: cross 
Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions "safe," VLMs may later describe, the full image or a text reference to the scene, as "safe." We define the core ability of VLMs enabling this attack as $\textit&#123;visual stitching&#125;$ -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each $(\texttt&#123;image&#125;, \texttt&#123;ID&#125;)$ pair into $\&#123;(\texttt&#123;patch&#125;, \texttt&#123;ID&#125;)\&#125;$ pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching.

### 摘要
缓解视觉-语言模型（VLM）风险的一种方法是清除训练数据中的危险样本。然而，当有害图像被分割成看似无害的小块并分散于多个训练样本时，这种数据过滤机制极易被绕过。此时VLM可能在训练过程中学会拼接这些碎片，并在推理阶段根据完整图像或文本提示生成有害响应。例如，若模型在训练时接触过带有"安全"描述的零散血腥场景图像块，后续面对完整场景图像或相关文本提示时，可能将其描述为"安全"。我们将VLM实现此类攻击的核心能力定义为$\textit&#123;视觉拼接&#125;$——即整合分散于多个共享相同文本描述的训练样本中的视觉信息的能力。本研究首先在三个标注唯一合成ID的数据集上验证了常见开源VLM的视觉拼接能力：将每个$(\texttt&#123;图像&#125;, \texttt&#123;ID&#125;)$对按不同粒度分割为$\&#123;(\texttt&#123;图像块&#125;, \texttt&#123;ID&#125;)\&#125;$对进行微调，发现调优后的模型能通过完整图像或文本提示准确输出对应ID。基于此，我们模拟上述对抗性数据投毒场景：使用危险图像块并将ID替换为"安全"或"危险"等文本描述，证明有害内容如何通过碎片化规避审核，并最终借由视觉拼接重构，从而造成严重的VLM安全隐患。代码见https://github.com/ZHZisZZ/visual-stitching。

---

## [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)

### Abstract
arXiv:2506.03785v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown to be effective evaluators across various domains such as machine translations or the scientific domain. Current LLM-as-a-Judge approaches rely mostly on individual assessments or a single round of pairwise assessments, preventing the judge LLM from developing a global ranking perspective. To address this, we present Knockout Assessment, an LLM-asa Judge method using a knockout tournament system with iterative pairwise comparisons. Experiments across three LLMs on two datasets show that knockout assessment improves scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations, aligning LLM assessments more closely with human scoring.

### 摘要
大语言模型（LLMs）已被证明在机器翻译或科学领域等多个领域能有效担任评估者。当前基于LLM的评判方法主要依赖个体评估或单轮成对评估，导致评判LLM无法形成全局排名视角。为解决这一问题，我们提出淘汰赛评估法——一种采用迭代成对比较的淘汰赛系统的LLM评判方法。在三个LLM上对两个数据集的实验表明，淘汰赛评估提高了评分准确性，在大学水平考试评分和机器翻译评估中，与专家评分的皮尔逊相关性平均提升0.07，使LLM评估结果更贴近人类评分。

---

## [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/abs/2506.03723)

### Abstract
arXiv:2506.03723v1 Announce Type: cross 
Abstract: Uncertainty calibration is essential for the safe deployment of large language models (LLMs), particularly when users rely on verbalized confidence estimates. While prior work has focused on classifiers or short-form generation, confidence calibration for chain-of-thought (CoT) reasoning remains largely unexplored. Surprisingly, we find that supervised fine-tuning with scalar confidence labels alone suffices to elicit self-verification behavior of language models, without any explicit reasoning supervision or reinforcement learning-based rewards. Despite being trained only to produce a verbalized confidence score without any self-verifying examples, the model learns to generate longer and self-checking responses for low-confidence queries while providing more concise answers for high-confidence ones. We further propose a simple rethinking method that boosts performance via test-time scaling based on calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning improves both calibration and accuracy, while also enhancing interpretability by aligning the model's reasoning path with its confidence.

### 摘要
不确定性校准对于大型语言模型（LLM）的安全部署至关重要，尤其是当用户依赖其口头表达的置信度估计时。尽管先前的研究主要集中在分类器或短文本生成任务上，但思维链（CoT）推理的置信度校准仍鲜有探索。令人惊讶的是，我们发现仅通过带有标量置信度标签的监督微调，就足以激发语言模型的自我验证行为，而无需任何显式的推理监督或基于强化学习的奖励机制。尽管模型仅被训练生成口头化置信度分数且未接触任何自我验证示例，它仍能学会对低置信度查询生成更长且包含自我检查的响应，同时对高置信度查询提供更简洁的答案。我们进一步提出一种简单的"再思考"方法，通过基于校准不确定性的测试时缩放来提升性能。在GSM8K及MATH-500、ARC-Challenge等保留推理任务上的实验表明，我们的置信度感知微调既改善了校准效果又提高了准确性，同时还通过使模型的推理路径与其置信度保持一致来增强可解释性。

---

## [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.03762)

### Abstract
arXiv:2506.03762v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.

### 摘要
大型语言模型（LLMs）显著推动了人工智能领域的发展，但其部署资源消耗巨大，这不仅源于模型参数量庞大，还因为推理过程中（键值）KV缓存占用大量内存。现有研究虽提出通过剔除不必要令牌来压缩KV缓存，但这些方法依赖累积注意力分数作为剔除标准以量化令牌重要性。我们发现累积注意力分数存在偏差，其数学期望值会随令牌位置后移而衰减，导致保留的令牌集中于初始位置，限制了模型获取全局上下文信息的能力。为解决该问题，我们提出自适应全局注意力KV缓存（AhaKV），该方法通过根据注意力分数信息熵的期望值自适应调整softmax缩放因子，有效消除累积注意力分数的偏差。为充分利用自注意力机制中的全局注意力信息，AhaKV引入先前研究忽视的值向量信息来优化自适应评分。理论分析表明我们的方法能有效减少偏差。在固定缓存预算下对多种模型部署AhaKV的实验表明，该方法成功缓解了注意力偏差，保留全局上下文中的关键令牌，在多项基准任务上取得了优于相关工作的性能。

---

## [From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation](https://arxiv.org/abs/2506.03801)

### Abstract
arXiv:2506.03801v1 Announce Type: cross 
Abstract: Traditional Business Process Management (BPM) struggles with rigidity, opacity, and scalability in dynamic environments while emerging Large Language Models (LLMs) present transformative opportunities alongside risks. This paper explores four real-world use cases that demonstrate how LLMs, augmented with trustworthy process intelligence, redefine process modeling, prediction, and automation. Grounded in early-stage research projects with industrial partners, the work spans manufacturing, modeling, life-science, and design processes, addressing domain-specific challenges through human-AI collaboration. In manufacturing, an LLM-driven framework integrates uncertainty-aware explainable Machine Learning (ML) with interactive dialogues, transforming opaque predictions into auditable workflows. For process modeling, conversational interfaces democratize BPMN design. Pharmacovigilance agents automate drug safety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable textile design employs multi-agent systems to navigate regulatory and environmental trade-offs. We intend to examine tensions between transparency and efficiency, generalization and specialization, and human agency versus automation. By mapping these trade-offs, we advocate for context-sensitive integration prioritizing domain needs, stakeholder values, and iterative human-in-the-loop workflows over universal solutions. This work provides actionable insights for researchers and practitioners aiming to operationalize LLMs in critical BPM environments.

### 摘要
传统业务流程管理（BPM）在动态环境中面临僵化性、不透明性和可扩展性挑战，而新兴大语言模型（LLMs）则带来变革机遇与潜在风险。本文通过四个现实用例，探讨LLMs如何结合可信流程智能重构流程建模、预测与自动化。基于与工业合作伙伴的早期研究项目，研究涵盖制造、建模、生命科学和设计领域，通过人机协作解决特定领域难题。在制造业中，LLM驱动框架将不确定性感知的可解释机器学习（ML）与交互式对话相结合，将不透明预测转化为可审计工作流；流程建模方面，对话式界面实现了BPMN设计的平民化；药物警戒代理通过知识图谱增强的LLMs实现药品安全监测自动化；可持续纺织品设计则采用多智能体系统权衡法规与环境因素。我们重点审视透明度与效率、通用性与专业性、人类决策与自动化之间的张力，通过厘清这些权衡关系，主张优先考虑领域需求、利益相关者价值和"人在回路"迭代工作流的上下文敏感集成，而非通用解决方案。本研究为关键BPM环境中LLMs的实际应用提供了可操作的见解。

---

## [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)

### Abstract
arXiv:2506.03880v1 Announce Type: cross 
Abstract: The rapid advancements in large language models (LLMs) have led to the emergence of routing techniques, which aim to efficiently select the optimal LLM from diverse candidates to tackle specific tasks, optimizing performance while reducing costs. Current LLM routing methods are limited in effectiveness due to insufficient exploration of the intrinsic connection between user queries and the characteristics of LLMs. To address this issue, in this paper, we present RadialRouter, a novel framework for LLM routing which employs a lightweight Transformer-based backbone with a radial structure named RadialFormer to articulate the query-LLMs relationship. The optimal LLM selection is performed based on the final states of RadialFormer. The pipeline is further refined by an objective function that combines Kullback-Leibler divergence with the query-query contrastive loss to enhance robustness. Experimental results on RouterBench show that RadialRouter significantly outperforms existing routing methods by 9.2\% and 5.8\% in the Balance and Cost First scenarios, respectively. Additionally, its adaptability toward different performance-cost trade-offs and the dynamic LLM pool demonstrates practical application potential.

### 摘要
大型语言模型（LLM）的快速发展催生了路由技术的兴起，该技术旨在从多样化的候选模型中高效选择最优LLM以处理特定任务，从而在优化性能的同时降低成本。现有LLM路由方法因未能充分探索用户查询与LLM特性之间的内在关联而效果受限。为此，本文提出RadialRouter——一种新型LLM路由框架，其采用基于轻量级Transformer的径向结构主干网络（RadialFormer）来显式建模查询-LLM关系，并依据RadialFormer的最终状态执行最优LLM选择。该流程通过结合Kullback-Leibler散度与查询-查询对比损失的损失函数进一步优化，以增强鲁棒性。RouterBench上的实验结果表明：RadialRouter在平衡优先和成本优先场景下分别以9.2%和5.8%的优势显著超越现有路由方法。此外，其对不同性能-成本权衡策略及动态LLM池的适应性，展现出实际应用潜力。

---

## [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/abs/2506.04044)

### Abstract
arXiv:2506.04044v1 Announce Type: cross 
Abstract: This paper describes LIBU (LoRA enhanced influence-based unlearning), an algorithm to solve the task of unlearning - removing specific knowledge from a large language model without retraining from scratch and compromising its overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models). The algorithm combines classical \textit&#123;influence functions&#125; to remove the influence of the data from the model and \textit&#123;second-order optimization&#125; to stabilize the overall utility. Our experiments show that this lightweight approach is well applicable for unlearning LLMs in different kinds of task.

### 摘要
本文介绍LIBU（基于LoRA增强的影响力遗忘算法），一种用于解决知识遗忘任务的算法——在不从头训练且不影响模型整体效用的前提下，从大语言模型中移除特定知识（SemEval-2025任务4：从大语言模型中遗忘敏感内容）。该算法结合经典影响力函数来消除数据对模型的影响，并采用二阶优化来稳定整体性能。实验表明，这种轻量级方法能有效适用于不同类型任务中的大语言模型遗忘。

---

## [Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems](https://arxiv.org/abs/2506.04038)

### Abstract
arXiv:2506.04038v1 Announce Type: cross 
Abstract: Developing safety-critical automotive software presents significant challenges due to increasing system complexity and strict regulatory demands. This paper proposes a novel framework integrating Generative Artificial Intelligence (GenAI) into the Software Development Lifecycle (SDLC). The framework uses Large Language Models (LLMs) to automate code generation in languages such as C++, incorporating safety-focused practices such as static verification, test-driven development and iterative refinement. A feedback-driven pipeline ensures the integration of test, simulation and verification for compliance with safety standards. The framework is validated through the development of an Adaptive Cruise Control (ACC) system. Comparative benchmarking of LLMs ensures optimal model selection for accuracy and reliability. Results demonstrate that the framework enables automatic code generation while ensuring compliance with safety-critical requirements, systematically integrating GenAI into automotive software engineering. This work advances the use of AI in safety-critical domains, bridging the gap between state-of-the-art generative models and real-world safety requirements.

### 摘要
开发安全关键型汽车软件面临着系统复杂性日益增加和严格法规要求的重大挑战。本文提出了一种将生成式人工智能（GenAI）集成到软件开发生命周期（SDLC）中的创新框架。该框架利用大语言模型（LLMs）实现C++等语言的自动化代码生成，同时融合静态验证、测试驱动开发和迭代优化等以安全为核心的实践方法。通过反馈驱动的流程设计，确保测试、仿真与验证环节的整合以满足安全标准要求。本研究以自适应巡航控制（ACC）系统开发为例验证了该框架的有效性。通过对不同LLMs进行对比基准测试，实现了精度与可靠性最优的模型选择。结果表明，该框架能够在确保符合安全关键要求的前提下实现自动化代码生成，系统性地将GenAI融入汽车软件工程领域。本研究成果推动了人工智能在安全关键领域的应用，弥合了前沿生成模型与实际安全需求之间的鸿沟。

---

## [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)

### Abstract
arXiv:2506.03930v1 Announce Type: cross 
Abstract: Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation.

### 摘要
大型语言模型（LLMs）在执行可视化任务（如绘制图表、曲线图）时常常表现不佳，这类任务的成功既取决于代码正确性，也依赖于视觉语义。现有的指令调优数据集缺乏基于执行的监督，且对迭代式代码修正的支持有限，导致生成的图表脆弱且不可靠。我们提出了VisCode-200K，一个面向Python可视化及自我修正的大规模指令调优数据集。该数据集包含超过20万个样本，来源包括：（1）开源仓库中经过验证的绘图代码，配以自然语言指令和渲染结果；（2）来自Code-Feedback的4.5万轮多回合修正对话，使模型能够利用运行时反馈修正错误代码。我们在VisCode-200K上对Qwen2.5-Coder-Instruct进行微调，创建了VisCoder，并在PandasPlotBench上评估其性能。VisCoder显著优于强大的开源基线模型，并接近GPT-4o-mini等专有模型的水平。我们进一步采用自调试评估协议来检验迭代修复能力，证明了反馈驱动学习对于生成可执行且视觉精确代码的益处。

---

## [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)

### Abstract
arXiv:2506.04039v1 Announce Type: cross 
Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive capabilities across multiple tasks. However, their trustworthiness is often challenged by hallucinations, which can be attributed to the modality misalignment and the inherent hallucinations of their underlying Large Language Models (LLMs) backbone. Existing preference alignment methods focus on aligning model responses with human preferences while neglecting image-text modality alignment, resulting in over-reliance on LLMs and hallucinations. In this paper, we propose Entity-centric Multimodal Preference Optimization (EMPO), which achieves enhanced modality alignment than existing human preference alignment methods. Besides, to overcome the scarcity of high-quality multimodal preference data, we utilize open-source instruction datasets to automatically construct high-quality preference data across three aspects: image, instruction, and response. Experiments on two human preference datasets and five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO, e.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on MM-HalBench.

### 摘要
大型视觉语言模型（LVLMs）在多项任务中展现出卓越性能，但其可信度常因幻觉问题受到质疑，这主要源于模态失准及底层大型语言模型（LLMs）固有的幻觉特性。现有偏好对齐方法仅关注模型响应与人类偏好的匹配，却忽视了图像-文本模态的对齐，导致对LLMs的过度依赖及幻觉现象。本文提出以实体为中心的多模态偏好优化方法（EMPO），其模态对齐效果优于现有人类偏好对齐方法。此外，针对高质量多模态偏好数据稀缺的问题，我们利用开源指令数据集，自动构建涵盖图像、指令和响应三个维度的高质量偏好数据。在两个人机偏好数据集和五个多模态幻觉基准测试上的实验表明，EMPO能显著降低幻觉率，例如在Object-HalBench上降低85.9%，在MM-HalBench上降低49.8%。

---

## [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)

### Abstract
arXiv:2506.04043v1 Announce Type: cross 
Abstract: Automated counter-narratives (CN) offer a promising strategy for mitigating online hate speech, yet concerns about their affective tone, accessibility, and ethical risks remain. We propose a framework for evaluating Large Language Model (LLM)-generated CNs across four dimensions: persona framing, verbosity and readability, affective tone, and ethical robustness. Using GPT-4o-Mini, Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting strategies on the MT-Conan and HatEval datasets. Our findings reveal that LLM-generated CNs are often verbose and adapted for people with college-level literacy, limiting their accessibility. While emotionally guided prompts yield more empathetic and readable responses, there remain concerns surrounding safety and effectiveness.

### 摘要
自动化反叙事（CN）为缓解网络仇恨言论提供了一种前景广阔的解决方案，但其情感基调、可及性和伦理风险仍存在隐忧。本研究提出一个评估框架，从四个维度对大型语言模型（LLM）生成的反叙事进行测评：人格框架、冗长性与可读性、情感基调及伦理稳健性。通过使用GPT-4o-Mini、Cohere的CommandR-7B和Meta的LLaMA 3.1-70B模型，我们在MT-Conan和HatEval数据集上评估了三种提示策略。研究发现，LLM生成的反叙事普遍冗长且仅适合具有大学文化水平的读者，可及性受限。虽然情感引导型提示能产生更具同理心且可读性更强的回应，但在安全性和有效性方面仍存在隐患。

---

## [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/abs/2506.04050)

### Abstract
arXiv:2506.04050v1 Announce Type: cross 
Abstract: Generative models, especially large language models (LLMs), have shown remarkable progress in producing text that appears human-like. However, they often exhibit patterns that make their output easier to detect than text written by humans. In this paper, we investigate how explainable AI (XAI) methods can be used to reduce the detectability of AI-generated text (AIGT) while also introducing a robust ensemble-based detection approach. We begin by training an ensemble classifier to distinguish AIGT from human-written text, then apply SHAP and LIME to identify tokens that most strongly influence its predictions. We propose four explainability-based token replacement strategies to modify these influential tokens. Our findings show that these token replacement approaches can significantly diminish a single classifier's ability to detect AIGT. However, our ensemble classifier maintains strong performance across multiple languages and domains, showing that a multi-model approach can mitigate the impact of token-level manipulations. These results show that XAI methods can make AIGT harder to detect by focusing on the most influential tokens. At the same time, they highlight the need for robust, ensemble-based detection strategies that can adapt to evolving approaches for hiding AIGT.

### 摘要
生成模型，尤其是大语言模型（LLM），在生成类人文本方面展现出显著进展。然而，其输出往往存在可识别模式，使得检测难度低于人类撰写文本。本研究探讨如何利用可解释人工智能（XAI）方法降低AI生成文本（AIGT）的可检测性，同时提出一种基于集成学习的鲁棒检测方案。我们首先训练集成分类器区分AIGT与人类文本，随后应用SHAP和LIME方法识别对预测影响最大的关键词元。基于此，我们提出四种基于可解释性的词元替换策略来修改这些关键词元。实验表明，这些替换方法能显著削弱单一分类器的AIGT检测能力。但我们的集成分类器在跨语言、跨领域场景中保持强劲性能，证明多模型方法可有效抵御词元级修改的影响。这些结果表明：XAI方法通过聚焦最具影响力的词元，能够提升AIGT的隐蔽性；同时也揭示需要采用能适应AIGT隐匿技术演进的、基于集成学习的鲁棒检测策略。

---

## [Privacy and Security Threat for OpenAI GPTs](https://arxiv.org/abs/2506.04036)

### Abstract
arXiv:2506.04036v1 Announce Type: cross 
Abstract: Large language models (LLMs) demonstrate powerful information handling capabilities and are widely integrated into chatbot applications. OpenAI provides a platform for developers to construct custom GPTs, extending ChatGPT's functions and integrating external services. Since its release in November 2023, over 3 million custom GPTs have been created. However, such a vast ecosystem also conceals security and privacy threats. For developers, instruction leaking attacks threaten the intellectual property of instructions in custom GPTs through carefully crafted adversarial prompts. For users, unwanted data access behavior by custom GPTs or integrated third-party services raises significant privacy concerns. To systematically evaluate the scope of threats in real-world LLM applications, we develop three phases instruction leaking attacks target GPTs with different defense level. Our widespread experiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are vulnerable to instruction leaking attacks via one or more adversarial prompts, and half of the remaining GPTs can also be attacked through multiround conversations. We also developed a framework to assess the effectiveness of defensive strategies and identify unwanted behaviors in custom GPTs. Our findings show that 77.5% of custom GPTs with defense strategies are vulnerable to basic instruction leaking attacks. Additionally, we reveal that 738 custom GPTs collect user conversational information, and identified 8 GPTs exhibiting data access behaviors that are unnecessary for their intended functionalities. Our findings raise awareness among GPT developers about the importance of integrating specific defensive strategies in their instructions and highlight users' concerns about data privacy when using LLM-based applications.

### 摘要
大型语言模型（LLMs）展现出强大的信息处理能力，被广泛集成至聊天机器人应用中。OpenAI为开发者提供了构建自定义GPT的平台，以扩展ChatGPT功能并整合外部服务。自2023年11月发布以来，已创建超过300万个自定义GPT。然而，如此庞大的生态系统也潜藏着安全与隐私威胁。对开发者而言，指令泄露攻击通过精心设计的对抗性提示，威胁自定义GPT中指令的知识产权；对用户而言，自定义GPT或集成第三方服务的不必要数据访问行为引发了重大隐私隐忧。为系统评估现实LLM应用中的威胁范围，我们开发了针对不同防御等级GPT的三阶段指令泄露攻击方法。通过对10,000个真实自定义GPT的大规模实验发现，超过98.8%的GPT易受一种或多种对抗性提示的指令泄露攻击，剩余GPT中半数亦可通过多轮对话被攻破。我们还构建了评估防御策略有效性及检测自定义GPT异常行为的框架。研究表明，77.5%采用防御策略的自定义GPT仍无法抵御基础指令泄露攻击。此外，我们发现738个自定义GPT会收集用户对话信息，并识别出8个GPT存在与功能需求无关的数据访问行为。这些发现警示GPT开发者需重视在指令中整合针对性防御策略，同时揭示了用户在使用基于LLM的应用时对数据隐私的合理关切。

---

## [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/abs/2506.04089)

### Abstract
arXiv:2506.04089v1 Announce Type: cross 
Abstract: As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.

### 摘要
作为具身智能体的一部分，大型语言模型（LLMs）通常用于根据用户的自然语言指令进行行为规划。然而，在现实环境中处理模糊指令仍是LLMs面临的挑战。目前已有多种任务模糊性检测方法被提出，但由于这些方法在不同数据集上进行测试且缺乏统一基准，难以进行有效比较。为此，我们提出AmbiK（厨房环境模糊任务数据集）——一个完全文本化的厨房环境机器人指令模糊数据集。该数据集在LLMs辅助下收集并经人工验证，包含1000对模糊任务及其明确对应版本，按模糊类型（人类偏好、常识知识、安全性）分类，同时提供环境描述、澄清问题与答案、用户意图及任务计划，共计2000项任务。我们期望AmbiK能帮助研究者对模糊性检测方法进行统一比较。数据集地址：https://github.com/cog-model/AmbiK-dataset。

---

## [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)

### Abstract
arXiv:2506.04079v1 Announce Type: cross 
Abstract: This report presents EuroLLM-9B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-9B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. We describe the pre-training data collection and filtering pipeline, including the creation of EuroFilter, an AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a novel synthetic dataset for post-training that enhances language coverage for European languages. Evaluation results demonstrate EuroLLM-9B's competitive performance on multilingual benchmarks and machine translation tasks, establishing it as the leading open European-made LLM of its size. To support open research and adoption, we release all major components of this work, including the base and instruction-tuned models, the EuroFilter classifier, and the synthetic post-training dataset.

### 摘要
本报告介绍了EuroLLM-9B大语言模型，该模型从零开始训练，旨在通过覆盖欧盟全部24种官方语言及11种附加语言来满足欧洲公民的需求。EuroLLM解决了现有开源大语言模型中欧洲语言代表性不足和服务欠缺的问题。我们全面概述了EuroLLM-9B的开发过程，包括分词器设计、架构规格、数据筛选及训练流程。详细阐述了预训练数据收集与过滤管道，其中包含基于人工智能的多语言过滤器EuroFilter的创建，以及用于增强欧洲语言覆盖度的新型合成后训练数据集EuroBlocks-Synthetic的设计。评估结果表明，EuroLLM-9B在多语言基准测试和机器翻译任务中展现出竞争优势，使其成为同规模欧洲本土开源LLM的领先代表。为支持开放研究与采用，我们公开了本项目的所有主要组件，包括基础模型、指令调优模型、EuroFilter分类器以及合成后训练数据集。

---

## [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)

### Abstract
arXiv:2506.04078v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) in medicine is crucial because medical applications require high accuracy with little room for error. Current medical benchmarks have three main types: medical exam-based, comprehensive medical, and specialized assessments. However, these benchmarks have limitations in question design (mostly multiple-choice), data sources (often not derived from real clinical scenarios), and evaluation methods (poor assessment of complex reasoning). To address these issues, we present LLMEval-Med, a new benchmark covering five core medical areas, including 2,996 questions created from real-world electronic health records and expert-designed clinical scenarios. We also design an automated evaluation pipeline, incorporating expert-developed checklists into our LLM-as-Judge framework. Furthermore, our methodology validates machine scoring through human-machine agreement analysis, dynamically refining checklists and prompts based on expert feedback to ensure reliability. We evaluate 13 LLMs across three categories (specialized medical models, open-source models, and closed-source models) on LLMEval-Med, providing valuable insights for the safe and effective deployment of LLMs in medical domains. The dataset is released in https://github.com/llmeval/LLMEval-Med.

### 摘要
评估大型语言模型（LLMs）在医学领域的应用至关重要，因为医疗场景要求极高的准确性且容错空间极小。现有医学基准测试主要分为三类：基于医学考试的评估、综合性医学测评以及专项能力测试。然而，这些基准在问题设计（多为选择题）、数据来源（通常非真实临床场景）和评估方法（复杂推理能力评测不足）方面存在局限。为解决这些问题，我们提出LLMEval-Med新基准，涵盖五大核心医学领域，包含2,996道源自真实电子健康档案和专家设计临床情境的题目。我们同时开发了自动化评估流程，将专家制定的核查清单整合至LLM-as-Judge框架中。此外，通过人机评分一致性分析验证机器评分可靠性，并基于专家反馈动态优化核查清单与提示模板。我们在LLMEval-Med上评估了三大类13个LLM（专业医疗模型、开源模型和闭源模型），为医学领域安全有效部署LLM提供了重要参考。数据集已发布于https://github.com/llmeval/LLMEval-Med。

---

## [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)

### Abstract
arXiv:2506.04088v1 Announce Type: cross 
Abstract: Tabular reasoning involves multi-step information extraction and logical inference over tabular data. While recent advances have leveraged large language models (LLMs) for reasoning over structured tables, such high-quality textual representations are often unavailable in real-world settings, where tables typically appear as images. In this paper, we tackle the task of tabular reasoning from table images, leveraging privileged structured information available during training to enhance multimodal large language models (MLLMs). The key challenges lie in the complexity of accurately aligning structured information with visual representations, and in effectively transferring structured reasoning skills to MLLMs despite the input modality gap. To address these, we introduce TabUlar Reasoning with Bridged infOrmation (&#123;\sc Turbo&#125;), a new framework for multimodal tabular reasoning with privileged structured tables. &#123;\sc Turbo&#125; benefits from a structure-aware reasoning trace generator based on DeepSeek-R1, contributing to high-quality modality-bridged data. On this basis, &#123;\sc Turbo&#125; repeatedly generates and selects the advantageous reasoning paths, further enhancing the model's tabular reasoning ability. Experimental results demonstrate that, with limited ($9$k) data, &#123;\sc Turbo&#125; achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across multiple datasets.

### 摘要
表格推理涉及对表格数据进行多步骤信息提取和逻辑推断。尽管近期研究利用大语言模型（LLM）实现了结构化表格的推理，但在现实场景中，此类高质量文本表示往往不可得——表格通常以图像形式呈现。本文致力于解决表格图像的推理任务，通过利用训练期间可获取的特权结构化信息来增强多模态大语言模型（MLLM）。核心挑战在于：如何精准对齐结构化信息与视觉表征的复杂性，以及如何跨越输入模态差异将结构化推理能力有效迁移至MLLM。为此，我们提出基于桥接信息的表格推理框架（&#123;\sc Turbo&#125;），该创新框架利用特权结构化表格实现多模态表格推理。&#123;\sc Turbo&#125;通过基于DeepSeek-R1的结构感知推理轨迹生成器，构建高质量的模态桥接数据。在此基础上，&#123;\sc Turbo&#125;通过循环生成与筛选优势推理路径，进一步提升模型的表格推理能力。实验结果表明，在有限数据量（9k）条件下，&#123;\sc Turbo&#125;在多个数据集上实现了最先进性能（较先前SOTA提升7.2%）。

---

## [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)

### Abstract
arXiv:2506.04098v1 Announce Type: cross 
Abstract: We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning.

### 摘要
我们提出TextAtari基准测试，用于评估语言智能体在长达100,000步的超长程决策任务中的表现。通过将经典Atari游戏的视觉状态表征转化为丰富的文本描述，TextAtari构建了一个连接序列决策与自然语言处理的挑战性测试平台。该基准包含近100项不同复杂度、动作空间和规划跨度的任务，均通过无监督表征学习框架（AtariARI）以文本形式呈现。我们评估了三种开源大语言模型（Qwen2.5-7B、Gemma-7B和Llama3.1-8B）在三种智能体框架（零样本、少量样本思维链和反思推理）下的表现，以探究不同先验知识形式对这些长程挑战任务的影响。四种实验场景——基础场景、模糊场景、人工增强场景和参考基准场景——研究了语义理解、指令解析和专家示范对智能体决策的影响。研究结果揭示了语言智能体与人类玩家在复杂规划任务中存在的显著性能差距，凸显了智能体在数万步决策过程中面临的序列推理、状态追踪和战略规划等挑战。TextAtari提供了标准化评估协议、基线实现方案以及推动语言模型与规划交叉领域研究的框架体系。

---

## [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/abs/2506.04051)

### Abstract
arXiv:2506.04051v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) currently respond to every prompt. However, they can produce incorrect answers when they lack knowledge or capability -- a problem known as hallucination. We instead propose post-training an LLM to generate content only when confident in its correctness and to otherwise (partially) abstain. Specifically, our method, HALT, produces capability-aligned post-training data that encodes what the model can and cannot reliably generate. We generate this data by splitting responses of the pretrained LLM into factual fragments (atomic statements or reasoning steps), and use ground truth information to identify incorrect fragments. We achieve capability-aligned finetuning responses by either removing incorrect fragments or replacing them with "Unsure from Here" -- according to a tunable threshold that allows practitioners to trade off response completeness and mean correctness of the response's fragments. We finetune four open-source models for biography writing, mathematics, coding, and medicine with HALT for three different trade-off thresholds. HALT effectively trades off response completeness for correctness, increasing the mean correctness of response fragments by 15% on average, while resulting in a 4% improvement in the F1 score (mean of completeness and correctness of the response) compared to the relevant baselines. By tuning HALT for highest correctness, we train a single reliable Llama3-70B model with correctness increased from 51% to 87% across all four domains while maintaining 53% of the response completeness achieved with standard finetuning.

### 摘要
当前大型语言模型（LLMs）会对所有提示作出回应。然而当模型缺乏相关知识或能力时，其可能产生错误答案——这种现象被称为幻觉。我们提出通过对LLM进行后训练，使其仅在确信答案正确时生成内容，否则（部分）保持克制。具体而言，我们提出的HALT方法能生成能力对齐的后训练数据，编码模型可可靠生成与不可靠生成的内容。该数据生成方法是将预训练LLM的响应拆分为事实片段（原子陈述或推理步骤），并利用真实信息识别错误片段。通过移除错误片段或将其替换为'此处不确定'（根据可调阈值，允许实践者在响应完整性与片段平均正确性之间权衡），我们实现了能力对齐的微调响应。我们在传记写作、数学、编程和医学四个领域，针对三种不同权衡阈值，对四个开源模型进行了HALT微调。HALT有效实现了响应完整性与正确性的权衡，将响应片段的平均正确性提升了15%，同时使F1分数（响应完整性与正确性的平均值）较相关基线平均提高4%。通过将HALT调至最高正确性，我们训练出单个可靠的Llama3-70B模型，其在所有四个领域的正确性从51%提升至87%，同时保持了标准微调所实现响应完整性的53%。

---

## [OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis](https://arxiv.org/abs/2506.04217)

### Abstract
arXiv:2506.04217v1 Announce Type: cross 
Abstract: The rapid progress of navigation, manipulation, and vision models has made mobile manipulators capable in many specialized tasks. However, the open-world mobile manipulation (OWMM) task remains a challenge due to the need for generalization to open-ended instructions and environments, as well as the systematic complexity to integrate high-level decision making with low-level robot control based on both global scene understanding and current agent state. To address this complexity, we propose a novel multi-modal agent architecture that maintains multi-view scene frames and agent states for decision-making and controls the robot by function calling. A second challenge is the hallucination from domain shift. To enhance the agent performance, we further introduce an agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM as the first dedicated foundation model for mobile manipulators with global scene understanding, robot state tracking, and multi-modal action generation in a unified model. Through experiments, we demonstrate that our model achieves SOTA performance compared to other foundation models including GPT-4o and strong zero-shot generalization in real world. The project page is at https://github.com/HHYHRHY/OWMM-Agent

### 摘要
导航、操作和视觉模型的快速发展使移动机械臂能够胜任许多专门任务。然而，开放世界移动操作（OWMM）任务仍面临挑战，这既需要针对开放式指令和环境进行泛化，又需基于全局场景理解和当前智能体状态，将高层决策与底层机器人控制进行系统性整合。为解决这一复杂性，我们提出一种新型多模态智能体架构，该架构通过维护多视角场景帧和智能体状态进行决策，并借助函数调用控制机器人。第二个挑战是领域迁移导致的幻觉问题。为提升智能体性能，我们进一步设计了面向OWMM任务的智能数据合成流程，通过指令微调使视觉语言模型（VLM）适配任务领域。我们强调所微调的OWMM-VLM是首个专用于移动机械臂的基础模型，其在一个统一框架内实现了全局场景理解、机器人状态追踪和多模态动作生成。实验表明，相较于包括GPT-4o在内的其他基础模型，我们的模型实现了最先进的性能，并在真实场景中展现出强大的零样本泛化能力。项目页面详见https://github.com/HHYHRHY/OWMM-Agent。

---

## [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/abs/2506.04226)

### Abstract
arXiv:2506.04226v1 Announce Type: cross 
Abstract: Knowledge editing methods like MEMIT are able to make data and compute efficient updates of factual knowledge by using a single sentence to update facts and their consequences. However, what is often overlooked is a "precomputation step", which requires a one-time but significant computational cost. The authors of MEMIT originally precompute approximately 44 million hidden vectors per edited layer, which requires a forward pass over 44 million tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this precomputation time grows with model size. In this paper, we show that this excessive computational cost is unnecessary. Knowledge editing using MEMIT and related methods, such as ROME and EMMET, can be performed by pre-computing a very small portion of the 44 million hidden vectors. We first present the theoretical minimum number of hidden vector precomputation required for solutions of these editing methods to exist. We then empirically show that knowledge editing using these methods can be done by pre-computing significantly fewer hidden vectors. Specifically, we show that the precomputation step can be done with less than 0.3% of the originally stipulated number of hidden vectors. This saves a significant amount of precomputation time and allows users to begin editing new models within a few minutes.

### 摘要
诸如MEMIT之类的知识编辑方法能够通过使用单个句子更新事实及其关联结果，实现数据和计算高效的知识更新。然而，一个常被忽视的"预计算步骤"需要承担一次性但显著的计算成本。MEMIT原作者最初为每个编辑层预计算约4400万个隐藏向量，这需要对4400万个标记进行前向传播。对于GPT-J（6B）模型，该预计算步骤在单GPU上耗时36小时，而Llama2-7B模型则需约40小时。此外，预计算时间随模型规模增长而增加。本文证明这种过高的计算成本是不必要的。使用MEMIT及相关方法（如ROME和EMMET）进行知识编辑时，仅需预计算4400万隐藏向量中极小部分即可实现。我们首先从理论上推导出这些编辑方法所需隐藏向量预计算的最小数量，随后通过实验证明：使用这些方法进行知识编辑时，预计算隐藏向量数量可大幅减少。具体而言，预计算步骤所需隐藏向量可少于原规定数量的0.3%。这显著节省了预计算时间，使用户能在数分钟内开始编辑新模型。

---

## [TracLLM: A Generic Framework for Attributing Long Context LLMs](https://arxiv.org/abs/2506.04202)

### Abstract
arXiv:2506.04202v1 Announce Type: cross 
Abstract: Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and/or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble/denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: https://github.com/Wang-Yanting/TracLLM.

### 摘要
长上下文大语言模型（LLMs）已被部署于众多现实应用中，如检索增强生成（RAG）、智能代理及广泛的LLM集成应用。给定指令和长上下文（如文档、PDF文件、网页），长上下文LLM能基于所提供上下文生成输出，旨在提供更准确、最新且可验证的输出，同时减少幻觉和缺乏支持的断言。这引发了一个研究问题：如何精确定位上下文中对LLM生成输出贡献最大或负有责任的文本（如句子、段落）？这一过程我们称之为上下文溯源，其现实应用包括：1）调试基于LLM的系统；2）对LLM遭受攻击（如提示注入攻击、知识污染攻击）进行事后取证分析；3）突出知识来源以增强用户对LLM生成输出的信任。现有特征归因方法（如Shapley）应用于长上下文LLM的溯源时，存在性能欠佳和/或计算成本高昂的问题。本研究提出TracLLM——首个专为长上下文LLM设计的通用溯源框架。该框架能提升现有特征归因方法的效能与效率。为提升效率，我们在TracLLM中开发了基于启发式搜索的算法，并采用贡献分数集成/去噪技术以提高准确性。评估结果表明，TracLLM能有效识别长上下文中导致LLM输出的关键文本。代码与数据详见：https://github.com/Wang-Yanting/TracLLM。

---

## [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)

### Abstract
arXiv:2506.04207v1 Announce Type: cross 
Abstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.

### 摘要
受Deepseek-R1在复杂文本任务中卓越推理能力的启发，许多研究尝试通过直接应用强化学习（RL）来激发多模态大语言模型（MLLMs）的类似能力。然而，这些方法仍难以激活复杂推理。本文并未孤立地研究多模态RL，而是深入分析了当前训练流程，发现三个关键现象：1）有效的冷启动初始化对提升MLLM推理能力至关重要。有趣的是，我们发现仅使用精选文本数据进行初始化，其性能即可超越许多近期多模态推理模型，甚至在多模态RL之前；2）标准GRPO应用于多模态RL时存在梯度停滞问题，会降低训练稳定性和性能；3）在多模态RL阶段后接续纯文本RL训练，能进一步强化多模态推理能力。这种分阶段训练方法有效平衡了感知基础与认知推理的发展。通过整合上述发现并解决多模态RL问题，我们提出了ReVisual-R1模型，在MathVerse、MathVision、WeMath、LogicVista、DynaMath等挑战性基准测试及AIME2024、AIME2025竞赛中，实现了开源7B参数MLLMs的最新最优性能。

---

## [Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development](https://arxiv.org/abs/2407.11784)

### Abstract
arXiv:2407.11784v3 Announce Type: replace 
Abstract: The emergence of multimodal large models has advanced artificial intelligence, introducing unprecedented levels of performance and functionality. However, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource utilization. In response, we present a new sandbox suite tailored for integrated data-model co-development. This sandbox provides a feedback-driven experimental platform, enabling cost-effective iteration and guided refinement of both data and models. Our proposed ``Probe-Analyze-Refine'' workflow, validated through practical use cases on multimodal tasks such as image-text pre-training with CLIP, image-to-text generation with LLaVA-like models, and text-to-video generation with DiT-based models, yields transferable and notable performance boosts, such as topping the VBench leaderboard. A comprehensive set of over 100 experiments demonstrated the suite's usability and extensibility, while also uncovering insights into the interplay between data quality, diversity, model behavior, and computational costs. All codes, datasets, and models are open-sourced to foster future research and applications that would otherwise be infeasible due to the lack of a dedicated co-development infrastructure.

### 摘要
多模态大模型的崛起推动了人工智能发展，带来了前所未有的性能水平和功能表现。然而由于历史上模型中心与数据中心发展路径的相互隔离，导致优化这些模型仍面临挑战，常出现次优结果和资源利用效率低下的问题。为此，我们提出一个专为数据-模型协同开发设计的新型沙箱套件。该沙箱提供反馈驱动的实验平台，支持对数据和模型进行高性价比的迭代与定向优化。我们提出的'探测-分析-精炼'工作流通过在CLIP图像文本预训练、类LLaVA图像到文本生成、基于DiT的文本到视频生成等多模态任务上的实际用例验证，取得了可迁移且显著的性能提升（例如登顶VBench排行榜）。超过100项综合实验证明了该套件的实用性和可扩展性，同时揭示了数据质量、多样性、模型行为与计算成本之间的内在关联。所有代码、数据集和模型均已开源，以促进那些因缺乏专用协同开发基础设施而难以开展的未来研究和应用。

---

## [Understanding Impact of Human Feedback via Influence Functions](https://arxiv.org/abs/2501.05790)

### Abstract
arXiv:2501.05790v2 Announce Type: replace 
Abstract: In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. In our experiments, we demonstrate two key applications of influence functions: (1) detecting common forms of labeler bias in human feedback datasets and (2) guiding labelers to refine their strategies to align more closely with expert feedback. By quantifying the impact of human feedback on reward models, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback. Source code is available at https://github.com/mintaywon/IF_RLHF

### 摘要
在基于人类反馈的强化学习（RLHF）中，从人类反馈中学习合适的奖励模型对于使大语言模型（LLM）与人类意图对齐至关重要。然而，人类反馈往往存在噪声、不一致性或偏见，特别是在评估复杂响应时。此类反馈可能导致奖励信号失准，进而在RLHF过程中产生意外副作用。为应对这些挑战，我们探索利用影响函数来量化人类反馈对奖励模型性能的影响。我们提出了一种计算高效的近似方法，使得影响函数能够应用于基于LLM的奖励模型和大规模偏好数据集。实验结果表明，影响函数具有两个关键应用：(1)检测人类反馈数据集中常见的标注者偏见形式；(2)指导标注者优化其策略以更贴近专家反馈。通过量化人类反馈对奖励模型的影响，我们认为影响函数能提升反馈可解释性，并有助于实现RLHF中的可扩展监督，从而帮助标注者提供更准确一致的反馈。

---

## [Trust-Oriented Adaptive Guardrails for Large Language Models](https://arxiv.org/abs/2408.08959)

### Abstract
arXiv:2408.08959v3 Announce Type: replace 
Abstract: Guardrail, an emerging mechanism designed to ensure that large language models (LLMs) align with human values by moderating harmful or toxic responses, requires a sociotechnical approach in their design. This paper addresses a critical issue: existing guardrails lack a well-founded methodology to accommodate the diverse needs of different user groups, particularly concerning access rights. Supported by trust modeling (primarily on `social' aspect) and enhanced with online in-context learning via retrieval-augmented generation (on `technical' aspect), we introduce an adaptive guardrail mechanism, to dynamically moderate access to sensitive content based on user trust metrics. User trust metrics, defined as a novel combination of direct interaction trust and authority-verified trust, enable the system to precisely tailor the strictness of content moderation by aligning with the user's credibility and the specific context of their inquiries. Our empirical evaluation demonstrates the effectiveness of the adaptive guardrail in meeting diverse user needs, outperforming existing guardrails while securing sensitive information and precisely managing potentially hazardous content through a context-aware knowledge base. To the best of our knowledge, this work is the first to introduce trust-oriented concept into a guardrail system, offering a scalable solution that enriches the discourse on ethical deployment for next-generation LLM service.

### 摘要
护栏（Guardrail）是一种新兴机制，旨在通过调节有害或不良响应来确保大语言模型（LLM）与人类价值观保持一致，其设计需要采用社会技术方法。本文探讨了一个关键问题：现有护栏缺乏完善的方法论来适应不同用户群体的多样化需求，特别是在访问权限方面。基于信任建模（主要针对"社会"层面）并通过检索增强生成技术（针对"技术"层面）增强在线上下文学习能力，我们提出了一种自适应护栏机制，可根据用户信任指标动态调节对敏感内容的访问权限。用户信任指标被定义为直接交互信任与权威验证信任的新型组合，使系统能够通过结合用户可信度及其查询的具体上下文，精确调整内容审核的严格程度。实证评估表明，该自适应护栏在满足多样化用户需求方面表现优异，其性能超越现有护栏，同时通过上下文感知知识库保护敏感信息并精准管理潜在危险内容。据我们所知，这是首次将面向信任的概念引入护栏系统，为下一代LLM服务的伦理部署提供了可扩展的解决方案，丰富了相关讨论。

---

## [A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization](https://arxiv.org/abs/2410.02165)

### Abstract
arXiv:2410.02165v2 Announce Type: replace 
Abstract: Open-ended short-answer questions (SAGs) have been widely recognized as a powerful tool for providing deeper insights into learners' responses in the context of learning analytics (LA). However, SAGs often present challenges in practice due to the high grading workload and concerns about inconsistent assessments. With recent advancements in natural language processing (NLP), automatic short-answer grading (ASAG) offers a promising solution to these challenges. Despite this, current ASAG algorithms are often limited in generalizability and tend to be tailored to specific questions. In this paper, we propose a unified multi-agent ASAG framework, GradeOpt, which leverages large language models (LLMs) as graders for SAGs. More importantly, GradeOpt incorporates two additional LLM-based agents - the reflector and the refiner - into the multi-agent system. This enables GradeOpt to automatically optimize the original grading guidelines by performing self-reflection on its errors. Through experiments on a challenging ASAG task, namely the grading of pedagogical content knowledge (PCK) and content knowledge (CK) questions, GradeOpt demonstrates superior performance in grading accuracy and behavior alignment with human graders compared to representative baselines. Finally, comprehensive ablation studies confirm the effectiveness of the individual components designed in GradeOpt.

### 摘要
开放式简答题（SAGs）在学习分析（LA）领域已被广泛认为是一种能够深入理解学习者反馈的有效工具。然而，由于评分工作量大且存在评估不一致的担忧，SAGs在实践中常面临挑战。随着自然语言处理（NLP）技术的最新进展，自动简答题评分（ASAG）为这些挑战提供了可行的解决方案。尽管如此，现有的ASAG算法通常在通用性方面存在局限，且往往针对特定问题定制。本文提出了一种统一的多智能体ASAG框架GradeOpt，该框架利用大语言模型（LLMs）作为SAGs的评分器。更重要的是，GradeOpt在其多智能体系统中引入了两个基于LLM的附加智能体——反思器与优化器，使其能够通过对自身错误的自我反思来自动优化原始评分标准。在一个具有挑战性的ASAG任务（即教学内容知识（PCK）与内容知识（CK）问题的评分）上的实验表明，相较于代表性基线方法，GradeOpt在评分准确性和与人类评分者行为一致性方面均表现出优越性能。最后，全面的消融研究验证了GradeOpt中各个设计组件的有效性。

---

## [From Intention To Implementation: Automating Biomedical Research via LLMs](https://arxiv.org/abs/2412.09429)

### Abstract
arXiv:2412.09429v3 Announce Type: replace 
Abstract: Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets. Artificial intelligence (AI), particularly Large Language Models (LLMs), has the potential to revolutionize this process by automating various steps. Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements. This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments. BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming. By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity. Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols. BioResearcher successfully achieves an average execution success rate of 63.07% across eight previously unmet research objectives. The generated protocols averagely outperform typical agent systems by 22.0% on five quality metrics. The system demonstrates significant potential to reduce researchers' workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.

### 摘要
传统生物医学研究因科学文献和数据集的指数级增长而日益劳动密集型。人工智能（AI），尤其是大语言模型（LLMs），有望通过自动化多个环节彻底改变这一流程。然而仍存在重大挑战，包括跨学科专业知识需求、实验设计的逻辑性及性能评估等问题。本文介绍首个端到端自动化系统BioResearcher，旨在优化涉及干实验的完整生物医学研究流程。该系统采用模块化多智能体架构，集成文献检索、文本处理、实验设计与编程等专用代理。通过将复杂任务分解为逻辑关联的子任务，并运用分层学习方法，BioResearcher有效解决了跨学科需求与逻辑复杂性的挑战。此外，系统内置基于LLM的评审模块进行过程质量控制，并引入创新性评估指标以量化实验方案的质量与自动化程度。在八项既往未实现的研究目标中，BioResearcher平均执行成功率达63.07%。所生成方案在五项质量指标上平均优于典型代理系统22.0%。该系统展现出减轻研究者工作量、加速生物医学发现的巨大潜力，为自动化研究系统的未来发展开辟了新路径。

---

## [HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims](https://arxiv.org/abs/2502.11753)

### Abstract
arXiv:2502.11753v2 Announce Type: replace 
Abstract: Misinformation can be countered with fact-checking, but the process is costly and slow. Identifying checkworthy claims is the first step, where automation can help scale fact-checkers' efforts. However, detection methods struggle with content that is (1) multimodal, (2) from diverse domains, and (3) synthetic. We introduce HintsOfTruth, a public dataset for multimodal checkworthiness detection with 27K real-world and synthetic image/claim pairs. The mix of real and synthetic data makes this dataset unique and ideal for benchmarking detection methods. We compare fine-tuned and prompted Large Language Models (LLMs). We find that well-configured lightweight text-based encoders perform comparably to multimodal models but the former only focus on identifying non-claim-like content. Multimodal LLMs can be more accurate but come at a significant computational cost, making them impractical for large-scale applications. When faced with synthetic data, multimodal models perform more robustly.

### 摘要
事实核查可有效应对错误信息，但该过程成本高昂且耗时。识别值得核查的主张是首要步骤，自动化技术可在此环节协助扩展事实核查工作。然而现有检测方法在应对以下内容时存在困难：(1)多模态内容；(2)跨领域内容；(3)合成内容。本研究推出HintsOfTruth公开数据集，包含2.7万组真实与合成的图像/主张配对数据，专用于多模态核查价值检测。真实数据与合成数据的混合使该数据集具有独特性，成为检测方法基准测试的理想选择。我们对比了微调大语言模型(LLM)与提示型LLM的表现，发现配置得当的轻量级文本编码器性能与多模态模型相当，但前者仅能识别非主张类内容。多模态LLM虽具有更高准确度，但需付出显著计算成本，难以应用于大规模场景。当处理合成数据时，多模态模型表现出更强的鲁棒性。

---

## [Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with Foundation Models](https://arxiv.org/abs/2501.14755)

### Abstract
arXiv:2501.14755v2 Announce Type: replace 
Abstract: The burgeoning field of foundation models necessitates advanced data processing mechanisms capable of harnessing vast and valuable data with various types used by these models. Nevertheless, the current landscape presents unique challenges that traditional data processing frameworks struggle to handle effectively, particularly in handling the complexity of multimodal data. In response, we present Data-Juicer 2.0, a data processing system backed by 100+ data processing operators spanning text, image, video, and audio modalities, supporting more critical tasks including data analysis, synthesis, annotation, and foundation model post-training. With seamless compatibility and dedicated optimization for popular dataset hubs like Hugging Face and computing engines like Ray, it improves upon its predecessor in terms of usability, efficiency, and programmability. It features an easily accessible user interface layer that supports decoupled Python interactions, RESTful APIs, and conversational commands. It contains a new runtime layer optimized for adaptive execution and management across varying dataset scales, processing demands, and computational environments, while hiding unnecessary system details. Extensive empirical evaluations demonstrate Data-Juicer 2.0's remarkable performance and scalability, highlighting its capability to efficiently process TB-level data with 10k+ CPU cores. The system is publicly available and has been widely adopted in diverse research fields and real-world products such as Alibaba Cloud PAI. We actively maintain it and share insights from practical feedback, with the goal of facilitating research and application of next-generation foundation models.

### 摘要
基础模型领域的蓬勃发展需要先进的数据处理机制，以有效利用这些模型所使用的海量多模态高价值数据。然而，当前技术环境存在传统数据处理框架难以应对的独特挑战，特别是在处理多模态数据复杂性方面。为此，我们提出Data-Juicer 2.0——一个包含100余种数据处理算子的系统，覆盖文本、图像、视频和音频模态，支持数据分析、合成、标注及基础模型后训练等关键任务。该系统通过深度优化实现对Hugging Face等主流数据集中心和Ray等计算引擎的无缝兼容，在可用性、效率和可编程性方面显著超越前代版本。其特点包括：支持Python解耦交互、RESTful API和对话式命令的易用接口层；针对不同数据集规模、处理需求和计算环境进行自适应执行优化的运行时层（同时隐藏系统细节）。大量实证评估表明，Data-Juicer 2.0在TB级数据（使用1万+CPU核心）处理中展现出卓越的性能与可扩展性。该系统已开源并被广泛应用于阿里云PAI等研究领域和实际产品中。我们将持续维护并根据实践反馈分享洞见，以促进下一代基础模型的研究与应用。

---

## [M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality](https://arxiv.org/abs/2503.02077)

### Abstract
arXiv:2503.02077v3 Announce Type: replace 
Abstract: Designing effective reward functions in multi-agent reinforcement learning (MARL) is a significant challenge, often leading to suboptimal or misaligned behaviors in complex, coordinated environments. We introduce Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality ($\text&#123;M&#125;^3\text&#123;HF&#125;$), a novel framework that integrates multi-phase human feedback of mixed quality into the MARL training process. By involving humans with diverse expertise levels to provide iterative guidance, $\text&#123;M&#125;^3\text&#123;HF&#125;$ leverages both expert and non-expert feedback to continuously refine agents' policies. During training, we strategically pause agent learning for human evaluation, parse feedback using large language models to assign it appropriately and update reward functions through predefined templates and adaptive weights by using weight decay and performance-based adjustments. Our approach enables the integration of nuanced human insights across various levels of quality, enhancing the interpretability and robustness of multi-agent cooperation. Empirical results in challenging environments demonstrate that $\text&#123;M&#125;^3\text&#123;HF&#125;$ significantly outperforms state-of-the-art methods, effectively addressing the complexities of reward design in MARL and enabling broader human participation in the training process.

### 摘要
设计有效的多智能体强化学习（MARL）奖励函数是一项重大挑战，在复杂的协同环境中常导致策略表现次优或行为失准。本文提出混合质量多阶段人类反馈的多智能体强化学习框架（$	ext&#123;M&#125;^3	ext&#123;HF&#125;$），该创新方法将混合质量的多阶段人类反馈整合至MARL训练过程。通过吸纳不同专业水平的人类提供迭代指导，$	ext&#123;M&#125;^3	ext&#123;HF&#125;$能同时利用专家与非专家反馈持续优化智能体策略。训练过程中，我们策略性地暂停智能体学习以获取人类评估，采用大语言模型解析反馈并进行合理分配，通过预定义模板和自适应权重（结合权重衰减与基于表现的调整）来更新奖励函数。该方法能整合不同质量层次的人类洞察，增强多智能体协作的可解释性与鲁棒性。在挑战性环境中的实证结果表明，$	ext&#123;M&#125;^3	ext&#123;HF&#125;$显著优于现有最优方法，有效解决了MARL奖励函数设计的复杂性，并拓宽了人类参与训练过程的途径。

---

## [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583)

### Abstract
arXiv:2308.09583v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM

### 摘要
以GPT-4为代表的大语言模型（LLMs）在自然语言处理（NLP）任务中展现出卓越性能，包括具有挑战性的数学推理任务。然而，现有大多数开源模型仅基于大规模互联网数据进行预训练，未针对数学领域进行优化。本文提出WizardMath模型，通过将我们提出的"基于进化指令反馈的强化学习"（RLEIF）方法应用于数学领域，在不依赖外部Python工具的情况下，显著提升了大语言模型的数学思维链（CoT）推理能力。通过在GSM8k和MATH两个数学推理基准测试上的大量实验，我们揭示了该模型的卓越能力。值得注意的是，WizardMath-Mistral 7B以更高的数据效率显著超越顶级开源大语言模型。此外，WizardMath 70B甚至优于GPT-3.5-Turbo、Claude 2、Gemini Pro和GPT-4早期版本。我们的初步探索还表明，指令进化和过程监督对于实现卓越数学性能具有关键作用。更多细节请参阅https://github.com/nlpxucan/WizardLM

---

## [Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?](https://arxiv.org/abs/2503.11207)

### Abstract
arXiv:2503.11207v2 Announce Type: replace 
Abstract: This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these symbolic analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles, and 2) we smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest accuracy reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.

### 摘要
本研究首次对两种先进的大规模推理模型（LRM）——OpenAI的o3-mini和DeepSeek R1——进行了类比推理能力评估，重点关注基于瑞文渐进矩阵的非言语人类智商测试。我们采用I-RAVEN数据集及其扩展版I-RAVEN-X作为基准，后者用于测试模型对更长推理规则和属性值范围的泛化能力。为评估视觉不确定性对这些符号类比推理测试的影响，我们对原本假设理想感知的I-RAVEN-X数据集进行了扩展：1）引入随机采样的干扰属性，这些属性对谜题正确答案预测无贡献；2）平滑输入属性值的分布。实验发现，OpenAI的o3-mini任务准确率从原始I-RAVEN的86.6%骤降至更具挑战性的I-RAVEN-X（增加输入长度、范围并模拟感知不确定性）的17.0%——接近随机概率，尽管其推理标记消耗增加了3.4倍。DeepSeek R1也呈现相似趋势（80.6%降至23.2%）。相比之下，在I-RAVEN上达到最先进性能的神经符号概率溯因模型ARLC，能在所有分布外测试中保持稳健推理，准确率仅从98.6%适度下降至88.0%。代码已开源：https://github.com/IBM/raven-large-language-models。

---

## [Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities](https://arxiv.org/abs/2309.16739)

### Abstract
arXiv:2309.16739v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, including split learning/inference, parameter-efficient fine-tuning, quantization, and parameter-sharing inference, to facilitate the efficient deployment of LLMs. This article serves as a position paper for thoroughly identifying the motivation, challenges, and pathway for empowering LLMs at the 6G edge.

### 摘要
大型语言模型（LLMs）以其卓越能力正推动人工智能发展的革命，并可能重塑未来格局。然而，鉴于其多模态特性，当前基于云端的部署模式面临三大关键挑战：1）响应延迟；2）带宽成本高昂；3）数据隐私泄露风险。6G移动边缘计算（MEC）系统有望解决这些紧迫问题。本文系统探究了6G边缘部署LLMs的潜力：首先通过机器人学和医疗保健等多模态LLMs驱动的杀手级应用，阐明终端用户侧部署的必要性；继而剖析边缘部署的核心挑战，并构建面向LLMs的6G MEC体系架构。进一步地，我们深入探讨边缘训练与边缘推理两大设计维度，针对边缘端固有资源限制，论述了包括分割学习/推理、参数高效微调、量化和参数共享推理在内的前沿技术方案，以实现LLMs的高效部署。本文作为立场文件，全面阐述了6G边缘赋能LLMs的动因、挑战与实施路径。

---

## [Quantifying Prediction Consistency Under Fine-Tuning Multiplicity in Tabular LLMs](https://arxiv.org/abs/2407.04173)

### Abstract
arXiv:2407.04173v2 Announce Type: replace-cross 
Abstract: Fine-tuning LLMs on tabular classification tasks can lead to the phenomenon of fine-tuning multiplicity where equally well-performing models make conflicting predictions on the same input. Fine-tuning multiplicity can arise due to variations in the training process, e.g., seed, weight initialization, minor changes to training data, etc., raising concerns about the reliability of Tabular LLMs in high-stakes applications such as finance, hiring, education, healthcare. Our work formalizes this unique challenge of fine-tuning multiplicity in Tabular LLMs and proposes a novel measure to quantify the consistency of individual predictions without expensive model retraining. Our measure quantifies a prediction's consistency by analyzing (sampling) the model's local behavior around that input in the embedding space. Interestingly, we show that sampling in the local neighborhood can be leveraged to provide probabilistic guarantees on prediction consistency under a broad class of fine-tuned models, i.e., inputs with sufficiently high local stability (as defined by our measure) also remain consistent across several fine-tuned models with high probability. We perform experiments on multiple real-world datasets to show that our local stability measure preemptively captures consistency under actual multiplicity across several fine-tuned models, outperforming competing measures.

### 摘要
在表格分类任务上对大型语言模型（LLM）进行微调时，可能会出现微调多重性现象，即性能相当的模型对相同输入做出相互冲突的预测。这种多重性可能源于训练过程中的变化因素，如随机种子、权重初始化、训练数据的微小调整等，从而引发对表格LLM在金融、招聘、教育、医疗等高风险领域应用可靠性的担忧。本研究首次系统阐述了表格LLM中微调多重性这一独特挑战，并提出了一种无需昂贵模型重训练即可量化单个预测一致性的新方法。该方法通过分析（采样）模型在嵌入空间中输入点周围的局部行为来量化预测一致性。值得注意的是，我们证明局部邻域采样可为广泛类别的微调模型提供预测一致性的概率保证——即具有足够高局部稳定性（由本方法定义）的输入，在多个微调模型间也能以高概率保持一致性。通过在多个真实数据集上的实验表明，我们的局部稳定性度量能够预先捕捉多个微调模型实际多重性下的一致性表现，其效果优于现有竞争方法。

---

## [Geometric Signatures of Compositionality Across a Language Model's Lifetime](https://arxiv.org/abs/2410.01444)

### Abstract
arXiv:2410.01444v4 Announce Type: replace-cross 
Abstract: By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.

### 摘要
借助语言的组合性，少数句法规则和有限词汇表即可生成无限数量的句子。这表明尽管语言看似高维，却能用相对较少的自由度来解释。一个悬而未决的问题是：当代语言模型（LMs）是否反映了组合性所赋予的语言内在简洁性？我们通过将数据集的组合程度与语言模型表征的内在维度（ID，即特征复杂度的度量）相关联，从几何视角探讨该问题。研究发现：不仅表征的ID能够反映数据集的组合程度，而且组合性与几何复杂度之间的关系源于训练过程中习得的语言特征。最终分析揭示了非线性和线性维度之间的显著对比——前者编码语言组合的语义层面，而后者编码其表层特征。

---

## [Nudging: Inference-time Alignment of LLMs via Guided Decoding](https://arxiv.org/abs/2410.09300)

### Abstract
arXiv:2410.09300v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose NUDGING, a simple, training-free algorithm that aligns any base model at inference time using a small aligned model. NUDGING is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, NUDGING employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high, with only a minor additional inference overhead. We evaluate NUDGING across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, NUDGING enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our code and demo are available at: https://fywalter.github.io/nudging/ .

### 摘要
大型语言模型（LLMs）需要通过对齐来有效且安全地遵循用户指令。这一过程通常需要为每个基础模型训练一个对齐版本，导致显著的计算开销。本研究提出NUDGING算法——一种简单、无需训练的推理时对齐方法，利用小型对齐模型引导任意基础模型。该方法的提出基于最新发现：对齐主要改变模型在少量风格标记（如话语标记）上的行为。我们发现基础模型在生成这类标记时表现出明显更高的不确定性。基于此，NUDGING在解码过程中当基础模型不确定性较高时，使用小型对齐模型生成引导标记来调控输出，仅带来微小的额外推理开销。我们在3个模型系列上针对多样化开放指令任务进行评估。未经任何训练，用7-14倍小的对齐模型引导大型基础模型时，其零样本性能可媲美甚至超越大型对齐模型。由于在标记级别操作，NUDGING支持不同模型系列间的即插即用协作。例如用Llama-27b-chat引导Gemma-2-27b时，其在多项任务上表现优于Llama-2-70b-chat。本研究为LLM对齐提供了模块化、高性价比的解决方案。代码与演示见：https://fywalter.github.io/nudging/。

---

## [Meaning-Typed Programming: Language Abstraction and Runtime for Model-Integrated Applications](https://arxiv.org/abs/2405.08965)

### Abstract
arXiv:2405.08965v4 Announce Type: replace-cross 
Abstract: Software development is shifting from traditional logical programming to model-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity.
  This paper presents Meaning-Typed Programming (MTP) model, a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the by operator for seamless LLM invocation, MT-IR, a meaning-based intermediate representation for semantic extraction, and MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a Python superset language and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. Our evaluation across diverse benchmarks and user studies demonstrates that MTP outperforms existing frameworks such as DSPy and LMQL by reducing lines of code by factors of 2.3-7.5X and 1.3-10.7X respectively. For math problems from the GSM8k dataset, MTP achieves accuracy rates approaching 90%, while reducing token usage in 10 out of 13 benchmarks. This leads to cost savings up to 4.5X and runtime speedups as high as 4.75X. Additionally, MTP demonstrates resilience even when 50% of naming conventions are suboptimal, establishing it as a practical, efficient solution for streamlining model-integrated application development.

### 摘要
软件开发正从传统的逻辑编程转向在运行时利用生成式AI和大语言模型（LLM）的模型集成应用。然而，LLM的集成仍然复杂，需要开发者手动编写提示词并处理输出。现有工具试图通过提示工程提供协助，但往往引入额外复杂性。
本文提出意义类型编程（MTP）模型，这是一种通过直观的语言级构造抽象化LLM集成的新范式。MTP利用代码固有的语义丰富性，无需开发者额外工作即可自动化提示生成与响应处理。我们引入`by`运算符实现无缝LLM调用、基于语义的中间表示MT-IR用于意义提取，以及自动化管理LLM交互的MT-Runtime系统。我们在Python超集语言Jac中实现MTP，发现其能在保持准确性与效率的同时显著降低编码复杂度。
跨多基准测试和用户研究的评估表明：MTP在代码行数上分别比DSPy和LMQL减少2.3-7.5倍与1.3-10.7倍；在GSM8k数学题集中准确率接近90%，同时在13项基准测试中有10项实现token用量下降。这使得成本最高降低4.5倍，运行速度提升达4.75倍。即使命名规范50%欠佳时，MTP仍表现稳健，证实其是简化模型集成应用开发的实用高效解决方案。

---

## [Improving Radiology Report Conciseness and Structure via Local Large Language Models](https://arxiv.org/abs/2411.05042)

### Abstract
arXiv:2411.05042v2 Announce Type: replace-cross 
Abstract: Radiology reports are often lengthy and unstructured, posing challenges for referring physicians to quickly identify critical imaging findings while increasing the risk of missed information. This retrospective study aimed to enhance radiology reports by making them concise and well-structured, with findings organized by relevant organs. To achieve this, we utilized private large language models (LLMs) deployed locally within our institution's firewall, ensuring data security and minimizing computational costs. Using a dataset of 814 radiology reports from seven board-certified body radiologists at Moffitt Cancer Center, we tested five prompting strategies within the LangChain framework. After evaluating several models, the Mixtral LLM demonstrated superior adherence to formatting requirements compared to alternatives like Llama. The optimal strategy involved condensing reports first and then applying structured formatting based on specific instructions, reducing verbosity while improving clarity. Across all radiologists and reports, the Mixtral LLM reduced redundant word counts by more than 53%. These findings highlight the potential of locally deployed, open-source LLMs to streamline radiology reporting. By generating concise, well-structured reports, these models enhance information retrieval and better meet the needs of referring physicians, ultimately improving clinical workflows.

### 摘要
放射学报告通常冗长且非结构化，这给转诊医师快速识别关键影像学发现带来挑战，同时增加了信息遗漏的风险。本回顾性研究旨在通过使报告简洁且结构清晰、按相关器官组织检查结果来提升放射学报告质量。为实现这一目标，我们采用了部署在机构防火墙内的私有大语言模型（LLMs），确保数据安全并降低计算成本。基于莫菲特癌症中心七位委员会认证体部放射科医师撰写的814份放射学报告数据集，我们在LangChain框架中测试了五种提示策略。经多模型评估后，Mixtral LLM在格式要求遵循度上显著优于Llama等替代模型。最优策略为先压缩报告内容，再根据特定指令进行结构化格式化，在减少冗余的同时提升清晰度。在所有放射科医师和报告中，Mixtral LLM将冗余词汇量降低超过53%。这些发现凸显了本地部署的开源LLMs在优化放射学报告流程方面的潜力。通过生成简明、结构良好的报告，这些模型提升了信息检索效率，更好地满足转诊医师需求，最终改善了临床工作流程。

---

## [REAL: Response Embedding-based Alignment for LLMs](https://arxiv.org/abs/2409.17169)

### Abstract
arXiv:2409.17169v4 Announce Type: replace-cross 
Abstract: Aligning large language models (LLMs) to human preferences is a crucial step in building helpful and safe AI tools, which usually involve training on supervised datasets. Popular algorithms such as Direct Preference Optimization (DPO) rely on pairs of AI-generated responses ranked according to human annotation. The response pair annotation process might bring human bias. Building a correct preference dataset is the costly part of the alignment pipeline. To improve annotation efficiency and quality in the LLMs alignment, we propose REAL: Response Embedding-based Alignment for LLMs, a strategy for constructing a high-quality training dataset that focuses on acquiring the less ambiguous preference pairs for labeling out of a set of response candidates. Our selection process is based on the similarity of embedding responses independently of prompts, which guarantees the selection process in an off-policy setting, avoiding adaptively measuring the similarity during the training. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF benchmarks indicate that choosing dissimilar response pairs enhances the direct alignment of LLMs while reducing inherited labeling errors. The model aligned with dissimilar response pairs obtained a better margin and win rate on the dialogue task. Our findings suggest that focusing on distinct pairs can reduce the label error and improve LLM alignment efficiency, saving up to $65\%$ of annotators' work.

### 摘要
将大语言模型（LLMs）与人类偏好对齐是构建有用且安全AI工具的关键步骤，这一过程通常涉及监督数据集的训练。直接偏好优化（DPO）等流行算法依赖于根据人工标注排序的AI生成响应对。响应对的标注过程可能引入人为偏差。构建正确的偏好数据集是对齐流程中成本最高的环节。为提高LLMs对齐中的标注效率与质量，我们提出REAL：基于响应嵌入的LLMs对齐策略，该策略通过从候选响应集中筛选歧义较小的偏好对进行标注，从而构建高质量训练数据集。我们的选择过程基于响应嵌入的相似性（与提示无关），确保选择过程在离线策略设置下完成，避免训练期间动态测量相似性。在真实数据集SHP2和合成HH-RLHF基准上的实验结果表明，选择差异较大的响应对能增强LLMs的直接对齐效果，同时减少继承性标注错误。使用差异响应对对齐的模型在对话任务中获得了更优的边际优势和胜率。研究发现，聚焦于差异明显的配对可降低标注错误率并提升LLM对齐效率，最高可节省65%的标注工作量。

---

## [Enabling LLM Knowledge Analysis via Extensive Materialization](https://arxiv.org/abs/2411.04920)

### Abstract
arXiv:2411.04920v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have majorly advanced NLP and AI, and next to their ability to perform a wide range of procedural tasks, a major success factor is their internalized factual knowledge. Since Petroni et al. (2019), analyzing this knowledge has gained attention. However, most approaches investigate one question at a time via modest-sized pre-defined samples, introducing an ``availability bias'' (Tversky&amp;Kahnemann, 1973) that prevents the analysis of knowledge (or beliefs) of LLMs beyond the experimenter's predisposition.
  To address this challenge, we propose a novel methodology to comprehensively materialize an LLM's factual knowledge through recursive querying and result consolidation. Our approach is a milestone for LLM research, for the first time providing constructive insights into the scope and structure of LLM knowledge (or beliefs).
  As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million relational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB to exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale, accuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible at https://gptkb.org

### 摘要
大型语言模型（LLMs）显著推动了自然语言处理与人工智能的发展，其成功不仅源于执行多样化程序性任务的能力，更关键因素在于模型内化的知识体系。自Petroni等人（2019）开创性研究以来，针对这类知识的分析日益受到关注。然而现有方法大多通过预设的小规模样本逐条检验问题，这种研究范式会引入"可得性偏差"（Tversky&amp;Kahnemann, 1973），导致分析结果受限于实验者预设而无法揭示LLMs知识（或信念）的全貌。

为解决这一局限，我们提出创新方法论：通过递归查询与结果整合系统化构建LLMs的事实知识体系。该研究首次为LLM知识（或信念）的范畴与结构提供了建设性分析框架，成为该领域的重要里程碑。

作为原型验证，我们基于GPT-4o-mini构建了包含290万实体、1.01亿关系三元组的知识库GPTKB。通过该知识库，我们同步分析了GPT-4o-mini在知识规模、准确性、偏差、时效性与一致性等方面的表现。GPTKB可通过https://gptkb.org访问。

---

## [Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria](https://arxiv.org/abs/2412.21006)

### Abstract
arXiv:2412.21006v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) rely on generating extensive intermediate reasoning units (e.g., tokens, sentences) to enhance final answer quality across a wide range of complex tasks. While this approach has proven effective, it inevitably increases substantial inference costs. Previous methods adopting token-level reduction without clear criteria result in poor performance compared to models trained with complete rationale. To address this challenge, we propose a novel sentence-level rationale reduction framework leveraging likelihood-based criteria, verbosity, to identify and remove redundant reasoning sentences. Unlike previous approaches, our method leverages verbosity to selectively remove redundant reasoning sentences while preserving reasoning capabilities. Our experimental results across various reasoning tasks demonstrate that our method improves performance by an average of 7.71% while reducing token generation by 19.87% compared to model trained with complete reasoning paths.

### 摘要
大型语言模型（LLMs）依赖生成大量中间推理单元（如词元、句子）来提升各类复杂任务的最终答案质量。虽然该方法已被证明有效，但不可避免地增加了显著的推理成本。先前采用无明确标准的词元级缩减方法，与使用完整推理链训练的模型相比性能较差。为解决这一挑战，我们提出了一种新颖的基于似然准则的句子级推理缩减框架——冗余度（verbosity），用于识别并移除冗余推理句子。与以往方法不同，我们的技术利用冗余度选择性删除冗余推理句，同时保留推理能力。在多种推理任务上的实验结果表明，相较于完整推理路径训练的模型，我们的方法平均提升7.71%的性能，同时减少19.87%的词元生成量。

---

## [Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation](https://arxiv.org/abs/2412.15255)

### Abstract
arXiv:2412.15255v2 Announce Type: replace-cross 
Abstract: In this paper, we show that knowledge distillation can be subverted to manipulate language model benchmark scores, revealing a critical vulnerability in current evaluation practices. We introduce "Data Laundering," a process that enables the covert transfer of benchmark-specific knowledge through seemingly legitimate intermediate training steps. Through extensive experiments with a 2-layer BERT student model, we show how this approach can achieve substantial improvements in benchmark accuracy (up to 75\% on GPQA) without developing genuine reasoning capabilities. Notably, this method can be exploited intentionally or even unintentionally, as researchers may inadvertently adopt this method and inflate scores without realising the implications. While our findings demonstrate the effectiveness of this technique, we present them as a cautionary tale highlighting the urgent need for more robust evaluation methods in AI. This work aims to contribute to the ongoing discussion about evaluation integrity in AI development and the need for benchmarks that more accurately reflect true model capabilities. The code is available at https://github.com/mbzuai-nlp/data_laundering.

### 摘要
本文揭示了知识蒸馏技术可被用于操纵语言模型基准测试分数，暴露出当前评估体系中的关键漏洞。我们提出"数据洗白"方法，通过看似合法的中间训练步骤实现基准测试特定知识的隐蔽迁移。通过使用双层BERT学生模型进行大量实验，我们证明该方法能在不发展真实推理能力的情况下显著提升基准准确率（GPQA数据集最高提升75%）。值得注意的是，该方法可能被有意或无意利用——研究人员可能在不知情的情况下采用该方法导致分数虚高。虽然实验结果验证了该技术的有效性，但我们将其视为警示案例，强调人工智能领域亟需更稳健的评估方法。本研究旨在推动关于AI发展中评估完整性问题的讨论，促进建立能更准确反映模型真实能力的基准测试。代码已开源：https://github.com/mbzuai-nlp/data_laundering。

---

## [Engagement-Driven Content Generation with Large Language Models](https://arxiv.org/abs/2411.13187)

### Abstract
arXiv:2411.13187v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate significant persuasive capabilities in one-on-one interactions, but their influence within social networks, where interconnected users and complex opinion dynamics pose unique challenges, remains underexplored. This paper addresses the research question: \emph&#123;Can LLMs generate meaningful content that maximizes user engagement on social networks?&#125;
  To answer this, we propose a pipeline using reinforcement learning with simulated feedback, where the network's response to LLM-generated content (i.e., the reward) is simulated through a formal engagement model. This approach bypasses the temporal cost and complexity of live experiments, enabling an efficient feedback loop between the LLM and the network under study. It also allows to control over endogenous factors such as the LLM's position within the social network and the distribution of opinions on a given topic. Our approach is adaptive to the opinion distribution of the underlying network and agnostic to the specifics of the engagement model, which is embedded as a plug-and-play component. Such flexibility makes it suitable for more complex engagement tasks and interventions in computational social science.
  Using our framework, we analyze the performance of LLMs in generating social engagement under different conditions, showcasing their full potential in this task. The experimental code is publicly available at https://github.com/mminici/Engagement-Driven-Content-Generation.

### 摘要
大型语言模型（LLMs）在一对一互动中展现出显著的 persuasive capabilities，但其在社交网络中的影响力——面对相互关联的用户和复杂的意见动态所带来的独特挑战——仍未被充分探索。本文研究核心问题为：\emph&#123;LLMs能否生成最大化社交网络用户参与度的有效内容？&#125;  为此，我们提出一种基于强化学习的模拟反馈 pipeline，其中网络对LLM生成内容的响应（即奖励）通过形式化的参与度模型进行模拟。该方法规避了实时实验的时间成本与复杂性，实现了LLM与被研究网络间的高效反馈循环，同时可控制内生变量（如LLM在社交网络中的位置及特定话题的意见分布）。我们的方法能自适应底层网络的意见分布，且与作为即插即用组件的参与度模型的具体细节无关。这种灵活性使其适用于更复杂的参与度任务及计算社会科学中的干预研究。  通过该框架，我们分析了LLM在不同条件下生成社交参与内容的性能，展示了其在此任务中的完整潜力。实验代码公开于：https://github.com/mminici/Engagement-Driven-Content-Generation。

---

## [Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946)

### Abstract
arXiv:2502.13946v2 Announce Type: replace-cross 
Abstract: The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.

### 摘要
大型语言模型（LLMs）的安全对齐机制仍存在脆弱性，即使相对简单的攻击也能轻易突破其初始行为防线。鉴于现有LLMs普遍采用在输入指令与初始模型输出之间填充固定模板的做法，我们假设该模板是其脆弱性的关键诱因：LLMs的安全相关决策过度依赖于模板区域的聚合信息，这些信息会显著影响模型的安全行为。我们将此问题称为"模板锚定式安全对齐"。本文通过大量实验验证了模板锚定式安全对齐现象在各种已对齐LLMs中的普遍性。机理分析表明，该现象导致模型在遭遇推理时越狱攻击时易受攻击。进一步研究表明，将安全机制与模板区域解耦可有效降低对越狱攻击的脆弱性。我们建议未来研究开发更鲁棒的安全对齐技术，减少对模板区域的依赖性。

---

## [RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2412.11050)

### Abstract
arXiv:2412.11050v3 Announce Type: replace-cross 
Abstract: Understanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-language models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant challenges, such as hallucination and insufficient real-world grounding, which compromise their performance in critical driving scenarios. In this work, RAC3, a novel framework designed to enhance the performance of VLMs in corner case comprehension, is proposed. RAC3 integrates a frequency-spatial fusion (FSF) image encoder, a cross-modal alignment training method for embedding models with hard and semi-hard negative mining, and a fast querying and retrieval pipeline based on K-Means clustering and hierarchical navigable small world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting strategy to guide analogical reasoning and reduce hallucinations during inference is introduced. Moreover, an update mechanism is integrated into RAC3 to ensure continual learning within the framework. Extensive experiments on the CODA and nuScenes datasets demonstrate that RAC3 significantly improves corner case comprehension across multiple downstream tasks. Compared to prior state-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the CODA-LM benchmark and shows consistent performance gains when integrated with end-to-end frameworks like DriveLM. These results demonstrate the effectiveness of retrieval-augmented strategies and cross-modal alignment for safer and more interpretable autonomous driving.

### 摘要
理解和处理极端案例对于确保自动驾驶系统的安全性和可靠性至关重要。视觉语言模型（VLMs）在提升场景理解能力方面发挥着关键作用，但仍面临幻觉现象和现实基础不足等重大挑战，这些因素会损害其在关键驾驶场景中的表现。本研究提出RAC3——一个旨在增强VLM极端案例理解性能的新型框架。该框架整合了频域-空间融合（FSF）图像编码器、采用困难与半困难负样本挖掘的跨模态对齐嵌入模型训练方法，以及基于K均值聚类与可导航小世界层次（HNSW）索引的快速查询检索流程。我们引入多模态思维链（CoT）提示策略以指导类比推理并减少推理过程中的幻觉现象。此外，RAC3还集成了更新机制以确保框架内的持续学习能力。在CODA和nuScenes数据集上的大量实验表明，RAC3在多项下游任务中显著提升了极端案例理解能力。相较于现有最优方法，RAC3在CODA-LM基准测试中以74.46分的最高成绩刷新记录，当与DriveLM等端到端框架集成时也展现出稳定的性能提升。这些结果验证了检索增强策略与跨模态对齐技术对实现更安全、可解释自动驾驶的有效性。

---

## [KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial Recomputation](https://arxiv.org/abs/2411.17089)

### Abstract
arXiv:2411.17089v2 Announce Type: replace-cross 
Abstract: Inference for Large Language Models (LLMs) is computationally demanding. To reduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to store intermediate activations, which significantly lowers the computational overhead for token generation. However, the memory required for the KV cache grows rapidly, often exceeding the capacity of GPU memory. A cost-effective alternative is to offload KV cache to CPU memory, which alleviates GPU memory pressure, but shifts the bottleneck to the limited bandwidth of the PCIe connection between the CPU and GPU. Existing methods attempt to address these issues by overlapping GPU computation with I/O or employing CPU-GPU heterogeneous execution, but they are hindered by excessive data movement and dependence on CPU capabilities. Fully overlapping PCIe communication latency gets challenging as the size of the KV cache grows and/or the GPU compute capabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware LLM inference method where the CPU first transfers a partial set of activations, from which the GPU can start recomputing the KV cache values. While the GPU recomputes the partial KV cache, the remaining portion of the KV cache is transferred concurrently from the CPU. This approach overlaps GPU recomputation with KV cache transfer to minimize idle GPU time and maximize inference performance. KVPR is fully automated by integrating a profiler module that utilizes input characteristics and system hardware information, a scheduler module to optimize the distribution of computation and communication workloads, and a runtime module to efficiently execute the derived execution plan. Experimental results show that KVPR achieves up to 35.8% lower latency and 46.2% higher throughput during decoding compared to state-of-the-art approaches. The code is available at https://github.com/chaoyij/KVPR.

### 摘要
大型语言模型（LLM）的推理过程对计算资源要求极高。为降低自回归解码成本，键值（KV）缓存被用于存储中间激活状态，从而显著减少令牌生成的计算开销。然而KV缓存的内存需求增长迅猛，常超出GPU显存容量。将KV缓存卸载至CPU内存虽能缓解GPU显存压力，却会使性能瓶颈转移至CPU与GPU间有限的PCIe连接带宽。现有方法尝试通过重叠GPU计算与I/O操作或采用CPU-GPU异构执行来解决问题，但均受限于过高的数据迁移量和CPU性能依赖。随着KV缓存规模扩大和/或GPU算力提升，完全重叠PCIe通信延迟变得愈发困难。本文提出KVPR——一种高效的I/O感知型LLM推理方法：CPU首先传输部分激活状态，GPU据此开始重计算KV缓存值。当GPU重计算部分KV缓存时，剩余缓存数据可并行从CPU传输。该方法通过重叠GPU重计算与KV缓存传输，最小化GPU空闲时间并最大化推理性能。KVPR通过集成分析器模块（利用输入特征与系统硬件信息）、调度器模块（优化计算与通信负载分配）及运行时模块（高效执行派生计划）实现全自动化。实验表明，相比前沿方案，KVPR在解码阶段可实现高达35.8%的延迟降低与46.2%的吞吐量提升。代码已开源：https://github.com/chaoyij/KVPR。

---

## [HashAttention: Semantic Sparsity for Faster Inference](https://arxiv.org/abs/2412.14468)

### Abstract
arXiv:2412.14468v2 Announce Type: replace-cross 
Abstract: Leveraging long contexts is crucial for advanced AI systems, but attention computation poses a scalability challenge. While scaled dot-product attention (SDPA) exhibits token sparsity, i.e. only a few pivotal tokens significantly contribute to output, exploiting this sparsity remains challenging. Existing methods either suffer from quality degradation or require substantial additional resources. We show that identifying pivotal tokens is a Maximum Inner Product Search (MIPS) problem. However, existing MIPS solutions are not well-suited for SDPA, as they are not GPU-friendly and often underperform due to the separated query and key distributions. This paper introduces HashAttention, framing pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space, capturing the required semantic similarity, using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query using bitwise operations and computes attention using only these tokens, improving the overall attention efficiency. Trained on generic data, HashAttention reduces tokens used by up to $16\times$ with minimal quality loss, requiring only 32 bits of auxiliary memory per token. Sparsity can be further improved to $32\times$ through task-specific fine-tuning. On A100 GPU, at $32\times$ sparsity, incorporating HashAttention reduces attention latency by up to $4.3\times$ in GPT-FAST and $2.54\times$ in FlashDecode, and achieves up to $3.12\times$ higher throughput for GPT-FAST.

### 摘要
有效利用长上下文对先进AI系统至关重要，但注意力计算存在可扩展性挑战。虽然缩放点积注意力（SDPA）存在令牌稀疏性（即仅有少量关键令牌对输出有显著贡献），但利用这种稀疏性仍具挑战性。现有方法要么存在质量下降问题，要么需要大量额外资源。我们证明关键令牌识别可转化为最大内积搜索（MIPS）问题。然而现有MIPS方案并不适用于SDPA，因其非GPU友好且常因查询与键分布分离导致性能不佳。本文提出HashAttention，将关键令牌识别构建为推荐问题：给定查询时，该方法通过学习到的映射函数在汉明空间编码键和查询，捕获所需语义相似性。HashAttention通过位运算高效识别关键令牌，并仅计算这些令牌的注意力，从而提升整体注意力效率。在通用数据训练后，HashAttention能以最小质量损失实现最高16倍的令牌缩减，每个令牌仅需32比特辅助内存。通过任务特定微调可进一步实现32倍的稀疏性提升。在A100 GPU上，32倍稀疏性时采用HashAttention可使GPT-FAST的注意力延迟降低4.3倍，FlashDecode降低2.54倍，并为GPT-FAST带来最高3.12倍的吞吐量提升。

---

## [Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models](https://arxiv.org/abs/2502.01419)

### Abstract
arXiv:2502.01419v2 Announce Type: replace-cross 
Abstract: Detailed image captioning is essential for tasks like data generation and aiding visually impaired individuals. High-quality captions require a balance between precision and recall, which remains challenging for current multimodal large language models (MLLMs). In this work, we hypothesize that this limitation stems from weakening and increasingly noisy visual attention as responses lengthen. To address this issue, we propose SPARC (Selective Progressive Attention ReCalibration), a training-free method that enhances the contribution of visual tokens during decoding. SPARC is founded on three key observations: (1) increasing the influence of all visual tokens reduces recall; thus, SPARC selectively amplifies visual tokens; (2) as captions lengthen, visual attention becomes noisier, so SPARC identifies critical visual tokens by leveraging attention differences across time steps; (3) as visual attention gradually weakens, SPARC reinforces it to preserve its influence. Our experiments, incorporating both automated and human evaluations, demonstrate that existing methods improve the precision of MLLMs at the cost of recall. In contrast, our proposed method enhances both precision and recall with minimal computational overhead.

### 摘要
细致的图像描述生成对于数据生成和辅助视障人士等任务至关重要。高质量的描述需要在精确率和召回率之间取得平衡，这对当前多模态大语言模型（MLLMs）仍具挑战性。本研究提出，该限制源于随着响应长度增加而逐渐弱化且噪声增多的视觉注意力。为解决此问题，我们提出SPARC（选择性渐进注意力重校准），一种无需训练的解码过程中增强视觉标记贡献的方法。SPARC基于三个关键发现：（1）增强所有视觉标记的影响会降低召回率，因此SPARC选择性放大视觉标记；（2）随着描述长度增加，视觉注意力噪声加剧，故SPARC通过利用时间步间的注意力差异识别关键视觉标记；（3）随着视觉注意力逐渐弱化，SPARC对其进行强化以保持其影响力。结合自动评估与人工评估的实验表明，现有方法以提高召回率为代价来提升MLLMs的精确率，而本方法能以最小计算开销同时提升精确率和召回率。

---

## [Aligning Compound AI Systems via System-level DPO](https://arxiv.org/abs/2502.17721)

### Abstract
arXiv:2502.17721v2 Announce Type: replace-cross 
Abstract: Compound AI systems, comprising multiple interacting components such as LLMs, foundation models, and external tools, have demonstrated remarkable improvements compared to single models in various tasks. To ensure their effective deployment in real-world applications, aligning these systems with human preferences is crucial. However, aligning the compound system via policy optimization, unlike the alignment of a single model, is challenging for two main reasons: (i) non-differentiable interactions between components make end-to-end gradient-based optimization method inapplicable, and (ii) system-level preferences cannot be directly transformed into component-level preferences. To address these challenges, we first formulate compound AI systems as Directed Acyclic Graphs (DAGs), explicitly modeling both component interactions and the associated data flows. Building on this formulation, we introduce $\textbf&#123;SysDPO&#125;$, a framework that extends Direct Preference Optimization (DPO) to enable joint system-level alignment. We propose two variants, SysDPO-Direct and SysDPO-Sampling, tailored for scenarios depending on whether we construct a system-specific preference dataset. We empirically demonstrate the effectiveness of our approach across two applications: the joint alignment of a language model and a diffusion model, and the joint alignment of an LLM collaboration system.

### 摘要
由大型语言模型、基础模型和外部工具等多个交互组件构成的复合人工智能系统，相较于单一模型已在多项任务中展现出显著性能提升。为确保此类系统在现实应用中的有效部署，使其与人类偏好保持一致至关重要。然而，与单一模型的对齐不同，通过策略优化实现复合系统的对齐面临两大核心挑战：（1）组件间的不可微分交互导致端到端梯度优化方法失效；（2）系统级偏好无法直接转化为组件级偏好。针对这些挑战，我们首先将复合AI系统建模为有向无环图（DAG），显式刻画组件交互及数据流。基于此形式化框架，我们提出$	extbf&#123;SysDPO&#125;$——一种将直接偏好优化（DPO）扩展至系统级联合对齐的新方法，并针对是否构建系统专属偏好数据集的不同场景，开发了SysDPO-Direct和SysDPO-Sampling两种变体。通过语言模型与扩散模型的联合对齐、以及LLM协作系统的联合对齐两个应用场景的实验，我们验证了该框架的有效性。

---

## [Sliding Window Attention Training for Efficient Large Language Models](https://arxiv.org/abs/2502.18845)

### Abstract
arXiv:2502.18845v2 Announce Type: replace-cross 
Abstract: Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at https://github.com/Fzkuji/swat-attention.

### 摘要
基于Transformer的大型语言模型（LLM）近期取得显著进展，在各种任务中展现出卓越性能。然而，其关于序列长度的二次计算复杂度仍是处理长文档时的重大瓶颈。为此，研究者提出了稀疏注意力、状态空间模型等多种方案来提升LLM在长序列上的效率。这些方法虽有效，但会牺牲模型性能或引入结构复杂性。因此亟需一种能保持基础Transformer架构的简洁高效模型。本文提出SWAT模型，通过滑动窗口注意力训练实现高效长上下文处理。研究首先将Transformer的低效性归因于softmax操作高方差导致的注意力汇聚现象，随后用sigmoid函数替代softmax，并采用平衡的ALiBi与旋转位置编码来实现高效信息压缩与保留。实验表明，SWAT在八个基准测试中相比最先进的线性循环架构实现了SOTA性能。代码已开源：https://github.com/Fzkuji/swat-attention。

---

## [Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination](https://arxiv.org/abs/2503.04149)

### Abstract
arXiv:2503.04149v2 Announce Type: replace-cross 
Abstract: The rapid evolution of code largelanguage models underscores the need for effective and transparent benchmarking of their reasoning capabilities. However, the current benchmarking approach heavily depends on publicly available, human-created datasets. The widespread use of these fixed benchmark datasets makes the benchmarking process to be static and thus particularly susceptible to data contamination, an unavoidable consequence of the extensive data collection processes used to train Code LLMs. Existing approaches that address data contamination often suffer from human effort limitations and imbalanced problem complexity. To tackle these challenges, we propose \tool, a novel benchmarking suite for evaluating Code LLMs under potential data contamination. Given a seed programming problem, \tool employs multiple agents to extract and modify the context without altering the core logic, generating semantically equivalent variations. We introduce a dynamic data generation methods and conduct empirical studies on two seed datasets across 21 Code LLMs. Results show that \tool effectively benchmarks reasoning capabilities under contamination risks while generating diverse problem sets to ensure consistent and reliable evaluations.

### 摘要
代码大语言模型的快速发展凸显了对其推理能力进行有效且透明评估的必要性。然而，当前基准测试方法严重依赖公开可用的人工创建数据集。这些固定基准数据集的广泛使用使得评估过程呈现静态化特征，因而特别容易受到数据污染的影响——这是训练代码大语言模型所采用的大规模数据收集过程中不可避免的结果。现有解决数据污染的方法常受限于人工工作量且存在问题复杂度不均衡的缺陷。为应对这些挑战，我们提出\tool这一创新基准测试套件，用于在潜在数据污染环境下评估代码大语言模型。给定种子编程问题后，\tool通过多智能体协作在不改变核心逻辑的前提下提取并修改上下文，生成语义等效的变体。我们提出动态数据生成方法，并在21个代码大语言模型上对两个种子数据集开展实证研究。结果表明，\tool能有效评估污染风险下的推理能力，同时生成多样化问题集以确保评估的一致性与可靠性。

---

## [Biased by Design: Leveraging AI Inherent Biases to Enhance Critical Thinking of News Readers](https://arxiv.org/abs/2504.14522)

### Abstract
arXiv:2504.14522v2 Announce Type: replace-cross 
Abstract: This paper explores the design of a propaganda detection tool using Large Language Models (LLMs). Acknowledging the inherent biases in AI models, especially in political contexts, we investigate how these biases might be leveraged to enhance critical thinking in news consumption. Countering the typical view of AI biases as detrimental, our research proposes strategies of user choice and personalization in response to a user's political stance, applying psychological concepts of confirmation bias and cognitive dissonance. We present findings from a qualitative user study, offering insights and design recommendations (bias awareness, personalization and choice, and gradual introduction of diverse perspectives) for AI tools in propaganda detection.

### 摘要
本文探讨了利用大语言模型(LLMs)设计政治宣传检测工具的方法。研究承认人工智能模型特别是政治语境中存在的固有偏见，并探究如何利用这些偏见来增强新闻消费中的批判性思维。与传统视AI偏见为有害的观点不同，我们基于确认偏误和认知失调等心理学概念，提出了针对用户政治立场的个性化选择策略。通过定性用户研究，我们为宣传检测AI工具提出了三项设计建议(偏见认知、个性化选择及渐进式多元观点引入)，并提供了相关研究发现与见解。

---

## [D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model Generation Using Large Language Models](https://arxiv.org/abs/2502.16540)

### Abstract
arXiv:2502.16540v2 Announce Type: replace-cross 
Abstract: In electronic design, engineers often manually search through extensive documents to retrieve component parameters required for constructing SPICE models, a process that is both labor-intensive and time-consuming. To address this challenge, we present an automated framework called D2S-FLOW that leverages large language models (LLMs) to extract electrical parameters from datasheets and generate SPICE models with high precision and efficiency, significantly reducing the need for manual intervention. Unlike traditional RAG systems, D2S-FLOW employs a workflow to enhance precision in handling unstructured documents and inconsistent naming conventions through three innovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity Normalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER utilizes document structure for precise parameter localization, and HNEN standardizes terminology via semantic inference. Experimental results demonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1 score of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the strongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it reduces API token consumption by 38% and minimizes the irrelevant information ratio to 4%, showcasing substantial improvements in resource efficiency. This research provides an effective automated solution for circuit design.

### 摘要
在电子设计领域，工程师通常需要手动检索大量文档以获取构建SPICE模型所需的元件参数，这一过程既费时又费力。为解决该问题，我们提出名为D2S-FLOW的自动化框架，通过利用大语言模型（LLMs）从数据手册中提取电气参数并高效精准地生成SPICE模型，显著减少人工干预需求。与传统RAG系统不同，D2S-FLOW采用创新工作流，通过三项机制提升处理非结构化文档和命名不一致的精度：注意力引导文档聚焦（AGDF）将检索范围限定于用户选定文档，层级化文档增强检索（HDER）利用文档结构实现参数精确定位，异构命名实体归一化（HNEN）通过语义推理实现术语标准化。实验结果表明，该框架在精确匹配（EM）、F1分数和完全正确率（EC）上分别达到0.86、0.92和0.96，较最优基线提升19.4%、5.7%和13.1%。此外，其API令牌消耗降低38%，无关信息比率控制在4%，显著提升了资源效率。本研究为电路设计提供了有效的自动化解决方案。

---

## [PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts](https://arxiv.org/abs/2503.06706)

### Abstract
arXiv:2503.06706v2 Announce Type: replace-cross 
Abstract: Process-driven dialogue systems, which operate under strict predefined process constraints, are essential in customer service and equipment maintenance scenarios. Although Large Language Models (LLMs) have shown remarkable progress in dialogue and reasoning, they still struggle to solve these strictly constrained dialogue tasks. To address this challenge, we construct Process Flow Dialogue (PFDial) dataset, which contains 12,705 high-quality Chinese dialogue instructions derived from 440 flowcharts containing 5,055 process nodes. Based on PlantUML specification, each UML flowchart is converted into atomic dialogue units i.e., structured five-tuples. Experimental results demonstrate that a 7B model trained with merely 800 samples, and a 0.5B model trained on total data both can surpass 90% accuracy. Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of 11.00%. We further evaluate models' performance on challenging backward transitions in process flows and conduct an in-depth analysis of various dataset formats to reveal their impact on model performance in handling decision and sequential branches. The data is released in https://github.com/KongLongGeFDU/PFDial.

### 摘要
流程驱动对话系统在严格预定义流程约束下运行，是客户服务和设备维护场景中的关键解决方案。尽管大语言模型（LLMs）在对话和推理方面展现出显著进步，但仍难以应对此类严格约束的对话任务。为解决这一挑战，我们构建了流程对话数据集（PFDial），包含源自440个流程图（含5,055个流程节点）的12,705条高质量中文对话指令。基于PlantUML规范，每个UML流程图被转化为原子对话单元——结构化五元组。实验结果表明：仅用800样本训练的7B模型，以及全量数据训练的0.5B模型，准确率均可突破90%；8B模型较GPT-4o最高可提升43.88%（平均11.00%）。我们进一步评估了模型在流程逆向跳转任务中的表现，并通过不同数据格式的深度分析揭示了其对模型处理决策分支和顺序分支能力的影响。数据已发布于https://github.com/KongLongGeFDU/PFDial。

---

## [Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding](https://arxiv.org/abs/2504.00030)

### Abstract
arXiv:2504.00030v3 Announce Type: replace-cross 
Abstract: Speculative decoding accelerates large language model (LLM) inference by using a smaller draft model to propose tokens, which are then verified by a larger target model. However, selecting an optimal speculation length is critical for maximizing speedup while minimizing wasted computation. We introduce \textit&#123;GammaTune&#125; and \textit&#123;GammaTune+&#125;, training-free adaptive algorithms that dynamically adjust speculation length based on token acceptance rates using a heuristic-based switching mechanism. Evaluated on SpecBench across multiple tasks and model pairs, our method outperforms other heuristic-based approaches and fixed-length speculative decoding, achieving an average speedup of 15\% ($\pm$5\%) with \textit&#123;GammaTune&#125; and 16\% ($\pm$3\%) with \textit&#123;GammaTune+&#125;, while reducing performance variance. This makes \textit&#123;GammaTune&#125; a robust and efficient solution for real-world deployment.

### 摘要
推测解码技术通过使用较小的草稿模型生成候选标记，再由较大的目标模型进行验证，从而加速大语言模型（LLM）的推理过程。然而，选择最优的推测长度对于最大化加速效果同时减少计算浪费至关重要。我们提出GammaTune和GammaTune+这两种无需训练的自适应算法，其基于启发式切换机制，根据标记接受率动态调整推测长度。在SpecBench多任务多模型对的测试中，我们的方法优于其他基于启发式的方案和固定长度推测解码，平均加速比达到15%（±5%）(GammaTune)和16%（±3%）(GammaTune+)，同时降低了性能波动。这使得GammaTune成为实际部署中鲁棒且高效的解决方案。

---

## [ROGRAG: A Robustly Optimized GraphRAG Framework](https://arxiv.org/abs/2503.06474)

### Abstract
arXiv:2503.06474v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) commonly struggle with specialized or emerging topics which are rarely seen in the training corpus. Graph-based retrieval-augmented generation (GraphRAG) addresses this by structuring domain knowledge as a graph for dynamic retrieval. However, existing pipelines involve complex engineering workflows, making it difficult to isolate the impact of individual components. It is also challenging to evaluate the retrieval effectiveness due to the overlap between the pretraining and evaluation datasets. In this work, we introduce ROGRAG, a Robustly Optimized GraphRAG framework. Specifically, we propose a multi-stage retrieval mechanism that integrates dual-level with logic form retrieval methods to improve retrieval robustness without increasing computational cost. To further refine the system, we incorporate various result verification methods and adopt an incremental database construction approach. Through extensive ablation experiments, we rigorously assess the effectiveness of each component. Our implementation includes comparative experiments on SeedBench, where Qwen2.5-7B-Instruct initially underperformed. ROGRAG significantly improves the score from 60.0% to 75.0% and outperforms mainstream methods. Experiments on domain-specific datasets reveal that dual-level retrieval enhances fuzzy matching, while logic form retrieval improves structured reasoning, highlighting the importance of multi-stage retrieval.ROGRAG is released as an open-source resource and supports installation with pip.

### 摘要
大语言模型（LLMs）在处理训练语料中罕见的专业或新兴主题时通常表现不佳。基于图的检索增强生成（GraphRAG）通过将领域知识构建为图结构以实现动态检索来解决这一问题。然而，现有流程涉及复杂的工程工作流，难以分离各组件的影响。由于预训练与评估数据集之间的重叠，评估检索效果也颇具挑战性。本研究提出ROGRAG框架，一种经过鲁棒优化的GraphRAG方法。具体而言，我们设计了一种多阶段检索机制，将双层级检索与逻辑形式检索方法相结合，在不增加计算成本的前提下提升检索鲁棒性。为进一步优化系统，我们整合了多种结果验证方法，并采用增量式数据库构建策略。通过大量消融实验，我们严格评估了各组件的有效性。实验部分包括在SeedBench上的对比测试：Qwen2.5-7B-Instruct初始表现欠佳，而ROGRAG将其得分从60.0%显著提升至75.0%，优于主流方法。在领域专用数据集上的实验表明，双层级检索增强了模糊匹配能力，逻辑形式检索则改善了结构化推理，凸显了多阶段检索的重要性。ROGRAG已作为开源资源发布，支持pip安装。

---

## [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)

### Abstract
arXiv:2504.13677v2 Announce Type: replace-cross 
Abstract: Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving their safety and reliability. Evaluations often use metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). We show that mutual biases--when both UQ methods and correctness functions are biased by the same factors--systematically distort evaluation. First, we formally prove that any mutual bias non-randomly skews AUROC rankings, compromising benchmark integrity. Second, we confirm this happens empirically by testing 7 widely used correctness functions, from lexical-based and embedding-based metrics to LM-as-a-judge approaches, across 4 datasets x 4 models x 8 UQ methods. Our analysis shows that length biases in correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LM-as-a-judge methods as the least length-biased, offering a promising path for a fairer UQ evaluation.

### 摘要
语言模型（LMs）中的不确定性量化（UQ）是提升其安全性与可靠性的关键。现有评估通常采用AUROC等指标来衡量UQ方法（如负序列概率）与任务正确性函数（如ROUGE-L）的关联程度。本文揭示当UQ方法与正确性函数受到相同因素影响而产生共同偏差时，会系统性扭曲评估结果。首先，我们通过形式化证明表明，任何非随机性的共同偏差都会导致AUROC排名失真，从而破坏基准的完整性。其次，我们在4个数据集×4个模型×8种UQ方法的组合下，测试了7种广泛使用的正确性函数（从基于词汇、嵌入的指标到LM-as-a-judge方法），实证验证了这一现象。分析表明，正确性函数中的长度偏差会与UQ方法中的长度偏差相互作用，导致UQ评估失真。研究发现LM-as-a-judge方法受长度偏差影响最小，为构建更公平的UQ评估体系提供了可行路径。

---

## [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)

### Abstract
arXiv:2504.13865v2 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative paradigm in human-computer interaction, evolving from rule-based automation scripts to sophisticated AI-driven systems capable of understanding and executing complex interface operations. This survey provides a comprehensive examination of the rapidly advancing field of LLM-based GUI Agents, systematically analyzing their architectural foundations, technical components, and evaluation methodologies. We identify and analyze four fundamental components that constitute modern GUI Agents: (1) perception systems that integrate text-based parsing with multimodal understanding for comprehensive interface comprehension; (2) exploration mechanisms that construct and maintain knowledge bases through internal modeling, historical experience, and external information retrieval; (3) planning frameworks that leverage advanced reasoning methodologies for task decomposition and execution; and (4) interaction systems that manage action generation with robust safety controls. Through rigorous analysis of these components, we reveal how recent advances in large language models and multimodal learning have revolutionized GUI automation across desktop, mobile, and web platforms. We critically examine current evaluation frameworks, highlighting methodological limitations in existing benchmarks while proposing directions for standardization. This survey also identifies key technical challenges, including accurate element localization, effective knowledge retrieval, long-horizon planning, and safety-aware execution control, while outlining promising research directions for enhancing GUI Agents' capabilities. Our systematic review provides researchers and practitioners with a thorough understanding of the field's current state and offers insights into future developments in intelligent interface automation.

### 摘要
图形用户界面（GUI）智能体已成为人机交互领域的革命性范式，其发展轨迹从基于规则的自动化脚本演进为能够理解并执行复杂界面操作的AI驱动系统。本综述对基于大语言模型的GUI智能体这一快速发展的领域进行了全面考察，系统分析了其架构基础、技术组件及评估方法。我们识别并剖析了构成现代GUI智能体的四大核心组件：(1)感知系统——结合文本解析与多模态理解实现界面全面认知；(2)探索机制——通过内部建模、历史经验和外部信息检索构建并维护知识库；(3)规划框架——运用先进推理方法进行任务分解与执行；(4)交互系统——在强健的安全控制下管理动作生成。通过对这些组件的严谨分析，我们揭示了大规模语言模型与多模态学习的最新进展如何彻底变革桌面端、移动端及网页平台的GUI自动化技术。本文批判性地审视现有评估框架，指出现有基准测试的方法学局限，并提出标准化发展方向。同时明确了一系列关键技术挑战，包括精确元素定位、高效知识检索、长程规划及安全感知执行控制，并勾勒出提升GUI智能体能力的潜在研究方向。本系统性综述为研究人员与实践者提供了对该领域现状的透彻理解，并为智能界面自动化的未来发展提供了前瞻性见解。

---

## [LLM Code Customization with Visual Results: A Benchmark on TikZ](https://arxiv.org/abs/2505.04670)

### Abstract
arXiv:2505.04670v2 Announce Type: replace-cross 
Abstract: With the rise of AI-based code generation, customizing existing code out of natural language instructions to modify visual results -such as figures or images -has become possible, promising to reduce the need for deep programming expertise. However, even experienced developers can struggle with this task, as it requires identifying relevant code regions (feature location), generating valid code variants, and ensuring the modifications reliably align with user intent. In this paper, we introduce vTikZ, the first benchmark designed to evaluate the ability of Large Language Models (LLMs) to customize code while preserving coherent visual outcomes. Our benchmark consists of carefully curated vTikZ editing scenarios, parameterized ground truths, and a reviewing tool that leverages visual feedback to assess correctness. Empirical evaluation with stateof-the-art LLMs shows that existing solutions struggle to reliably modify code in alignment with visual intent, highlighting a gap in current AI-assisted code editing approaches. We argue that vTikZ opens new research directions for integrating LLMs with visual feedback mechanisms to improve code customization tasks in various domains beyond TikZ, including image processing, art creation, Web design, and 3D modeling.

### 摘要
随着基于AI的代码生成技术兴起，通过自然语言指令定制现有代码以修改可视化结果（如图形或图像）已成为可能，这有望降低对深度编程专业知识的依赖。然而，即使是经验丰富的开发者也难以胜任该任务，因为它需要识别相关代码区域（特征定位）、生成有效代码变体，并确保修改内容与用户意图可靠对齐。本文提出vTikZ——首个用于评估大语言模型（LLMs）在保持视觉连贯性的同时定制代码能力的基准测试。我们的基准包含精心设计的vTikZ编辑场景、参数化真实案例，以及利用视觉反馈评估正确性的审查工具。针对前沿LLMs的实证评估表明，现有解决方案难以可靠地按照视觉意图修改代码，揭示了当前AI辅助代码编辑方法的不足。我们认为vTikZ为整合LLMs与视觉反馈机制开辟了新研究方向，可改进TikZ之外多个领域的代码定制任务，包括图像处理、艺术创作、网页设计和3D建模。

---

