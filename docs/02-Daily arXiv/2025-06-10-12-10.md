# 2025-06-10-12-10

## [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)

### Abstract
arXiv:2506.06472v1 Announce Type: new 
Abstract: We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47x on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory.

### 摘要
我们提出了一种新型生命周期感知的张量卸载框架TERAIO的设计与实现，该框架利用基于PCIe的低成本固态硬盘(SSD)实现GPU内存扩展。TERAIO专为多GPU多SSD环境下的大语言模型(LLM)训练而开发。设计动机源于我们观察到：在每次LLM训练迭代中，活跃张量仅占GPU已分配内存的很小部分（平均1.7%），非活跃张量通常体积庞大且长期不被使用，这为在SSD与GPU间进行张量卸载/预取创造了充分机会，且不会中断GPU训练过程。TERAIO通过分析训练过程前几次迭代的性能特征，精确预估每个张量在GPU内存中的生命周期（活跃时段）。基于张量生命周期分析，TERAIO将生成优化的张量卸载/预取方案，并通过PyTorch将其集成到编译后的LLM程序中。该框架配备运行时张量迁移引擎，通过GPUDirect存储技术执行卸载/预取计划，实现GPU与SSD间的直接张量传输，从而缓解CPU瓶颈并最大化SSD带宽利用率。与ZeRO-Offload、ZeRO-Infinity等前沿研究相比，TERAIO将各类LLM的训练性能平均提升1.47倍，在假设GPU内存无限的情况下可达到理想性能的80.7%。

---

## [Hierarchical Debate-Based Large Language Model (LLM) for Complex Task Planning of 6G Network Management](https://arxiv.org/abs/2506.06519)

### Abstract
arXiv:2506.06519v1 Announce Type: new 
Abstract: 6G networks have become increasingly complicated due to novel network architecture and newly emerging signal processing and transmission techniques, leading to significant burdens to 6G network management. Large language models (LLMs) have recently been considered a promising technique to equip 6G networks with AI-native intelligence. Different from most existing studies that only consider a single LLM, this work involves a multi-LLM debate-based scheme for 6G network management, where multiple LLMs can collaboratively improve the initial solution sequentially. Considering the complex nature of 6G domain, we propose a novel hierarchical debate scheme: LLMs will first debate the sub-task decomposition, and then debate each subtask step-by-step. Such a hierarchical approach can significantly reduce the overall debate difficulty by sub-task decomposition, aligning well with the complex nature of 6G networks and ensuring the final solution qualities. In addition, to better evaluate the proposed technique, we have defined a novel dataset named 6GPlan, including 110 complex 6G network management tasks and 5000 keyword solutions. Finally, the experiments show that the proposed hierarchical debate can significantly improve performance compared to baseline techniques, e.g. more than 30% coverage rate and global recall rate improvement.

### 摘要
由于新型网络架构及新兴信号处理与传输技术的出现，6G网络变得日益复杂，这给网络管理带来了巨大负担。近期，大语言模型（LLMs）被视为赋予6G网络原生人工智能能力的重要技术。与现有大多数仅考虑单一LLM的研究不同，本研究提出了一种基于多LLM辩论的6G网络管理方案，通过多个LLM的协作逐步优化初始解决方案。针对6G领域的高度复杂性，我们创新性地设计了分层辩论机制：LLMs首先对子任务分解进行辩论，随后逐步对每个子任务展开辩论。这种分层方法通过任务分解显著降低了整体辩论难度，既契合6G网络的复杂特性，又能确保最终解决方案的质量。此外，为更好评估所提技术，我们构建了名为6GPlan的新型数据集，包含110项复杂6G网络管理任务及5000个关键词解决方案。实验结果表明，相较于基线技术，所提出的分层辩论方案能显著提升性能指标，例如覆盖率与全局召回率均提高30%以上。

---

## [Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach](https://arxiv.org/abs/2506.06282)

### Abstract
arXiv:2506.06282v1 Announce Type: new 
Abstract: Effective financial reasoning demands not only textual understanding but also the ability to interpret complex visual data such as charts, tables, and trend graphs. This paper introduces a new benchmark designed to evaluate how well AI models - especially large language and multimodal models - reason in finance-specific contexts. Covering 3,200 expert-level question-answer pairs across 15 core financial topics, the benchmark integrates both textual and visual modalities to reflect authentic analytical challenges in finance. To address limitations in current reasoning approaches, we propose an error-aware learning framework that leverages historical model mistakes and feedback to guide inference, without requiring fine-tuning. Our experiments across state-of-the-art models show that multimodal inputs significantly enhance performance and that incorporating error feedback leads to consistent and measurable improvements. The results highlight persistent challenges in visual understanding and mathematical logic, while also demonstrating the promise of self-reflective reasoning in financial AI systems. Our code and data can be found at https://anonymous/FinMR/CodeData.

### 摘要
有效的金融推理不仅需要文本理解能力，还需具备解读图表、表格和趋势图等复杂视觉数据的能力。本文提出了一种新的基准测试，旨在评估AI模型——特别是大语言模型和多模态模型——在金融特定语境中的推理表现。该基准涵盖15个核心金融主题的3,200个专家级问答对，整合了文本与视觉模态以反映真实的金融分析挑战。针对当前推理方法的局限性，我们提出了一种误差感知学习框架，该框架利用历史模型错误和反馈来指导推理，且无需微调。通过对前沿模型的实验表明，多模态输入能显著提升性能，而融入误差反馈可带来持续且可量化的改进。研究结果揭示了视觉理解和数理逻辑方面存在的持续挑战，同时也证明了自反思推理在金融AI系统中的潜力。代码与数据详见https://anonymous/FinMR/CodeData。

---

## [ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search](https://arxiv.org/abs/2506.06524)

### Abstract
arXiv:2506.06524v1 Announce Type: new 
Abstract: There is much interest in using large pre-trained models in Automatic Game Design (AGD), whether via the generation of code, assets, or more abstract conceptualization of design ideas. But so far this interest largely stems from the ad hoc use of such generative models under persistent human supervision. Much work remains to show how these tools can be integrated into longer-time-horizon AGD pipelines, in which systems interface with game engines to test generated content autonomously. To this end, we introduce ScriptDoctor, a Large Language Model (LLM)-driven system for automatically generating and testing games in PuzzleScript, an expressive but highly constrained description language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates and tests game design ideas in an iterative loop, where human-authored examples are used to ground the system's output, compilation errors from the PuzzleScript engine are used to elicit functional code, and search-based agents play-test generated games. ScriptDoctor serves as a concrete example of the potential of automated, open-ended LLM-based workflows in generating novel game content.

### 摘要
在自动游戏设计（AGD）领域，利用大型预训练模型生成代码、资源或更抽象的设计概念正受到广泛关注。然而目前这种兴趣主要源于人类持续监督下对这些生成模型的临时性使用。如何将这些工具整合到长期AGD流程中——即系统通过与游戏引擎交互自主测试生成内容——仍需大量研究。为此，我们提出ScriptDoctor系统，这是一个基于大语言模型（LLM）的自动化解决方案，专门用于在PuzzleScript（一种针对二维网格世界回合制解谜游戏的高度受限描述语言）中生成并测试游戏。该系统通过迭代循环实现设计生成与测试：以人工编写样本为基础约束输出内容，利用PuzzleScript引擎的编译错误引导功能代码生成，并采用基于搜索的智能体对生成游戏进行试玩测试。ScriptDoctor为基于LLM的自动化开放式工作流在生成新颖游戏内容方面的潜力提供了具体范例。

---

## [Deep Research Bench: Evaluating AI Web Research Agents](https://arxiv.org/abs/2506.06287)

### Abstract
arXiv:2506.06287v1 Announce Type: new 
Abstract: Amongst the most common use cases of modern AI is LLM chat with web search enabled. However, no direct evaluations of the quality of web research agents exist that control for the continually-changing web. We introduce Deep Research Bench, consisting of 89 multi-step web research task instances of varying difficulty across 8 diverse task categories, with the answers carefully worked out by skilled humans. We provide a "RetroSearch" environment with a large frozen set of scraped web pages, and demonstrate that offline "RetroSearch" agents perform comparably to "live web" agents, enabling reliable evaluations of models over time. We provide robust agent tooling and scaffolding to benchmark major LLMs as they are released, including "thinking" models like o3 and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent traces to report progress over time in hallucinations, tool use, and forgetting. Finally, we evaluate the major web research products branded as "Deep Research", "Deep Search", "Search", or "Research." Results are available on a public leaderboard at https://drb.futuresearch.ai/.

### 摘要
现代人工智能最常见的应用场景之一是支持网络搜索的大语言模型聊天。然而目前尚缺乏针对持续变化的网络环境所设计的网络研究智能体质量直接评估方法。本研究推出深度研究基准（Deep Research Bench），包含8个不同任务类别中89个难度各异的多步骤网络研究任务实例，所有答案均由专业人员精心验证完成。我们构建了包含大量冻结网页抓取数据的"回溯搜索"（RetroSearch）环境，并证明离线"回溯搜索"智能体的表现与"实时网络"智能体相当，从而实现了模型评估的长期可靠性。我们提供完善的智能体工具链和框架，用于对包括o3和Gemini 2.5 Pro等"思考型"模型在内的主流大语言模型进行版本迭代基准测试。通过自动化评估冗长的智能体操作轨迹，我们持续追踪幻觉现象、工具使用和记忆遗忘等指标的进展。最后，我们对标榜为"深度研究"、"深度搜索"、"搜索"或"研究"的主流网络研究产品进行了评估。所有结果均发布于公开排行榜：https://drb.futuresearch.ai/。

---

## [Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review](https://arxiv.org/abs/2506.06301)

### Abstract
arXiv:2506.06301v1 Announce Type: new 
Abstract: Roadway safety and mobility remain critical challenges for modern transportation systems, demanding innovative analytical frameworks capable of addressing complex, dynamic, and heterogeneous environments. While traditional engineering methods have made progress, the complexity and dynamism of real-world traffic necessitate more advanced analytical frameworks. Large Language Models (LLMs), with their unprecedented capabilities in natural language understanding, knowledge integration, and reasoning, represent a promising paradigm shift. This paper comprehensively reviews the application and customization of LLMs for enhancing roadway safety and mobility. A key focus is how LLMs are adapted -- via architectural, training, prompting, and multimodal strategies -- to bridge the "modality gap" with transportation's unique spatio-temporal and physical data. The review systematically analyzes diverse LLM applications in mobility (e.g., traffic flow prediction, signal control) and safety (e.g., crash analysis, driver behavior assessment,). Enabling technologies such as V2X integration, domain-specific foundation models, explainability frameworks, and edge computing are also examined. Despite significant potential, challenges persist regarding inherent LLM limitations (hallucinations, reasoning deficits), data governance (privacy, bias), deployment complexities (sim-to-real, latency), and rigorous safety assurance. Promising future research directions are highlighted, including advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI collaboration, continuous learning, and the development of efficient, verifiable systems. This review provides a structured roadmap of current capabilities, limitations, and opportunities, underscoring LLMs' transformative potential while emphasizing the need for responsible innovation to realize safer, more intelligent transportation systems.

### 摘要
道路安全与通行效率仍是现代交通系统面临的关键挑战，这要求我们建立能够应对复杂、动态、异构环境的新型分析框架。尽管传统工程方法已取得进展，但现实交通的复杂性与动态性需要更先进的分析范式。大型语言模型（LLM）凭借其在自然语言理解、知识整合与推理方面的卓越能力，正带来革命性的范式转变。本文全面综述了LLM在提升道路安全与通行效率中的应用与定制化实践，重点探讨如何通过架构调整、训练优化、提示工程及多模态策略来弥合交通领域特有的时空-物理数据"模态鸿沟"。研究系统分析了LLM在通行效率（如交通流预测、信号控制）与安全领域（如事故分析、驾驶行为评估）的多样化应用，同时考察了V2X集成、领域专用基础模型、可解释性框架及边缘计算等支撑技术。尽管潜力巨大，LLM仍面临固有局限（幻觉现象、推理缺陷）、数据治理（隐私保护、偏差控制）、部署复杂性（仿真到现实的迁移、延迟优化）及严格的安全验证等挑战。本文进一步指出了未来研究方向，包括先进多模态融合、时空推理增强、人机协作、持续学习以及高效可验证系统的开发。本综述通过结构化路线图阐明了当前技术能力、局限与发展机遇，既彰显了LLM的变革潜力，也强调需要负责任的技术创新来实现更安全、更智能的交通系统。

---

## [Memory OS of AI Agent](https://arxiv.org/abs/2506.06326)

### Abstract
arXiv:2506.06326v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at https://github.com/BAI-LAB/MemoryOS.

### 摘要
大语言模型（LLMs）面临固定上下文窗口和内存管理不足的关键挑战，导致其长期记忆能力严重不足，与AI代理的交互体验中个性化程度有限。为克服这一挑战，我们创新性地提出了一种内存操作系统——MemoryOS，以实现对AI代理全面高效的内存管理。受操作系统内存管理原理启发，MemoryOS设计了分层存储架构，包含四个核心模块：内存存储、更新、检索与生成。具体而言，该架构由三级存储单元构成：短期记忆、中期记忆和长期个人记忆。MemoryOS中的关键操作包括存储单元间的动态更新：短期到中期的更新遵循基于对话链的FIFO原则，而中期到长期的更新采用分段页组织策略。我们开创性的MemoryOS实现了分层内存整合与动态更新。在LoCoMo基准测试上的大量实验表明，相较于GPT-4o-mini的基线模型，其F1值平均提升49.11%，BLEU-1值平均提升46.18%，验证了其在长对话中保持上下文连贯性与个性化记忆留存的能力。实现代码已开源：https://github.com/BAI-LAB/MemoryOS。

---

## [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)

### Abstract
arXiv:2506.06470v1 Announce Type: new 
Abstract: Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.

### 摘要
通过简单扩大数据集来增强大型语言模型的方法已开始呈现收益递减趋势，研究焦点随之转向数据质量。蒙特卡洛树搜索（MCTS）作为一种生成高质量思维链数据的技术崭露头角，但传统方法通常仅保留搜索树中的最高分轨迹，而丢弃包含宝贵局部洞见、重复错误模式和替代推理策略的兄弟节点。这种对非最优推理分支的无条件舍弃可能导致整个搜索树中大量信息数据的浪费。我们提出SIGMA（兄弟节点引导的蒙特卡洛增强框架），该创新框架通过重新整合这些被丢弃的兄弟节点来优化LLM推理。SIGMA沿每条搜索路径构建兄弟节点间的语义关联，并实施两阶段优化：批判模型识别兄弟节点集中被忽视的优势与缺陷，修订模型则基于这种对比反馈进行文本反向传播以精炼最高分轨迹。通过挖掘并强化非最优推理分支中未被充分利用但极具价值的信号，SIGMA显著提升了推理轨迹质量。在具有挑战性的MATH基准测试中，经SIGMA调优的70亿参数模型仅使用3万样本即达到54.92%准确率，优于基于59万样本训练的现有最优模型。这一结果表明，我们的兄弟节点引导优化不仅能大幅降低数据用量，更能显著提升LLM的推理能力。

---

## [\textit&#123;QuantMCP&#125;: Grounding Large Language Models in Verifiable Financial Reality](https://arxiv.org/abs/2506.06622)

### Abstract
arXiv:2506.06622v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing financial analysis and decision-making, yet their direct application is often hampered by issues of data hallucination and lack of access to real-time, verifiable financial information. This paper introduces QuantMCP, a novel framework designed to rigorously ground LLMs in financial reality. By leveraging the Model Context Protocol (MCP) for standardized and secure tool invocation, QuantMCP enables LLMs to accurately interface with a diverse array of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can interact via natural language to precisely retrieve up-to-date financial data, thereby overcoming LLM's inherent limitations in factual data recall. More critically, once furnished with this verified, structured data, the LLM's analytical capabilities are unlocked, empowering it to perform sophisticated data interpretation, generate insights, and ultimately support more informed financial decision-making processes. QuantMCP provides a robust, extensible, and secure bridge between conversational AI and the complex world of financial data, aiming to enhance both the reliability and the analytical depth of LLM applications in finance.

### 摘要
大语言模型（LLMs）在革新金融分析与决策方面具有巨大潜力，但其直接应用常受数据幻觉和缺乏实时可验证金融信息的问题阻碍。本文提出QuantMCP框架，旨在将LLMs严格锚定于金融现实。通过采用模型上下文协议（MCP）实现标准化且安全的工具调用，QuantMCP使LLMs能准确对接多种Python可访问的金融数据API（如Wind、yfinance）。用户可通过自然语言交互精准获取最新金融数据，从而克服LLM在事实数据召回方面的固有局限。更关键的是，当模型获得此类经过验证的结构化数据后，其分析能力将被激活，使其能够执行复杂的数据解读、生成洞察，并最终支持更明智的金融决策流程。QuantMCP在对话式AI与复杂金融数据世界之间构建了稳健、可扩展且安全的桥梁，致力于提升LLM金融应用的可靠性与分析深度。

---

## [AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method](https://arxiv.org/abs/2506.06740)

### Abstract
arXiv:2506.06740v1 Announce Type: new 
Abstract: Psychological counseling faces huge challenges due to the growing demand for mental health services and the shortage of trained professionals. Large language models (LLMs) have shown potential to assist psychological counseling, especially in empathy and emotional support. However, existing models lack a deep understanding of emotions and are unable to generate personalized treatment plans based on fine-grained emotions. To address these shortcomings, we present AI PsyRoom, a multi-agent simulation framework designed to enhance psychological counseling by generating empathetic and emotionally nuanced conversations. By leveraging fine-grained emotion classification and a multi-agent framework, we construct a multi-agent PsyRoom A for dialogue reconstruction, generating a high-quality dialogue dataset EmoPsy, which contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues. We also propose PsyRoom B for generating personalized treatment plans. Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms state-of-the-art methods, achieving 18% improvement in problem orientation, 23% in expression, 24% in Empathy, and 16% in interactive communication quality. The datasets and models are publicly available, providing a foundation for advancing AI-assisted psychological counseling research.

### 摘要
由于心理健康服务需求日益增长而专业咨询师短缺，心理咨询面临巨大挑战。大型语言模型在共情与情感支持方面展现出辅助心理咨询的潜力，但现有模型缺乏对情绪的深层理解，无法基于细粒度情绪生成个性化治疗方案。为此，我们提出AI PsyRoom——一个通过生成具有情感细微变化的共情对话来增强心理咨询效果的多智能体仿真框架。通过细粒度情绪分类与多智能体架构，我们构建了用于对话重构的多智能体PsyRoom A，生成包含35种子情绪、423个具体情绪场景和12,350组对话的高质量数据集EmoPsy。同时提出用于生成个性化治疗方案的PsyRoom B。定量评估表明，AI PsyRoom在问题导向性（提升18%）、表达质量（23%）、共情能力（24%）及交互沟通质量（16%）上显著优于现有最优方法。相关数据集与模型均已开源，为推进AI辅助心理咨询研究奠定基础。

---

## [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)

### Abstract
arXiv:2506.06698v1 Announce Type: new 
Abstract: Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.

### 摘要
大语言模型（LLM）智能体已被应用于网页导航等序列决策任务，但由于缺乏特定环境经验，其在复杂任务中往往表现不佳。现有LLM智能体在推理阶段未设计持续学习机制，而这一能力对获取环境特定经验至关重要。为此，我们提出上下文经验回放（CER）框架，该免训练方案可使语言智能体在上下文窗口内实现高效自我提升。具体而言，CER通过动态记忆缓冲区积累并整合历史经验，这些经验涵盖环境动态与常见决策模式，使智能体在新任务中能检索并利用相关知识进行自我增强，从而提升复杂环境下的适应能力。我们在WebArena和VisualWebArena基准测试中评估CER性能：在VisualWebArena上达到31.9%的竞争性指标；在WebArena上获得36.7%的平均成功率，相较GPT-4o基线实现51.0%的相对提升。通过全面分析，我们验证了该框架的高效性、有效性并深化了其机理认知。

---

## [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)

### Abstract
arXiv:2506.06725v1 Announce Type: new 
Abstract: Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model's predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.

### 摘要
大语言模型（LLMs）具备通用世界知识，但在模拟等结构化、特定领域的场景中往往难以生成精确预测。这种局限性源于其无法将宽泛的非结构化理解与特定环境相结合。为此，我们提出WorldLLM框架，通过结合贝叶斯推断、自主主动探索与强化学习，增强基于LLM的世界建模能力。WorldLLM利用LLM的上下文学习特性，通过提示中给定的自然语言假设来指导基于LLM的世界模型预测。这些假设通过贝叶斯推断框架迭代优化，该框架采用第二个LLM作为给定收集证据的提案分布。证据收集通过好奇心驱动的强化学习策略实现，该策略探索环境以发现当前假设下基于LLM的预测模型中对数似然较低的转移。通过交替优化假设与收集新证据，我们的框架自主驱动预测能力的持续提升。实验表明，WorldLLM在需要智能体操作和组合对象的文本游戏环境中表现优异。该框架不仅提高了预测准确性，还生成了人类可理解的环境动态理论。

---

## [AI-Generated Compromises for Coalition Formation](https://arxiv.org/abs/2506.06837)

### Abstract
arXiv:2506.06837v1 Announce Type: new 
Abstract: The challenge of finding compromises between agent proposals is fundamental to AI subfields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. A crucial step in this process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals remains an open question. We address this gap by formalizing a model that incorporates agent bounded rationality and uncertainty, and by developing AI methods to generate compromise proposals. We focus on the domain of collaborative document writing, such as the democratic drafting of a community constitution. Our approach uses natural language processing techniques and large language models to induce a semantic metric space over text. Based on this space, we design algorithms to suggest compromise points likely to receive broad support. To evaluate our methods, we simulate coalition formation processes and show that AI can facilitate large-scale democratic text editing, a domain where traditional tools are limited.

### 摘要
在智能体提案间寻求折衷方案是人工智能论证、调解与协商等子领域的基础性问题。基于这一传统，Elkind等人（2021年）提出了一种联盟形成流程，通过在度量空间中利用每个智能体的理想点，寻找优于现状且获得多数支持的提案。该流程的关键步骤在于识别能使智能体联盟达成一致的折衷提案，而如何有效发现此类提案仍是开放问题。本研究通过形式化包含智能体有限理性与不确定性的模型，并开发生成折衷提案的人工智能方法来解决这一空白。我们聚焦于协作文档撰写领域（如社区宪章的民主起草），采用自然语言处理技术和大型语言模型构建文本语义度量空间，据此设计能生成可能获得广泛支持的折衷点算法。通过模拟联盟形成过程评估方法，我们证明人工智能可促进传统工具难以应对的大规模民主化文本编辑。

---

## [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)

### Abstract
arXiv:2506.06832v1 Announce Type: new 
Abstract: Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.

### 摘要
大语言模型（LLMs）定义了文本上的概率测度。通过探讨LLM"知晓"此类测度的隐含知识问题及其算法内涵，我们自然引出了一系列超越生成采样的任务形式，包括摘要生成、反事实思考、异常检测、原创性搜索、逆向提示、辩论、创造性解题等。这些任务可被表述为基于LLM测度的博弈形式，我们称之为交叉熵（Xent）博弈。Xent博弈既可以是单人形式也可以是多玩家参与，其核心涉及交叉熵评分与交叉熵约束，并能通过简单的计算图和程序予以表达。我们证明Xent博弈空间具有足够广度以涵盖大量有趣案例，同时其构建过程符合基本博弈论一致性公理。进而我们讨论如何利用Xent博弈空间来衡量LLMs的能力，由此引出了Xent博弈测度的构建方法：通过从给定范围提取覆盖测度，形成可作为能力基准测试的有限Xent博弈族。针对衡量通用能力时面临的无限范围难题，我们提出采用受演化动力学启发的思路，以连贯方式探索Xent博弈空间的解决方案。

---

## [United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory](https://arxiv.org/abs/2506.06843)

### Abstract
arXiv:2506.06843v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit a notable performance ceiling on complex, multi-faceted tasks, as they often fail to integrate diverse information or adhere to multiple constraints. We posit that such limitation arises when the demands of a task exceed the LLM's effective cognitive load capacity. This interpretation draws a strong analogy to Cognitive Load Theory (CLT) in cognitive science, which explains similar performance boundaries in the human mind, and is further supported by emerging evidence that reveals LLMs have bounded working memory characteristics. Building upon this CLT-grounded understanding, we introduce CoThinker, a novel LLM-based multi-agent framework designed to mitigate cognitive overload and enhance collaborative problem-solving abilities. CoThinker operationalizes CLT principles by distributing intrinsic cognitive load through agent specialization and managing transactional load via structured communication and a collective working memory. We empirically validate CoThinker on complex problem-solving tasks and fabricated high cognitive load scenarios, demonstrating improvements over existing multi-agent baselines in solution quality and efficiency. Our analysis reveals characteristic interaction patterns, providing insights into the emergence of collective cognition and effective load management, thus offering a principled approach to overcoming LLM performance ceilings.

### 摘要
大型语言模型（LLMs）在处理复杂多层面任务时表现出明显的性能瓶颈，因其往往难以整合多样化信息或同时满足多重约束。我们认为这种局限性源于任务需求超出了LLM的有效认知负载能力。该解释与认知科学中的认知负荷理论（CLT）形成强烈类比——该理论同样揭示了人类心智中的类似性能边界，且最新证据表明LLMs具有有限工作记忆特性，进一步支持了这一观点。基于这种CLT理论框架，我们提出CoThinker：一种新型多智能体框架，旨在缓解认知过载并增强协作问题解决能力。CoThinker通过智能体专业化分配内在认知负荷，借助结构化通信和集体工作记忆管理交互负荷，从而实现了CLT原则的操作化。我们在复杂问题解决任务和构建的高认知负荷场景中实证验证了CoThinker，其解决方案质量和效率均优于现有多智能体基线。分析揭示了特征性交互模式，为集体认知涌现和有效负荷管理提供了机制性解释，从而为突破LLM性能瓶颈提供了理论指导的方法路径。

---

## [Causal Graph based Event Reasoning using Semantic Relation Experts](https://arxiv.org/abs/2506.06910)

### Abstract
arXiv:2506.06910v1 Announce Type: new 
Abstract: Understanding how events in a scenario causally connect with each other is important for effectively modeling and reasoning about events. But event reasoning remains a difficult challenge, and despite recent advances, Large Language Models (LLMs) still struggle to accurately identify causal connections between events. This struggle leads to poor performance on deeper reasoning tasks like event forecasting and timeline understanding. To address this challenge, we investigate the generation of causal event graphs (e.g., A enables B) as a parallel mechanism to help LLMs explicitly represent causality during inference. This paper evaluates both how to generate correct graphs as well as how graphs can assist reasoning. We propose a collaborative approach to causal graph generation where we use LLMs to simulate experts that focus on specific semantic relations. The experts engage in multiple rounds of discussions which are then consolidated by a final expert. Then, to demonstrate the utility of causal graphs, we use them on multiple downstream applications, and also introduce a new explainable event prediction task that requires a causal chain of events in the explanation. These explanations are more informative and coherent than baseline generations. Finally, our overall approach not finetuned on any downstream task, achieves competitive results with state-of-the-art models on both forecasting and next event prediction tasks.

### 摘要
理解场景中事件之间的因果关联对于有效建模和推理事件至关重要。然而事件推理仍面临严峻挑战，尽管大型语言模型（LLMs）近期取得进展，其在准确识别事件间因果关系方面仍存在困难，这导致其在事件预测和时间线理解等深层推理任务中表现欠佳。为解决该问题，本研究探索将因果事件图（如'A促成B'）生成作为并行机制，以帮助LLMs在推理过程中显式表征因果关系。本文同时评估了如何生成正确图谱及图谱如何辅助推理。我们提出一种协作式因果图生成方法，利用LLMs模拟专注于特定语义关系的专家模型，通过多轮专家讨论后由终审专家整合结果。为验证因果图的实用性，我们在多个下游应用中对其进行测试，并引入新的可解释事件预测任务——该任务要求解释必须包含事件因果链。实验表明，这些解释比基线生成结果更具信息量和连贯性。最终，我们的整体方法无需任何下游任务微调，就在事件预测和后续事件预测任务中取得了与最先进模型相当的成果。

---

## [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)

### Abstract
arXiv:2506.06905v1 Announce Type: new 
Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.

### 摘要
大型多模态模型（LMMs）通常依赖上下文学习（ICL）在最小监督下执行新任务。然而，ICL性能（尤其是较小规模的LMMs）表现不稳定，且不总是随示例增加而单调提升。我们假设这种现象源于图像嵌入中包含的额外信息淹没了模型，而这些信息对下游任务并非必需。为此，我们提出一种元学习方法，通过从任务相关图像特征中蒸馏出一组固定的软提示（可在测试时通过少量示例适配），为LMMs提供另一种少样本能力诱导方案。为实现这种蒸馏，我们引入了一个注意力映射模块，该模块可轻松集成至流行的LLaVA v1.5架构，并与软提示联合学习，仅需少量梯度步即可实现低数据条件下的LMM任务适配。VL-ICL基准测试表明，即使在图像扰动情况下，我们的方法也持续优于ICL及相关提示调优方法，显著提升了视觉问答任务中的任务诱导和推理能力。

---

## [An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design](https://arxiv.org/abs/2506.06935)

### Abstract
arXiv:2506.06935v1 Announce Type: new 
Abstract: Recent significant advances in integrating multiple Large Language Model (LLM) systems have enabled Agentic Frameworks capable of performing complex tasks autonomously, including novel scientific research. We develop and demonstrate such a framework specifically for the inverse design of photonic metamaterials. When queried with a desired optical spectrum, the Agent autonomously proposes and develops a forward deep learning model, accesses external tools via APIs for tasks like simulation and optimization, utilizes memory, and generates a final design via a deep inverse method. The framework's effectiveness is demonstrated in its ability to automate, reason, plan, and adapt. Notably, the Agentic Framework possesses internal reflection and decision flexibility, permitting highly varied and potentially novel outputs.

### 摘要
近期在集成多个大型语言模型（LLM）系统方面取得的重大进展，使得智能体框架能够自主执行复杂任务，包括新颖的科学研究。我们开发并展示了这样一种专门用于光子超材料逆向设计的框架。当输入所需的光学光谱时，该智能体能够自主提出并开发前向深度学习模型，通过API访问外部工具以执行模拟和优化等任务，利用记忆功能，并通过深度逆向方法生成最终设计。该框架的有效性体现在其自动化、推理、规划和适应能力上。值得注意的是，该智能体框架具备内部反思和决策灵活性，能够产生高度多样化且可能具有新颖性的输出。

---

## [KnowCoder-V2: Deep Knowledge Analysis](https://arxiv.org/abs/2506.06881)

### Abstract
arXiv:2506.06881v1 Announce Type: new 
Abstract: Deep knowledge analysis tasks always involve the systematic extraction and association of knowledge from large volumes of data, followed by logical reasoning to discover insights. However, to solve such complex tasks, existing deep research frameworks face three major challenges: 1) They lack systematic organization and management of knowledge; 2) They operate purely online, making it inefficient for tasks that rely on shared and large-scale knowledge; 3) They cannot perform complex knowledge computation, limiting their abilities to produce insightful analytical results. Motivated by these, in this paper, we propose a \textbf&#123;K&#125;nowledgeable \textbf&#123;D&#125;eep \textbf&#123;R&#125;esearch (\textbf&#123;KDR&#125;) framework that empowers deep research with deep knowledge analysis capability. Specifically, it introduces an independent knowledge organization phase to preprocess large-scale, domain-relevant data into systematic knowledge offline. Based on this knowledge, it extends deep research with an additional kind of reasoning steps that perform complex knowledge computation in an online manner. To enhance the abilities of LLMs to solve knowledge analysis tasks in the above framework, we further introduce \textbf&#123;\KCII&#125;, an LLM that bridges knowledge organization and reasoning via unified code generation. For knowledge organization, it generates instantiation code for predefined classes, transforming data into knowledge objects. For knowledge computation, it generates analysis code and executes on the above knowledge objects to obtain deep analysis results. Experimental results on more than thirty datasets across six knowledge analysis tasks demonstrate the effectiveness of \KCII. Moreover, when integrated into the KDR framework, \KCII can generate high-quality reports with insightful analytical results compared to the mainstream deep research framework.

### 摘要
深度知识分析任务通常涉及从海量数据中系统性地提取并关联知识，继而通过逻辑推理发现深层洞见。然而现有深度研究框架在解决此类复杂任务时面临三大挑战：1) 缺乏对知识的系统性组织与管理；2) 完全在线运行模式导致依赖共享大规模知识的任务效率低下；3) 无法执行复杂知识计算，限制了产生深刻分析结果的能力。基于此，本文提出具备深度知识分析能力的**知识化深度研究框架(KDR)**，其创新性体现在：首先通过独立的知识组织阶段离线预处理领域相关的大规模数据，将其转化为系统化知识；在此基础上扩展深度研究流程，新增一类支持在线复杂知识计算的推理步骤。为提升大语言模型在该框架下解决知识分析任务的能力，我们进一步提出**\KCII**模型，通过统一的代码生成桥接知识组织与推理：在知识组织阶段生成预定义类的实例化代码，将数据转化为知识对象；在知识计算阶段生成分析代码并在知识对象上执行以获得深度分析结果。在六类知识分析任务、超过三十个数据集上的实验验证了\KCII的有效性。当集成至KDR框架时，相较于主流深度研究框架，\KCII能生成包含深刻分析结果的高质量研究报告。

---

## [Boosting LLM Reasoning via Spontaneous Self-Correction](https://arxiv.org/abs/2506.06923)

### Abstract
arXiv:2506.06923v1 Announce Type: new 
Abstract: While large language models (LLMs) have demonstrated remarkable success on a broad range of tasks, math reasoning remains a challenging one. One of the approaches for improving math reasoning is self-correction, which designs self-improving loops to let the model correct its own mistakes. However, existing self-correction approaches treat corrections as standalone post-generation refinements, relying on extra prompt and system designs to elicit self-corrections, instead of performing real-time, spontaneous self-corrections in a single pass. To address this, we propose SPOC, a spontaneous self-correction approach that enables LLMs to generate interleaved solutions and verifications in a single inference pass, with generation dynamically terminated based on verification outcomes, thereby effectively scaling inference time compute. SPOC considers a multi-agent perspective by assigning dual roles -- solution proposer and verifier -- to the same model. We adopt a simple yet effective approach to generate synthetic data for fine-tuning, enabling the model to develop capabilities for self-verification and multi-agent collaboration. We further improve its solution proposal and verification accuracy through online reinforcement learning. Experiments on mathematical reasoning benchmarks show that SPOC significantly improves performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24, respectively.

### 摘要
尽管大语言模型（LLMs）在广泛任务中展现出卓越成效，数学推理仍是其面临的重要挑战。提升数学推理的方法之一为自我校正，该方法通过设计自我改进循环使模型自行修正错误。然而，现有自我校正方法将修正视为独立的后生成优化步骤，依赖额外提示和系统设计来引发自我校正，而非在单次推理中实现实时、自发的自我校正。为此，我们提出SPOC（自发自我校正方法），使LLMs能在单次推理过程中交错生成解决方案与验证步骤，并根据验证结果动态终止生成，从而高效扩展推理计算资源。SPOC采用多智能体视角，为同一模型分配双重角色——方案提出者与验证者。我们采用简单而有效的方法生成合成数据进行微调，使模型具备自我验证与多智能体协作能力。通过在线强化学习，我们进一步提升了其方案提出与验证的准确性。数学推理基准测试表明，SPOC显著提升了模型性能。值得注意的是，SPOC使Llama-3.1-8B和70B Instruct模型的准确率分别获得显著提升：在MATH500上提高8.8%和11.6%，在AMC23上提升10.0%和20.0%，在AIME24上增长3.3%和6.7%。

---

## [Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth](https://arxiv.org/abs/2506.06991)

### Abstract
arXiv:2506.06991v1 Announce Type: new 
Abstract: The recent success of generative AI highlights the crucial role of high-quality human feedback in building trustworthy AI systems. However, the increasing use of large language models (LLMs) by crowdsourcing workers poses a significant challenge: datasets intended to reflect human input may be compromised by LLM-generated responses. Existing LLM detection approaches often rely on high-dimension training data such as text, making them unsuitable for annotation tasks like multiple-choice labeling. In this work, we investigate the potential of peer prediction -- a mechanism that evaluates the information within workers' responses without using ground truth -- to mitigate LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our approach quantifies the correlations between worker answers while conditioning on (a subset of) LLM-generated labels available to the requester. Building on prior research, we propose a training-free scoring mechanism with theoretical guarantees under a crowdsourcing model that accounts for LLM collusion. We establish conditions under which our method is effective and empirically demonstrate its robustness in detecting low-effort cheating on real-world crowdsourcing datasets.

### 摘要
生成式人工智能的最新成功凸显了高质量人类反馈在构建可信AI系统中的关键作用。然而，众包工作者日益频繁使用大语言模型（LLM）带来了重大挑战：旨在反映人类输入的标注数据集可能被LLM生成的回答所污染。现有LLM检测方法通常依赖文本等高维训练数据，使其不适用于多项选择标注等任务。本研究探讨了同伴预测机制——一种通过评估工作者回答内部信息而非依赖真实标签的验证方法——在众包标注任务中抑制LLM辅助作弊的潜力。我们的方法量化了工作者答案之间的相关性，同时以请求者可获得的（部分）LLM生成标签为条件。基于前期研究，我们提出了一种无需训练的评分机制，该机制在考虑LLM共谋的众包模型下具有理论保证。我们确立了该方法有效的条件，并通过真实众包数据集实证验证了其在检测低努力作弊行为方面的鲁棒性。

---

## [Mathesis: Towards Formal Theorem Proving from Natural Languages](https://arxiv.org/abs/2506.07047)

### Abstract
arXiv:2506.07047v1 Announce Type: new 
Abstract: Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.

### 摘要
大语言模型的最新进展为形式化推理展现出强大潜力。然而，大多数基于LLM的定理证明器长期受限于需要专家编写的形式化陈述作为输入，这限制了其在自然语言表达的现实问题中的适用性。我们通过Mathesis系统解决了这一差距，这是首个处理非形式化问题陈述的端到端定理证明流程。该系统包含Mathesis-Autoformalizer——首个采用强化学习增强自然语言问题形式化能力的自动形式化器，并辅以我们新颖的LeanScorer框架进行细致的形式化质量评估；同时提出Mathesis-Prover，用于从形式化陈述生成形式化证明。为评估端到端形式化定理证明的现实适用性，我们构建了Gaokao-Formal基准测试集，包含488道中国高考真题。我们的方法经过精心设计，对各组件进行了深入研究。实验表明Mathesis具有显著效果：自动形式化器在Gaokao-Formal上的通过率比最佳基线高出22%；完整系统在MiniF2F上以pass@32达到64%准确率，在Gaokao-Formal上以18%的准确率刷新了当前最优水平。

---

## [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)

### Abstract
arXiv:2506.06941v1 Announce Type: new 
Abstract: Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.

### 摘要
最新一代语言模型引入了大型推理模型（LRMs），这类模型在给出答案前会生成详细的思维过程。虽然这些模型在推理基准测试中表现出性能提升，但其基本能力、扩展特性和局限性仍未得到充分理解。当前评估主要集中于成熟的数学和编程基准，强调最终答案的准确性。然而，这种评估范式常受数据污染影响，且无法深入理解推理轨迹。本研究通过可控的谜题环境系统性地探究这些问题，该环境能在保持逻辑结构一致的同时精确调控问题复杂度。这一设置不仅可分析最终答案，还能研究内部推理轨迹，从而揭示LRMs的思考方式。大量实验表明，LRMs在超过特定复杂度后会出现准确性完全崩溃的现象。此外，它们展现出反直觉的扩展极限：推理努力随问题复杂度增加而提升，但在仍有剩余token预算时却开始下降。通过将LRMs与相同计算资源下的标准LLMs对比，我们识别出三种性能模式：（1）低复杂度任务中标准模型优于LRMs；（2）中等复杂度任务中LRMs显现优势；（3）高复杂度任务中两种模型均完全失效。研究发现LRMs在精确计算方面存在局限：它们无法运用显式算法，且在不同规模下推理不一致。我们进一步深入分析推理轨迹，研究探索解决方案的模式，并剖析模型的计算行为，从而阐明其优势与局限，同时对其推理能力提出新的质疑。

---

## [Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT](https://arxiv.org/abs/2506.07173)

### Abstract
arXiv:2506.07173v1 Announce Type: new 
Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL framework that is easy to use by ML&amp;AI developers who do not need to be professional programmers and is also amenable to LLMs. In the previous research, generic federated learning algorithms provided by this framework were manually translated into the CSP processes and algorithms' safety and liveness properties were automatically verified by the model checker PAT. In this paper, a simple translation process is introduced wherein the ChatGPT is used to automate the translation of the mentioned federated learning algorithms in Python into the corresponding CSP processes. Within the process, the minimality of the used context is estimated based on the feedback from ChatGPT. The proposed translation process was experimentally validated by successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.

### 摘要
Python联邦学习算法测试平台是一个简易的Python FL框架，便于无需专业编程背景的ML&amp;AI开发者使用，同时也适合大语言模型操作。在先前研究中，该框架提供的通用联邦学习算法被手动转换为CSP进程，并通过模型检测器PAT自动验证了算法的安全性与活性属性。本文提出一种简易的翻译流程，利用ChatGPT将上述Python联邦学习算法自动转换为对应的CSP进程。在此过程中，基于ChatGPT的反馈评估了所使用上下文的最小化程度。通过成功翻译（经模型检测器PAT验证）通用集中式与分布式联邦学习算法，对所提出的翻译流程进行了实验验证。

---

## [BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite](https://arxiv.org/abs/2506.07116)

### Abstract
arXiv:2506.07116v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both structurally clean and semantically coherent. BRIGHT is a recent and influential benchmark designed to evaluate complex multi-hop retrieval across diverse, high-reasoning domains. However, its practical effectiveness is limited by common web-crawled artifacts - such as content redundancy and semantic discontinuity - that impair retrieval accuracy and downstream reasoning. Notably, we find that such issues are concentrated in seven StackExchange-derived subdomains, while other domains (e.g., Coding and Theorem-based content) remain relatively clean.
  In this study, we present MARCUS, a multi-agent pipeline that leverages large language models (LLMs) to systematically clean and re-chunk BRIGHT into a higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for structural noise removal and semantic segmentation, preserving answer-bearing spans while improving contextual integrity. Experimental evaluations demonstrate that BRIGHT-Plus yields consistent and significant improvements in both retrieval accuracy and multi-hop reasoning across a diverse set of retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to support future research on robust, reasoning-centric retrieval.

### 摘要
检索增强生成（RAG）系统需要结构清晰且语义连贯的语料库。BRIGHT是近期提出的重要基准，旨在评估跨领域复杂多跳检索能力。然而，其实际效果受限于常见的网络爬取缺陷——如内容冗余和语义断裂——这些问题会损害检索准确性和下游推理性能。值得注意的是，我们发现此类问题主要集中在七个源自StackExchange的子领域，而其他领域（如编程和定理类内容）则相对规范。本研究提出MARCUS，一种基于大语言模型（LLM）的多智能体流程，可系统性地清理并重新分块BRIGHT语料库，生成更高质量的BRIGHT-Plus版本。MARCUS通过专用智能体实现结构噪声消除和语义分割，在保留答案片段的同时提升上下文完整性。实验评估表明，BRIGHT-Plus在不同检索器上均能带来检索准确率和多跳推理能力的持续显著提升。我们同步开源BRIGHT-Plus语料库和MARCUS流程，以支持未来面向鲁棒性、以推理为核心的检索研究。

---

## [Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues](https://arxiv.org/abs/2506.07194)

### Abstract
arXiv:2506.07194v1 Announce Type: new 
Abstract: This study investigates effective strategies for developing a customised GPT agent to code classroom dialogue. While classroom dialogue is widely recognised as a crucial element of education, its analysis remains challenging due to the need for a nuanced understanding of dialogic functions and the labour-intensive nature of manual transcript coding. Recent advancements in large language models offer promising avenues for automating this process. However, existing studies predominantly focus on training large-scale models or evaluating pre-trained models with fixed codebooks, which are often not applicable or replicable for dialogue researchers working with small datasets or customised coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its baseline performance in coding classroom dialogue with a human codebook and examines how performance varies with different example inputs through a variable control method. Through a design-based research approach, it identifies a set of practical strategies, based on MyGPT's unique features, for configuring effective agents with limited data. The findings suggest that, despite some limitations, a MyGPT agent developed with these strategies can serve as a useful coding assistant by generating coding suggestions.

### 摘要
本研究探讨了开发定制化GPT代理以编码课堂对话的有效策略。尽管课堂对话被普遍视为教育的关键要素，但由于需要对对话功能进行细致理解且人工转录编码工作繁重，其分析仍具挑战性。大型语言模型的最新进展为自动化这一过程提供了可行路径。然而现有研究主要集中于训练大规模模型或使用固定编码本评估预训练模型，这对于使用小型数据集或定制编码方案的对话研究者往往缺乏适用性和可复现性。本研究以GPT-4的MyGPT代理为例，通过变量控制法评估其采用人工编码本进行课堂对话编码的基线性能，并探究不同示例输入对性能的影响。基于设计研究方法，本研究结合MyGPT的独有特性，提出了一套在有限数据条件下配置高效代理的实用策略。研究结果表明，尽管存在某些局限，采用这些策略开发的MyGPT代理仍可作为有用的编码辅助工具，通过生成编码建议发挥作用。

---

## [BIMgent: Towards Autonomous Building Modeling via Computer-use Agents](https://arxiv.org/abs/2506.07217)

### Abstract
arXiv:2506.07217v1 Announce Type: new 
Abstract: Existing computer-use agents primarily focus on general-purpose desktop automation tasks, with limited exploration of their application in highly specialized domains. In particular, the 3D building modeling process in the Architecture, Engineering, and Construction (AEC) sector involves open-ended design tasks and complex interaction patterns within Building Information Modeling (BIM) authoring software, which has yet to be thoroughly addressed by current studies. In this paper, we propose BIMgent, an agentic framework powered by multimodal large language models (LLMs), designed to enable autonomous building model authoring via graphical user interface (GUI) operations. BIMgent automates the architectural building modeling process, including multimodal input for conceptual design, planning of software-specific workflows, and efficient execution of the authoring GUI actions. We evaluate BIMgent on real-world building modeling tasks, including both text-based conceptual design generation and reconstruction from existing building design. The design quality achieved by BIMgent was found to be reasonable. Its operations achieved a 32% success rate, whereas all baseline models failed to complete the tasks (0% success rate). Results demonstrate that BIMgent effectively reduces manual workload while preserving design intent, highlighting its potential for practical deployment in real-world architectural modeling scenarios.

### 摘要
现有计算机使用代理主要集中于通用桌面自动化任务，对其在高度专业化领域应用的探索较为有限。尤其在建筑、工程与施工（AEC）领域的3D建筑建模过程中，涉及开放式设计任务和建筑信息模型（BIM）创作软件中的复杂交互模式，当前研究尚未深入解决这一问题。本文提出BIMgent——一个由多模态大语言模型（LLMs）驱动的代理框架，旨在通过图形用户界面（GUI）操作实现自主建筑模型创作。该框架可自动化完成建筑建模全过程，包括概念设计的多元输入、软件专用工作流规划以及创作GUI操作的高效执行。我们在真实建筑建模任务中评估BIMgent，涵盖基于文本的概念设计生成和既有建筑设计重建两方面。实验表明BIMgent达到的设计质量合理，其操作成功率为32%，而所有基线模型均未能完成任务（成功率0%）。结果表明BIMgent在保持设计意图的同时有效减少人工工作量，凸显了其在真实建筑建模场景中的实际部署潜力。

---

## [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)

### Abstract
arXiv:2506.07184v1 Announce Type: new 
Abstract: While multimodal large language models excel at various tasks, they still suffer from hallucinations, which limit their reliability and scalability for broader domain applications. To address this issue, recent research mainly focuses on objective hallucination. However, for sequential images, besides objective hallucination, there is also behavioral hallucination, which is less studied. This work aims to fill in the gap. We first reveal that behavioral hallucinations mainly arise from two key factors: prior-driven bias and the snowball effect. Based on these observations, we introduce SHE (Sequence Hallucination Eradication), a lightweight, two-stage framework that (1) detects hallucinations via visual-textual alignment check using our proposed adaptive temporal window and (2) mitigates them via orthogonal projection onto the joint embedding space. We also propose a new metric (BEACH) to quantify behavioral hallucination severity. Empirical results on standard benchmarks demonstrate that SHE reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy.

### 摘要
尽管多模态大语言模型在各类任务中表现卓越，但其仍存在幻觉问题，这限制了其在更广泛领域应用中的可靠性与可扩展性。当前研究主要聚焦于客观幻觉，然而对于序列图像而言，除客观幻觉外还存在行为幻觉，后者尚未得到充分研究。本研究旨在填补这一空白。我们首先揭示行为幻觉主要源于两个关键因素：先验驱动偏差与雪球效应。基于这些发现，我们提出了SHE（序列幻觉消除框架）——一个轻量级的两阶段框架：（1）通过自适应时间窗口进行视觉-文本对齐检测以识别幻觉；（2）通过联合嵌入空间的正交投影来缓解幻觉。同时，我们提出新度量指标BEACH以量化行为幻觉的严重程度。在标准基准测试上的实证结果表明，SHE在保持描述准确性的同时，能将行为幻觉在BEACH指标上降低超过10%。

---

## [Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation](https://arxiv.org/abs/2506.07202)

### Abstract
arXiv:2506.07202v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) show impressive vision-language benchmark performance, yet growing concerns about data contamination (test set exposure during training) risk masking true generalization. This concern extends to reasoning MLLMs, often fine-tuned via reinforcement learning from potentially contaminated base models. We propose a novel dynamic evaluation framework to rigorously assess MLLM generalization, moving beyond static benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the same visual input, models are evaluated across a family of tasks (e.g., QA, captioning, question posing, verification) to probe diverse capabilities. This task perturbation reveals whether model performance is robust or reliant on superficial task-specific cues. Our approach is analogous to loss landscape sharpness: models overfit or contaminated for a single task (sharp minima) falter under task shifts, unlike models with generalizable solutions (flatter minima). We developed an automated pipeline with a calibrated judge scoring open-ended generations (captions, questions) using paraphrase and corruption sampling. Applying this framework to leading image/video MLLMs on benchmarks including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task "ability vector." We demonstrate that fine-tuning on simulated test data (extreme contamination) drastically sharpens task-specific performance but harms overall generalization. Our dynamic task perturbation offers deeper insights into MLLM generalization, distinguishing genuine understanding from spurious leakage or overfitting.

### 摘要
多模态大语言模型（MLLMs）在视觉语言基准测试中展现出卓越性能，但日益严重的数据污染问题（训练期间测试集暴露）可能掩盖其真实泛化能力。这一担忧尤其适用于通过强化学习微调的推理型MLLMs，其基础模型本身可能已受污染。我们提出了一种新颖的动态评估框架，通过超越静态基准的测试来严格评估MLLM的泛化能力。该方法不扰动输入数据，而是对任务本身进行扰动：使用相同视觉输入，让模型执行任务族（如问答、描述、提问、验证）以探测多样化能力。这种任务扰动能揭示模型性能是具备鲁棒性，还是依赖于特定任务的表面线索。我们的方法类似于损失景观锐度分析：对单一任务过拟合或受污染的模型（尖锐最小值）在任务切换时表现不佳，而具有可泛化解的模型（平缓最小值）则不然。我们开发了自动化流程，通过校准评估者使用复述和损坏采样对开放式生成（描述、问题）进行评分。将该框架应用于MME、RealWorldQA和CVRR-ES等基准测试中的领先图像/视频MLLMs，我们分析了各模型的跨任务'能力向量'。研究表明，在模拟测试数据（极端污染）上微调会显著提高特定任务性能，但损害整体泛化能力。这种动态任务扰动方法为区分真实理解能力与虚假泄漏或过拟合现象提供了更深入的MLLM泛化分析视角。

---

## [An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning](https://arxiv.org/abs/2506.07411)

### Abstract
arXiv:2506.07411v1 Announce Type: new 
Abstract: As the scale and complexity of cloud-based AI systems continue to increase, the detection and adaptive recovery of system faults have become the core challenges to ensure service reliability and continuity. In this paper, we propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to realize a fault recovery framework with semantic understanding and policy optimization capabilities in cloud AI systems. On the basis of the traditional DRL-based control model, the proposed method constructs a two-stage hybrid architecture: (1) an LLM-driven fault semantic interpretation module, which can dynamically extract deep contextual semantics from multi-source logs and system indicators to accurately identify potential fault modes; (2) DRL recovery strategy optimizer, based on reinforcement learning, learns the dynamic matching of fault types and response behaviors in the cloud environment. The innovation of this method lies in the introduction of LLM for environment modeling and action space abstraction, which greatly improves the exploration efficiency and generalization ability of reinforcement learning. At the same time, a memory-guided meta-controller is introduced, combined with reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve continuous adaptation to new failure modes and avoid catastrophic forgetting. Experimental results on the cloud fault injection platform show that compared with the existing DRL and rule methods, the IFSHM framework shortens the system recovery time by 37% with unknown fault scenarios.

### 摘要
随着基于云平台的AI系统规模和复杂性持续增长，系统故障的检测与自适应恢复已成为保障服务可靠性与连续性的核心挑战。本文提出一种融合大语言模型（LLM）与深度强化学习（DRL）的智能故障自愈机制（IFSHM），旨在云AI系统中实现具备语义理解与策略优化能力的故障恢复框架。该方法在传统DRL控制模型基础上构建了两阶段混合架构：（1）LLM驱动的故障语义解析模块，可动态提取多源日志与系统指标的深层上下文语义，精准识别潜在故障模式；（2）DRL恢复策略优化器，基于强化学习实现云环境中故障类型与响应行为的动态匹配。该方法创新性引入LLM进行环境建模与动作空间抽象，显著提升强化学习的探索效率与泛化能力；同时引入记忆引导的元控制器，结合强化学习回放与LLM提示微调策略，实现对新故障模式的持续适应并避免灾难性遗忘。在云故障注入平台上的实验表明，相较于现有DRL与规则方法，IFSHM框架在未知故障场景下将系统恢复时间缩短了37%。

---

## [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)

### Abstract
arXiv:2506.07390v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the models' ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement in the accuracy. The source code and data are available at https://github.com/Xin-Cheng-Wen/PO4Vul.

### 摘要
大语言模型（LLMs）在众多代码相关任务中展现出显著能力，但其在软件漏洞检测方面的性能仍存在局限。这一局限主要源于两个因素：（1）缺乏与漏洞相关的推理数据，阻碍了模型捕捉潜在漏洞模式的能力；（2）模型侧重于学习语义表征而非其背后的逻辑，因而无法识别语义相似的漏洞样本。此外，在高质量数据集稀缺的环境下，开发专注于漏洞检测的LLMs尤为困难。本文提出新型框架ReVD，其优势在于通过推理数据合成和漏洞特异性偏好优化来挖掘漏洞模式。具体而言，我们为漏洞及对应修复代码构建前向与后向推理流程，确保合成高质量推理数据；同时设计三重监督微调与渐进式在线偏好优化方案，使ReVD能更深入理解漏洞模式。在PrimeVul和SVEN数据集上的大量实验表明，ReVD为基于LLM的软件漏洞检测设立了新标杆，例如准确率提升12.24%-22.77%。源代码与数据详见https://github.com/Xin-Cheng-Wen/PO4Vul。

---

## [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)

### Abstract
arXiv:2506.07400v1 Announce Type: new 
Abstract: The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.

### 摘要
深度学习青光眼检测技术与大语言模型（LLM）的整合，为缓解眼科医师短缺问题并提升临床报告效率提供了自动化解决方案。然而，通用大语言模型在医学影像领域的应用仍面临幻觉效应、可解释性不足及领域专业知识匮乏等挑战，这些因素可能降低临床诊断准确性。尽管近期研究通过结合影像模型与LLM推理机制改善了报告生成质量，但现有方案通常依赖单一通用代理，难以模拟多学科医疗团队所具备的多样化和复杂性推理能力。为此，我们提出MedChat——一个融合专用视觉模型与多角色定制LLM代理的协同诊断框架及平台，所有代理均由导演代理统一协调。该设计显著提升了系统可靠性，降低了幻觉风险，并通过专为临床审查与教学设计的交互界面实现了动态诊断报告生成。代码详见https://github.com/Purdue-M2/MedChat。

---

## [LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments](https://arxiv.org/abs/2506.07223)

### Abstract
arXiv:2506.07223v1 Announce Type: new 
Abstract: In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun exploring agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates inference delays in decision-making into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios.

### 摘要
在具身智能领域，大型语言模型（LLMs）的发展显著提升了智能体的决策能力。为此，研究者开始探索智能体在动态变化的高风险场景（如HAZARD基准测试中的火灾、洪水和飓风场景）中的表现。在这些极端条件下，决策延迟成为一个关键但尚未充分研究的问题。我们提出了一种时间转换机制（TCM），将决策过程中的推理延迟转换为等效的模拟帧数，从而在基于FPS的单一指标下统一认知成本与物理成本。通过为HAZARD引入响应延迟（RL）和延迟-行动比（LAR）指标，我们建立了一套完整的延迟感知评估方案。此外，我们提出了快速反射异步反思智能体（RRARA），该架构将轻量级LLM引导的反馈模块与基于规则的智能体相结合，实现了即时反应行为和原位异步反思优化。HAZARD上的实验表明，RRARA在延迟敏感场景中显著优于现有基线方法。

---

## [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)

### Abstract
arXiv:2506.07398v1 Announce Type: new 
Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both $\textit&#123;high-level, generalizable insights&#125;$ that enable the system to leverage cross-trial knowledge, and $\textit&#123;fine-grained, condensed interaction trajectories&#125;$ that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\%$ and $10.12\%$, respectively, without any modifications to the original frameworks. Our codes are available at https://github.com/bingreeky/GMemory.

### 摘要
基于大语言模型（LLM）的多智能体系统（MAS）已展现出远超单一LLM智能体的认知与执行能力，但其自我进化能力仍受限于不完善的内存架构。经深入分析，我们注意到当前主流MAS内存机制存在两大缺陷：（1）过度简化设计，完全忽视了智能体间复杂的协作轨迹；（2）缺乏跨任务试验和智能体专属定制能力，与为单一智能体开发的表达性内存形成鲜明对比。为弥补这一差距，我们提出G-Memory——一个受组织记忆理论启发的分层式多智能体内存系统，通过三层图结构（洞察图、查询图和交互图）管理MAS的长期交互。当接收到新用户查询时，G-Memory执行双向内存检索，既能获取支持跨任务知识迁移的高层可泛化洞察，又能提取紧凑编码历史协作经验的细粒度浓缩交互轨迹。任务执行过程中，整个层级结构通过吸收新协作轨迹实现动态演化，促进智能体团队的持续进化。在五个基准测试、三种LLM主干模型和三种主流MAS框架上的实验表明，G-Memory在不修改原框架的前提下，将具身行动成功率提升最高达20.89%，知识问答准确率提高最高达10.12%。代码已开源：https://github.com/bingreeky/GMemory。

---

## [Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests](https://arxiv.org/abs/2506.07418)

### Abstract
arXiv:2506.07418v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) promise advanced vision language capabilities, yet their effectiveness in visually presented mathematics remains underexplored. This paper analyzes the development and evaluation of MLLMs for mathematical problem solving, focusing on diagrams, multilingual text, and symbolic notation. We then assess several models, including GPT 4o, Pixtral, Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our experiments reveal four key findings. First, overall precision remains moderate across geometry, visual algebra, logic, patterns, and combinatorics: no single model excels in every topic. Second, while most models see improved accuracy with questions that do not have images, the gain is often limited; performance for some remains nearly unchanged without visual input, indicating underutilization of diagrammatic information. Third, substantial variation exists across languages and difficulty levels: models frequently handle easier items but struggle with advanced geometry and combinatorial reasoning. Notably, Gemini 2.0 Flash achieves the highest precision on image based tasks, followed by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance. Fourth, a complementary analysis aimed at distinguishing whether models reason or simply recite reveals that Gemini and GPT 4o stand out for their structured reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less consistent reasoning, often defaulting to heuristics or randomness when unable to align their outputs with the given answer options.

### 摘要
多模态大语言模型（MLLMs）展现出先进的视觉语言能力，但其在视觉呈现数学问题上的有效性仍待深入探索。本文分析了面向数学解题的MLLMs开发与评估方法，重点关注图表、多语言文本及符号表示。我们在涵盖英语、法语、西班牙语和加泰罗尼亚语的多语言袋鼠式基准测试中，评估了包括GPT-4o、Pixtral、Qwen-VL、Llama-3.2-Vision变体及Gemini-2.0-Flash在内的多个模型。实验揭示了四项关键发现：首先，在几何、视觉代数、逻辑、模式与组合数学领域，模型整体精度保持中等水平，且无单一模型能在所有主题上表现优异；其次，尽管多数模型对无图像问题的准确率有所提升，但增益有限，部分模型在无视觉输入时性能几乎不变，表明其对图表信息的利用不足；第三，不同语言与难度级别间存在显著差异——模型常能处理简单题目，但在高阶几何与组合推理上表现欠佳。值得注意的是，Gemini-2.0-Flash在图像任务中精度最高，其次为Qwen-VL-2.5-72B与GPT-4o，但均未达到人类水平；最后，一项旨在区分模型是推理还是简单复述的补充分析表明，Gemini与GPT-4o因其结构化推理和稳定准确性脱颖而出，而Pixtral与Llama则表现出较低的逻辑一致性，当无法匹配给定选项时往往依赖启发式或随机策略。

---

## [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)

### Abstract
arXiv:2506.07232v1 Announce Type: new 
Abstract: Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \textit&#123;Learn as Individuals, Evolve as a Team (LIET)&#125; paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.

### 摘要
大型语言模型（LLMs）拥有广泛的知识库和强大的推理能力，这使其成为具身环境中复杂多智能体规划的有力工具。然而，尽管LLMs具备先进能力且代理方法采用精巧的模块化设计，现有基于LLM的规划算法仍受限于对多智能体具身场景的弱适应能力。我们通过引入一个框架来解决这一局限，该框架使LLM智能体能够在测试前后进行学习与进化，为其配备环境相关知识以优化规划，并通过增强通信提升协作能力。受多智能体强化学习中"集中训练分散执行"的启发，我们提出\textit&#123;个体学习、团队进化（LIET）&#125;的多智能体LLM适应范式。在个体层面，LLM智能体从探索性数据集中学习局部效用函数以更好理解具身环境，该函数在测试时被调用以支持知情决策；在团队层面，LLM智能体基于新经验协作式迭代维护并更新共享合作知识列表，用以指导更有效的通信。通过将个体学习与团队进化相结合，LIET实现了LLM智能体全面而灵活的适应。我们在Communicative Watch-And-Help和ThreeD-World Multi-Agent Transport基准上的实验表明，由LLaMA和GPT-4o实例化的LIET优于现有基线，并展现出强大的协作规划能力。

---

## [Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents](https://arxiv.org/abs/2506.07388)

### Abstract
arXiv:2506.07388v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show strong collaborative performance in multi-agent systems with predefined roles and workflows. However, in open-ended environments lacking coordination rules, agents tend to act in self-interested ways. The central challenge in achieving coordination lies in credit assignment -- fairly evaluating each agent's contribution and designing pricing mechanisms that align their heterogeneous goals. This problem is critical as LLMs increasingly participate in complex human-AI collaborations, where fair compensation and accountability rely on effective pricing mechanisms. Inspired by how human societies address similar coordination challenges (e.g., through temporary collaborations such as employment or subcontracting), we propose a cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley Chain-of-Thought -- leveraging marginal contributions as a principled basis for pricing -- with structured negotiation protocols for effective price matching, enabling LLM agents to coordinate through rational task-time pricing and post-task reward redistribution. This approach aligns agent incentives, fosters cooperation, and maintains autonomy. We evaluate Shapley-Coop across two multi-agent games and a software engineering simulation, demonstrating that it consistently enhances LLM agent collaboration and facilitates equitable credit assignment. These results highlight the effectiveness of Shapley-Coop's pricing mechanisms in accurately reflecting individual contributions during task execution.

### 摘要
大语言模型（LLMs）在具有预定义角色和工作流程的多智能体系统中展现出强大的协作性能。然而，在缺乏协调规则的开放环境中，智能体往往倾向于采取利己行为。实现协调的核心挑战在于信用分配——公平评估每个智能体的贡献，并设计能够协调其异质目标的定价机制。这一问题至关重要，因为LLMs正日益参与复杂的人机协作，而公平补偿和问责制都依赖于有效的定价机制。受人类社会解决类似协调挑战方式（例如通过雇佣或分包等临时协作）的启发，我们提出了一种协作工作流程Shapley-Coop。该方法将Shapley思维链（利用边际贡献作为定价的原则性基础）与结构化协商协议相结合，实现有效的价格匹配，使LLM智能体能够通过理性的任务-时间定价和任务后奖励再分配进行协调。这一方法能协调智能体激励、促进合作并保持自主性。我们在两个多智能体游戏和一个软件工程模拟中评估了Shapley-Coop，证明其能持续增强LLM智能体的协作能力，并促进公平的信用分配。这些结果凸显了Shapley-Coop定价机制在准确反映任务执行过程中个体贡献方面的有效性。

---

## [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736)

### Abstract
arXiv:2506.07736v1 Announce Type: new 
Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite deliberate safety alignment efforts, posing significant risks to users and society. To safeguard against the risk of policy-violating content, system-level moderation via external guard models-designed to monitor LLM inputs and outputs and block potentially harmful content-has emerged as a prevalent mitigation strategy. Existing approaches of training guard models rely heavily on extensive human curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks. To address these limitations, we propose RSafe, an adaptive reasoning-based safeguard that conducts guided safety reasoning to provide robust protection within the scope of specified safety policies. RSafe operates in two stages: 1) guided reasoning, where it analyzes safety risks of input content through policy-guided step-by-step reasoning, and 2) reinforced alignment, where rule-based RL optimizes its reasoning paths to align with accurate safety prediction. This two-stage training paradigm enables RSafe to internalize safety principles to generalize safety protection capability over unseen or adversarial safety violation scenarios. During inference, RSafe accepts user-specified safety policies to provide enhanced safeguards tailored to specific safety requirements.

### 摘要
尽管经过刻意的安全对齐努力，大型语言模型（LLMs）仍持续表现出脆弱性，对用户和社会构成重大风险。为防范违反政策内容的风险，通过外部防护模型进行系统级审核——即监控LLM输入输出并拦截潜在有害内容——已成为主流缓解策略。现有防护模型训练方法高度依赖人工标注数据集，且难以应对分布外威胁（如新兴有害类别或越狱攻击）。针对这些局限性，我们提出RSafe：一种基于自适应推理的安全防护框架，通过引导式安全推理在指定安全政策范围内提供鲁棒保护。RSafe采用两阶段运行机制：1）引导推理阶段——通过政策引导的逐步推理分析输入内容的安全风险；2）强化对齐阶段——基于规则的强化学习优化推理路径以实现准确安全预测。这种两阶段训练范式使RSafe能够内化安全原则，从而对未知或对抗性安全违规场景实现泛化保护。在推理阶段，RSafe可接收用户定制的安全政策，为特定安全需求提供增强型防护。

---

## [NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models](https://arxiv.org/abs/2506.07731)

### Abstract
arXiv:2506.07731v1 Announce Type: new 
Abstract: Existing benchmarks have proven effective for assessing the performance of fully trained large language models. However, we find striking differences in the early training stages of small models, where benchmarks often fail to provide meaningful or discriminative signals. To explore how these differences arise, this competition tackles the challenge of designing scientific knowledge evaluation tasks specifically tailored for measuring early training progress of language models. Participants are invited to develop novel evaluation methodologies or adapt existing benchmarks to better capture performance differences among language models. To support this effort, we provide three pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate checkpoints sampled during training up to 200B tokens. All experiments and development work can be run on widely available free cloud-based GPU platforms, making participation accessible to researchers with limited computational resources. Submissions will be evaluated based on three criteria: the quality of the performance signal they produce, the consistency of model rankings at 1 trillion tokens of training, and their relevance to the scientific knowledge domain. By promoting the design of tailored evaluation strategies for early training, this competition aims to attract a broad range of participants from various disciplines, including those who may not be machine learning experts or have access to dedicated GPU resources. Ultimately, this initiative seeks to make foundational LLM research more systematic and benchmark-informed from the earliest phases of model development.

### 摘要
现有基准测试已被证明能有效评估完全训练后的大型语言模型性能。然而我们发现，在小模型的早期训练阶段存在显著差异，此时基准测试往往无法提供有意义或区分性的信号。为探究这些差异的产生机制，本次竞赛致力于设计专门用于衡量语言模型早期训练进度的科学知识评估任务。我们邀请参与者开发新型评估方法或改造现有基准测试，以更好地捕捉语言模型间的性能差异。为支持此项工作，我们提供了三个预训练小模型（0.5B、1B和3B参数）及其在200B tokens训练过程中采样的中间检查点。所有实验和开发工作均可在广泛可用的免费云端GPU平台上运行，使计算资源有限的研究者也能参与。提交成果将根据三个标准进行评估：生成性能信号的质量、1万亿tokens训练时模型排名的稳定性，以及与科学知识领域的相关性。通过推动针对早期训练的定制化评估策略设计，本次竞赛旨在吸引来自不同学科的广泛参与者，包括非机器学习专家或缺乏专用GPU资源的研究者。最终，该倡议致力于从模型开发的最初阶段就使基础LLM研究更具系统性和基准指导性。

---

## [SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling](https://arxiv.org/abs/2506.07636)

### Abstract
arXiv:2506.07636v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced rapidly from conversational problem solving to addressing real-world tasks involving tool use, such as software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex and Cursor, have offered end-to-end automation of the software development process. However, building effective SWE agents remains challenging due to the lack of high-quality training data and effective test cases. To address this issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we develop a robust pipeline to synthesize test cases for patch evaluation. Second, we scale up agent trajectories to construct the training data for building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the SWE-Dev models can achieve top performance among all open SWE agents. Specifically, the success rates of the SWE-Dev 7B and 32B parameter models reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source models. All code, models, and datasets are publicly available at https://github.com/THUDM/SWE-Dev.

### 摘要
大语言模型（LLMs）已从对话式问题解决快速发展到处理涉及工具使用的现实任务，例如软件工程（SWE）。近期基于LLM的工具包（如OpenAI Codex和Cursor）已实现软件开发流程的端到端自动化。然而，由于缺乏高质量训练数据和有效测试用例，构建高效SWE智能体仍具挑战性。为此，我们提出SWE-Dev——一个基于开源LLMs的SWE智能体。首先，我们开发了稳健的测试用例合成流程以评估代码补丁；其次，通过扩展智能体轨迹构建训练数据。在SWE-bench-Verified基准测试中，SWE-Dev模型在所有开源SWE智能体中表现最佳：7B和32B参数模型的成功率分别达到23.4%和36.6%，优于当前最先进的开源模型。所有代码、模型及数据集已公开于https://github.com/THUDM/SWE-Dev。

---

## [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)

### Abstract
arXiv:2506.07527v1 Announce Type: new 
Abstract: Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \textbf&#123;ReLIFT&#125; (\textbf&#123;Re&#125;inforcement \textbf&#123;L&#125;earning \textbf&#123;I&#125;nterleaved with Online \textbf&#123;F&#125;ine-\textbf&#123;T&#125;uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.

### 摘要
大语言模型（LLM）推理领域的最新进展表明，通过强化学习（RL）可以涌现出诸如规划与自我反思等复杂行为。然而尽管取得这些成果，现有形式的强化学习仍不足以突破基础模型的能力局限，因其主要基于模型已有知识进行优化，而非促进新信息的获取。为应对这一局限，我们采用监督微调（SFT）来学习强化学习无法掌握的内容，通过利用高质量示范数据实现新知识与推理模式的整合。通过分析LLM推理中强化学习与监督微调的训练动态，发现强化学习擅长维持并提升模型原有能力范围内问题的表现，而监督微调更有效推动模型解决当前能力边界之外的问题。基于两者优势互补，我们提出创新训练方法\textbf&#123;ReLIFT&#125;（\textbf&#123;在线微调交替强化学习&#125;），该方法以强化学习为主训练模型，当遇到难题时收集高质量解决方案进行微调，通过强化学习与微调的交替执行增强模型推理能力。在五个竞赛级基准和一个分布外基准测试中，ReLIFT相较其他零强化学习模型平均提升超过5.2分。此外，我们证明ReLIFT在使用仅13%详细示范数据的情况下，性能同时超越纯强化学习与监督微调，彰显其可扩展性。这些结果为ReLIFT突破强化学习根本局限提供了有力证据，并凸显其重大潜力。

---

## [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)

### Abstract
arXiv:2506.07564v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.

### 摘要
大语言模型（LLM）与视觉语言模型（VLM）的最新进展催生了能够执行复杂推理与多模态工具使用的自主智能体。尽管其能力不断增强，现有智能体框架仍存在脆弱性，缺乏保障信息安全流动、可靠性与多智能体协同的机制化方法。本研究提出SAFEFLOW——一种构建可信LLM/VLM智能体的协议级框架，通过细粒度信息流控制（IFC）精确追踪智能体、工具、用户及环境间所有数据的来源、完整性与机密性。通过约束LLM推理过程遵循这些安全标签，SAFEFLOW能防止非可信或对抗性输入污染高完整性决策。为确保多智能体并发场景的鲁棒性，SAFEFLOW引入事务性执行、冲突解决及共享状态安全调度机制，维护跨智能体的全局一致性。我们进一步整合预写日志、回滚与安全缓存等机制，显著提升运行时错误与策略违规的容错能力。为验证性能，我们构建了SAFEFLOWBENCH基准测试套件，专门评估对抗性、噪声及并发操作条件下的智能体可靠性。大量实验表明，基于SAFEFLOW构建的智能体在恶劣环境中仍能保持卓越的任务性能与安全保证，显著优于现有最优方案。SAFEFLOW与SAFEFLOWBENCH共同为原则化、鲁棒且安全的智能体生态系统奠定基础，推动可靠自主系统的前沿发展。

---

## [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)

### Abstract
arXiv:2506.07675v1 Announce Type: new 
Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.

### 摘要
查询重写将SQL查询转换为语义等效但执行效率更高的形式。现有方法主要依赖预定义的重写规则，但仅能处理有限查询子集且可能导致性能回退。这一局限源于基于规则的查询重写面临的三个挑战：(1)新规则的发现与验证困难；(2)固定规则无法泛化至新查询模式；(3)部分重写技术无法表示为固定规则。鉴于人类专家展现出色但缺乏扩展性的重写能力，而大语言模型(LLMs)已表现出接近人类水平的语义理解和推理能力，我们提出利用LLMs实现超越规则的SQL查询重写新方法。由于LLMs存在幻觉问题，直接应用常导致非等效和次优查询。为此，我们提出QUITE系统——基于LLM智能体的免训练、反馈感知系统，可将SQL查询重写为语义等效且性能显著提升的形式，相比基于规则的方法覆盖更广的查询模式和重写策略。首先，我们设计由有限状态机(FSM)控制的多智能体框架，使LLMs具备使用外部工具的能力，并通过实时数据库反馈增强重写过程；其次，开发重写中间件以提升LLMs生成优化查询等效项的能力；最后，采用新型提示注入技术改进重写查询的执行计划。大量实验表明，QUITE较现有最优方法最高降低35.8%查询执行时间，重写产出量增加24.1%，并能处理早期系统未覆盖的查询案例。

---

## [REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models](https://arxiv.org/abs/2506.07759)

### Abstract
arXiv:2506.07759v1 Announce Type: new 
Abstract: Multi-objective optimization is fundamental in complex decision-making tasks. Traditional algorithms, while effective, often demand extensive problem-specific modeling and struggle to adapt to nonlinear structures. Recent advances in Large Language Models (LLMs) offer enhanced explainability, adaptability, and reasoning. This work proposes Reflective Evolution of Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with LLM-based heuristic generation. A key innovation is a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity. The approach is evaluated on the Flexible Job Shop Scheduling Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate that REMoH achieves competitive results compared to state-of-the-art approaches with reduced modeling effort and enhanced adaptability. These findings underscore the potential of LLMs to augment traditional optimization, offering greater flexibility, interpretability, and robustness in multi-objective scenarios.

### 摘要
多目标优化是复杂决策任务中的基础性问题。传统算法虽然有效，但通常需要大量针对特定问题的建模工作，且难以适应非线性结构。大型语言模型（LLMs）的最新进展提供了更强的可解释性、适应性和推理能力。本研究提出多目标启发式反射进化框架（REMoH），该创新方法将NSGA-II算法与基于LLM的启发式生成相结合。其核心创新在于采用聚类和搜索空间反射机制来引导生成多样化、高质量的启发式规则，从而改善收敛性并保持解集的多样性。该方法在柔性作业车间调度问题（FJSSP）上进行了系统评估，通过Dauzere、Barnes和Brandimarte三个实例数据集与前沿方法进行了深度基准测试。结果表明，REMoH在减少建模工作量和增强适应性的同时，取得了与最先进方法相媲美的优化效果。这些发现印证了LLMs在增强传统优化方法方面的潜力，为多目标场景提供了更优的灵活性、可解释性和鲁棒性。

---

## [MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents](https://arxiv.org/abs/2506.07672)

### Abstract
arXiv:2506.07672v1 Announce Type: new 
Abstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative technique to automate human-computer interaction. However, existing CUA benchmarks predominantly target GUI agents, whose evaluation methods are susceptible to UI changes and ignore function interactions exposed by application APIs, e.g., Model Context Protocol (MCP). To this end, we propose MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those with source code availability and can be revised/re-compiled as needed (e.g., adding MCP support), with two notable advantages:
  (1) It greatly broadens the design space of CUA, such as what and how the app features to be exposed/extracted as CUA-callable APIs.
  (2) It allows MCPWorld to programmatically verify task completion by directly monitoring application behavior through techniques like dynamic code instrumentation, offering robust, accurate CUA evaluation decoupled from specific agent implementations or UI states.
  Currently, MCPWorld includes 201 well curated and annotated user tasks, covering diversified use cases and difficulty levels. MCPWorld is also fully containerized with GPU acceleration support for flexible adoption on different OS/hardware environments. Our preliminary experiments, using a representative LLM-powered CUA framework, achieve 75.12% task completion accuracy, simultaneously providing initial evidence on the practical effectiveness of agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate and standardize the benchmarking of next-generation computer use agents that can leverage rich external tools. Our code and dataset are publicly available at https://github.com/SAAgent/MCPWorld.

### 摘要
LLM驱动的计算机使用代理(CUA)正成为一种变革性技术，旨在实现人机交互自动化。然而，现有CUA基准测试主要针对GUI代理，其评估方法易受界面变化影响，且忽略了应用程序API(如模型上下文协议MCP)暴露的功能交互。为此，我们提出首个面向API、GUI及API-GUI混合代理的自动测试平台MCPWorld。MCPWorld的核心原则是采用"白盒应用"(即源代码可获取并可按需修改/重新编译的应用，例如添加MCP支持)，具有两大显著优势：(1)极大拓展了CUA的设计空间，例如确定哪些应用功能以及如何将其暴露/提取为CUA可调用API；(2)允许通过动态代码插装等技术直接监控应用行为，以编程方式验证任务完成情况，从而提供与具体代理实现或界面状态解耦的稳健、精确的CUA评估。目前MCPWorld包含201项精心设计并标注的用户任务，涵盖多样化用例及难度级别。该平台采用全容器化设计并支持GPU加速，可灵活适配不同操作系统/硬件环境。基于典型LLM驱动CUA框架的初步实验实现了75.12%的任务完成准确率，同时为利用MCP实现代理自动化的实际有效性提供了初步证据。我们期待MCPWorld能够促进并标准化下一代能利用丰富外部工具的计算机使用代理的基准测试。代码与数据集已开源：https://github.com/SAAgent/MCPWorld。

---

## [Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs](https://arxiv.org/abs/2506.07824)

### Abstract
arXiv:2506.07824v1 Announce Type: new 
Abstract: Multi-digit addition is a clear probe of the computational power of large language models. To dissect the internal arithmetic processes in LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection. Inspired by the step-by-step manner in which humans perform addition, we propose and analyze a coherent four-stage trajectory in the forward pass:Formula-structure representations become linearly decodable first, while the answer token is still far down the candidate list.Core computational features then emerge prominently.At deeper activation layers, numerical abstractions of the result become clearer, enabling near-perfect detection and decoding of the individual digits in the sum.Near the output, the model organizes and generates the final content, with the correct token reliably occupying the top rank.This trajectory suggests a hierarchical process that favors internal computation over rote memorization. We release our code and data to facilitate reproducibility.

### 摘要
多位数加法是检验大型语言模型计算能力的有效探针。为剖析LLaMA-3-8B-Instruct模型的内部算术过程，我们结合线性探测与logit-lens检测技术。受人类逐步执行加法运算的启发，我们提出并分析了一个连贯的前向传播四阶段轨迹：公式结构表征最先实现线性解码，而此时答案标记仍位于候选列表较后位置；核心计算特征随后显著浮现；在更深层激活层中，结果的数值抽象逐渐清晰，使得求和的各位数字能被近乎完美地检测和解码；接近输出层时，模型组织并生成最终内容，正确标记可靠地占据首位。该轨迹揭示了偏向内部计算而非机械记忆的层次化处理过程。我们公开代码与数据以促进研究可复现性。

---

## [Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation](https://arxiv.org/abs/2506.07820)

### Abstract
arXiv:2506.07820v1 Announce Type: new 
Abstract: Human reasoning is flexible, adaptive, and grounded in prior experience-qualities that large language models (LLMs) still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner. We propose Guideline Forest, a framework that enhances LLMs reasoning by inducing structured reasoning strategies-called guidelines-from verified examples and executing them via step-wise aggregation. Unlike test-time search or single-path distillation, our method draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants. Much like human reasoning, these variants reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step-enabling the model to adaptively resolve uncertainty and synthesize robust solutions.We evaluate Guideline Forest on four benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and programmatic reasoning. Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation, underscoring the Guideline Forest's adaptability and generalization potential.

### 摘要
人类推理具有灵活性、适应性和基于先验经验的特点——这些特质仍是当前大语言模型（LLMs）难以企及的。现有方法要么在推理时探索多样化路径，要么通过高成本操作搜索最优工作流，但均未能以结构化、高效的方式利用多重可复用策略。我们提出Guideline Forest框架，该框架通过从已验证案例中归纳结构化推理策略（称为"指南"），并通过逐步聚合执行这些策略来增强LLMs的推理能力。与测试时搜索或单路径蒸馏不同，我们的方法通过归纳可复用指南并将其扩展为多样化变体来借鉴已验证的推理经验。这些变体类似人类推理模式：体现替代性思维路径、并行执行、通过自我修正优化，并逐步聚合——使模型能自适应消除不确定性并综合稳健解决方案。我们在GSM8K、MATH-500、MBPP和HumanEval四个涵盖数学与编程推理的基准上评估Guideline Forest，其表现始终优于CoT、ReAct、ToT、FoT和AFlow等强基线。消融实验进一步验证了多路径推理与逐步聚合的有效性，彰显了Guideline Forest的适应性与泛化潜力。

---

## [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)

### Abstract
arXiv:2506.07896v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have revitalized philosophical debates surrounding artificial intelligence. Two of the most fundamental challenges - namely, the Frame Problem and the Symbol Grounding Problem - have historically been viewed as unsolvable within traditional symbolic AI systems. This study investigates whether modern LLMs possess the cognitive capacities required to address these problems. To do so, I designed two benchmark tasks reflecting the philosophical core of each problem, administered them under zero-shot conditions to 13 prominent LLMs (both closed and open-source), and assessed the quality of the models' outputs across five trials each. Responses were scored along multiple criteria, including contextual reasoning, semantic coherence, and information filtering. The results demonstrate that while open-source models showed variability in performance due to differences in model size, quantization, and instruction tuning, several closed models consistently achieved high scores. These findings suggest that select modern LLMs may be acquiring capacities sufficient to produce meaningful and stable responses to these long-standing theoretical challenges.

### 摘要
大型语言模型（LLMs）的最新进展重新激发了围绕人工智能的哲学争论。其中两个最根本的挑战——框架问题与符号接地问题——历来被认为无法在传统符号人工智能系统中解决。本研究探讨现代LLMs是否具备解决这些问题所需的认知能力。为此，我设计了两项反映每个问题哲学核心的基准任务，在零样本条件下对13个主流LLMs（包括闭源和开源模型）进行测试，并在五次重复试验中评估模型输出的质量。响应结果根据上下文推理、语义连贯性和信息过滤等多重标准进行评分。研究表明：开源模型因模型规模、量化和指令微调的差异表现出性能波动，而部分闭源模型则持续获得高分。这些发现表明，某些现代LLMs可能正在获得足以对长期理论挑战产生有意义且稳定回应的能力。

---

## [HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains](https://arxiv.org/abs/2506.07837)

### Abstract
arXiv:2506.07837v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have shown great potential in general domains but perform poorly in some specific domains due to a lack of domain-specific data, such as image-text data or vedio-text data. In some specific domains, there is abundant graphic and textual data scattered around, but lacks standardized arrangement. In the field of medical ultrasound, there are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic diagnostic reports, and so on. However, these ultrasonic materials are often saved in the forms of PDF, images, etc., and cannot be directly used for the training of MLLMs. This paper proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, and answer) from domain-specific materials. A medical ultrasound domain dataset ReMUD is established, containing over 45,000 reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound field. To facilitate research, the ReMUD dataset, data generation codebase, and ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD, addressing the data shortage issue in specific domain MLLMs.

### 摘要
多模态大语言模型（MLLMs）在通用领域展现出巨大潜力，但由于缺乏特定领域数据（如图像-文本数据或视频-文本数据），在某些特定领域表现欠佳。部分特定领域虽存在大量分散的图文数据，但缺乏标准化整理。在医学超声领域，存在超声诊断书籍、超声临床指南、超声诊断报告等资料，但这些材料通常以PDF、图像等形式保存，无法直接用于MLLMs的训练。本文提出一种新颖的图文推理监督微调数据生成流程，可从领域特定材料中构建四元组数据（图像、问题、思维轨迹和答案）。基于此建立的医学超声领域数据集ReMUD包含超过45,000条推理与非推理监督微调问答（QA）及视觉问答（VQA）数据。经Qwen2.5-VL-7B-Instruct微调得到的ReMUD-7B模型在医学超声领域性能优于通用领域MLLMs。为促进研究，ReMUD数据集、数据生成代码库及ReMUD-7B参数将发布于https://github.com/ShiDaizi/ReMUD，以解决特定领域MLLMs的数据短缺问题。

---

## [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)

### Abstract
arXiv:2506.07963v1 Announce Type: new 
Abstract: Building upon large language models (LLMs), recent large multimodal models (LMMs) unify cross-model understanding and generation into a single framework. However, LMMs still struggle to achieve accurate image-text alignment, prone to generating text responses contradicting the visual input or failing to follow the text-to-image prompts. Current solutions require external supervision (e.g., human feedback or reward models) and only address unidirectional tasks-either understanding or generation. In this work, based on the observation that understanding and generation are inverse dual tasks, we introduce a self-supervised dual reward mechanism to reinforce the understanding and generation capabilities of LMMs. Specifically, we sample multiple outputs for a given input in one task domain, then reverse the input-output pairs to compute the dual likelihood of the model as self-rewards for optimization. Extensive experimental results on visual understanding and generation benchmarks demonstrate that our method can effectively enhance the performance of the model without any external supervision, especially achieving remarkable improvements in text-to-image tasks.

### 摘要
基于大型语言模型（LLMs）的最新大型多模态模型（LMMs）将跨模态理解与生成统一至单一框架。然而，LMMs仍难以实现精确的图文对齐，易生成与视觉输入矛盾的文本响应，或无法遵循文本到图像的提示。现有解决方案需依赖外部监督（如人类反馈或奖励模型），且仅解决单向任务——理解或生成中的一种。本研究基于理解与生成互为逆对偶任务的观察，提出一种自监督的双重奖励机制以增强LMMs的理解与生成能力。具体而言，我们在单任务域中对给定输入采样多个输出，随后反转输入-输出对以计算模型的对偶似然作为自我奖励进行优化。在视觉理解与生成基准测试上的大量实验结果表明，本方法无需任何外部监督即可有效提升模型性能，尤其在文本到图像任务中实现了显著改进。

---

## [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)

### Abstract
arXiv:2506.07915v1 Announce Type: new 
Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent's internal model and the evolving reality of its operational context. This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making. To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable. However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge. To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system. This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions. Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent's learning process, and as zero-shot exploration facilitators guiding the agent's action selection process during exploration. We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies. Our findings show the potential of context-driven decision-making, where autonomous systems leverage human contextual knowledge for operational success.

### 摘要
在动态环境中，既有环境知识的快速过时会导致智能体内部模型与其运行环境的演化现实之间出现差距。这种先验环境评估与更新后评估之间的差异从根本上限制了自主决策的有效性。为弥合这一差距，人类领域利益相关者通过直接实时观察自然积累的洞察力所形成的语境偏差变得不可或缺。然而，如何将其蕴含丰富语境信息的精细化输入转化为自主系统可执行的智能信息仍是一个开放挑战。为此，我们提出LUCIFER（语言理解与语境融合的探索与行为优化框架），这是一个领域无关的框架，将分层决策架构与强化学习（RL）和大语言模型（LLMs）集成到统一系统中。该架构模拟人类分解复杂任务的方式，使高层规划器能够协调专注于不同目标和具有时间依赖性动作的专用子代理。与LLMs仅限于单一角色的传统应用不同，LUCIFER将其整合到两个协同角色中：作为语境提取器，将利益相关者的言语输入结构化处理为领域感知表征，通过注意力空间机制使LLM衍生的洞察与智能体学习过程相协调；同时作为零样本探索引导者，在探索过程中指导智能体的动作选择过程。我们对不同LLM在这两种角色中的表现进行了基准测试，结果表明LUCIFER在探索效率和决策质量上均优于扁平化的目标条件策略。本研究揭示了语境驱动决策的潜力，即自主系统通过利用人类语境知识实现操作成功。

---

## [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)

### Abstract
arXiv:2506.07940v1 Announce Type: new 
Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML platforms rely on single optimisation strategies that explore only a fraction of viable hyperparameter configurations. In this white paper, We introduce Gradients, a decentralised AutoML platform that transforms hyperparameter optimisation into a competitive marketplace where independent miners compete to discover optimal configurations. Economic incentives align individual exploration with collective optimisation goals, driving systematic investigation of hyperparameter regions that centralised methods miss. We evaluate our approach across 180 controlled experiments spanning diverse model architectures (70M to 70B parameters) and task types. Gradients achieves an 82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI, Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\% respectively. Complex reasoning and retrieval tasks show particularly strong gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for person-specific generation. These results demonstrate that competitive, economically-driven approaches can systematically discover superior configurations that centralised AutoML consistently miss.

### 摘要
基础模型微调面临一个根本性挑战：现有AutoML平台依赖单一优化策略，仅能探索可行超参数配置中的一小部分。本白皮书介绍Gradients——一个将超参数优化转化为竞争性市场的去中心化AutoML平台，其中独立矿工通过竞争发现最优配置。经济激励使个体探索与集体优化目标保持一致，推动了对集中式方法遗漏的超参数区域的系统性研究。我们在涵盖多样化模型架构（70M至70B参数）和任务类型的180组对照实验中评估该方法。Gradients对HuggingFace AutoTrain的胜率达82.8%，对TogetherAI、Databricks和Google Cloud的胜率为100%，平均改进率分别为11.8%和42.1%。复杂推理和检索任务显示出30-40%的显著提升，而扩散模型在人物特定生成任务中实现23.4%的改进。这些结果表明，经济驱动下的竞争性方法能系统性地发现集中式AutoML持续遗漏的优越配置。

---

## [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)

### Abstract
arXiv:2506.07927v1 Announce Type: new 
Abstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.

### 摘要
不等式证明作为跨越众多科学与数学领域的关键能力，能够检验发现紧界和策略性定理应用等高级推理技能。这使其成为大型语言模型(LLM)一个独特且具有挑战性的前沿领域，其研究价值超越一般数学问题求解。该领域进展受限于现有数据集通常规模有限、合成生成或形式僵化等问题。为此，我们提出一种非正式但可验证的任务框架，将不等式证明分解为两个可自动检验的子任务：边界估计与关系预测。基于此，我们发布了IneqMath数据集——一个由专家精心策划的奥赛级不等式题库，包含测试集与训练集，并附有分步解答和定理标注。我们还开发了创新的"LLM作为评判者"评估框架，通过最终答案评判员与四个分步评判员的组合来检测常见推理缺陷。对29个主流LLM在IneqMath上的系统评估揭示了一个惊人事实：即使在逐步严格审查下，像o1这样的顶级模型总体准确率也不足10%，相较仅考虑最终答案等效性的评估标准，其表现下滑幅度高达65.5%。这种差异暴露出当前LLM在构建严密证明方面存在脆弱的演绎链条和关键能力缺口。模型规模扩展和测试时计算量增加对整体证明正确性的提升效果有限。相反，我们的研究结果指明了定理引导推理和自我优化等具有前景的研究方向。代码与数据详见https://ineqmath.github.io/

---

## [MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4](https://arxiv.org/abs/2406.00971)

### Abstract
arXiv:2406.00971v2 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have recently seen significant advancements through integrating with Large Language Models (LLMs). The VLMs, which process image and text modalities simultaneously, have demonstrated the ability to learn and understand the interaction between images and texts across various multi-modal tasks. Reverse designing, which could be defined as a complex vision-language task, aims to predict the edits and their parameters, given a source image, an edited version, and an optional high-level textual edit description. This task requires VLMs to comprehend the interplay between the source image, the edited version, and the optional textual context simultaneously, going beyond traditional vision-language tasks. In this paper, we extend and fine-tune MiniGPT-4 for the reverse designing task. Our experiments demonstrate the extensibility of off-the-shelf VLMs, specifically MiniGPT-4, for more complex tasks such as reverse designing. Code is available at this \href&#123;https://github.com/VahidAz/MiniGPT-Reverse-Designing&#125;

### 摘要
视觉语言模型（VLMs）近期通过与大型语言模型（LLMs）的融合取得了显著进展。这类同时处理图像与文本模态的模型，已在多种多模态任务中展现出学习并理解图文交互的能力。逆向设计作为一种复杂的视觉语言任务，其目标是在给定源图像、编辑后版本及可选的高级文本编辑描述条件下，预测具体的编辑操作及其参数。该任务要求VLMs同步理解源图像、编辑版本与可选文本语境之间的关联，其复杂性超越了传统视觉语言任务范畴。本文通过扩展并微调MiniGPT-4模型实现逆向设计任务，实验证明现成VLM（特别是MiniGPT-4）具备向逆向设计等复杂任务扩展的潜力。代码已发布于\href&#123;https://github.com/VahidAz/MiniGPT-Reverse-Designing&#125;。

---

## [GOLFer: Smaller LM-Generated Documents Hallucination Filter &amp; Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)

### Abstract
arXiv:2506.04762v1 Announce Type: cross 
Abstract: Large language models (LLMs)-based query expansion for information retrieval augments queries with generated hypothetical documents with LLMs. However, its performance relies heavily on the scale of the language models (LMs), necessitating larger, more advanced LLMs. This approach is costly, computationally intensive, and often has limited accessibility. To address these limitations, we introduce GOLFer - Smaller LMs-Generated Documents Hallucination Filter &amp; Combiner - a novel method leveraging smaller open-source LMs for query expansion. GOLFer comprises two modules: a hallucination filter and a documents combiner. The former detects and removes non-factual and inconsistent sentences in generated documents, a common issue with smaller LMs, while the latter combines the filtered content with the query using a weight vector to balance their influence. We evaluate GOLFer alongside dominant LLM-based query expansion methods on three web search and ten low-resource datasets. Experimental results demonstrate that GOLFer consistently outperforms other methods using smaller LMs, and maintains competitive performance against methods using large-size LLMs, demonstrating its effectiveness.

### 摘要
基于大语言模型（LLMs）的信息检索查询扩展技术通过LLMs生成假设文档来增强查询。然而，该方法的性能严重依赖于语言模型（LMs）的规模，需要使用更大、更先进的大语言模型。这种方案成本高昂、计算密集，且往往可访问性有限。为解决这些局限性，我们提出GOLFer——基于较小开源语言模型的生成文档幻觉过滤与组合器——这是一种利用较小开源语言模型进行查询扩展的新方法。GOLFer包含两个模块：幻觉过滤器和文档组合器。前者检测并去除生成文档中非事实性和不一致的句子（这是较小语言模型的常见问题），后者则通过权重向量将过滤后的内容与查询相结合以平衡其影响。我们在三个网络搜索和十个低资源数据集上评估了GOLFer与主流基于大语言模型的查询扩展方法。实验结果表明，GOLFer在使用较小语言模型时始终优于其他方法，并在与使用大型大语言模型的方法对比中保持竞争力，证明了其有效性。

---

## [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)

### Abstract
arXiv:2506.08012v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.

### 摘要
多模态大语言模型（MLLMs）在图形用户界面（GUI）自动化领域展现出革命性潜力。然而现有GUI模型主要依赖于近乎无误差的离线轨迹学习，缺乏自我反思与错误恢复能力。为弥补这一缺陷，我们提出GUI-Reflection框架，通过专门训练阶段（GUI特定预训练、离线监督微调及在线反思调优）将自我反思与纠错能力显式整合到端到端多模态GUI模型中。该框架通过全自动数据生成与学习流程实现反思行为的涌现，无需人工标注：1）首先提出可扩展数据管道，从现有成功轨迹自动构建反思与纠错数据。针对现有模型主要关注 grounding 和 UI 理解能力，我们设计GUI-Reflection任务套件来显式学习与评估反思导向能力；2）构建移动端多样化高效环境用于GUI模型的在线训练与数据采集；3）提出基于该环境的迭代式在线反思调优算法，持续增强模型的反思与纠错能力。本框架赋予GUI代理自我反思与修正能力，为构建更鲁棒、自适应和智能的GUI自动化奠定基础，所有数据、模型、环境及工具将全面开源。

---

## [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)

### Abstract
arXiv:2506.06292v1 Announce Type: cross 
Abstract: During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.

### 摘要
在大型语言模型（LLMs）的偏好优化过程中，新生成的模型样本与用于训练奖励模型（RM）的数据之间可能出现分布偏移。这种偏移会降低RM的有效性，进而对策略模型（PM）的性能产生负面影响。为解决这一挑战，我们提出Mutual-Taught——一种无需额外人工标注即可迭代改进PM和RM的自训练方法。该方法与期望最大化（EM）算法相呼应：在E步中，利用当前RM的反馈更新PM，引导PM更接近潜在最优偏好分布；在M步中，通过构建E步更新前后PM输出的训练数据来更新RM，使RM能够适应不断演化的策略分布。实验结果表明，这种迭代方法可使两个模型持续改进。具体而言，我们的80亿参数策略模型LLaMA-3-8B-Instruct-MT在AlpacaEval-2上实现了54.1%的长度控制胜率，而80亿参数奖励模型FsfairX-LLaMA3-RM-MT在RewardBench上的表现与GPT-4o-2024-08-06相当。

---

## [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)

### Abstract
arXiv:2506.06295v1 Announce Type: cross 
Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large Language Models. Recently, a new paradigm has emerged in the form of diffusion-based Large Language Models (dLLMs), which generate text by iteratively denoising masked segments. This approach has shown significant advantages and potential. However, dLLMs suffer from high inference latency. Traditional ARM acceleration techniques, such as Key-Value caching, are incompatible with dLLMs due to their bidirectional attention mechanism. To address this specific challenge, our work begins with a key observation that dLLM inference involves a static prompt and a partially dynamic response, where most tokens remain stable across adjacent denoising steps. Based on this, we propose dLLM-Cache, a training-free adaptive caching framework that combines long-interval prompt caching with partial response updates guided by feature similarity. This design enables efficient reuse of intermediate computations without compromising model performance. Extensive experiments on representative dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1 x speedup over standard inference without compromising output quality. Notably, our method brings dLLM inference latency close to that of ARMs under many settings. Codes are provided in the supplementary material and will be released publicly on GitHub.

### 摘要
自回归模型（ARMs）长期以来主导着大语言模型的发展格局。近期，基于扩散机制的大语言模型（dLLMs）作为一种新范式出现，其通过迭代去噪掩码文本来生成内容，展现出显著优势和发展潜力。然而，dLLMs存在推理延迟高的问题。传统ARM加速技术（如键值缓存）因其双向注意力机制而与dLLMs不兼容。针对这一特殊挑战，我们的研究首先发现：dLLM推理过程包含静态提示词和部分动态响应，其中多数标记在相邻去噪步骤间保持稳定。基于此，我们提出dLLM-Cache——一种免训练的自适应缓存框架，通过结合长间隔提示词缓存与特征相似性指导的部分响应更新，实现中间计算量的高效复用且不影响模型性能。在LLaDA 8B和Dream 7B等代表性dLLMs上的大量实验表明，dLLM-Cache在保证输出质量前提下，相比标准推理可实现最高9.1倍的加速效果。值得注意的是，本方法使dLLMs的推理延迟在多场景下接近ARMs水平。代码详见补充材料并将开源发布于GitHub。

---

## [A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing](https://arxiv.org/abs/2506.06316)

### Abstract
arXiv:2506.06316v1 Announce Type: cross 
Abstract: For personalized marketing, a new challenge of how to effectively algorithm the A/B testing to maximize user response is urgently to be overcome. In this paper, we present a new approach, the RL-LLM-AB test framework, for using reinforcement learning strategy optimization combined with LLM to automate and personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained instruction-tuned language model. It first generates A/B versions of candidate content variants using a Prompt-Conditioned Generator, and then dynamically embeds and fuses the user portrait and the context of the current query with the multi-modal perception module to constitute the current interaction state. The content version is then selected in real-time through the policy optimization module with an Actor-Critic structure, and long-term revenue is estimated according to real-time feedback (such as click-through rate and conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded into the framework to capture long-term user preference drift, which helps to generalize policy across multiple users and content contexts. Numerical results demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B testing methods, including classical A/B testing, Contextual Bandits, and benchmark reinforcement learning approaches on real-world marketing data.

### 摘要
针对个性化营销中如何有效算法化A/B测试以最大化用户响应这一新挑战，本文提出RL-LLM-AB测试框架，通过强化学习策略优化结合大语言模型实现自动化与个性化的A/B测试。该框架基于预训练指令调优语言模型构建：首先通过提示条件生成器产生候选内容变体的A/B版本；继而利用多模态感知模块动态嵌入并融合用户画像与当前查询上下文，构成交互状态；再通过具有Actor-Critic结构的策略优化模块实时选择内容版本，并根据实时反馈（如点击率与转化率）估算长期收益。框架中嵌入了记忆增强奖励估计器以捕捉用户长期偏好漂移，从而提升策略在多元用户与内容场景中的泛化能力。数值实验表明，在真实营销数据上，RL-LLM-AB测试方法相较于传统A/B测试、情境赌博机及基准强化学习方法具有显著优势。

---

## [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)

### Abstract
arXiv:2506.06313v1 Announce Type: cross 
Abstract: Long document understanding has become increasingly crucial in natural language processing, with retrieval-based methods emerging as a promising solution to address the context length limitations of large language models (LLMs). However, existing approaches either treat documents as flat sequences or employ arbitrary chunking strategies, failing to capture the inherent discourse structure that guides human comprehension. We present DISRetrieval, a novel hierarchical retrieval framework that leverages linguistic discourse structure to enhance long document understanding. Our approach introduces three key innovations: (1) a discourse-aware document organization framework that utilizes rhetorical structure theory (RST) to create sentence-level hierarchical representations, preserving both semantic relationships and natural document flow; (2) an LLM-enhanced node representation technique that combines discourse structure with adaptive summarization to enrich tree nodes with contextual information; and (3) a hierarchical evidence retrieval mechanism that effectively selects relevant content while maintaining discourse coherence. Through comprehensive experiments on QASPER and QuALITY datasets, DISRetrieval demonstrates substantial improvements over existing methods in both token-level retrieval metrics and downstream question answering tasks. Our ablation studies confirm that incorporating discourse structure significantly enhances retrieval effectiveness across different document lengths and query types, validating the importance of linguistically-informed document representation in long-text understanding. Our code and datasets are publicly available at github/DreamH1gh/DISRetrieval to facilitate future research.

### 摘要
长文档理解在自然语言处理中日益重要，基于检索的方法成为解决大语言模型（LLMs）上下文长度限制的有效方案。然而现有方法要么将文档视为扁平序列，要么采用任意分块策略，均未能捕捉指导人类理解的固有篇章结构。本文提出DISRetrieval——一种利用语言学篇章结构增强长文档理解的新型分层检索框架，包含三项关键创新：（1）基于修辞结构理论（RST）的篇章感知文档组织框架，构建句子级分层表征，同时保留语义关系和自然文档流；（2）结合篇章结构与自适应摘要的LLM增强节点表示技术，通过上下文信息丰富树节点；（3）分层证据检索机制，在保持篇章连贯性的同时有效筛选相关内容。在QASPER和QuALITY数据集上的综合实验表明，DISRetrieval在词元级检索指标和下游问答任务上均显著优于现有方法。消融研究证实，引入篇章结构能显著提升不同文档长度和查询类型的检索效果，验证了基于语言学知识的文档表征对长文本理解的重要性。相关代码和数据集已开源于github/DreamH1gh/DISRetrieval以促进后续研究。

---

## [Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support](https://arxiv.org/abs/2506.06340)

### Abstract
arXiv:2506.06340v1 Announce Type: cross 
Abstract: The advent of large language models (LLMs) has opened new avenues for analyzing complex, unstructured data, particularly within the medical domain. Electronic Health Records (EHRs) contain a wealth of information in various formats, including free text clinical notes, structured lab results, and diagnostic codes. This paper explores the application of advanced language models to leverage these diverse data sources for improved clinical decision support. We will discuss how text-based features, often overlooked in traditional high dimensional EHR analysis, can provide semantically rich representations and aid in harmonizing data across different institutions. Furthermore, we delve into the challenges and opportunities of incorporating medical codes and ensuring the generalizability and fairness of AI models in healthcare.

### 摘要
大型语言模型（LLMs）的出现为分析复杂的非结构化数据开辟了新途径，尤其是在医疗领域。电子健康记录（EHRs）包含多种格式的丰富信息，包括自由文本临床记录、结构化实验室结果和诊断代码。本文探讨了如何应用先进语言模型整合这些多样化数据源以提升临床决策支持。我们将讨论传统高维EHR分析中常被忽视的文本特征如何提供语义丰富的表征，并帮助协调不同机构间的数据。此外，我们深入研究了整合医疗代码的挑战与机遇，以及如何确保医疗人工智能模型的普适性和公平性。

---

## [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)

### Abstract
arXiv:2506.06331v1 Announce Type: cross 
Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented generation (GraphRAG) enhances large language models (LLMs) to generate quality answers for user questions. Many GraphRAG methods have been proposed and reported inspiring performance in answer quality. However, we observe that the current answer evaluation framework for GraphRAG has two critical flaws, i.e., unrelated questions and evaluation biases, which may lead to biased or even wrong conclusions on performance. To tackle the two flaws, we propose an unbiased evaluation framework that uses graph-text-grounded question generation to produce questions that are more related to the underlying dataset and an unbiased evaluation procedure to eliminate the biases in LLM-based answer assessment. We apply our unbiased framework to evaluate 3 representative GraphRAG methods and find that their performance gains are much more moderate than reported previously. Although our evaluation framework may still have flaws, it calls for scientific evaluations to lay solid foundations for GraphRAG research.

### 摘要
通过从知识图谱中检索上下文，基于图谱的检索增强生成（GraphRAG）能够提升大语言模型（LLM）为用户问题生成高质量答案的能力。目前已有多项GraphRAG方法被提出，并报告了令人鼓舞的答案质量表现。然而，我们发现当前GraphRAG的答案评估框架存在两个关键缺陷：无关问题与评估偏差，这可能导致对模型性能得出有偏甚至错误的结论。为解决这两个缺陷，我们提出了一种无偏评估框架：采用图文本锚定问题生成技术产生与底层数据集更相关的问题，并通过无偏评估流程消除基于LLM的答案评估中的偏差。应用该框架对3种代表性GraphRAG方法进行评估后，我们发现其性能提升幅度较先前报道结果更为温和。尽管我们的评估框架可能仍存在不足，但这项工作呼吁通过科学评估为GraphRAG研究奠定坚实基础。

---

## [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)

### Abstract
arXiv:2506.06303v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) is a human-designed framework for solving sequential decision making problems. In this work, we demonstrate that, surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a phenomenon known as in-context RL (ICRL). Specifically, we propose a novel multi-round prompting framework called ICRL prompting. The goal is to prompt the LLM to complete a task. After the LLM generates a response at the current round, we give numerical scalar feedbacks for the response, called the rewards. At the next round, we prompt the LLM again with the same task and a context consisting of all previous responses and rewards. We observe that the quality of the LLM's response increases as the context grows. In other words, the LLM is able to maximize the scalar reward signal in the inference time, just like an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24, creative writing, and ScienceWorld) and demonstrate significant performance improvements over baseline methods such as Self-Refine and Reflexion. Surprisingly, in some experiments the reward signals are generated by the LLM itself, yet performance improvements are still observed from ICRL prompting, offering a promising paradigm for scaling test-time compute.

### 摘要
强化学习（RL）是人类设计的用于解决序列决策问题的框架。本研究发现，令人惊讶的是，RL会在大语言模型（LLM）的推理过程中自然涌现——这种现象被称为上下文强化学习（ICRL）。具体而言，我们提出了一种名为ICRL提示的新型多轮提示框架，其目标是引导LLM完成任务。当LLM在当前轮次生成响应后，我们会为该响应提供数值标量反馈（即奖励）。在下一轮次中，我们会用相同任务及包含所有历史响应与奖励的上下文再次提示LLM。实验观察到，随着上下文内容的增加，LLM的响应质量持续提升。这表明LLM能够在推理阶段实现标量奖励信号的最大化，其行为模式与RL算法高度相似。我们在三个基准测试（24点游戏、创意写作和ScienceWorld）中评估了ICRL提示方法，结果表明其性能显著优于Self-Refine和Reflexion等基线方法。尤为值得注意的是，在某些实验中奖励信号完全由LLM自身生成，但ICRL提示仍能带来性能提升，这为扩展测试时计算提供了具有前景的新范式。

---

## [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)

### Abstract
arXiv:2506.06347v1 Announce Type: cross 
Abstract: Toxicity detection in gaming communities faces significant scaling challenges when expanding across multiple games and languages, particularly in real-time environments where computational efficiency is crucial. We present two key findings to address these challenges while building upon our previous work on ToxBuster, a BERT-based real-time toxicity detection system. First, we introduce a soft-prompting approach that enables a single model to effectively handle multiple games by incorporating game-context tokens, matching the performance of more complex methods like curriculum learning while offering superior scalability. Second, we develop an LLM-assisted label transfer framework using GPT-4o-mini to extend support to seven additional languages. Evaluations on real game chat data across French, German, Portuguese, and Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with particularly strong performance in German, surpassing the English benchmark of 45.39%. In production, this unified approach significantly reduces computational resources and maintenance overhead compared to maintaining separate models for each game and language combination. At Ubisoft, this model successfully identifies an average of 50 players, per game, per day engaging in sanctionable behavior.

### 摘要
游戏社区中的毒性检测在扩展到多游戏和多语言环境时面临显著的规模化挑战，尤其在需要计算效率的实时场景中更为突出。基于我们先前开发的基于BERT的实时毒性检测系统ToxBuster，本研究提出两项关键发现：首先，我们采用软提示技术，通过引入游戏上下文标记使单一模型能有效处理多款游戏，其性能与课程学习等复杂方法相当，同时具备更优的可扩展性；其次，我们开发了基于GPT-4o-mini的大语言模型辅助标签迁移框架，将支持语言扩展至七种。在法语、德语、葡萄牙语和俄语的真实游戏聊天数据评估中，宏观F1分数达到32.96%至58.88%，其中德语表现尤为突出，甚至超过英语基准的45.39%。实际应用中，这种统一方案相比为每种游戏和语言组合维护独立模型，显著降低了计算资源和维护成本。在育碧公司的部署中，该模型平均每天每款游戏能成功识别50名存在可制裁行为的玩家。

---

## [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)

### Abstract
arXiv:2506.06343v1 Announce Type: cross 
Abstract: Recent advances in speech-enabled language models have shown promising results in building intelligent voice assistants. However, most existing approaches rely on large-scale paired speech-text data and extensive computational resources, which pose challenges in terms of scalability and accessibility. In this paper, we present \textbf&#123;TESU-LLM&#125;, a novel framework that enables training speech-capable language models using only text data. Our key insight is to leverage a unified encoder that maps semantically equivalent text and speech inputs to a shared latent space. By aligning the encoder output with the embedding space of a LLM via a lightweight projection network, we enable the model to generalize from text-only supervision to speech-based inference. Despite being trained exclusively on text, TESU-LLM achieves strong performance on various speech-related benchmarks, comparable to baseline methods trained with large-scale multimodal datasets and substantial computational resources. These results highlight the effectiveness and efficiency of our approach, offering a scalable path toward building speech LLMs without speech data.

### 摘要
语音增强语言模型的最新进展在构建智能语音助手方面显示出令人鼓舞的成果。然而，现有方法大多依赖大规模配对语音-文本数据和大量计算资源，这给可扩展性和可访问性带来了挑战。本文提出TESU-LLM——一种仅使用文本数据即可训练语音语言模型的新框架。我们的核心思路是通过统一编码器将语义等效的文本与语音输入映射至共享潜在空间。通过轻量级投影网络将编码器输出与LLM的嵌入空间对齐，该模型能够从纯文本监督泛化至基于语音的推理。尽管仅接受文本训练，TESU-LLM在多项语音相关基准测试中表现优异，其性能可与基于大规模多模态数据集和大量计算资源训练的基线方法相媲美。这些结果凸显了我们方法的有效性与高效性，为无需语音数据构建语音语言模型提供了可扩展的路径。

---

## [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)

### Abstract
arXiv:2506.06339v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture for combining the precision of retrieval systems with the fluency of large language models. While several studies have investigated RAG pipelines for high-resource languages, the optimization of RAG components for Arabic remains underexplored. This study presents a comprehensive empirical evaluation of state-of-the-art RAG components-including chunking strategies, embedding models, rerankers, and language models-across a diverse set of Arabic datasets. Using the RAGAS framework, we systematically compare performance across four core metrics: context precision, context recall, answer faithfulness, and answer relevancy. Our experiments demonstrate that sentence-aware chunking outperforms all other segmentation methods, while BGE-M3 and Multilingual-E5-large emerge as the most effective embedding models. The inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness in complex datasets, and Aya-8B surpasses StableLM in generation quality. These findings provide critical insights for building high-quality Arabic RAG pipelines and offer practical guidelines for selecting optimal components across different document types.

### 摘要
检索增强生成（RAG）架构通过结合检索系统的精确性与大语言模型的流畅性，已成为一种强大范式。尽管已有诸多研究针对高资源语言的RAG流程展开探讨，但阿拉伯语RAG组件的优化研究仍显不足。本研究对阿拉伯语多数据集中的前沿RAG组件（包括文本分块策略、嵌入模型、重排序器及语言模型）进行了全面实证评估。基于RAGAS框架，我们系统比较了四项核心指标的表现：上下文精确度、上下文召回率、答案忠实度及答案相关性。实验表明：句子感知分块法优于其他分割方法；BGE-M3与Multilingual-E5-large成为最有效的嵌入模型；引入重排序器（bge-reranker-v2-m3）显著提升复杂数据集中的忠实度；Aya-8B在生成质量上超越StableLM。这些发现为构建高质量阿拉伯语RAG流程提供了关键洞见，并为不同文档类型下的组件选择提供了实用指南。

---

## [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)

### Abstract
arXiv:2506.06353v1 Announce Type: cross 
Abstract: The growing convergence between Large Language Models (LLMs) and electroencephalography (EEG) research is enabling new directions in neural decoding, brain-computer interfaces (BCIs), and affective computing. This survey offers a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications. We organize the literature into four domains: (1) LLM-inspired foundation models for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal generation including image and 3D object synthesis, and (4) clinical applications and dataset management tools. The survey highlights how transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance. By offering a structured overview of modeling strategies, system designs, and application areas, this work serves as a foundational resource for future work to bridge natural language processing and neural signal analysis through language models.

### 摘要
大型语言模型（LLMs）与脑电图（EEG）研究的日益融合，正在为神经解码、脑机接口（BCIs）和情感计算等领域开辟新的研究方向。本文系统综述了利用LLMs进行EEG分析与应用的最新进展，并提出了结构化分类法。我们将相关文献归纳为四个领域：（1）用于EEG表征学习的LLM启发式基础模型，（2）EEG到语言的解码，（3）包括图像和3D物体合成在内的跨模态生成，以及（4）临床应用与数据集管理工具。本综述重点阐述了基于Transformer架构的模型如何通过微调、少样本和零样本学习，使EEG模型能够执行自然语言生成、语义解释和诊断辅助等复杂任务。通过提供建模策略、系统设计和应用领域的结构化概览，本研究为未来通过语言模型 bridging 自然语言处理与神经信号分析的研究提供了基础性资源。

---

## [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)

### Abstract
arXiv:2506.06355v1 Announce Type: cross 
Abstract: Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes. Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios. This study examines multiple LLMs to proactively estimate perceived earthquake impacts. Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales. Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real reports at the zip code level. Techniques such as RAG and ICL can improve simulation performance, while visual inputs notably enhance accuracy compared to structured numerical data alone. These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning.

### 摘要
高效模拟对于提升地震等突发性灾害的主动应对能力至关重要。近期大型语言模型（LLM）作为世界模型的发展，为复杂场景模拟提供了新可能。本研究通过多模型评估，前瞻性预测地震感知影响。我们构建的多模态框架整合地理空间、社会经济、建筑及街景数据，在邮编和县郡尺度生成修正麦卡利烈度（MMI）预测。基于2014年纳帕地震和2019年里奇克雷斯特地震的USGS"Did You Feel It? (DYFI)"报告验证显示：邮编级别的预测结果与实际报告高度吻合（相关系数0.88，均方根误差0.77）。研究表明，检索增强生成（RAG）和上下文学习（ICL）技术可提升模拟性能，而视觉数据较纯结构化数值数据能显著提高预测精度。这些发现证实了LLM在灾害影响模拟中的应用潜力，可为灾前规划提供决策支持。

---

## [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)

### Abstract
arXiv:2506.06382v1 Announce Type: cross 
Abstract: This paper explains \textbf&#123;why it is impossible to create large language models that do not hallucinate and what are the trade-offs we should be looking for&#125;. It presents a formal \textbf&#123;impossibility theorem&#125; demonstrating that no inference mechanism can simultaneously satisfy four fundamental properties: \textbf&#123;truthful (non-hallucinatory) generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality&#125;. By modeling LLM inference as an \textbf&#123;auction of ideas&#125; where neural components compete to contribute to responses, we prove the impossibility using the Green-Laffont theorem. That mathematical framework provides a rigorous foundation for understanding the nature of inference process, with implications for model architecture, training objectives, and evaluation methods.

### 摘要
本文阐述了为何无法构建不产生幻觉的大语言模型，以及我们应当权衡哪些关键因素。通过提出一个形式化的不可能性定理，我们证明了任何推理机制都无法同时满足四个基本属性：真实（无幻觉）生成、语义信息守恒、相关知识揭示以及知识约束下的最优性。通过将LLM推理建模为神经组件竞争贡献响应的"思想拍卖"过程，我们利用格林-拉丰定理完成了该不可能性证明。这一数学框架为理解推理过程的本质提供了严格基础，对模型架构、训练目标和评估方法均具有重要启示意义。

---

## [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)

### Abstract
arXiv:2506.06335v1 Announce Type: cross 
Abstract: In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs' practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.

### 摘要
在自然语言处理（NLP）领域，研究重心已从BERT等仅编码器微型语言模型转向GPT-3等仅解码器大语言模型（LLM）。然而LLM在金融领域的实际应用暴露出三大局限性：（1）在判别任务（如财报市场情绪分析）中，LLM表现常逊于微调后的BERT，却消耗更高计算资源；（2）生成任务应用高度依赖检索增强生成（RAG）方法获取实时专业信息，而通用检索器在领域特定检索任务中表现欠佳；（3）其他基于特征的场景（如主题建模）存在额外不足。我们提出FinBERT2——基于320亿token高质量金融语料预训练的专业双向编码器，这是同类参数规模模型中已知最大的中文金融预训练语料。作为更优基础模型，FinBERT2通过以下成果弥合LLM金融领域部署的鸿沟：（1）判别式微调模型（Fin-Labelers）在五项金融分类任务中平均优于其他（Fin）BERT变体0.4%-3.3%，领先主流LLM达9.7%-12.3%；（2）对比微调模型（Fin-Retrievers）在五项金融检索任务中超越开源（如较BGE-base-zh平均提升6.8%）与商用（如较OpenAI的text-embedding-3-large平均提升4.2%）嵌入模型；（3）基于FinBERT2变体构建的Fin-TopicModel可实现金融标题的优越聚类与主题表征。本研究通过与当代LLM的对比分析重新审视金融BERT模型，为LLM时代高效利用FinBERT提供了实践启示。

---

## [AI Agent Behavioral Science](https://arxiv.org/abs/2506.06366)

### Abstract
arXiv:2506.06366v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled AI systems to behave in increasingly human-like ways, exhibiting planning, adaptation, and social dynamics across increasingly diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the models' internal architecture, but emerge from their integration into agentic systems that operate within situated contexts, where goals, feedback, and interactions shape behavior over time. This shift calls for a new scientific lens: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this paradigm emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize a growing body of research across individual, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as a necessary complement to traditional approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems.

### 摘要
大型语言模型（LLMs）的最新进展使得人工智能系统能够以越来越类人的方式运作，在日益多样化、交互式和开放式的场景中展现出规划、适应和社会动态等能力。这些行为不仅是模型内部架构的产物，更源于其被整合到具有情境感知的智能体系统中——在该系统中，目标、反馈和交互会随时间推移塑造行为模式。这一转变需要新的科学视角：AI智能体行为科学。该范式不再仅关注内部机制，而是强调通过系统行为观察、假设验证的干预设计以及理论指导的行为解读，来研究AI智能体如何随时间推移行动、适应和交互。我们对个体智能体、多智能体以及人机交互场景下的新兴研究进行了系统化梳理，并进一步论证了如何通过将公平性、安全性、可解释性、问责制和隐私视为行为属性，来推动负责任AI的发展。通过整合最新研究成果并规划未来方向，我们将AI智能体行为科学定位为传统方法的必要补充，为理解、评估和治理日益自主的AI系统在现实世界中的行为提供了关键工具。

---

## [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)

### Abstract
arXiv:2506.06376v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural language processing tasks, yet they encounter challenges in complex decision-making scenarios that require long-term reasoning and alignment with high-level objectives. Existing methods either rely on short-term auto-regressive action generation or face limitations in accurately simulating rollouts and assessing outcomes, leading to sub-optimal decisions. This paper introduces a novel LLM-based Actor-Critic framework, termed LAC, that effectively improves LLM policies with long-term action evaluations in a principled and scalable way. Our approach addresses two key challenges: (1) extracting robust action evaluations by computing Q-values via token logits associated with positive/negative outcomes, enhanced by future trajectory rollouts and reasoning; and (2) enabling efficient policy improvement through a gradient-free mechanism. Experiments across diverse environments -- including high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text), and large action spaces (WebShop) -- demonstrate the framework's generality and superiority over state-of-the-art methods. Notably, our approach achieves competitive performance using 7B/8B parameter LLMs, even outperforming baseline methods employing GPT-4 in complex tasks. These results underscore the potential of integrating structured policy optimization with LLMs' intrinsic knowledge to advance decision-making capabilities in multi-step environments.

### 摘要
大语言模型（LLMs）在自然语言处理任务中取得了显著进展，但在需要长期推理和符合高层目标的复杂决策场景中仍面临挑战。现有方法要么依赖短时自回归动作生成，要么在准确模拟推演和评估结果方面存在局限，导致决策次优。本文提出了一种新颖的基于LLM的演员-评论家框架LAC，通过原则性且可扩展的方式，利用长期动作评估有效改进了LLM策略。我们的方法解决了两个关键问题：（1）通过计算与正/负结果相关联的词元逻辑的Q值，结合未来轨迹推演和推理，提取稳健的动作评估；（2）通过无梯度机制实现高效策略改进。在多样化环境中的实验——包括高层决策（ALFWorld）、低层动作空间（BabyAI-Text）和大动作空间（WebShop）——验证了该框架的通用性和对现有最先进方法的优越性。值得注意的是，使用7B/8B参数的LLMs时，我们的方法实现了具有竞争力的性能，甚至在复杂任务中超越了采用GPT-4的基线方法。这些结果凸显了将结构化策略优化与LLMs内在知识相结合，在多步环境中提升决策能力的潜力。

---

## [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)

### Abstract
arXiv:2506.06380v1 Announce Type: cross 
Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.

### 摘要
极端事件（如市场崩盘、自然灾害和流行病）虽罕见却具有灾难性，常引发跨互联系统的级联故障。精准预测与早期预警有助于减少损失并提升应对能力。尽管数据驱动方法为极端事件建模提供了强大工具，但其需要大量训练数据，而极端事件数据本身稀缺，构成了根本性挑战。合成数据生成已成为有效解决方案。然而现有综述多聚焦于通用数据且侧重隐私保护，未能涵盖极端事件的特殊性能需求。本综述首次系统阐述面向极端事件的合成数据生成方法：我们系统回顾了生成建模技术与大语言模型，特别是经统计理论增强及采用特殊训练与采样机制以捕捉厚尾分布的方法；汇总了基准数据集，并提出涵盖统计特性、依赖性、可视化及任务导向指标的定制化评估框架。核心贡献在于深入分析各指标在极端性刻画与领域适配中的适用性，为极端场景下的模型评估提供实操指南。我们划分了关键应用领域，并指出行为金融、野火、地震、风暴和传染病爆发等研究不足的方向。最后，我们提出开放挑战，为推进合成稀有事件研究奠定结构化基础。

---

## [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)

### Abstract
arXiv:2506.06359v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has long promised to improve energy management in smart grids by enhancing situational awareness and supporting more effective decision-making. While traditional machine learning has demonstrated notable results in forecasting and optimization, it often struggles with generalization, situational awareness, and heterogeneous data integration. Recent advances in foundation models such as Transformer architecture and Large Language Models (LLMs) have demonstrated improved capabilities in modelling complex temporal and contextual relationships, as well as in multi-modal data fusion which is essential for most AI applications in the energy sector. In this review we synthesize the rapid expanding field of AI applications in the energy domain focusing on Transformers and LLMs. We examine the architectural foundations, domain-specific adaptations and practical implementations of transformer models across various forecasting and grid management tasks. We then explore the emerging role of LLMs in the field: adaptation and fine tuning for the energy sector, the type of tasks they are suited for, and the new challenges they introduce. Along the way, we highlight practical implementations, innovations, and areas where the research frontier is rapidly expanding. These recent developments reviewed underscore a broader trend: Generative AI (GenAI) is beginning to augment decision-making not only in high-level planning but also in day-to-day operations, from forecasting and grid balancing to workforce training and asset onboarding. Building on these developments, we introduce the concept of the Agentic Digital Twin, a next-generation model that integrates LLMs to bring autonomy, proactivity, and social interaction into digital twin-based energy management systems.

### 摘要
长期以来，人工智能（AI）一直有望通过增强态势感知和支持更有效的决策来改善智能电网中的能源管理。虽然传统机器学习在预测和优化方面已展现出显著成果，但其在泛化能力、态势感知和异构数据整合方面仍存在不足。基于Transformer架构和大语言模型（LLMs）的基础模型最新进展，在建模复杂时序与上下文关系以及多模态数据融合方面表现出更强的能力——这些对能源领域大多数AI应用至关重要。本文综述了聚焦Transformer和LLMs的能源领域AI应用的快速扩展研究：我们考察了Transformer模型在不同预测和电网管理任务中的架构基础、领域适应性调整及实际应用；探究了LLMs在能源领域的新兴作用——包括针对能源行业的适配与微调、适用任务类型及其带来的新挑战。在梳理过程中，我们重点分析了实际应用案例、创新成果以及研究前沿快速拓展的领域。这些最新进展揭示了一个更广泛的趋势：生成式人工智能（GenAI）正开始从高层规划到日常运营（包括预测、电网平衡、员工培训和资产接入等）全面增强决策能力。基于这些发展，我们提出了“代理型数字孪生”的概念——这是一种集成LLMs的新一代模型，旨在为基于数字孪生的能源管理系统赋予自主性、主动性和社交交互能力。

---

## [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)

### Abstract
arXiv:2506.06384v1 Announce Type: cross 
Abstract: With the widespread adoption of Large Language Models (LLMs), prompt injection attacks have emerged as a significant security threat. Existing defense mechanisms often face critical trade-offs between effectiveness and generalizability. This highlights the urgent need for efficient prompt injection detection methods that are applicable across a wide range of LLMs. To address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion detection framework. It integrates a pretrained language model with heuristic feature engineering to detect prompt injection attacks. Specifically, the framework employs DeBERTa-v3-base as a feature extractor to transform input text into semantic vectors enriched with contextual information. In parallel, we design heuristic rules based on known attack patterns to extract explicit structural features commonly observed in attacks. Features from both channels are subsequently fused and passed through a fully connected neural network to produce the final prediction. This dual-channel approach mitigates the limitations of relying only on DeBERTa to extract features. Experimental results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms existing methods in terms of accuracy, recall, and F1-score. Furthermore, when deployed actually, it significantly reduces attack success rates across mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

### 摘要
随着大语言模型（LLMs）的广泛采用，提示注入攻击已成为重大安全威胁。现有防御机制往往面临有效性与泛化性之间的关键权衡，这凸显了亟需开发适用于多种LLMs的高效提示注入检测方法。为此，我们提出DMPI-PMHFE——一种双通道特征融合检测框架，通过整合预训练语言模型与启发式特征工程来检测提示注入攻击。具体而言，该框架采用DeBERTa-v3-base作为特征提取器，将输入文本转化为富含上下文信息的语义向量；同时基于已知攻击模式设计启发式规则，提取攻击中常见的显式结构特征。双通道特征经融合后通过全连接神经网络生成最终预测。这种双通道方法缓解了仅依赖DeBERTa提取特征的局限性。在多样化基准数据集上的实验表明，DMPI-PMHFE在准确率、召回率和F1分数上均优于现有方法。实际部署时，该框架能显著降低GLM-4、LLaMA 3、Qwen 2.5和GPT-4o等主流LLMs的攻击成功率。

---

## [Benchmarking Large Language Models on Homework Assessment in Circuit Analysis](https://arxiv.org/abs/2506.06390)

### Abstract
arXiv:2506.06390v1 Announce Type: cross 
Abstract: Large language models (LLMs) have the potential to revolutionize various fields, including code development, robotics, finance, and education, due to their extensive prior knowledge and rapid advancements. This paper investigates how LLMs can be leveraged in engineering education. Specifically, we benchmark the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama 3 70B, in assessing homework for an undergraduate-level circuit analysis course. We have developed a novel dataset consisting of official reference solutions and real student solutions to problems from various topics in circuit analysis. To overcome the limitations of image recognition in current state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX format. Using this dataset, a prompt template is designed to test five metrics of student solutions: completeness, method, final answer, arithmetic error, and units. The results show that GPT-4o and Llama 3 70B perform significantly better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B each having distinct advantages in different evaluation aspects. Additionally, we present insights into the limitations of current LLMs in several aspects of circuit analysis. Given the paramount importance of ensuring reliability in LLM-generated homework assessment to avoid misleading students, our results establish benchmarks and offer valuable insights for the development of a reliable, personalized tutor for circuit analysis -- a focus of our future work. Furthermore, the proposed evaluation methods can be generalized to a broader range of courses for engineering education in the future.

### 摘要
大型语言模型（LLMs）凭借其丰富的先验知识和快速发展的技术，有望在代码开发、机器人学、金融和教育等多个领域引发变革。本文探究如何将LLMs应用于工程教育领域，具体通过比较不同模型（包括GPT-3.5 Turbo、GPT-4o和Llama 3 70B）在本科生电路分析课程作业评估中的表现。我们构建了一个新型数据集，包含电路分析各专题的官方参考答案与真实学生解答。为克服当前最先进LLMs在图像识别方面的局限，数据集中的解答均转换为LaTeX格式。基于该数据集，我们设计了提示模板以测试学生解答的五个指标：完整性、方法、最终答案、算术错误和单位。结果表明，GPT-4o和Llama 3 70B在所有指标上均显著优于GPT-3.5 Turbo，且两者在不同评估维度上各具优势。此外，我们揭示了当前LLMs在电路分析多个方面的局限性。鉴于LLM生成作业评估的可靠性对避免误导学生至关重要，本研究不仅建立了性能基准，更为开发可靠的电路分析个性化辅导系统（我们未来工作的重点）提供了关键洞见。所提出的评估方法未来可推广至更广泛的工程教育课程领域。

---

## [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)

### Abstract
arXiv:2506.06404v1 Announce Type: cross 
Abstract: The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs that align with human values. However, aligning these models with individual values raises significant safety concerns, as certain values may correlate with harmful information. In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety risks, supported by psychological hypotheses. This study offers insights into the "black box" of value alignment and proposes in-context alignment methods to enhance the safety of value-aligned LLMs.

### 摘要
大型语言模型（LLMs）的应用范围持续扩大，使得符合人类价值观的个性化LLMs受到日益关注。然而，将这些模型与个体价值观对齐会引发重大安全隐患，因为某些价值观可能与有害信息相关联。本文识别了价值对齐LLMs的特定安全风险，并探究了这些挑战背后的心理学原理。研究发现揭示了两个关键结论：（1）相较于未经微调的模型，价值对齐LLMs更容易产生有害行为；在传统安全评估中，其风险也略高于其他经过微调的模型。（2）这些安全问题源于价值对齐LLMs会真实地按照所对齐的价值观生成文本，从而可能放大有害后果。通过使用具有详细安全分类的数据集，我们发现价值对齐与安全风险之间存在显著相关性，这一结论得到了心理学假设的支持。本研究为价值对齐的"黑箱"问题提供了新见解，并提出了情境对齐方法来增强价值对齐LLMs的安全性。

---

## [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)

### Abstract
arXiv:2506.06401v1 Announce Type: cross 
Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.

### 摘要
轻量化大型语言模型（LwLLMs）是通过参数缩减和优化设计、能在消费级硬件上高效运行的模型，具有资源效率高、成本效益好和数据隐私性强等显著优势。然而这类模型常因推理和认知能力受限，在复杂任务中表现欠佳，制约了其实用性。现有提示优化方法通常依赖大量人工干预或顶尖LLMs的元认知能力，对LwLLMs效果有限。为此，我们提出DeBoP——一种源自思维链（CoT）提示技术的直接行为优化范式。与CoT提示不同，DeBoP是自动优化方法，直接针对LwLLMs的行为进行优化：通过无梯度蒙特卡洛树搜索，将复杂提示优化转化为离散可量化执行序列的优化。我们在七项顶尖LLMs擅长而LwLLMs普遍表现不佳的挑战性任务上评估DeBoP。实验表明，DeBoP在多数任务上显著优于最新提示优化方法。经DeBoP优化的LwLLMs在多数任务超越GPT-3.5，同时较其他自动提示优化方法减少约60%计算时间。

---

## [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)

### Abstract
arXiv:2506.06391v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are widely used across sectors, yet their alignment with International Humanitarian Law (IHL) is not well understood. This study evaluates eight leading LLMs on their ability to refuse prompts that explicitly violate these legal frameworks, focusing also on helpfulness - how clearly and constructively refusals are communicated. While most models rejected unlawful requests, the clarity and consistency of their responses varied. By revealing the model's rationale and referencing relevant legal or safety principles, explanatory refusals clarify the system's boundaries, reduce ambiguity, and help prevent misuse. A standardised system-level safety prompt significantly improved the quality of the explanations expressed within refusals in most models, highlighting the effectiveness of lightweight interventions. However, more complex prompts involving technical language or requests for code revealed ongoing vulnerabilities. These findings contribute to the development of safer, more transparent AI systems and propose a benchmark to evaluate the compliance of LLM with IHL.

### 摘要
大型语言模型（LLMs）在各行业得到广泛应用，但其与国际人道法（IHL）的契合度尚未被充分认知。本研究评估了八种主流LLMs拒绝明显违反该法律框架提示的能力，并着重考察了拒绝行为的帮助性——即拒绝信息传达的清晰度和建设性。尽管多数模型能拒绝非法请求，但其回应的明确性与一致性存在差异。通过揭示模型推理逻辑并援引相关法律或安全原则，解释性拒绝能明确系统边界、降低歧义并防止滥用。标准化系统级安全提示显著提升了多数模型拒绝说明的质量，印证了轻量级干预的有效性。然而，涉及技术术语或代码请求的复杂提示仍暴露出持续存在的漏洞。这些发现为开发更安全、透明的AI系统提供了依据，并提出了评估LLMs是否符合IHL的基准框架。

---

## [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)

### Abstract
arXiv:2506.06406v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling large language models, with growing interest in extending them to multimodal tasks. Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models. To address this, we propose Soft ModalityAware Routing (SMAR), a novel regularization technique that uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data. Experiments on visual instruction tuning show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance. Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.

### 摘要
混合专家（MoE）架构已成为扩展大语言模型的关键方法，学界对其在多模态任务中的应用兴趣日益增长。现有构建多模态MoE模型的方法要么训练成本高昂，要么在适配预训练模型时导致语言能力下降。为此，我们提出软模态感知路由（SMAR），这是一种新颖的正则化技术，利用Kullback-Leibler散度控制跨模态的路由概率分布，在不修改模型架构或过度依赖文本数据的情况下促进专家专业化。视觉指令调优实验表明，SMAR仅需2.5%纯文本数据即可保持86.6%的语言能力保留率，在维持强大多模态性能的同时优于基线方法。该研究为平衡多模态MoE模型中的模态分化与语言能力提供了实用高效的解决方案。

---

## [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)

### Abstract
arXiv:2506.06446v1 Announce Type: cross 
Abstract: State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations--the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.

### 摘要
当前最先进的大型语言模型通过使用称为分词器的工具从原始文本中提取大量标记进行训练。关键在于，分词器不仅决定了模型在推理过程中使用的（标记）词汇表，也在原则上决定了（标记）语言。这是因为，虽然标记词汇表可能允许对字符串进行不同的标记化处理，但分词器始终将字符串映射到其中一种标记化形式——即规范标记化。然而，多项实证证据表明，大型语言模型并不总是生成规范标记序列，这会带来若干负面影响。本研究首先证明，要生成规范标记序列，模型需在其自回归生成过程的每个步骤中生成（部分）规范标记序列。基于这一理论结果，我们提出了规范采样方法——一种简单高效的采样技术，可防止给定模型生成非规范标记序列。此外，我们还证明，与标准采样相比，采用规范采样生成的标记序列分布可被严格证明更接近训练期间使用的真实标记序列分布。

---

## [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)

### Abstract
arXiv:2506.06396v1 Announce Type: cross 
Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of Battlefield Things (IoBT), gives rise to new opportunities for enhancing situational awareness. To increase the potential of IoBT for situational awareness in critical decision making, the data from these devices must be processed into consumer-ready information objects, and made available to consumers on demand. To address this challenge we propose a workflow that makes use of natural language processing (NLP) to query a database technology and return a response in natural language. Our solution utilizes Large Language Models (LLMs) that are sized for edge devices to perform NLP as well as graphical databases which are well suited for dynamic connected networks which are pervasive in the IoBT. Our architecture employs LLMs for both mapping questions in natural language to Cypher database queries as well as to summarize the database output back to the user in natural language. We evaluate several medium sized LLMs for both of these tasks on a database representing publicly available data from the US Army's Multipurpose Sensing Area (MSA) at the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion parameters) outperforms the other models across all the considered metrics. Most importantly, we note that, unlike current methods, our two step approach allows the relaxation of the Exact Match (EM) requirement of the produced Cypher queries with ground truth code and, in this way, it achieves a 19.4% increase in accuracy. Our workflow lays the ground work for deploying LLMs on edge devices to enable natural language interactions with databases containing information objects for critical decision making.

### 摘要
战场物联网（IoBT）的扩展为增强态势感知提供了新的机遇。为提升IoBT在关键决策中态势感知的潜力，必须将来自这些设备的数据处理成可供终端用户直接使用的信息对象，并按需提供给用户。针对这一挑战，我们提出一种利用自然语言处理（NLP）技术查询数据库并以自然语言返回响应的工作流程。该方案采用适合边缘设备部署的大语言模型（LLMs）执行NLP任务，并采用适用于IoBT中普遍存在的动态连接网络的图数据库。我们的架构通过LLMs实现双重功能：将自然语言问题映射为Cypher数据库查询语句，以及将数据库输出结果以自然语言形式汇总反馈给用户。我们在代表美国陆军拉斯克鲁塞斯 jornada 靶场多用途传感区（MSA）公开数据的数据库上，评估了多个中等规模LLMs在这两项任务中的表现。实验表明，Llama 3.1（80亿参数）在所有评估指标上均优于其他模型。最关键的是，我们发现与现有方法不同，这种两步走策略放宽了对生成Cypher查询语句与真实代码完全匹配（EM）的要求，从而实现了19.4%的准确率提升。该工作流程为在边缘设备部署LLMs奠定了基础，使得用户能够通过自然语言交互访问包含关键决策信息对象的数据库。

---

## [Benchmarking Misuse Mitigation Against Covert Adversaries](https://arxiv.org/abs/2506.06414)

### Abstract
arXiv:2506.06414v1 Announce Type: cross 
Abstract: Existing language model safety evaluations focus on overt attacks and low-stakes tasks. Realistic attackers can subvert current safeguards by requesting help on small, benign-seeming tasks across many independent queries. Because individual queries do not appear harmful, the attack is hard to &#123;detect&#125;. However, when combined, these fragments uplift misuse by helping the attacker complete hard and dangerous tasks. Toward identifying defenses against such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data generation pipeline that automates evaluations of covert attacks and corresponding defenses. Using this pipeline, we curate two new datasets that are consistently refused by frontier models and are too difficult for weaker open-weight models. Our evaluations indicate that decomposition attacks are effective misuse enablers, and highlight stateful defenses as a countermeasure.

### 摘要
现有语言模型安全评估主要关注显性攻击和低风险任务。现实攻击者可通过大量独立查询请求看似良性的小型任务协助，从而规避现有防护机制。由于单个查询看似无害，此类攻击难以被检测。然而当这些任务片段被组合使用时，能帮助攻击者完成高难度危险任务，从而助长滥用行为。为识别针对此类策略的防御方法，我们开发了状态化防御基准测试（BSD）——一个能自动评估隐蔽攻击及相应防御措施的数据生成流程。通过该流程，我们构建了两个新数据集，这些数据集持续被前沿模型拒绝，且对较弱开源模型而言难度过高。评估结果表明分解攻击是有效的滥用促成手段，同时凸显了状态化防御作为应对措施的重要性。

---

## [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)

### Abstract
arXiv:2506.06409v1 Announce Type: cross 
Abstract: Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks - such as coding - where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater

### 摘要
大语言模型（LLM）水印技术能够验证文本来源，遏制机器生成文本的滥用，并促进对人工智能系统的信任。现有水印通过改变LLM输出的下一标记预测实现，更新后的（即带水印的）预测依赖于随机辅助信息（例如通过哈希先前生成标记产生）。在低熵生成任务（如编程）中，由于下一标记预测近乎确定性，LLM水印面临特殊挑战。本文提出水印设计的优化框架，旨在探究如何最有效地利用随机辅助信息以最大化水印检测概率并最小化生成文本失真。基于此分析，我们设计了两种新型水印：HeavyWater与SimplexWater。二者均具备可调性，能在检测精度与文本失真间实现优雅权衡，且适用于任意LLM，与辅助信息生成方式无关。通过多项基准测试验证，这两种水印在低熵场景下能以极小的文本生成质量妥协实现高检测准确率。理论分析还揭示了LLM水印与编码理论间的新颖联系。代码实现详见https://github.com/DorTsur/HeavyWater_SimplexWater

---

## [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)

### Abstract
arXiv:2506.06485v1 Announce Type: cross 
Abstract: Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. We propose a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.

### 摘要
大型语言模型在执行任务时通常依赖于上下文输入和参数化知识。然而，这些知识源可能产生冲突，尤其是当检索到的文档与模型的参数化知识相矛盾时。我们提出一个诊断框架来系统评估LLM在上下文记忆冲突（即上下文信息与模型参数信念相背离）下的行为表现。通过构建诱发此类冲突的诊断数据，我们分析了模型在多种任务类型中的表现。研究发现：(1) 知识冲突对无需知识利用的任务影响甚微；(2) 当上下文知识与参数化知识一致时，模型表现持续更优；(3) 即使收到明确指令，模型仍无法完全抑制其内部知识；(4) 提供解释冲突的理性依据会增加模型对上下文的依赖。这些发现引发了关于模型评估有效性的担忧，并强调了在LLM部署中考虑知识冲突的必要性。

---

## [Private GPTs for LLM-driven testing in software development and machine learning](https://arxiv.org/abs/2506.06509)

### Abstract
arXiv:2506.06509v1 Announce Type: cross 
Abstract: In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results -where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios: a simple "Hello World" program and a digit classification model, showing that structured prompts lead to higher-quality test outputs.

### 摘要
在本研究中，我们探讨了私有GPT模型基于需求自动生成可执行测试代码的能力。具体而言，我们以现代开发流程中常见的史诗（epic）或用户故事（story）所含的验收标准作为输入，使产品负责人或商业智能部门能够通过大型语言模型直接生成可测试标准。我们从两个维度评估生成测试的质量：i）直接由语言模型根据需求生成代码；ii）通过Gherkin语法作为中间步骤的生成方式。结果表明，两步法能产生更优结果——此处"更优"定义为更高的人类可读性及更佳编码实践（如代码行数及测试常用库的使用情况）。具体而言，我们通过两个场景评估提示有效性：简单的"Hello World"程序和数字分类模型，实验证明结构化提示能生成更高质量的测试输出。

---

## [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)

### Abstract
arXiv:2506.06444v1 Announce Type: cross 
Abstract: Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .

### 摘要
现有安全保证研究主要集中于通过训练阶段对齐来向大语言模型(LLM)灌输安全行为。然而最新研究表明这些方法易受各类越狱攻击影响。与此同时，推理扩展技术显著提升了LLM的推理能力，但在安全保证领域的应用尚未得到探索。针对这一空白，本研究开创性地将推理扩展应用于抵御新兴威胁的LLM安全强化。我们发现传统推理扩展技术在安全场景下表现欠佳，甚至不及基础方法如最佳N采样法(N-best sampling)，这与其在推理任务中的成功形成鲜明对比。我们将此低效归因于新发现的"探索-效率困境"——该困境源于频繁调用过程奖励模型(PRM)带来的高计算开销。为解决此困境，我们提出SAFFRON这一专为安全保证设计的新型推理扩展范式。该范式的核心是引入多叉奖励模型(MRM)，可大幅减少所需奖励模型评估次数。为实现该范式，我们进一步提出：(1)MRM的部分监督训练目标；(2)防止分布外探索的保守探索约束；(3)基于字典树(Trie)的键值缓存策略，实现树搜索过程中跨序列的缓存共享。大量实验验证了我们方法的有效性。此外，我们公开了训练好的多叉奖励模型(Saffron-1)及配套的token级安全奖励数据集(Safety4M)，以加速LLM安全领域的未来研究。相关代码、模型和数据已发布于https://github.com/q-rz/saffron，项目主页见https://q-rz.github.io/p/saffron。

---

## [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)

### Abstract
arXiv:2506.06532v1 Announce Type: cross 
Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various real-world applications. However, the control and optimization of multi-UAV systems remain a significant challenge, particularly in dynamic and constrained environments. This work explores the joint motion and communication control of multiple UAVs operating within integrated terrestrial and non-terrestrial networks that include high-altitude platform stations (HAPS). Specifically, we consider an aerial highway scenario in which UAVs must accelerate, decelerate, and change lanes to avoid collisions and maintain overall traffic flow. Different from existing studies, we propose a novel hierarchical and collaborative method based on large language models (LLMs). In our approach, an LLM deployed on the HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning and control. This LLM-based framework leverages the rich knowledge embedded in pre-trained models to enable both high-level strategic planning and low-level tactical decisions. This knowledge-driven paradigm holds great potential for the development of next-generation 3D aerial highway systems. Experimental results demonstrate that our proposed collaborative LLM-based method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline approaches.

### 摘要
无人机（UAV）已在各类现实应用中得到广泛采用。然而，多无人机系统的控制与优化仍面临重大挑战，尤其在动态受限环境中。本研究探索了在包含高空平台站（HAPS）的天地一体化网络内运行的多无人机联合运动与通信控制问题。具体而言，我们考虑一种空中高速公路场景，其中无人机需通过加速、减速和变道来避免碰撞并维持整体交通流。与现有研究不同，我们提出了一种基于大语言模型（LLM）的新型分层协作方法：部署于HAPS的LLM负责无人机接入控制，而各无人机搭载的LLM则处理运动规划与控制。该框架利用预训练模型中蕴含的丰富知识，同时实现高层战略规划与底层战术决策。这种知识驱动范式为下一代三维空中高速公路系统的开发提供了巨大潜力。实验结果表明，与基线方法相比，我们提出的协作式LLM方法能获得更高的系统收益、更低的运行成本，并显著降低无人机碰撞率。

---

## [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)

### Abstract
arXiv:2506.06540v1 Announce Type: cross 
Abstract: After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.

### 摘要
在发生破坏性事件或冲击后（例如2025年联邦政府效率部（DOGE）裁员事件），专家判断会受到结果认知的影响。这使得重建研究事件关联因素所需的事前认知变得困难甚至不可能。本立场文件提出，当传统测量方法因冲击失效时，基于海量数字媒体数据训练的大语言模型（LLMs）可作为专家政治调查的可行替代方案。我们以DOGE裁员事件作为具体案例进行研究，通过LLMs的成对比较提示推导出联邦行政机构的意识形态评分。这些评分复现了裁员前的专家评估结果，并预测出哪些机构会成为DOGE的裁撤目标。采用相同方法还发现，即便控制意识形态变量，某些联邦机构作为知识机构的认知仍能预测其被DOGE裁撤的可能性。该案例研究表明，利用LLMs能快速便捷地检验冲击事件背后的假设关联因素。更广泛而言，本研究通过近期事件的案例分析，展示了当传统测量技术失效时，LLMs如何为冲击的关联因素提供研究洞见。最后我们提出两阶段标准，用于判断研究者何时可采用LLMs替代专家政治调查。

---

## [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)

### Abstract
arXiv:2506.06561v1 Announce Type: cross 
Abstract: Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.

### 摘要
图表标题对于帮助读者理解和记忆图表的核心信息至关重要。目前已有多种模型被开发用于自动生成这类标题，以协助作者更轻松地撰写出更高质量的标题。然而，作者几乎总是需要对通用AI生成的标题进行修改，以匹配其写作风格和特定领域的风格要求，这凸显了个性化定制的必要性。尽管语言模型个性化（LaMP）技术取得了进展，但这些技术通常仅针对纯文本场景，很少涉及输入和用户档案均为多模态的情况。本文介绍了LaMP-Cap数据集，这是一个支持多模态图表档案的个性化图表标题生成数据集。对于每个目标图表，LaMP-Cap不仅提供所需的输入（如图表图像），还提供来自同一文档的至多三个其他图表作为上下文特征档案——每个档案包含图表图像、标题及提及该图表的段落。通过四种大型语言模型的实验表明，利用档案信息能持续帮助生成更接近作者原创标题的文本。消融研究显示，档案中的图像比提及图表的段落更具帮助性，这凸显了多模态档案相较于纯文本档案的优势。

---

## [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)

### Abstract
arXiv:2506.06539v1 Announce Type: cross 
Abstract: When exposed to complex queries containing multiple conditions, today's large language models (LLMs) tend to produce responses that only partially satisfy the query while neglecting certain conditions. We therefore introduce the concept of Intent Hallucination. In this phenomenon, LLMs either omit (neglecting to address certain parts) or misinterpret (responding to invented query parts) elements of the given query, leading to intent hallucinated generation. To systematically evaluate intent hallucination, we introduce FAITHQA, a novel benchmark for intent hallucination that contains 20,068 problems, covering both query-only and retrieval-augmented generation (RAG) setups with varying topics and difficulty. FAITHQA is the first hallucination benchmark that goes beyond factual verification, tailored to identify the fundamental cause of intent hallucination. By evaluating various LLMs on FAITHQA, we find that (1) intent hallucination is a common issue even for state-of-the-art models, and (2) the phenomenon stems from omission or misinterpretation of LLMs. To facilitate future research, we introduce an automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting intent hallucination. Human evaluation results demonstrate that CONSTRAINT SCORE is closer to human performance for intent hallucination compared to baselines.

### 摘要
当面对包含多重条件的复杂查询时，当前的大语言模型（LLMs）往往仅能部分满足查询要求，而忽略某些条件。为此，我们提出"意图幻觉"概念。该现象表现为LLMs对给定查询要素的遗漏（未处理特定部分）或误读（对虚构查询内容作出响应），从而导致意图幻觉式生成。为系统评估意图幻觉，我们推出FAITHQA基准测试，该创新基准包含20,068个问题，涵盖纯查询与检索增强生成（RAG）两种设置，涉及不同主题与难度等级。FAITHQA是首个超越事实核查的幻觉基准，专门用于识别意图幻觉的根本成因。通过对各LLMs在FAITHQA上的评估，我们发现：（1）意图幻觉对最先进模型仍是普遍问题；（2）该现象源于LLMs的遗漏或误读行为。为推进未来研究，我们提出自动评估指标"约束分数"（CONSTRAINT SCORE）用于检测意图幻觉。人工评估结果表明，相较于基线方法，该指标在意图幻觉检测方面更接近人类表现水平。

---

## [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)

### Abstract
arXiv:2506.06499v1 Announce Type: cross 
Abstract: Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24\%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.

### 摘要
基于大语言模型（LLM）的合成数据生成已成为提升模型推理能力的有效方法。然而，现有方法多将先进大模型蒸馏为小型学生模型，或依赖自然真实问题陈述以保证问题质量，这限制了方法在更复杂多样问题领域的扩展性。为此，我们提出SPARQ：基于质量-多样性算法的推理问题合成生成方法，通过测量问题解决率（问题难度的代理指标），仅使用单一模型即可生成高质量、多样化的数学问题与解答对。从7.5K种子数据集出发，我们生成了超过2000万新问题-解答对。研究表明，按难度筛选生成数据后，用所得数据微调原模型可使性能相对提升高达24%。我们还通过消融实验探究了合成数据数量、质量及多样性对模型泛化能力的影响：以问题难度衡量的更高质量数据能显著提升分布内性能；而多样性数据虽对分布内性能提升有限，但筛选多样性更高的数据可增强分布外泛化鲁棒性。实验同时验证了合成问题生成中存在模型与数据规模法则，这些法则对下游模型泛化具有积极影响。

---

## [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)

### Abstract
arXiv:2506.06605v1 Announce Type: cross 
Abstract: Existing LLM-based medical question-answering systems lack citation generation and evaluation capabilities, raising concerns about their adoption in practice. In this work, we introduce \name, the first end-to-end framework that facilitates the design and evaluation of citation generation with LLMs for medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation method that generates high-quality citations. Our evaluation highlights the challenges and opportunities of citation generation for medical tasks, while identifying important design choices that have a significant impact on the final citation quality. Our proposed method achieves superior citation precision and recall improvements compared to strong baseline methods, and we show that evaluation results correlate well with annotation results from professional experts.

### 摘要
现有基于大语言模型（LLM）的医疗问答系统缺乏引文生成与评估能力，这引发了对其实际应用可行性的担忧。本研究提出首个端到端框架\name，用于支持医疗任务中基于LLM的引文生成设计与评估。同时，我们创新性地提出一种多轮检索-引文生成方法，能够产出高质量引文。评估结果揭示了医疗任务引文生成面临的挑战与机遇，并识别出对最终引文质量具有显著影响的关键设计选择。与强基线方法相比，本研究所提方法在引文精确率与召回率上均实现显著提升，且评估结果与专业医师的标注结果呈现良好相关性。

---

## [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)

### Abstract
arXiv:2506.06522v1 Announce Type: cross 
Abstract: Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.

### 摘要
近期关于大语言模型（LLM）的研究日益集中于后训练阶段，并通过精选数据集进行对齐以提升指令遵循、世界知识和专业技能。然而，主流开源与闭源LLM所使用的后训练数据集大多未向公众开放，其构建过程亦鲜有披露。这种透明度的缺失促使了开源后训练语料库的近期发展。尽管基于这些开放替代方案训练能达到与领先模型相当的性能，但由于大规模系统比较所需的高昂计算成本，严谨的对比研究仍面临挑战，因此相关分析基本空缺。这导致在评估数据质量时，具体样本、任务类型或筛选策略如何影响下游性能仍不明确。本研究首次对两个著名开源后训练数据集（Tulu-3-SFT-Mix与SmolTalk）进行了全面并行分析。通过Magpie框架，我们为每个样本标注了细粒度质量指标（包括对话结构[单轮vs多轮]、任务类别、输入质量及响应质量），并统计揭示了两数据集在结构与质量上的异同。基于这些发现，我们设计了一种原则性筛选方案，构建出新数据混合集TuluTalk——其样本量比源数据集减少14%，但在关键基准测试中达到或超越源数据性能。本研究为在有限资源下构建更高效后训练数据集以提升模型性能提供了可操作的见解。为支持后续研究，我们公开了标注后的源数据集及精选的TuluTalk混合集。

---

## [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)

### Abstract
arXiv:2506.06579v1 Announce Type: cross 
Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field of natural language processing (NLP), excelling at tasks like text generation, summarization, and question answering. However, their inference remains computationally expensive and energy intensive, especially in settings with limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in mobile, edge, or cost sensitive environments. To address these challenges, recent approaches have introduced multi LLM intelligent model selection strategies that dynamically allocate computational resources based on query complexity -- using lightweight models for simpler queries and escalating to larger models only when necessary. This survey explores two complementary strategies for efficient LLM inference: (i) routing, which selects the most suitable model based on the query, and (ii) cascading or hierarchical inference (HI), which escalates queries through a sequence of models until a confident response is found. Both approaches aim to reduce computation by using lightweight models for simpler tasks while offloading only when needed. We provide a comparative analysis of these techniques across key performance metrics, discuss benchmarking efforts, and outline open challenges. Finally, we outline future research directions to enable faster response times, adaptive model selection based on task complexity, and scalable deployment across heterogeneous environments, making LLM based systems more efficient and accessible for real world applications.

### 摘要
语言模型（LMs）的最新进展显著推动了自然语言处理（NLP）领域的发展，在文本生成、摘要和问答等任务中表现出色。然而，其推理过程仍然计算成本高昂且能耗密集，尤其在硬件、功耗或带宽受限的环境中。这使得语言模型难以部署在移动、边缘或成本敏感的场景中。为应对这些挑战，近期研究提出了多LLM智能模型选择策略，根据查询复杂度动态分配计算资源——对简单查询使用轻量级模型，仅在必要时调用更大模型。本综述探讨了两种高效LLM推理的互补策略：（i）路由策略，基于查询选择最合适的模型；（ii）级联或分层推理（HI），通过模型序列逐步处理查询直至获得可靠响应。两种方法均旨在通过轻量级模型处理简单任务，仅在必要时调用复杂模型来减少计算量。我们从关键性能指标对这些技术进行了比较分析，讨论了基准测试工作，并概述了开放挑战。最后，我们提出了未来研究方向，以实现更快响应时间、基于任务复杂度的自适应模型选择，以及在异构环境中的可扩展部署，从而使基于LLM的系统在实际应用中更高效、更易用。

---

## [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)

### Abstract
arXiv:2506.06632v1 Announce Type: cross 
Abstract: We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method.

### 摘要
我们的目标是通过强化学习（RL）提升语言模型的推理能力。近期经过RL后训练的模型（如DeepSeek-R1）已在数学和编程任务中展现出推理能力。然而，先前研究表明，单独使用RL改进固有困难任务的推理效果有限。受课程学习启发，我们提出从易到难（E2H）的任务调度方法，使大语言模型能够逐步构建推理能力，该方法称为E2H推理器。实证研究发现：虽然简单任务在初期至关重要，但通过合理调度逐步淡化这些任务对防止过拟合具有关键作用。理论上，我们在近似策略迭代框架内为E2H推理器建立了收敛性保证，推导出有限样本复杂度边界，并证明当任务被恰当分解和条件化时，分阶段课程学习所需总样本量少于直接学习。跨领域实验表明，E2H推理器显著提升了小规模语言模型（15亿至30亿参数）的推理能力——这些模型在单独使用传统RL训练时表现欠佳，从而凸显了本方法的有效性。

---

## [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)

### Abstract
arXiv:2506.06657v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown promise in structured prediction tasks, including regression, but existing approaches primarily focus on point estimates and lack systematic comparison across different methods. We investigate probabilistic regression using LLMs for unstructured inputs, addressing challenging text-to-distribution prediction tasks such as price estimation where both nuanced text understanding and uncertainty quantification are critical. We propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. Through extensive experiments across three diverse price prediction datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration. Our systematic comparison of LLM approaches, model architectures, training approaches, and data scaling reveals that Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods. Our experiments also reveal the effectiveness of LLM-assisted label correction in achieving human-level accuracy without systematic bias. Our curated datasets are made available at https://github.com/vnik18/llm-price-quantile-reg/ to support future research.

### 摘要
大语言模型（LLMs）在结构化预测任务（包括回归）中展现出潜力，但现有方法主要关注点估计，且缺乏不同方法的系统比较。我们研究了利用LLMs对非结构化输入进行概率回归的方法，解决了具有挑战性的文本到分布预测任务（如价格估计），其中细致的文本理解和不确定性量化至关重要。我们提出了一种新颖的分位数回归方法，使LLMs能够生成完整的预测分布，从而改进传统的点估计。通过在三个不同的价格预测数据集上进行大量实验，我们发现采用分位数头微调的Mistral-7B模型在预测准确性和分布校准方面（各使用三个既定指标衡量）显著优于传统方法。我们对LLM方法、模型架构、训练方法和数据扩展的系统比较表明，Mistral-7B始终优于编码器架构、基于嵌入的方法和少样本学习方法。实验还揭示了LLM辅助标签校正在实现人类水平准确性且无系统偏差方面的有效性。我们整理的数据集已发布于https://github.com/vnik18/llm-price-quantile-reg/，以支持未来研究。

---

## [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)

### Abstract
arXiv:2506.06699v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context learning (ICL). However, the effectiveness of ICL is often sensitive to the selection and ordering of demonstration examples. To address this, we present MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that selects hard demonstration examples for the ICL prompt, adapting to each test instance. Our approach achieves 2-7% absolute improvement in F1-score across classification tasks, compared to a random selection of examples. We also provide theoretical insights and empirical evidence showing that MarginSel induces max-margin behavior in LLMs by effectively increasing the margin for hard examples, analogous to support vectors, thereby shifting the decision boundary in a beneficial direction.

### 摘要
大语言模型（LLMs）通过上下文学习（ICL）在少样本学习中表现优异。然而，ICL的效果往往对演示示例的选择和排序敏感。为此，我们提出MarginSel：面向LLMs的最大间隔演示选择方法，这是一种两步法，能够为ICL提示选择针对每个测试实例的困难演示示例。与随机选择示例相比，我们的方法在分类任务中实现了F1分数2-7%的绝对提升。我们还提供了理论分析和实证证据，表明MarginSel通过有效增加困难示例（类似于支持向量）的间隔，诱导LLMs产生最大间隔行为，从而将决策边界向有益方向调整。

---

## [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)

### Abstract
arXiv:2506.06607v1 Announce Type: cross 
Abstract: We present a training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as a sparse linear combination of shared tokens, in two phases: first, compute each new token's representation in the donor embedding space with a small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of the base model's performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment.

### 摘要
我们提出一种无需训练的预训练大语言模型(LLM)分词器移植方法，通过正交匹配追踪(OMP)算法重构未见过的词元嵌入。具体而言，我们将词汇表外词元近似为共享词元的稀疏线性组合，该过程分为两个阶段：首先利用少量共享锚词元构成的词典，计算新词元在供体嵌入空间中的表示；然后将这些相同的稀疏系数转移回基础模型的嵌入空间。在两个具有挑战性的跨分词器任务(Llama→Mistral NeMo 12B和Qwen→Llama 1B)上，OMP在多个基准测试中实现了最佳的基础模型性能零样本保持，而其他零样本方法性能显著下降。与基线方法(零初始化、均值初始化及WECHSEL、FOCUS、ZETT等现有方法)相比，OMP始终获得最佳综合性能，无需梯度更新即可有效弥合大型分词器间的差异。我们的分析进一步指出数值分词方案不匹配是保持数学推理能力的关键挑战。该技术可直接复用预训练模型权重与新分词器，促进跨分词器知识蒸馏、推测解码、集成、合并及领域专用词汇适配。我们已将本方法集成至开源工具mergekit-tokensurgeon中，用于事后词汇重新对齐。

---

## [RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks](https://arxiv.org/abs/2506.06683)

### Abstract
arXiv:2506.06683v1 Announce Type: cross 
Abstract: Dual-arm robots play a crucial role in improving efficiency and flexibility in complex multitasking scenarios. While existing methods have achieved promising results in task planning, they often fail to fully optimize task parallelism, limiting the potential of dual-arm collaboration. To address this issue, we propose RoboPARA, a novel large language model (LLM)-driven framework for dual-arm task parallelism planning. RoboPARA employs a two-stage process: (1) Dependency Graph-based Planning Candidates Generation, which constructs directed acyclic graphs (DAGs) to model task dependencies and eliminate redundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which optimizes DAG traversal to maximize parallelism while maintaining task coherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task dataset (X-DAPT dataset), the first dataset specifically designed to evaluate dual-arm task parallelism across diverse scenarios and difficulty levels. Extensive experiments on the X-DAPT dataset demonstrate that RoboPARA significantly outperforms existing methods, achieving higher efficiency and reliability, particularly in complex task combinations. The code and dataset will be released upon acceptance.

### 摘要
双机械臂机器人在提升复杂多任务场景下的效率和灵活性方面发挥着关键作用。现有方法虽然在任务规划方面取得了显著成果，但往往未能充分优化任务并行性，限制了双臂协作的潜力。针对这一问题，我们提出RoboPARA——一种新型大语言模型驱动的双机械臂任务并行规划框架。该框架采用两阶段处理流程：(1)基于依赖图的规划候选生成，通过构建有向无环图(DAG)建模任务依赖关系并消除冗余；(2)基于图重遍历的双臂并行规划，通过优化DAG遍历在保持任务连贯性的同时最大化并行度。此外，我们首次提出跨场景双臂并行任务数据集(X-DAPT数据集)，这是专为评估不同场景和难度等级下双臂任务并行性而设计的基准数据集。在X-DAPT数据集上的大量实验表明，RoboPARA显著优于现有方法，尤其在复杂任务组合中实现了更高效率和可靠性。代码与数据集将在论文录用后公开。

---

## [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)

### Abstract
arXiv:2506.06705v1 Announce Type: cross 
Abstract: Detecting LLM-generated text in specialized and high-stakes domains like medicine and law is crucial for combating misinformation and ensuring authenticity. However, current zero-shot detectors, while effective on general text, often fail when applied to specialized content due to domain shift. We provide a theoretical analysis showing this failure is fundamentally linked to the KL divergence between human, detector, and source text distributions. To address this, we propose DivScore, a zero-shot detection framework using normalized entropy-based scoring and domain knowledge distillation to robustly identify LLM-generated text in specialized domains. We also release a domain-specific benchmark for LLM-generated text detection in the medical and legal domains. Experiments on our benchmark show that DivScore consistently outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0% higher recall (0.1% false positive rate threshold). In adversarial settings, DivScore demonstrates superior robustness than other baselines, achieving on average 22.8% advantage in AUROC and 29.5% in recall. Code and data are publicly available.

### 摘要
在医学和法律等专业高风险领域检测大语言模型（LLM）生成的文本对于打击错误信息和确保真实性至关重要。然而，当前的零样本检测器虽然在通用文本上表现良好，但由于领域偏移，在应用于专业内容时往往失效。我们通过理论分析表明，这种失效本质上与人类文本、检测器文本及源文本分布之间的KL散度有关。为解决这一问题，我们提出了DivScore，一种基于归一化熵评分和领域知识蒸馏的零样本检测框架，用于在专业领域鲁棒地识别LLM生成的文本。我们还发布了针对医学和法律领域的LLM生成文本检测的领域特定基准测试。实验结果表明，DivScore在基准测试中始终优于现有最先进的检测器，AUROC提高了14.4%，召回率（0.1%假阳性率阈值）提高了64.0%。在对抗性设置下，DivScore表现出比其他基线更强的鲁棒性，平均在AUROC上具有22.8%的优势，在召回率上具有29.5%的优势。代码和数据已公开。

---

## [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)

### Abstract
arXiv:2506.06808v1 Announce Type: cross 
Abstract: Can language models reliably predict that possible events are more likely than merely improbable ones? By teasing apart possibility, typicality, and contextual relatedness, we show that despite the results of previous work, language models' ability to do this is far from robust. In fact, under certain conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo - perform at worse-than-chance level, assigning higher probabilities to impossible sentences such as 'the car was given a parking ticket by the brake' than to merely unlikely sentences such as 'the car was given a parking ticket by the explorer'.

### 摘要
语言模型能否可靠地预测可能事件比仅具低概率事件更易发生？通过区分可能性、典型性和上下文关联性，我们发现尽管已有研究得出肯定结论，但语言模型在此方面的能力远非稳健。事实上，在特定条件下，所有测试模型（包括Llama 3、Gemma 2和Mistral NeMo）表现均低于随机水平——它们为不可能句（如"刹车给汽车开了张罚单"）分配的概率值，反而高于仅具低概率句（如"探险家给汽车开了张罚单"）。

---

## [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)

### Abstract
arXiv:2506.06737v1 Announce Type: cross 
Abstract: Navigating healthcare systems can be complex and overwhelming, creating barriers for patients seeking timely and appropriate medical attention. In this paper, we introduce C-PATH (Conversational Patient Assistance and Triage in Healthcare), a novel conversational AI system powered by large language models (LLMs) designed to assist patients in recognizing symptoms and recommending appropriate medical departments through natural, multi-turn dialogues. C-PATH is fine-tuned on medical knowledge, dialogue data, and clinical summaries using a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of this work is a GPT-based data augmentation framework that transforms structured clinical knowledge from DDXPlus into lay-person-friendly conversations, allowing alignment with patient communication norms. We also implement a scalable conversation history management strategy to ensure long-range coherence. Evaluation with GPTScore demonstrates strong performance across dimensions such as clarity, informativeness, and recommendation accuracy. Quantitative benchmarks show that C-PATH achieves superior performance in GPT-rewritten conversational datasets, significantly outperforming domain-specific baselines. C-PATH represents a step forward in the development of user-centric, accessible, and accurate AI tools for digital health assistance and triage.

### 摘要
在医疗系统中进行导航可能复杂且令人不知所措，这为患者寻求及时适当的医疗服务设置了障碍。本文介绍C-PATH（医疗领域对话式患者辅助与分诊系统），这是一种基于大语言模型的新型对话式人工智能系统，旨在通过自然的多轮对话帮助患者识别症状并推荐合适的医疗科室。该系统基于LLaMA3架构构建多阶段训练流程，对医学知识、对话数据和临床摘要进行微调。本研究的核心贡献是开发了一个基于GPT的数据增强框架，该框架将DDXPlus中的结构化临床知识转化为通俗易懂的对话内容，使其符合患者沟通规范。我们还实现了可扩展的对话历史管理策略以确保长程连贯性。GPTScore评估表明，该系统在清晰度、信息量和推荐准确性等维度表现优异。定量基准测试显示，C-PATH在GPT重写的对话数据集上实现了卓越性能，显著优于特定领域基线模型。C-PATH代表了以用户为中心、易获取且精准的数字健康辅助与分诊人工智能工具发展的重要进展。

---

## [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)

### Abstract
arXiv:2506.06842v1 Announce Type: cross 
Abstract: Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models' knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection.

### 摘要
虚假信息检测是媒体素养的核心环节。心理学研究表明，掌握说服谬误知识有助于个体识别虚假信息。受此启发，我们通过大型语言模型（LLMs）实验验证了注入说服知识是否能增强虚假信息检测能力。据此，我们提出"说服增强思维链"（PCoT）这一创新方法，利用说服机制改进零样本分类中的虚假信息检测。我们在在线新闻和社交媒体帖子上对PCoT进行了全面评估，并发布两个新颖的当代虚假信息数据集：EUDisinfo和MultiDis。这些数据集包含模型知识截断后发布的全新内容，可评估PCoT在LLMs完全未见数据上的表现。实验表明，在五种LLMs和五个数据集上，PCoT平均性能超越竞争方法15%。这些发现凸显了说服知识在强化零样本虚假信息检测中的价值。

---

## [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)

### Abstract
arXiv:2506.06821v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through test case generation remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning.

### 摘要
大语言模型（LLMs）在代码生成方面展现出卓越能力，能够处理推理过程中的复杂任务。然而，LLMs通过测试用例生成实现代码检查或调试的潜力尚未得到充分探索。本研究从竞赛级编程（CP）程序的角度出发，提出TCGBench——一个用于（LLM生成）测试用例生成器的基准。该基准包含两项任务，旨在研究LLMs在以下方面的能力：（1）为给定CP问题生成有效测试用例生成器；（2）进一步生成能暴露人工编写代码缺陷的针对性测试用例生成器。实验结果表明，尽管最先进的LLMs在多数情况下能生成有效测试用例生成器，但大多数模型难以有效生成揭示人工代码缺陷的针对性测试用例。值得注意的是，即使是高级推理模型（如o3-mini）在生成针对性生成器任务中的表现也显著低于人类水平。此外，我们构建了一个高质量、人工标注的针对性生成器指令数据集。分析表明，借助该数据集通过提示或微调方式，可有效提升LLMs的性能。

---

## [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)

### Abstract
arXiv:2506.06862v1 Announce Type: cross 
Abstract: Grounding language to a navigating agent's observations can leverage pretrained multimodal foundation models to match perceptions to object or event descriptions. However, previous approaches remain disconnected from environment mapping, lack the spatial precision of geometric maps, or neglect additional modality information beyond vision. To address this, we propose multimodal spatial language maps as a spatial map representation that fuses pretrained multimodal features with a 3D reconstruction of the environment. We build these maps autonomously using standard exploration. We present two instances of our maps, which are visual-language maps (VLMaps) and their extension to audio-visual-language maps (AVLMaps) obtained by adding audio information. When combined with large language models (LLMs), VLMaps can (i) translate natural language commands into open-vocabulary spatial goals (e.g., "in between the sofa and TV") directly localized in the map, and (ii) be shared across different robot embodiments to generate tailored obstacle maps on demand. Building upon the capabilities above, AVLMaps extend VLMaps by introducing a unified 3D spatial representation integrating audio, visual, and language cues through the fusion of features from pretrained multimodal foundation models. This enables robots to ground multimodal goal queries (e.g., text, images, or audio snippets) to spatial locations for navigation. Additionally, the incorporation of diverse sensory inputs significantly enhances goal disambiguation in ambiguous environments. Experiments in simulation and real-world settings demonstrate that our multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation and improve recall by 50% in ambiguous scenarios. These capabilities extend to mobile robots and tabletop manipulators, supporting navigation and interaction guided by visual, audio, and spatial cues.

### 摘要
将语言与导航智能体的观测相锚定，可利用预训练多模态基础模型实现感知与物体或事件描述的匹配。然而现有方法仍与环境建图脱节，缺乏几何地图的空间精确性，或忽视了视觉以外的多模态信息。为此，我们提出多模态空间语言地图作为新型空间表征，将预训练多模态特征与环境三维重建相融合。该地图通过标准探索自主构建，具体呈现为视觉-语言地图（VLMaps）及其扩展版本——通过添加音频信息获得的视听-语言地图（AVLMaps）。当与大型语言模型（LLMs）结合时，VLMaps能够：（1）将自然语言指令转化为可直接在地图中定位的开放词汇空间目标（如"沙发与电视之间"）；（2）跨不同机器人平台共享，按需生成定制化障碍物地图。基于上述功能，AVLMaps通过融合预训练多模态基础模型特征，构建了整合音频、视觉与语言线索的统一三维空间表征。这使得机器人能将多模态目标查询（如文本、图像或音频片段）锚定至导航空间位置。此外，多样化感官输入的引入显著提升了模糊环境中的目标消歧能力。仿真与真实环境实验表明，我们的多模态空间语言地图可实现零样本空间与多模态目标导航，并在模糊场景中将召回率提升50%。该能力可扩展至移动机器人与桌面机械臂，支持基于视觉、音频及空间线索的导航与交互。

---

## [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)

### Abstract
arXiv:2506.06930v1 Announce Type: cross 
Abstract: Recent advances in text summarization have predominantly leveraged large language models to generate concise summaries. However, language models often do not maintain long-term discourse structure, especially in news articles, where organizational flow significantly influences reader engagement. We introduce a novel approach to integrating discourse structure into summarization processes, focusing specifically on news articles across various media. We present a novel summarization dataset where news articles are summarized multiple times in different ways across different social media platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse schema to describe summarization structures and a novel algorithm, DiscoSum, which employs beam search technique for structure-aware summarization, enabling the transformation of news stories to meet different stylistic and structural demands. Both human and automatic evaluation results demonstrate the efficacy of our approach in maintaining narrative fidelity and meeting structural requirements.

### 摘要
文本摘要领域的最新进展主要利用大语言模型生成简洁摘要。然而，语言模型往往难以保持长期话语结构，尤其在新闻文章中，组织流程对读者参与度具有重要影响。我们提出了一种将话语结构整合到摘要生成过程的新方法，特别关注跨媒体新闻文章。我们构建了一个新颖的摘要数据集，其中新闻文章在不同社交媒体平台（如领英、脸书等）上以多种方式被重复摘要。我们开发了新的新闻话语模式来描述摘要结构，并提出创新算法DiscoSum——该算法采用束搜索技术实现结构感知的摘要生成，能够根据不同文体和结构需求转换新闻报道。人工评估与自动评估结果均表明，我们的方法在保持叙事忠实度和满足结构要求方面具有显著成效。

---

## [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)

### Abstract
arXiv:2506.06955v1 Announce Type: cross 
Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.

### 摘要
我们推出BIS Reasoning 1.0——首个专为评估大语言模型（LLMs）中信念不一致推理能力而设计的大规模日语三段论推理数据集。与NeuBAROCO和JFLD等关注通用或信念一致推理的现有数据集不同，BIS Reasoning 1.0通过引入逻辑有效但信念不一致的三段论，揭示了基于人类对齐语料训练的LLMs存在的推理偏差。我们对包括GPT系列、Claude系列及主流日语LLMs在内的前沿模型进行基准测试，发现性能存在显著差异（GPT-4o准确率达79.54%）。分析表明当前LLMs在处理逻辑有效但信念冲突的输入时存在关键缺陷。这些发现对在法律、医疗和科学文献等高风险领域部署LLMs具有重要启示——当事实与直觉信念冲突时，必须确保模型优先遵循真理以维护系统完整性与安全性。

---

## [Position: Simulating Society Requires Simulating Thought](https://arxiv.org/abs/2506.06958)

### Abstract
arXiv:2506.06958v1 Announce Type: cross 
Abstract: Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior -- it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior -- primarily through prompting and supervised fine-tuning. Yet they often lack internal coherence, causal reasoning, and belief traceability -- making them unreliable for analyzing how people reason, deliberate, or respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought -- not just language -- for social simulations.

### 摘要
我们认为，用大语言模型（LLMs）模拟社会不仅需要生成看似合理的行为，更要求具备结构化、可修正且可追溯的认知基础推理能力。当前基于LLM的智能体主要通过提示和监督微调来模拟个体和群体行为，但它们往往缺乏内在一致性、因果推理和信念可追溯性——这使得它们在分析人类如何推理、商议或应对干预时不可靠。

为解决这一问题，我们提出了一种概念建模范式"生成式心智"（GenMinds），该范式借鉴认知科学原理，为生成式智能体提供结构化信念表征支持。为评估此类智能体，我们开发了RECAP（重构因果路径）框架，这一基准通过因果可追溯性、人口统计基础和干预一致性来评估推理保真度。这些贡献推动了一个更广泛的转向：从表层模仿转向能模拟思维（而不仅是语言）的生成式智能体，以实现社会模拟。

---

## [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)

### Abstract
arXiv:2506.06975v1 Announce Type: cross 
Abstract: As API access becomes a primary interface to large language models (LLMs), users often interact with black-box systems that offer little transparency into the deployed model. To reduce costs or maliciously alter model behaviors, API providers may discreetly serve quantized or fine-tuned variants, which can degrade performance and compromise safety. Detecting such substitutions is difficult, as users lack access to model weights and, in most cases, even output logits. To tackle this problem, we propose a rank-based uniformity test that can verify the behavioral equality of a black-box LLM to a locally deployed authentic model. Our method is accurate, query-efficient, and avoids detectable query patterns, making it robust to adversarial providers that reroute or mix responses upon the detection of testing attempts. We evaluate the approach across diverse threat scenarios, including quantization, harmful fine-tuning, jailbreak prompts, and full model substitution, showing that it consistently achieves superior statistical power over prior methods under constrained query budgets.

### 摘要
随着API访问成为大型语言模型（LLM）的主要交互方式，用户往往面对的是缺乏透明度的黑盒系统。为降低成本或恶意篡改模型行为，API提供商可能暗中部署量化或微调变体，这些操作可能导致性能下降并危及安全性。由于用户无法获取模型权重且在多数情况下甚至无法获得输出对数概率，检测此类替换行为具有挑战性。针对该问题，我们提出一种基于秩的均匀性检验方法，可验证黑盒LLM与本地部署的原始模型在行为上的等效性。该方法具有高准确性、查询高效性，且能避免可检测的查询模式，使其对可能通过重定向或混合响应来对抗检测尝试的恶意提供商具有鲁棒性。我们在量化、有害微调、越狱提示及完整模型替换等多种威胁场景下评估该方法，结果表明在有限查询预算条件下，其统计功效始终优于现有方法。

---

## [LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models](https://arxiv.org/abs/2506.06874)

### Abstract
arXiv:2506.06874v1 Announce Type: cross 
Abstract: There is growing interest in understanding how people interact with large language models (LLMs) and whether such models elicit dependency or even addictive behaviour. Validated tools to assess the extent to which individuals may become dependent on LLMs are scarce and primarily build on classic behavioral addiction symptoms, adapted to the context of LLM use. We view this as a conceptual limitation, as the LLM-human relationship is more nuanced and warrants a fresh and distinct perspective. To address this gap, we developed and validated a new 12-item questionnaire to measure LLM dependency, referred to as LLM-D12. The scale was based on the authors' prior theoretical work, with items developed accordingly and responses collected from 526 participants in the UK. Exploratory and confirmatory factor analyses, performed on separate halves of the total sample using a split-sample approach, supported a two-factor structure: Instrumental Dependency (six items) and Relationship Dependency (six items). Instrumental Dependency reflects the extent to which individuals rely on LLMs to support or collaborate in decision-making and cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs as socially meaningful, sentient, or companion-like entities. The two-factor structure demonstrated excellent internal consistency and clear discriminant validity. External validation confirmed both the conceptual foundation and the distinction between the two subscales. The psychometric properties and structure of our LLM-D12 scale were interpreted in light of the emerging view that dependency on LLMs does not necessarily indicate dysfunction but may still reflect reliance levels that could become problematic in certain contexts.

### 摘要
人们对于理解人类如何与大型语言模型（LLM）互动以及此类模型是否会引发依赖甚至成瘾行为的兴趣日益增长。目前用于评估个体对LLM依赖程度的有效工具稀缺，且主要基于经典行为成瘾症状，并适应于LLM使用情境。我们认为这存在概念局限性，因为LLM与人类的关系更为复杂，需要一种全新且独特的视角。为填补这一空白，我们开发并验证了一份包含12个条目的新问卷（称为LLM-D12）来测量LLM依赖。该量表基于作者先前的理论工作，相应开发条目，并收集了英国526名参与者的反馈。采用分样本方法对总样本进行探索性和验证性因子分析，支持了两因子结构：工具性依赖（6个条目）和关系性依赖（6个条目）。工具性依赖反映个体在决策和认知任务中依赖LLM支持或协作的程度；关系性依赖则捕捉将LLM视为具有社会意义、感知力或类似伴侣实体的倾向。两因子结构表现出优异的内部一致性和清晰的区分效度。外部验证证实了概念基础及两个子量表的区分性。我们根据新兴观点解释了LLM-D12量表的心理测量特性与结构，即对LLM的依赖未必意味着功能障碍，但仍可能反映在特定情境下可能引发问题的依赖水平。

---

## [AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization](https://arxiv.org/abs/2506.07035)

### Abstract
arXiv:2506.07035v1 Announce Type: cross 
Abstract: Deciphering protein function remains a fundamental challenge in protein representation learning. The task presents significant difficulties for protein language models (PLMs) due to the sheer volume of functional annotation categories and the highly imbalanced distribution of annotated instances across biological ontologies. Inspired by the remarkable success of reinforcement learning from human feedback (RLHF) in large language model (LLM) alignment, we propose AnnoDPO, a novel multi-modal framework for protein function prediction that leverages Direct Preference Optimization (DPO) to enhance annotation learning. Our methodology addresses the dual challenges of annotation scarcity and category imbalance through preference-aligned training objectives, establishing a new paradigm for biological knowledge integration in protein representation learning.

### 摘要
破译蛋白质功能仍是蛋白质表征学习领域的核心难题。由于功能注释类别数量庞大且生物本体学中注释实例分布极不均衡，该任务对蛋白质语言模型（PLMs）提出了重大挑战。受大规模语言模型（LLM）对齐中人类反馈强化学习（RLHF）显著成功的启发，我们提出AnnoDPO——一个创新的多模态蛋白质功能预测框架，该框架利用直接偏好优化（DPO）来增强注释学习。我们的方法通过偏好对齐的训练目标，解决了注释稀缺和类别失衡的双重挑战，为蛋白质表征学习中的生物知识整合建立了新范式。

---

## [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)

### Abstract
arXiv:2506.07031v1 Announce Type: cross 
Abstract: Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and reasoning tasks, showcasing exceptional capabilities. However, the enhancement of reasoning abilities and the exposure of their internal reasoning processes introduce new safety vulnerabilities. One intriguing concern is: when reasoning is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs exhibit? To address this issue, we introduce HauntAttack, a novel and general-purpose black-box attack framework that systematically embeds harmful instructions into reasoning questions. Specifically, we treat reasoning questions as carriers and substitute one of their original conditions with a harmful instruction. This process creates a reasoning pathway in which the model is guided step by step toward generating unsafe outputs. Based on HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results reveal that even the most advanced LRMs exhibit significant safety vulnerabilities. Additionally, we perform a detailed analysis of different models, various types of harmful instructions, and model output patterns, providing valuable insights into the security of LRMs.

### 摘要
新兴的大型推理模型（LRMs）在数学和推理任务中持续表现出色，展现出卓越的能力。然而，推理能力的提升及其内部推理过程的暴露也带来了新的安全漏洞。一个值得关注的问题是：当推理与危害性高度纠缠时，LRMs会表现出怎样的安全-推理权衡？为解决这一问题，我们提出了HauntAttack，一种新颖且通用的黑盒攻击框架，该系统性地将有害指令嵌入推理问题中。具体而言，我们将推理问题视为载体，并将其原始条件之一替换为有害指令。这一过程构建了一条推理路径，使模型逐步被引导至生成不安全输出。基于HauntAttack，我们在多个LRMs上进行了全面实验。结果表明，即使最先进的LRMs也存在显著的安全漏洞。此外，我们对不同模型、各类有害指令及模型输出模式进行了详细分析，为LRMs的安全性提供了有价值的见解。

---

## [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)

### Abstract
arXiv:2506.07022v1 Announce Type: cross 
Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their ability to refuse malicious prompts, especially jailbreak attacks, is essential for safe and reliable use. Recently, activation steering has emerged as an effective approach for enhancing LLM safety by adding a refusal direction vector to internal activations of LLMs during inference, which will further induce the refusal behaviors of LLMs. However, indiscriminately applying activation steering fundamentally suffers from the trade-off between safety and utility, since the same steering vector can also lead to over-refusal and degraded performance on benign prompts. Although prior efforts, such as vector calibration and conditional steering, have attempted to mitigate this trade-off, their lack of theoretical grounding limits their robustness and effectiveness. To better address the trade-off between safety and utility, we present a theoretically grounded and empirically effective activation steering method called AlphaSteer. Specifically, it considers activation steering as a learnable process with two principled learning objectives: utility preservation and safety enhancement. For utility preservation, it learns to construct a nearly zero vector for steering benign data, with the null-space constraints. For safety enhancement, it learns to construct a refusal direction vector for steering malicious data, with the help of linear regression. Experiments across multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness of AlphaSteer, which significantly improves the safety of LLMs without compromising general capabilities. Our codes are available at https://github.com/AlphaLab-USTC/AlphaSteer.

### 摘要
随着大语言模型（LLMs）在现实应用中的广泛部署，确保其能够拒绝恶意提示（尤其是越狱攻击）对安全可靠使用至关重要。近期，激活导向技术通过在大语言模型推理过程中向内部激活添加拒绝方向向量来增强模型安全性，从而进一步诱导模型的拒绝行为。然而，盲目应用激活导向技术本质上会面临安全性与实用性之间的权衡问题，因为相同的导向向量也可能导致良性提示的过度拒绝和性能下降。尽管先前研究（如向量校准和条件导向）尝试缓解这一矛盾，但其缺乏理论依据限制了方法的鲁棒性和有效性。为更好地解决安全性与实用性的权衡问题，我们提出了一种理论完备且实证有效的激活导向方法AlphaSteer。该方法将激活导向视为具有两个原则性学习目标的可学习过程：实用性保持和安全性增强。在实用性保持方面，通过零空间约束学习构建接近零向量的良性数据导向向量；在安全性增强方面，借助线性回归学习构建恶意数据的拒绝方向向量。在多种越狱攻击和实用性基准测试上的实验表明，AlphaSteer能显著提升大语言模型的安全性而不损害其通用能力。代码已开源：https://github.com/AlphaLab-USTC/AlphaSteer。

---

## [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)

### Abstract
arXiv:2506.07045v1 Announce Type: cross 
Abstract: The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection methods. Although existing approaches often attain high accuracy, they typically operate as black boxes without providing human-understandable justifications. Multi-modal Large Language Models (MLLMs), while not originally intended for forgery detection, exhibit strong analytical and reasoning capabilities. When properly fine-tuned, they can effectively identify AI-generated images and offer meaningful explanations. However, existing MLLMs still struggle with hallucination and often fail to align their visual interpretations with actual image content and human reasoning. To bridge this gap, we construct a dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, establishing a foundation for human-aligned visual-textual grounded reasoning. We then finetune MLLMs through a multi-stage optimization strategy that progressively balances the objectives of accurate detection, visual localization, and coherent textual explanation. The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly outperforming baseline methods.

### 摘要
图像生成技术的快速发展增强了对可解释且鲁棒的检测方法的需求。尽管现有方法通常能达到较高准确率，但它们往往作为黑箱运行，无法提供人类可理解的判断依据。多模态大语言模型（MLLMs）虽非专为伪造检测设计，却展现出强大的分析和推理能力。经过适当微调后，这些模型能有效识别AI生成图像并提供有意义的解释。然而，现有MLLMs仍存在幻觉问题，其视觉解释常与实际图像内容及人类推理逻辑不一致。为弥合这一差距，我们构建了一个包含边界框标注和描述性文本的数据集，这些标注突出显示了合成伪影，为人类对齐的视觉-文本基础推理奠定了基础。随后通过多阶段优化策略对MLLMs进行微调，逐步平衡准确检测、视觉定位和连贯文本解释三大目标。最终模型在检测AI生成图像和定位视觉缺陷方面均表现出卓越性能，显著优于基线方法。

---

## [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)

### Abstract
arXiv:2506.07044v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...

### 摘要
多模态大语言模型（MLLMs）在理解常见视觉元素方面展现出卓越能力，这主要归功于其大规模数据集和先进的训练策略。然而，由于医疗场景与通用领域在数据和任务上存在固有差异，其在医疗应用中的有效性仍受限制。具体而言，现有医疗MLLMs面临以下关键局限：（1）医学知识覆盖范围局限于影像数据；（2）因数据筛选流程欠佳导致幻觉敏感性加剧；（3）缺乏针对复杂医疗场景的定制化推理能力。为应对这些挑战，我们首先提出综合性数据筛选流程：（1）高效获取涵盖医学影像、海量医学文本及通用领域数据的丰富医学知识；（2）合成精准的医学描述、视觉问答（VQA）及推理样本。由此构建出富含广谱医学知识的多模态数据集。基于筛选数据，我们推出医疗专用MLLM——灵枢。该模型通过多阶段训练逐步嵌入医学专业知识并增强任务解决能力。此外，我们初步探索了可验证奖励机制的强化学习对提升灵枢医疗推理能力的潜力。同时开发了MedEvalKit统一评估框架，整合主流多模态与文本医疗基准以实现标准化、公平且高效的模型评估。我们在三项基础医疗任务（多模态问答、文本问答及医疗报告生成）上评估灵枢性能，结果表明其在多数任务上持续超越现有开源多模态模型...

---

## [Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search](https://arxiv.org/abs/2506.07062)

### Abstract
arXiv:2506.07062v1 Announce Type: cross 
Abstract: The problem of relocating a set of objects to designated areas amidst movable obstacles can be framed as a Geometric Task and Motion Planning (G-TAMP) problem, a subclass of task and motion planning (TAMP). Traditional approaches to G-TAMP have relied either on domain-independent heuristics or on learning from planning experience to guide the search, both of which typically demand significant computational resources or data. In contrast, humans often use common sense to intuitively decide which objects to manipulate in G-TAMP problems. Inspired by this, we propose leveraging Large Language Models (LLMs), which have common sense knowledge acquired from internet-scale data, to guide task planning in G-TAMP problems. To enable LLMs to perform geometric reasoning, we design a predicate-based prompt that encodes geometric information derived from a motion planning algorithm. We then query the LLM to generate a task plan, which is then used to search for a feasible set of continuous parameters. Since LLMs are prone to mistakes, instead of committing to LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action space and use the LLM to guide the search. Unlike the previous approach that calls an LLM at every node and incurs high computational costs, we use it to warm-start the MCTS with the nodes explored in completing the LLM's task plan. On six different G-TAMP problems, we show our method outperforms previous LLM planners and pure search algorithms. Code can be found at: https://github.com/iMSquared/prime-the-search

### 摘要
在可移动障碍物环境中将一组物体重新定位至指定区域的问题可被表述为几何任务与运动规划（G-TAMP）问题，这是任务与运动规划（TAMP）的一个子类。传统G-TAMP方法通常依赖领域无关启发式或从规划经验中学习来指导搜索，这两种方式通常需要大量计算资源或数据。与之相反，人类在解决G-TAMP问题时往往凭借常识直觉判断需要操纵哪些物体。受此启发，我们提出利用从互联网规模数据中获取常识知识的大型语言模型（LLMs）来指导G-TAMP问题中的任务规划。为使LLMs能够进行几何推理，我们设计了一种基于谓词的提示机制，该机制编码了来自运动规划算法的几何信息。随后通过查询LLM生成任务计划，并据此搜索可行的连续参数集。鉴于LLMs易出错，我们并未直接采用其输出，而是将蒙特卡洛树搜索（MCTS）扩展至混合动作空间，并利用LLM指导搜索过程。与先前在每个节点调用LLM导致高计算成本的方法不同，我们使用LLM在完成其任务计划时探索的节点来预热MCTS。在六种不同G-TAMP问题上的实验表明，本方法优于现有LLM规划器和纯搜索算法。代码详见：https://github.com/iMSquared/prime-the-search

---

## [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)

### Abstract
arXiv:2506.07064v1 Announce Type: cross 
Abstract: Large language models (LLMs) have mastered abundant simple and explicit commonsense knowledge through pre-training, enabling them to achieve human-like performance in simple commonsense reasoning. Nevertheless, LLMs struggle to reason with complex and implicit commonsense knowledge that is derived from simple ones (such as understanding the long-term effects of certain events), an aspect humans tend to focus on more. Existing works focus on complex tasks like math and code, while complex commonsense reasoning remains underexplored due to its uncertainty and lack of structure. To fill this gap and align with real-world concerns, we propose a benchmark Com$^2$ focusing on complex commonsense reasoning. We first incorporate causal event graphs to serve as structured complex commonsense. Then we adopt causal theory~(e.g., intervention) to modify the causal event graphs and obtain different scenarios that meet human concerns. Finally, an LLM is employed to synthesize examples with slow thinking, which is guided by the logical relationships in the modified causal graphs. Furthermore, we use detective stories to construct a more challenging subset. Experiments show that LLMs struggle in reasoning depth and breadth, while post-training and slow thinking can alleviate this. The code and data are available at https://github.com/Waste-Wood/Com2.

### 摘要
大型语言模型（LLMs）通过预训练掌握了大量简单显式的常识知识，使其在简单常识推理任务中能够达到类人水平。然而，当涉及由简单常识衍生的复杂隐式常识推理（例如理解特定事件的长期影响）时，LLMs表现欠佳，而这类推理正是人类更关注的领域。现有研究多集中于数学、代码等复杂任务，由于不确定性和缺乏结构化特征，复杂常识推理领域仍探索不足。为填补这一空白并契合现实关切，我们提出专注于复杂常识推理的基准测试Com$^2$。首先整合因果事件图作为结构化复杂常识的载体，继而采用因果理论（如干预）修改因果事件图以生成符合人类关切的多样化场景，最后通过慢思考机制引导LLM基于修改后因果图的逻辑关系合成样本。此外，我们利用侦探故事构建了更具挑战性的子集。实验表明，LLMs在推理深度和广度上存在局限，而事后训练与慢思考能缓解这一问题。代码与数据详见https://github.com/Waste-Wood/Com2。

---

## [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)

### Abstract
arXiv:2506.06998v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by emitting long chains of thought. Yet, these verbose traces slow down inference and often drift into unnecessary detail, known as the overthinking phenomenon. To better understand LRMs' behavior, we systematically analyze the token-level misalignment between reasoning and non-reasoning models. While it is expected that their primary difference lies in the stylistic "thinking cues", LRMs uniquely exhibit two pivotal, previously under-explored phenomena: a Global Misalignment Rebound, where their divergence from non-reasoning models persists or even grows as response length increases, and more critically, a Local Misalignment Diminish, where the misalignment concentrates at the "thinking cues" each sentence starts with but rapidly declines in the remaining of the sentence. Motivated by the Local Misalignment Diminish, we propose FoReaL-Decoding, a collaborative fast-slow thinking decoding method for cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few tokens for each sentence, and then a weaker draft model completes the following tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to smoothly interpolate between the small and the large model. On four popular math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23), FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by up to 40%, while preserving 86 to 100% of model performance. These results establish FoReaL-Decoding as a simple, plug-and-play route to controllable cost-quality trade-offs in reasoning-centric tasks.

### 摘要
大型推理模型（LRMs）通过生成长链式思维轨迹实现强大的推理性能。然而这些冗长的推理过程会降低推断速度，且经常陷入不必要的细节描述，这种现象被称为过度思考。为深入理解LRMs的行为机制，我们系统分析了推理模型与非推理模型在token级别的错位现象。尽管二者差异主要体现于风格化的"思维提示"是预期结果，但LRMs独特地展现出两个关键且未被充分研究的现象：全局错位反弹（随着响应长度增加，其与非推理模型的差异持续存在甚至扩大）以及更关键的局部错位衰减（错位集中在每句开头的"思维提示"处，但在句子后续部分迅速减弱）。基于局部错位衰减现象，我们提出FoReaL-Decoding解码方法——一种实现成本-质量权衡的协作式快慢思维解码策略。该方法由主导模型负责每句开头的若干token生成，随后由较弱的草稿模型完成该句剩余部分。通过随机门控机制在大小模型之间实现平滑插值。在四大数学推理基准测试（AIME24、GPQA-Diamond、MATH500、AMC23）上，FoReaL-Decoding在保持模型性能86%-100%的同时，将理论FLOPs降低30%-50%，思维链长度缩减达40%。这些结果表明FoReaL-Decoding为以推理为核心的任务提供了一种简单即插即用的成本-质量可控权衡方案。

---

## [Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models](https://arxiv.org/abs/2506.07077)

### Abstract
arXiv:2506.07077v1 Announce Type: cross 
Abstract: Differential Privacy (DP) is a widely adopted technique, valued for its effectiveness in protecting the privacy of task-specific datasets, making it a critical tool for large language models. However, its effectiveness in Multimodal Large Language Models (MLLMs) remains uncertain. Applying Differential Privacy (DP) inherently introduces substantial computation overhead, a concern particularly relevant for MLLMs which process extensive textual and visual data. Furthermore, a critical challenge of DP is that the injected noise, necessary for privacy, scales with parameter dimensionality, leading to pronounced model degradation; This trade-off between privacy and utility complicates the application of Differential Privacy (DP) to complex architectures like MLLMs. To address these, we propose Dual-Priv Pruning, a framework that employs two complementary pruning mechanisms for DP fine-tuning in MLLMs: (i) visual token pruning to reduce input dimensionality by removing redundant visual information, and (ii) gradient-update pruning during the DP optimization process. This second mechanism selectively prunes parameter updates based on the magnitude of noisy gradients, aiming to mitigate noise impact and improve utility. Experiments demonstrate that our approach achieves competitive results with minimal performance degradation. In terms of computational efficiency, our approach consistently utilizes less memory than standard DP-SGD. While requiring only 1.74% more memory than zeroth-order methods which suffer from severe performance issues on A100 GPUs, our method demonstrates leading memory efficiency on H20 GPUs. To the best of our knowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is coming soon.

### 摘要
差分隐私（DP）作为一种广泛采用的技术，因其在保护任务特定数据集隐私方面的有效性而受到重视，成为大语言模型的关键工具。然而，其在多模态大语言模型（MLLMs）中的有效性仍不确定。差分隐私的应用本质上会引入大量计算开销，这对处理海量文本和视觉数据的MLLMs尤为关键。此外，DP面临的核心挑战在于：为保护隐私而注入的噪声会随参数维度增加而放大，导致显著的模型性能下降；这种隐私与效用的权衡使得DP在MLLMs等复杂架构中的应用变得困难。为此，我们提出双重隐私剪枝框架Dual-Priv Pruning，该框架通过两种互补的剪枝机制实现MLLMs的DP微调：（1）视觉令牌剪枝通过去除冗余视觉信息降低输入维度；（2）在DP优化过程中实施梯度更新剪枝，该机制根据噪声梯度幅度选择性修剪参数更新，旨在减轻噪声影响并提升效用。实验表明，我们的方法以最小性能损失取得了具有竞争力的结果。在计算效率方面，我们的方法始终比标准DP-SGD占用更少内存。虽然仅比存在A100 GPU严重性能问题的零阶方法多消耗1.74%内存，但我们的方案在H20 GPU上展现出领先的内存效率。据我们所知，这是首次探索MLLMs的DP微调研究。代码即将发布。

---

## [Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings](https://arxiv.org/abs/2506.07109)

### Abstract
arXiv:2506.07109v1 Announce Type: cross 
Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a longstanding goal. However, unlike domains such as language or vision, where scaling structured data has driven generalization, progress in offline BBO remains hindered by the lack of unified representations for heterogeneous numerical spaces. Thus, existing offline BBO approaches are constrained to single-task and fixed-dimensional settings, failing to achieve cross-domain universal optimization. Recent advances in language models (LMs) offer a promising path forward: their embeddings capture latent relationships in a unifying way, enabling universal optimization across different data types possible. In this paper, we discuss multiple potential approaches, including an end-to-end learning framework in the form of next-token prediction, as well as prioritizing the learning of latent spaces with strong representational capabilities. To validate the effectiveness of these methods, we collect offline BBO tasks and data from open-source academic works for training. Experiments demonstrate the universality and effectiveness of our proposed methods. Our findings suggest that unifying language model priors and learning string embedding space can overcome traditional barriers in universal BBO, paving the way for general-purpose BBO algorithms. The code is provided at https://github.com/lamda-bbo/universal-offline-bbo.

### 摘要
对通用黑盒优化（BBO）算法的探索是一个长期目标。然而与语言或视觉等领域不同——这些领域通过扩展结构化数据实现了泛化，离线BBO的进展仍因缺乏异构数值空间的统一表示而受阻。因此现有离线BBO方法被限制在单任务和固定维度设置中，无法实现跨领域通用优化。语言模型（LM）的最新进展提供了可行路径：其嵌入表示能以统一方式捕捉潜在关系，使得跨数据类型通用优化成为可能。本文讨论了多种潜在方法，包括采用下一词预测形式的端到端学习框架，以及优先学习具有强表征能力的潜在空间。为验证这些方法的有效性，我们从开源学术作品中收集离线BBO任务和数据用于训练。实验证明了所提方法的通用性和有效性。研究表明：统一语言模型先验和学习字符串嵌入空间能突破通用BBO的传统障碍，为通用BBO算法开辟道路。

---

## [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)

### Abstract
arXiv:2506.07104v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving capabilities through extended Chain-of-Thought (CoT) reasoning but often produce excessively verbose and redundant reasoning traces. This inefficiency incurs high inference costs and limits practical deployment. While existing fine-tuning methods aim to improve reasoning efficiency, assessing their efficiency gains remains challenging due to inconsistent evaluations. In this work, we introduce the reasoning efficiency frontiers, empirical upper bounds derived from fine-tuning base LRMs across diverse approaches and training configurations. Based on these frontiers, we propose the Reasoning Efficiency Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from these frontiers. Systematic evaluation on challenging mathematical benchmarks reveals significant gaps in current methods: they either sacrifice accuracy for short length or still remain inefficient under tight token budgets. To reduce the efficiency gap, we propose REO-RL, a class of Reinforcement Learning algorithms that minimizes REG by targeting a sparse set of token budgets. Leveraging numerical integration over strategically selected budgets, REO-RL approximates the full efficiency objective with low error using a small set of token budgets. Through systematic benchmarking, we demonstrate that our efficiency metric, REG, effectively captures the accuracy-length trade-off, with low-REG methods reducing length while maintaining accuracy. Our approach, REO-RL, consistently reduces REG by &gt;=50 across all evaluated LRMs and matching Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy loss. Ablation studies confirm the effectiveness of our exponential token budget strategy. Finally, our findings highlight that fine-tuning LRMs to perfectly align with the efficiency frontiers remains an open challenge.

### 摘要
大规模推理模型（LRMs）通过扩展的思维链（CoT）推理展现出卓越的问题解决能力，但往往产生过度冗长和冗余的推理轨迹。这种低效性导致高昂的推理成本并限制实际部署。虽然现有微调方法旨在提升推理效率，但由于评估标准不一致，衡量其效率增益仍具挑战性。本研究提出推理效率前沿——通过多种方法和训练配置对基础LRMs进行微调得出的经验性上限。基于此，我们提出推理效率差距（REG），这一统一指标可量化任何微调后LRM与效率前沿的偏离程度。在具有挑战性的数学基准上的系统评估表明，当前方法存在显著差距：它们要么为缩短长度牺牲准确性，要么在严格令牌预算下仍效率低下。为缩小效率差距，我们提出REO-RL算法，这类强化学习方法通过瞄准稀疏令牌预算集合来最小化REG。利用对策略性选定预算的数值积分，REO-RL能以低误差近似完整效率目标。系统基准测试表明，我们的效率指标REG能有效捕捉准确性与长度间的权衡关系，低REG方法可在保持精度的同时缩短推理长度。REO-RL在所有评估的LRMs上持续降低REG≥50%，并在16K令牌预算下以最小精度损失匹配Qwen3-4B/8B的效率前沿。消融研究验证了我们指数级令牌预算策略的有效性。最终，研究发现使LRMs完全对齐效率前沿仍是一个开放挑战。

---

## [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)

### Abstract
arXiv:2506.07106v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.

### 摘要
大语言模型（LLMs）在自然语言推理任务中展现出强大性能，但其推理过程仍存在脆弱性且难以解释。思维链（CoT）等提示技术通过引导中间推理步骤或聚合多个输出提升了可靠性，但缺乏强制执行逻辑结构和评估内部一致性的机制。本文提出思维定理（ToTh）——一种创新框架，将推理建模为三个并行智能体（分别模拟溯因、演绎和归纳三种推理模式）的协作过程。每个智能体生成推理轨迹，并结构化形成正式推理图。为评估一致性，我们采用自然语言推理（NLI）指导的贝叶斯信念传播方法，为每个步骤分配置信度分数。最终选择最具一致性的推理图导出答案。在符号推理（WebOfLies）和数值推理（MultiArith）基准测试中，ToTh在多种LLMs上持续优于CoT、自洽性检验和CoT解码方法，同时生成可解释且逻辑严密的推理链。研究表明，该方法为构建更鲁棒且受认知启发的LLM推理提供了可行方向。实现代码详见https://github.com/KurbanIntelligenceLab/theorem-of-thought。

---

## [Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models](https://arxiv.org/abs/2506.07121)

### Abstract
arXiv:2506.07121v1 Announce Type: cross 
Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a systematic approach to identifying adversarial prompts that elicit harmful responses from target LLMs--has emerged as a crucial safety evaluation method. Within this framework, the diversity of adversarial prompts is essential for comprehensive safety assessments. We find that previous approaches to red-teaming may suffer from two key limitations. First, they often pursue diversity through simplistic metrics like word frequency or sentence embedding similarity, which may not capture meaningful variation in attack strategies. Second, the common practice of training a single attacker model restricts coverage across potential attack styles and risk categories. This paper introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to address these limitations. QDRT achieves goal-driven diversity through behavior-conditioned training and implements a behavioral replay buffer in an open-ended manner. Additionally, it trains multiple specialized attackers capable of generating high-quality attacks across diverse styles and risk categories. Our empirical evaluation demonstrates that QDRT generates attacks that are both more diverse and more effective against a wide range of target LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the field of LLM safety by providing a systematic and effective approach to automated red-teaming, ultimately supporting the responsible deployment of LLMs.

### 摘要
确保大型语言模型（LLMs）的安全性至关重要。红队测试——一种通过系统性方法识别能诱发目标LLMs生成有害回复的对抗性提示的技术——已成为关键的安全评估手段。该框架下，对抗性提示的多样性对全面安全评估具有决定性意义。研究发现，现有红队测试方法可能存在两大局限：其一，通常仅依赖词频或句子嵌入相似度等简单指标追求多样性，难以捕捉攻击策略的本质差异；其二，普遍采用单一攻击者模型的训练方式，限制了攻击风格与风险类别的覆盖范围。本文提出质量-多样性红队测试（QDRT）新框架以解决上述问题：通过行为条件训练实现目标导向的多样性，采用开放式行为回放缓冲机制，并训练多个能跨风格/风险类别生成高质量攻击的专项攻击者。实验表明，QDRT生成的攻击在GPT-2、Llama-3、Gemma-2和Qwen2.5等各类目标LLMs上兼具更高多样性与更强攻击效力。本研究通过提供系统化自动红队测试方案，为LLMs的安全部署提供支持，推动了LLM安全领域的发展。

---

## [AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models](https://arxiv.org/abs/2506.07165)

### Abstract
arXiv:2506.07165v1 Announce Type: cross 
Abstract: Existing multi-objective preference alignment methods for large language models (LLMs) face limitations: (1) the inability to effectively balance various preference dimensions, and (2) reliance on auxiliary reward/reference models introduces computational complexity. To address these challenges, we propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel framework that achieves dynamic balance across preference dimensions. By introducing the multi-objective optimization paradigm to use the dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with diverse preferences without additional reward models or reference models. We introduce an adaptive weight assignment mechanism that models the generation space as a Gaussian distribution, allowing dynamic prioritization of preference dimensions. Empirical results demonstrate that AMoPO outperforms state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B models reveal the scaling ability of AMoPO. Moreover, additional analysis of multiple dimensions verifies its adaptability and effectiveness. These findings validate AMoPO's capability to achieve dimension-aware preference alignment, highlighting its superiority. Our codes and datasets are available at https://github.com/Javkonline/AMoPO.

### 摘要
现有的大型语言模型（LLM）多目标偏好对齐方法存在两个局限性：（1）无法有效平衡各偏好维度；（2）依赖辅助奖励/参考模型导致计算复杂度增加。为解决这些问题，我们提出自适应多目标偏好优化框架（AMoPO），该框架通过引入多目标优化范式，将维度感知生成指标作为隐式奖励，无需额外奖励模型或参考模型即可实现跨偏好维度的动态平衡。我们设计了一种自适应权重分配机制，将生成空间建模为高斯分布，从而动态调整偏好维度优先级。实验结果表明，AMoPO以28.5%的优势超越现有最优基线，在7B、14B和32B模型上的测试验证了其可扩展性。多维度分析进一步证实了该框架的适应性和有效性。这些发现验证了AMoPO实现维度感知偏好对齐的能力，凸显了其优越性。代码和数据集已开源：https://github.com/Javkonline/AMoPO。

---

## [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)

### Abstract
arXiv:2506.07138v1 Announce Type: cross 
Abstract: Large multimodal models (LMMs) suffer significant computational challenges due to the high cost of Large Language Models (LLMs) and the quadratic complexity of processing long vision token sequences. In this paper, we explore the spatial redundancy among vision tokens and shorten the length of vision token sequences for inference acceleration. Specifically, we propose a Spatial Token Fusion (STF) method to learn compact vision tokens for short vision token sequence, where spatial-adjacent tokens are fused into one. Meanwhile, weight-frozen vision encoder can not well adapt to the demand of extensive downstream vision-language tasks. To this end, we further introduce a Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features for the reduced token sequence. Overall, we combine STF and MBTF module to balance token reduction and information preservation, thereby improving inference efficiency without sacrificing multimodal reasoning capabilities. Experimental results demonstrate that our method based on LLaVA-1.5 achieves comparable or even superior performance to the baseline on 8 popular vision-language benchmarks with only $25\%$ vision tokens of baseline. The source code and trained weights are available at https://github.com/visresearch/LLaVA-STF.

### 摘要
大型多模态模型（LMMs）因大语言模型（LLMs）的高计算成本以及长视觉令牌序列的二次处理复杂度而面临显著计算挑战。本文通过探索视觉令牌间的空间冗余性，提出缩短视觉令牌序列长度以实现推理加速。具体而言，我们提出空间令牌融合（STF）方法，通过将空间相邻令牌融合为单一令牌，为短视觉令牌序列学习紧凑的视觉表示。同时，权重冻结的视觉编码器难以适应多样化下游视觉-语言任务的需求。为此，我们进一步引入多区块令牌融合（MBTF）模块，为缩减后的令牌序列补充多粒度特征。整体方案通过结合STF与MBTF模块，在令牌压缩与信息保留间实现平衡，从而在不牺牲多模态推理能力的前提下提升推理效率。实验结果表明，基于LLaVA-1.5的该方法仅需基线25%的视觉令牌，便在8个主流视觉-语言基准测试中取得与基线相当或更优的性能。源代码与训练权重已发布于https://github.com/visresearch/LLaVA-STF。

---

## [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)

### Abstract
arXiv:2506.07154v1 Announce Type: cross 
Abstract: Controlling the syntactic structure of text generated by language models is valuable for applications requiring clarity, stylistic consistency, or interpretability, yet it remains a challenging task. In this paper, we argue that sampling algorithms based on the posterior inference can effectively enforce a target constituency structure during generation. Our approach combines sequential Monte Carlo, which estimates the posterior distribution by sampling from a proposal distribution, with a syntactic tagger that ensures that each generated token aligns with the desired syntactic structure. Our experiments with GPT2 and Llama3-8B models show that with an appropriate proposal distribution, we can improve syntactic accuracy, increasing the F1 score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both cases without compromising the language model's fluency. These results underscore both the complexity of syntactic control and the effectiveness of sampling algorithms, offering a promising approach for applications where precise control over syntax is essential.

### 摘要
控制语言模型生成文本的句法结构对于需要清晰度、风格一致性或可解释性的应用具有重要价值，但这仍是一项具有挑战性的任务。本文提出基于后验推断的采样算法能够在生成过程中有效执行目标句法结构约束。我们的方法将序贯蒙特卡洛（通过从提议分布中采样来估计后验分布）与句法标注器相结合，确保每个生成标记均符合预期句法结构。在GPT2和Llama3-8B模型上的实验表明：当采用合适的提议分布时，我们能在保持语言模型流畅性的前提下，将句法准确率F1分数从GPT2-large的12.31和Llama3-8B的35.33均提升至约93。这些结果既揭示了句法控制的复杂性，也证明了采样算法的有效性，为需要精确句法控制的应用提供了可行方案。

---

## [Taxonomy of migration scenarios for Qiskit refactoring using LLMs](https://arxiv.org/abs/2506.07135)

### Abstract
arXiv:2506.07135v1 Announce Type: cross 
Abstract: As quantum computing advances, quantum programming libraries' heterogeneity and steady evolution create new challenges for software developers. Frequent updates in software libraries break working code that needs to be refactored, thus adding complexity to an already complex landscape. These refactoring challenges are, in many cases, fundamentally different from those known in classical software engineering due to the nature of quantum computing software. This study addresses these challenges by developing a taxonomy of quantum circuit's refactoring problems, providing a structured framework to analyze and compare different refactoring approaches. Large Language Models (LLMs) have proven valuable tools for classic software development, yet their value in quantum software engineering remains unexplored. This study uses LLMs to categorize refactoring needs in migration scenarios between different Qiskit versions. Qiskit documentation and release notes were scrutinized to create an initial taxonomy of refactoring required for migrating between Qiskit releases. Two taxonomies were produced: one by expert developers and one by an LLM. These taxonomies were compared, analyzing differences and similarities, and were integrated into a unified taxonomy that reflects the findings of both methods. By systematically categorizing refactoring challenges in Qiskit, the unified taxonomy is a foundation for future research on AI-assisted migration while enabling a more rigorous evaluation of automated refactoring techniques. Additionally, this work contributes to quantum software engineering (QSE) by enhancing software development workflows, improving language compatibility, and promoting best practices in quantum programming.

### 摘要
随着量子计算的发展，量子编程库的异构性和持续演进为软件开发人员带来了新的挑战。软件库的频繁更新会导致原有代码失效而需要进行重构，这进一步加剧了本就复杂的开发环境。由于量子计算软件的特殊性，这些重构挑战在许多方面与传统软件工程中的问题存在本质差异。本研究通过建立量子电路重构问题的分类体系，为解决这些挑战提供了一个结构化分析框架，可用于比较不同重构方法。大型语言模型（LLM）已被证明是传统软件开发的有力工具，但其在量子软件工程中的价值仍有待探索。本研究利用LLM对不同Qiskit版本间迁移场景中的重构需求进行分类。通过系统分析Qiskit文档和版本发布说明，我们首先建立了Qiskit版本迁移所需重构操作的初始分类体系。研究最终生成两个分类体系：一个来自开发专家，另一个由LLM产生。通过对比分析这两个分类体系的异同点，我们将其整合成一个统一分类框架。该统一分类体系不仅为未来AI辅助迁移研究奠定了基础，还能更严谨地评估自动化重构技术。此外，本研究通过优化软件开发流程、提升语言兼容性以及推广量子编程最佳实践，对量子软件工程（QSE）领域做出了贡献。

---

## [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)

### Abstract
arXiv:2506.07168v1 Announce Type: cross 
Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural networks (GNNs) often fall short due to the complex textual information associated with each node. Recent methods have improved node representations by leveraging large language models (LLMs) to enhance node text features, but these approaches typically require extensive annotations or fine-tuning across all nodes, which is both time-consuming and costly. To overcome these challenges, we introduce GAGA, an efficient framework for TAG representation learning. GAGA reduces annotation time and cost by focusing on annotating only representative nodes and edges. It constructs an annotation graph that captures the topological relationships among these annotations. Furthermore, GAGA employs a two-level alignment module to effectively integrate the annotation graph with the TAG, aligning their underlying structures. Experiments show that GAGA achieves classification accuracies on par with or surpassing state-of-the-art methods while requiring only 1% of the data to be annotated, demonstrating its high efficiency.

### 摘要
在文本属性图（TAG）领域，传统图神经网络（GNN）常因节点关联的复杂文本信息而表现欠佳。现有方法通过利用大语言模型（LLM）增强节点文本特征来改进节点表示，但这些方法通常需要对所有节点进行大量标注或微调，耗时且成本高昂。为解决这些问题，我们提出GAGA——一种高效的TAG表示学习框架。该框架通过仅标注代表性节点和边来降低标注时间和成本，并构建捕获这些标注间拓扑关系的标注图。此外，GAGA采用两级对齐模块将标注图与TAG有效整合，实现底层结构对齐。实验表明，GAGA在仅需标注1%数据的情况下，分类准确率达到或超越现有最优方法，展现出显著的高效性。

---

## [Mind the Web: The Security of Web Use Agents](https://arxiv.org/abs/2506.07153)

### Abstract
arXiv:2506.07153v1 Announce Type: cross 
Abstract: Web-use agents are rapidly being deployed to automate complex web tasks, operating with extensive browser capabilities including multi-tab navigation, DOM manipulation, JavaScript execution and authenticated session access. However, these powerful capabilities create a critical and previously unexplored attack surface. This paper demonstrates how attackers can exploit web-use agents' high-privilege capabilities by embedding malicious content in web pages such as comments, reviews, or advertisements that agents encounter during legitimate browsing tasks. In addition, we introduce the task-aligned injection technique that frame malicious commands as helpful task guidance rather than obvious attacks. This technique exploiting fundamental limitations in LLMs' contextual reasoning: agents struggle in maintaining coherent contextual awareness and fail to detect when seemingly helpful web content contains steering attempts that deviate from their original task goal. Through systematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do Browser, OpenOperator), we demonstrate nine payload types that compromise confidentiality, integrity, and availability, including unauthorized camera activation, user impersonation, local file exfiltration, password leakage, and denial of service, with validation across multiple LLMs achieving success rates of 80%-100%. These payloads succeed across agents with built-in safety mechanisms, requiring only the ability to post content on public websites, creating unprecedented risks given the ease of exploitation combined with agents' high-privilege access. To address this attack, we propose comprehensive mitigation strategies including oversight mechanisms, execution constraints, and task-aware reasoning techniques, providing practical directions for secure development and deployment.

### 摘要
网络使用代理正被快速部署以自动化复杂网络任务，其操作具备包括多标签页导航、DOM操作、JavaScript执行及认证会话访问在内的广泛浏览器能力。然而，这些强大能力也构成了一个关键且尚未被探索的攻击面。本文揭示了攻击者如何通过将恶意内容嵌入网页（如评论、评价或广告等代理在合法浏览任务中遭遇的内容）来利用网络代理的高权限能力。此外，我们提出任务对齐注入技术，该技术将恶意指令伪装成有用的任务指导而非明显攻击。此技术利用了LLM上下文推理的根本局限：代理难以保持连贯的上下文感知能力，无法识别看似有益的网页内容是否包含偏离其原始任务目标的操控企图。通过对四种主流代理（OpenAI Operator、Browser Use、Do Browser、OpenOperator）的系统性评估，我们展示了九种危害机密性、完整性和可用性的攻击载荷，包括非授权摄像头激活、用户身份伪造、本地文件窃取、密码泄露及拒绝服务，经多种LLM验证成功率可达80%-100%。这些攻击载荷可突破内置安全机制的代理，仅需在公共网站发布内容即可实现，鉴于其易实施性与代理高权限访问的结合，造成了前所未有的风险。为应对此类攻击，我们提出了包括监督机制、执行约束和任务感知推理技术在内的综合缓解策略，为安全开发和部署提供了实践方向。

---

## [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)

### Abstract
arXiv:2506.07142v1 Announce Type: cross 
Abstract: This is the second in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate Chain-of-Thought (CoT) prompting, a technique that encourages a large language model (LLM) to "think step by step" (Wei et al., 2022). CoT is a widely adopted method for improving reasoning tasks, however, our findings reveal a more nuanced picture of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending on the type of task and model. For non-reasoning models, CoT generally improves average performance by a small amount, particularly if the model does not inherently engage in step-by-step processing by default. However, CoT can introduce more variability in answers, sometimes triggering occasional errors in questions the model would otherwise get right. We also found that many recent models perform some form of CoT reasoning even if not asked; for these models, a request to perform CoT had little impact. Performing CoT generally requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting often results in only marginal, if any, gains in answer accuracy. However, it significantly increases the time and tokens needed to generate a response.

### 摘要
这是系列短篇报告中的第二篇，旨在通过严谨测试帮助商业、教育和政策领导者理解人工智能应用的技术细节。本报告研究了思维链（Chain-of-Thought，CoT）提示技术——一种促使大语言模型（LLM）"逐步思考"的方法（Wei等，2022）。虽然CoT是被广泛采用的推理任务改进手段，但我们的研究发现其有效性存在更复杂的表现。我们证明了两点：
- 思维链提示的效果随任务类型和模型差异而显著变化。对于非推理型模型，CoT通常小幅提升平均表现（尤其当模型默认不进行逐步处理时），但会引入更高回答波动性，有时甚至会在模型本可正确回答的问题上触发偶然错误。我们还发现许多新模型即使未被要求也会执行某种形式的CoT推理，这类模型受CoT提示影响甚微。执行CoT通常比直接回答消耗更多token（增加成本与时间）。
- 对于具备显式推理能力的模型，CoT提示即便能带来答案准确率的提升也极为有限，但会显著增加生成响应所需的时间与token消耗。

---

## [Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation](https://arxiv.org/abs/2506.07211)

### Abstract
arXiv:2506.07211v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) presents a dual challenge in the fight against disinformation. These powerful tools, capable of generating human-like text at scale, can be weaponised to produce sophisticated and persuasive disinformation, yet they also hold promise for enhancing detection and mitigation strategies. This paper investigates the complex dynamics between LLMs and disinformation through a communication game that simulates online forums, inspired by the game Werewolf, with 25 participants. We analyse how Disinformers, Moderators, and Users leverage LLMs to advance their goals, revealing both the potential for misuse and combating disinformation. Our findings highlight the varying uses of LLMs depending on the participants' roles and strategies, underscoring the importance of understanding their effectiveness in this context. We conclude by discussing implications for future LLM development and online platform design, advocating for a balanced approach that empowers users and fosters trust while mitigating the risks of LLM-assisted disinformation.

### 摘要
大型语言模型（LLMs）的出现为打击虚假信息带来了双重挑战。这些能够大规模生成类人文本的强大工具，既可能被武器化以生产复杂且具有说服力的虚假信息，同时也为增强检测与缓解策略提供了可能。本研究通过一个受狼人杀游戏启发的通信游戏（包含25名参与者）模拟在线论坛，探究LLMs与虚假信息之间的复杂互动关系。我们分析了虚假信息传播者、版主和普通用户如何利用LLMs达成各自目标，既揭示了滥用风险，也展现了对抗虚假信息的潜力。研究发现，参与者根据角色和策略对LLMs的差异化使用，突显了理解其在此情境下有效性的重要意义。最后，我们探讨了未来LLM开发和在线平台设计的启示，主张采取平衡策略——在降低LLM辅助虚假信息风险的同时，赋予用户权力并培育信任。

---

## [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Disserta\c&#123;c&#125;\~oes e Trabalhos de Gradua\c&#123;c&#125;\~ao em SI -- XXI Simp\'osio Brasileiro de Sistemas de Informa\c&#123;c&#125;\~ao](https://arxiv.org/abs/2506.07169)

### Abstract
arXiv:2506.07169v1 Announce Type: cross 
Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule of more: more data, more computing power and more complexity, best exemplified by the Large Language Models. However, training (or fine-tuning) large dense models for specific applications usually requires significant amounts of computing resources. This \textbf&#123;Ph.D. dissertation&#125; focuses on an under-investi\-gated NLP data engineering technique, whose potential is enormous in the current scenario known as Instance Selection (IS). The IS goal is to reduce the training set size by removing noisy or redundant instances while maintaining the effectiveness of the trained models and reducing the training process cost. We provide a comprehensive and scientifically sound comparison of IS methods applied to an essential NLP task -- Automatic Text Classification (ATC), considering several classification solutions and many datasets. Our findings reveal a significant untapped potential for IS solutions. We also propose two novel IS solutions that are noise-oriented and redundancy-aware, specifically designed for large datasets and transformer architectures. Our final solution achieved an average reduction of 41\% in training sets, while maintaining the same levels of effectiveness in all datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x (up to 2.46x), making them scalable for datasets with hundreds of thousands of documents.

### 摘要
自然语言处理（NLP）的进展始终遵循"更多"法则：更多数据、更强算力与更高复杂度，这一趋势在大型语言模型中体现得尤为显著。然而，针对特定应用训练（或微调）大型稠密模型通常需要消耗大量计算资源。本博士论文聚焦于当前研究不足但潜力巨大的NLP数据工程技术——实例选择（IS）。该技术旨在通过剔除噪声或冗余实例来缩减训练集规模，同时保持模型效能并降低训练成本。我们针对自然语言处理核心任务——自动文本分类（ATC），结合多种分类方案与数据集，开展了全面且科学严谨的IS方法比较研究。研究发现IS解决方案存在显著未开发潜力。我们进一步提出两种新型IS方案：面向噪声处理与冗余识别的解决方案，专为海量数据集与Transformer架构设计。最终方案在保持所有数据集分类效能不变的前提下，平均实现训练集规模缩减41%。尤为重要的是，我们的方案实现了1.67倍（最高达2.46倍）的加速提升，使其能够有效扩展到包含数十万文档的数据集。

---

## [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)

### Abstract
arXiv:2506.07180v1 Announce Type: cross 
Abstract: As video large language models (Video-LLMs) become increasingly integrated into real-world applications that demand grounded multimodal reasoning, ensuring their factual consistency and reliability is of critical importance. However, sycophancy, the tendency of these models to align with user input even when it contradicts the visual evidence, undermines their trustworthiness in such contexts. Current sycophancy research has largely overlooked its specific manifestations in the video-language domain, resulting in a notable absence of systematic benchmarks and targeted evaluations to understand how Video-LLMs respond under misleading user input. To fill this gap, we propose VISE (Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated benchmark designed to evaluate sycophantic behavior in state-of-the-art Video-LLMs across diverse question formats, prompt biases, and visual reasoning tasks. Specifically, VISE pioneeringly brings linguistic perspectives on sycophancy into the visual domain, enabling fine-grained analysis across multiple sycophancy types and interaction patterns. In addition, we explore key-frame selection as an interpretable, training-free mitigation strategy, which reveals potential paths for reducing sycophantic bias by strengthening visual grounding.

### 摘要
随着视频大语言模型（Video-LLMs）日益融入需要多模态推理的现实应用，确保其事实一致性与可靠性变得至关重要。然而，谄媚现象——即这些模型即使在与视觉证据相矛盾时仍倾向于迎合用户输入的倾向——严重削弱了其在此类场景中的可信度。当前关于谄媚行为的研究大多忽视了其在视频-语言领域的具体表现，导致缺乏系统性基准和针对性评估来理解视频大语言模型如何在误导性用户输入下作出响应。为填补这一空白，我们提出VISE（视频大语言模型谄媚行为基准评估），这是首个专门用于评估前沿视频大语言模型在不同问题形式、提示偏见和视觉推理任务中谄媚行为的基准。具体而言，VISE开创性地将语言学视角的谄媚研究引入视觉领域，实现了跨多种谄媚类型与交互模式的细粒度分析。此外，我们探索了关键帧选择作为一种可解释、无需训练的缓解策略，揭示了通过强化视觉基础来减少谄媚偏见的潜在路径。

---

## [VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code](https://arxiv.org/abs/2506.07239)

### Abstract
arXiv:2506.07239v1 Announce Type: cross 
Abstract: Modern chip design is complex, and there is a crucial need for early-stage prediction of key design-quality metrics like timing and routing congestion directly from Verilog code (a commonly used programming language for hardware design). It is especially important yet complex to predict individual lines of code that cause timing violations or downstream routing congestion. Prior works have tried approaches like converting Verilog into an intermediate graph representation and using LLM embeddings alongside other features to predict module-level quality, but did not consider line-level quality prediction. We propose VeriLoC, the first method that predicts design quality directly from Verilog at both the line- and module-level. To this end, VeriLoC leverages recent Verilog code-generation LLMs to extract local line-level and module-level embeddings, and train downstream classifiers/regressors on concatenations of these embeddings. VeriLoC achieves high F1-scores of 0.86-0.95 for line-level congestion and timing prediction, and reduces the mean average percentage error from 14% - 18% for SOTA methods down to only 4%. We believe that VeriLoC embeddings and insights from our work will also be of value for other predictive and optimization tasks for complex hardware design.

### 摘要
现代芯片设计复杂度高，亟需直接从Verilog代码（硬件设计常用编程语言）中早期预测时序和布线拥塞等关键设计质量指标。尤其重要且复杂的是预测导致时序违规或下游布线拥塞的代码行级别问题。先前研究尝试过将Verilog转换为中间图表示，并利用LLM嵌入结合其他特征预测模块级质量，但未考虑代码行级质量预测。我们提出首个能在代码行和模块级别直接从Verilog预测设计质量的方法VeriLoC。该方法利用最新Verilog代码生成LLM提取局部行级和模块级嵌入，通过拼接这些嵌入训练下游分类器/回归器。VeriLoC在行级拥塞和时序预测中达到0.86-0.95的高F1分数，将现有最佳方法的平均绝对百分比误差从14%-18%降至仅4%。我们相信VeriLoC的嵌入特征及研究见解对其他复杂硬件设计的预测和优化任务也具有重要价值。

---

## [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs](https://arxiv.org/abs/2506.07240)

### Abstract
arXiv:2506.07240v1 Announce Type: cross 
Abstract: Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model's internal "thinking" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model's planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this "overclocking" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available.

### 摘要
近期，显式结构化推理等技术通过强制分离模型内部"思考"过程与最终响应，展现出强大的测试时扩展性能。该场景下影响答案质量的关键因素是思考阶段的长度。当推理过短时，模型可能无法捕捉任务的复杂性；反之，当推理过长时，模型可能陷入过度思考，导致不必要的计算和性能下降。本文探究并利用大语言模型在显式思维过程中理解和调节推理长度的内在机制。首先，我们证明大语言模型会编码其推理过程进度，并引入交互式进度条可视化方法，用以揭示模型规划动态的深层特征。其次，我们在推理过程中操纵内部进度编码，以减少冗余步骤并生成更简洁、更果断的思维链。实验结果表明，这种"超频"方法能有效缓解过度思考现象，提升答案准确性，同时降低推理延迟。相关代码已公开。

---

## [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)

### Abstract
arXiv:2506.07218v1 Announce Type: cross 
Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data.

### 摘要
增强多模态大语言模型（MLLMs）的多模态推理能力是一项具有挑战性的任务，已引起学界日益关注。近期研究尝试将可验证奖励的强化学习（RLVR）应用于多模态领域以提升MLLMs的推理能力，但这些工作大多忽视了模型多模态感知能力的增强——该能力是复杂多模态推理的核心前提与基础组成部分。通过麦克尼马尔检验，我们发现现有RLVR方法未能有效提升MLLMs的多模态感知能力，从而限制了其在多模态推理方面的进一步改进。针对这一局限，我们提出Perception-R1方法，通过引入新型视觉感知奖励机制，显式激励模型准确感知视觉内容，从而有效同步提升其多模态感知与推理能力。具体而言，我们首先从多模态问题的思维链轨迹中收集文本化视觉标注作为奖励分配的视觉参照；在RLVR训练阶段，采用评判大语言模型评估MLLMs生成响应与视觉标注的一致性，并基于一致性判断分配视觉感知奖励。在多个多模态推理基准上的大量实验表明，Perception-R1仅使用1,442个训练数据即在大多数基准上实现了最先进的性能。

---

## [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)

### Abstract
arXiv:2506.07245v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have significantly improved performance on the Text-to-SQL task. However, prior approaches typically rely on static, pre-processed database information provided at inference time, which limits the model's ability to fully understand the database contents. Without dynamic interaction, LLMs are constrained to fixed, human-provided context and cannot autonomously explore the underlying data. To address this limitation, we propose SDE-SQL, a framework that enables large language models to perform self-driven exploration of databases during inference. This is accomplished by generating and executing SQL probes, which allow the model to actively retrieve information from the database and iteratively update its understanding of the data. Unlike prior methods, SDE-SQL operates in a zero-shot setting, without relying on any question-SQL pairs as in-context demonstrations. When evaluated on the BIRD benchmark with Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing a new state-of-the-art among methods based on open-source models without supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the performance of SDE-SQL can be further enhanced, yielding an additional 0.52% improvement.

### 摘要
大型语言模型（LLMs）的最新进展显著提升了文本到SQL任务的性能。然而，现有方法通常依赖于推理时提供的静态预处理数据库信息，这限制了模型对数据库内容的完整理解。由于缺乏动态交互，LLMs受限于固定的人工提供上下文，无法自主探索底层数据。为解决这一局限，我们提出SDE-SQL框架，使大型语言模型能在推理过程中对数据库进行自主探索。该框架通过生成并执行SQL探针实现，使模型能主动从数据库检索信息并迭代更新对数据的理解。与先前方法不同，SDE-SQL在零样本设置下运行，不依赖任何问题-SQL对作为上下文示例。在BIRD基准测试中使用Qwen2.5-72B-Instruct进行评估时，SDE-SQL相比原始Qwen2.5-72B-Instruct基线实现了8.02%的相对执行准确率提升，在没有监督微调（SFT）或模型集成的情况下，基于开源模型的方法中确立了新的最优性能。此外，结合SFT可进一步提升SDE-SQL性能，获得额外0.52%的改进。

---

## [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)

### Abstract
arXiv:2506.07274v1 Announce Type: cross 
Abstract: Code-switching presents a complex challenge for syntactic analysis, especially in low-resource language settings where annotated data is scarce. While recent work has explored the use of large language models (LLMs) for sequence-level tagging, few approaches systematically investigate how well these models capture syntactic structure in code-switched contexts. Moreover, existing parsers trained on monolingual treebanks often fail to generalize to multilingual and mixed-language input. To address this gap, we introduce the BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal Dependencies (UD) annotations for code-switched text. First, we develop a prompt-based framework for Spanish-English and Spanish-Guaran\'i data, combining few-shot LLM prompting with expert review. Second, we release two annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus. Third, we conduct a detailed syntactic analysis of switch points across language pairs and communicative contexts. Experimental results show that BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly outperforming prior baselines and multilingual parsers. These results show that LLMs, when carefully guided, can serve as practical tools for bootstrapping syntactic resources in under-resourced, code-switched environments. Data and source code are available at https://github.com/N3mika/ParsingProject

### 摘要
代码转换现象为句法分析带来了复杂挑战，尤其在标注数据稀缺的低资源语言环境中。尽管近期研究探索了利用大语言模型（LLM）进行序列标注的方法，但少有研究系统性地考察这些模型在代码转换语境中对句法结构的捕捉能力。此外，基于单语树库训练的现有解析器往往难以泛化至多语言及混合语言输入。为填补这一空白，我们提出BiLingua解析器——一个基于LLM的标注流程，专门为代码转换文本生成通用依存关系（UD）标注。首先，我们开发了面向西班牙语-英语和西班牙语-瓜拉尼语的提示框架，将小样本LLM提示与专家审核相结合；其次，我们发布两个标注数据集，包含首个西班牙语-瓜拉尼语UD解析语料库；最后，我们对不同语言对及交际情境中的转换点展开细粒度句法分析。实验结果表明，经专家修订后BiLingua解析器标注准确率（LAS）最高达95.29%，显著优于现有基线模型和多语言解析器。这些发现证明，经过精心引导的大语言模型可成为资源匮乏的代码转换环境中句法资源构建的有效工具。数据与源代码详见https://github.com/N3mika/ParsingProject。

---

## [Tokenized Bandit for LLM Decoding and Alignment](https://arxiv.org/abs/2506.07276)

### Abstract
arXiv:2506.07276v1 Announce Type: cross 
Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB), variants of linear and stochastic multi-armed bandit problems inspired by LLM decoding and alignment. In these problems, at each round $t \in [T]$, a user submits a query (context), and the decision maker (DM) sequentially selects a token irrevocably from a token set. Once the sequence is complete, the DM observes a random utility from the user, whose expectation is presented by a sequence function mapping the chosen token sequence to a nonnegative real value that depends on the query.
  In both problems, we first show that learning is impossible without any structure on the sequence function. We introduce a natural assumption, diminishing distance with more commons (DDMC), and propose algorithms with regret $\tilde&#123;O&#125;(L\sqrt&#123;T&#125;)$ and $\tilde&#123;O&#125;(L\sqrt&#123;T^&#123;2/3&#125;&#125;)$ for TLB and TMAB, respectively. As a side product, we obtain an (almost) optimality of the greedy decoding for LLM decoding algorithm under DDMC, which justifies the unresaonable effectiveness of greedy decoding in several tasks. This also has an immediate application to decoding-time LLM alignment, when the misaligned utility can be represented as the frozen LLM's utility and a linearly realizable latent function. We finally validate our algorithm's performance empirically as well as verify our assumptions using synthetic and real-world datasets.

### 摘要
我们提出了令牌化线性赌博机（TLB）和令牌化多臂赌博机（TMAB），这是受大语言模型解码和对齐启发而设计的线性与随机多臂赌博机问题变体。在这两类问题中，每轮$t \in [T]$用户提交查询（上下文）后，决策者（DM）需从令牌集中不可逆地逐个选择令牌。当序列生成完成后，DM将观测到来自用户的随机效用，其期望值由序列函数表示——该函数将选定令牌序列映射为依赖于查询的非负实数值。

针对这两个问题，我们首先证明若序列函数不具备任何结构约束则无法进行学习。为此我们提出了"公共信息递增则距离递减"（DDMC）的自然假设，并分别设计了具有$\tilde&#123;O&#125;(L\sqrt&#123;T&#125;)$和$\tilde&#123;O&#125;(L\sqrt&#123;T^&#123;2/3&#125;&#125;)$遗憾界的TLB与TMAB算法。作为副产品，我们证明了在DDMC假设下贪婪解码算法（几乎）达到最优，这解释了贪婪解码在多项任务中异常有效的现象。该结论可立即应用于解码时的大语言模型对齐问题——当未对齐效用可表示为冻结语言模型效用与线性可实现的潜在函数之和时。最后，我们通过合成数据集和真实数据集对算法性能进行实证验证，并检验了所提假设的合理性。

---

## [Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://arxiv.org/abs/2506.07311)

### Abstract
arXiv:2506.07311v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during long-context inference due to conventional handling of key-value (KV) caches. In this work, we introduce a novel integration of PagedAttention with PyTorch's FlexAttention, addressing internal fragmentation and inefficiencies associated with monolithic KV cache allocations. Implemented within IBM's Foundation Model Stack (FMS), our fused attention kernel efficiently gathers scattered KV data. Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced inference latency, growing only linearly (~2x) with sequence length from 128 to 2048 tokens when utilizing a global KV cache, compared to exponential latency increases without caching. While peak memory usage remains largely unchanged for single-step evaluations (dominated by model weights and activations), paged attention causes minimal incremental memory usage, observable only at sequence lengths exceeding 2048 tokens due to its power-of-two cache allocations. We open-source the full implementation and discuss its implications for future long-context model deployment.

### 摘要
大型语言模型（LLMs）在长上下文推理过程中，由于传统键值（KV）缓存处理方式而面临严重的内存效率低下问题。本研究提出了一种将PagedAttention与PyTorch的FlexAttention相结合的新方法，以解决单一KV缓存分配导致的内存碎片化和效率低下问题。该融合注意力内核在IBM基础模型堆栈（FMS）中实现，可高效收集分散的KV数据。我们在NVIDIA L4 GPU（24GB）上的测试表明，当使用全局KV缓存时，推理延迟显著降低——序列长度从128增长到2048个标记时，延迟仅呈线性增加（约2倍）；而未使用缓存时延迟呈指数级增长。尽管单步评估的峰值内存使用量（主要由模型权重和激活决定）基本保持不变，但分页注意力仅导致极小的增量内存占用，这一现象仅在序列长度超过2048个标记时（因其采用二次幂缓存分配）才可观测到。我们开源了完整实现方案，并探讨了其对未来长上下文模型部署的影响。

---

## [HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval](https://arxiv.org/abs/2506.07296)

### Abstract
arXiv:2506.07296v1 Announce Type: cross 
Abstract: We present HotelMatch-LLM, a multimodal dense retrieval model for the travel domain that enables natural language property search, addressing the limitations of traditional travel search engines which require users to start with a destination and editing search parameters. HotelMatch-LLM features three key innovations: (1) Domain-specific multi-task optimization with three novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model (SLM) for efficient online query processing and a large language model (LLM) for embedding hotel data; and (3) Extensive image processing to handle all property image galleries. Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, including VISTA and MARVEL. Specifically, on the test set -- main query type -- we achieve 0.681 for HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our analysis highlights the impact of our multi-task optimization, the generalizability of HotelMatch-LLM across LLM architectures, and its scalability for processing large image galleries.

### 摘要
我们提出HotelMatch-LLM，这是一个面向旅游领域的多模态稠密检索模型，能够实现自然语言属性搜索，解决了传统旅游搜索引擎需要用户先指定目的地并手动调整搜索参数的局限性。该模型具有三大关键创新：(1) 领域特定的多任务优化框架，包含检索、视觉和语言建模三项新颖目标；(2) 非对称稠密检索架构，结合小型语言模型(SLM)实现高效在线查询处理，以及大型语言模型(LLM)完成酒店数据嵌入；(3) 全面的图像处理能力以处理所有物业图库。在四个多样化测试集上的实验表明，HotelMatch-LLM显著优于VISTA和MARVEL等最先进模型。具体而言，在主查询类型的测试集上，本模型达到0.681的评分，而效果最佳的基线模型MARVEL仅为0.603。我们的分析突显了多任务优化的成效、模型在不同LLM架构间的泛化能力，以及处理大规模图库的可扩展性优势。

---

## [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)

### Abstract
arXiv:2506.07298v1 Announce Type: cross 
Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)$\unicode&#123;x2013&#125;$their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequences$\unicode&#123;x2013&#125;$an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.

### 摘要
隐马尔可夫模型（HMMs）是建模具有潜在马尔可夫结构的序列数据的基础工具，但将其拟合到现实世界数据仍存在计算挑战。本研究证明，预训练大型语言模型（LLMs）能够通过上下文学习（ICL）有效建模HMM生成的数据——这种能力使其能够从提示中的示例推断模式。在多样化的合成HMM数据集上，LLMs的预测准确率接近理论最优值。我们揭示了受HMM特性影响的新颖缩放趋势，并为这些实证观察提供了理论猜想。同时，我们为科研人员提供了使用ICL作为复杂数据诊断工具的实用指南。在真实世界动物决策任务中，ICL取得了与人类专家设计模型相竞争的性能。据我们所知，这是首次证明ICL能够学习并预测HMM生成的序列——这一进展深化了我们对LLMs上下文学习的理解，并确立了其作为揭示复杂科学数据隐藏结构的强大工具的潜力。

---

## [Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation](https://arxiv.org/abs/2506.07315)

### Abstract
arXiv:2506.07315v1 Announce Type: cross 
Abstract: Generative AI, particularly large language models (LLMs), is beginning to transform the financial industry by automating tasks and helping to make sense of complex financial information. One especially promising use case is the automatic creation of fundamental analysis reports, which are essential for making informed investment decisions, evaluating credit risks, guiding corporate mergers, etc. While LLMs attempt to generate these reports from a single prompt, the risks of inaccuracy are significant. Poor analysis can lead to misguided investments, regulatory issues, and loss of trust. Existing financial benchmarks mainly evaluate how well LLMs answer financial questions but do not reflect performance in real-world tasks like generating financial analysis reports. In this paper, we propose FinAR-Bench, a solid benchmark dataset focusing on financial statement analysis, a core competence of fundamental analysis. To make the evaluation more precise and reliable, we break this task into three measurable steps: extracting key information, calculating financial indicators, and applying logical reasoning. This structured approach allows us to objectively assess how well LLMs perform each step of the process. Our findings offer a clear understanding of LLMs current strengths and limitations in fundamental analysis and provide a more practical way to benchmark their performance in real-world financial settings.

### 摘要
生成式人工智能，尤其是大语言模型（LLMs），正通过自动化任务和辅助解析复杂金融信息的方式开始重塑金融行业。其中一项极具前景的应用是自动化生成基本面分析报告，这类报告对于形成投资决策、评估信用风险以及指导企业并购等至关重要。尽管LLMs尝试通过单一提示生成此类报告，但存在显著的准确性风险。低质量分析可能导致投资失误、监管问题及信任丧失。现有金融基准主要评估LLMs回答金融问题的能力，但未能反映其在生成财务分析报告等实际任务中的表现。本文提出FinAR-Bench——一个专注于财务报表分析的基准数据集，该能力是基本面分析的核心竞争力。为使评估更精确可靠，我们将该任务分解为三个可量化步骤：关键信息提取、财务指标计算和逻辑推理应用。这种结构化方法使我们能客观评估LLMs在每个流程步骤中的表现。研究结果清晰揭示了LLMs在基本面分析中的当前优势与局限，并为衡量其在实际金融场景中的表现提供了更具实践意义的基准方法。

---

## [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)

### Abstract
arXiv:2506.07326v1 Announce Type: cross 
Abstract: Reward modeling has emerged as a crucial component in aligning large language models with human values. Significant attention has focused on using reward models as a means for fine-tuning generative models. However, the reward models themselves -- which directly encode human value judgments by turning prompt-response pairs into scalar rewards -- remain relatively understudied. We present a novel approach to reward model interpretability through exhaustive analysis of their responses across their entire vocabulary space. By examining how different reward models score every possible single-token response to value-laden prompts, we uncover several striking findings: (i) substantial heterogeneity between models trained on similar objectives, (ii) systematic asymmetries in how models encode high- vs low-scoring tokens, (iii) significant sensitivity to prompt framing that mirrors human cognitive biases, and (iv) overvaluation of more frequent tokens. We demonstrate these effects across ten recent open-source reward models of varying parameter counts and architectures. Our results challenge assumptions about the interchangeability of reward models, as well as their suitability as proxies of complex and context-dependent human values. We find that these models can encode concerning biases toward certain identity groups, which may emerge as unintended consequences of harmlessness training -- distortions that risk propagating through the downstream large language models now deployed to millions.

### 摘要
奖励建模已成为将大型语言模型与人类价值观对齐的关键组件。现有研究主要关注将奖励模型作为生成模型微调的工具，而对奖励模型本身——通过将提示-响应对转化为标量奖励来直接编码人类价值判断的机制——仍缺乏深入探究。本研究提出了一种通过全面分析词汇空间响应来增强奖励模型可解释性的新方法。通过考察不同奖励模型如何对蕴含价值观的提示的所有可能单标记响应进行评分，我们发现了若干显著现象：（1）目标相似模型间存在显著异质性；（2）模型编码高/低分标记时存在系统性不对称；（3）对提示表述的敏感性反映出人类认知偏差；（4）对高频标记的过度估值。我们在十个不同参数量级和架构的开源奖励模型中验证了这些效应。研究结果挑战了关于奖励模型可互换性及其作为复杂情境依赖的人类价值观代理适用性的假设。研究发现这些模型可能编码针对特定身份群体的有害偏见，这些偏见可能源于无害性训练带来的非预期后果——此类扭曲风险将通过当前部署给数百万用户的下游大型语言模型持续传播。

---

## [Speech Recognition on TV Series with Video-guided Post-Correction](https://arxiv.org/abs/2506.07323)

### Abstract
arXiv:2506.07323v1 Announce Type: cross 
Abstract: Automatic Speech Recognition (ASR) has achieved remarkable success with deep learning, driving advancements in conversational artificial intelligence, media transcription, and assistive technologies. However, ASR systems still struggle in complex environments such as TV series, where overlapping speech, domain-specific terminology, and long-range contextual dependencies pose significant challenges to transcription accuracy. Existing multimodal approaches fail to correct ASR outputs with the rich temporal and contextual information available in video. To address this limitation, we propose a novel multimodal post-correction framework that refines ASR transcriptions by leveraging contextual cues extracted from video. Our framework consists of two stages: ASR Generation and Video-based Post-Correction, where the first stage produces the initial transcript and the second stage corrects errors using Video-based Contextual Information Extraction and Context-aware ASR Correction. We employ the Video-Large Multimodal Model (VLMM) to extract key contextual information using tailored prompts, which is then integrated with a Large Language Model (LLM) to refine the ASR output. We evaluate our method on a multimodal benchmark for TV series ASR and demonstrate its effectiveness in improving ASR performance by leveraging video-based context to enhance transcription accuracy in complex multimedia environments.

### 摘要
自动语音识别（ASR）凭借深度学习取得了显著成功，推动了会话人工智能、媒体转录和辅助技术的进步。然而，ASR系统在电视剧等复杂环境中仍面临挑战，其中重叠语音、领域特定术语和长程上下文依赖关系对转录准确性构成重大障碍。现有多模态方法未能利用视频中丰富的时序和上下文信息来修正ASR输出。为解决这一局限，我们提出了一种新颖的多模态后修正框架，通过利用视频提取的上下文线索优化ASR转录。该框架包含两个阶段：ASR生成和基于视频的后修正。第一阶段生成初始转录文本，第二阶段通过基于视频的上下文信息提取和上下文感知ASR修正来纠正错误。我们采用视频大型多模态模型（VLMM）通过定制化提示提取关键上下文信息，随后将其与大型语言模型（LLM）结合以优化ASR输出。我们在电视剧ASR的多模态基准上评估了该方法，结果表明通过利用视频上下文增强复杂多媒体环境下的转录准确性，该方法能有效提升ASR性能。

---

## [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)

### Abstract
arXiv:2506.07335v1 Announce Type: cross 
Abstract: Role-playing has emerged as an effective technique for enhancing the reasoning capabilities of large language models (LLMs). However, existing methods primarily rely on prompt engineering, which often lacks stability and interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing Steering (SRPS), a novel framework that identifies and manipulates internal model features associated with role-playing behavior. Our approach extracts latent representations from role-play prompts, selects the most relevant features based on activation patterns, and constructs a steering vector that can be injected into the model's residual stream with controllable intensity. Our method enables fine-grained control over role-specific behavior and offers insights into how role information influences internal model activations. Extensive experiments across various reasoning benchmarks and model sizes demonstrate consistent performance gains. Notably, in the zero-shot chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to 45.10%. These results highlight the potential of SRPS to enhance reasoning ability in LLMs, providing better interpretability and stability compared to traditional prompt-based role-playing.

### 摘要
角色扮演已成为增强大语言模型（LLMs）推理能力的有效技术。然而，现有方法主要依赖提示工程，往往缺乏稳定性和可解释性。本文提出稀疏自编码器角色扮演导向（SRPS）这一新型框架，通过识别和操控与角色扮演行为相关的内部模型特征来实现优化。我们的方法从角色扮演提示中提取潜在表征，根据激活模式选择最相关特征，并构建可调控强度的导向向量注入模型残差流。该方法能实现角色特异性行为的细粒度控制，并揭示角色信息如何影响内部模型激活。跨多种推理基准和模型规模的实验表明，该方法能持续提升性能。值得注意的是，在零样本思维链（CoT）设置下，Llama3.1-8B在CSQA上的准确率从31.86%提升至39.80%，Gemma2-9B在SVAMP上的准确率从37.50%提升至45.10%。这些结果凸显了SRPS在增强LLMs推理能力方面的潜力，相比传统基于提示的角色扮演方法具有更好的可解释性和稳定性。

---

## [InverseScope: Scalable Activation Inversion for Interpreting Large Language Models](https://arxiv.org/abs/2506.07406)

### Abstract
arXiv:2506.07406v1 Announce Type: cross 
Abstract: Understanding the internal representations of large language models (LLMs) is a central challenge in interpretability research. Existing feature interpretability methods often rely on strong assumptions about the structure of representations that may not hold in practice. In this work, we introduce InverseScope, an assumption-light and scalable framework for interpreting neural activations via input inversion. Given a target activation, we define a distribution over inputs that generate similar activations and analyze this distribution to infer the encoded features. To address the inefficiency of sampling in high-dimensional spaces, we propose a novel conditional generation architecture that significantly improves sample efficiency compared to previous methods. We further introduce a quantitative evaluation protocol that tests interpretability hypotheses using feature consistency rate computed over the sampled inputs. InverseScope scales inversion-based interpretability methods to larger models and practical tasks, enabling systematic and quantitative analysis of internal representations in real-world LLMs.

### 摘要
理解大型语言模型（LLMs）的内部表征是可解释性研究的核心挑战。现有特征解释方法通常依赖于对表征结构的强假设，这些假设在实践中可能并不成立。本研究提出InverseScope，一种假设宽松且可扩展的框架，通过输入反演来解读神经激活。给定目标激活，我们定义了能产生相似激活的输入分布，并通过分析该分布来推断编码特征。针对高维空间采样效率低下的问题，我们提出了一种新颖的条件生成架构，相比先前方法显著提升了采样效率。我们进一步引入定量评估协议，利用采样输入计算的特征一致性率来验证可解释性假设。InverseScope将基于反演的解释方法扩展至更大模型和实际任务，实现了对现实世界LLMs内部表征的系统化定量分析。

---

## [JavelinGuard: Low-Cost Transformer Architectures for LLM Security](https://arxiv.org/abs/2506.07330)

### Abstract
arXiv:2506.07330v1 Announce Type: cross 
Abstract: We present JavelinGuard, a suite of low-cost, high-performance model architectures designed for detecting malicious intent in Large Language Model (LLM) interactions, optimized specifically for production deployment. Recent advances in transformer architectures, including compact BERT(Devlin et al. 2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build highly accurate classifiers with as few as approximately 400M parameters that achieve rapid inference speeds even on standard CPU hardware. We systematically explore five progressively sophisticated transformer-based architectures: Sharanga (baseline transformer classifier), Mahendra (enhanced attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid neural ensemble architectures), and Raudra (an advanced multi-task framework with specialized loss functions). Our models are rigorously benchmarked across nine diverse adversarial datasets, including popular sets like the NotInject series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly introduced JavelinBench, specifically crafted to test generalization on challenging borderline and hard-negative cases. Additionally, we compare our architectures against leading open-source guardrail models as well as large decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance trade-offs in terms of accuracy, and latency. Our findings reveal that while Raudra's multi-task design offers the most robust performance overall, each architecture presents unique trade-offs in speed, interpretability, and resource requirements, guiding practitioners in selecting the optimal balance of complexity and efficiency for real-world LLM security applications.

### 摘要
我们提出JavelinGuard——一套专为生产环境优化、用于检测大语言模型（LLM）交互中恶意意图的低成本高性能模型架构。基于Transformer架构的最新进展（包括紧凑型BERT变体如ModernBERT），我们构建了仅需约4亿参数即可实现高精度的分类器，即使在标准CPU硬件上也能获得快速推理速度。我们系统探索了五种渐进式复杂度的基于Transformer的架构：Sharanga（基线Transformer分类器）、Mahendra（采用更深层注意力加权池化）、Vaishnava与Ashwina（混合神经集成架构）以及Raudra（配备专用损失函数的先进多任务框架）。这些模型在九个多样化对抗数据集上经过严格基准测试，包括NotInject系列、BIPIA、Garak、ImprovedLLM、ToxicChat、WildGuard等主流数据集，以及我们新开发的专门用于挑战性边界案例和困难负样本泛化测试的JavelinBench。通过与领先的开源防护模型及GPT-4o等纯解码器LLM的对比实验，我们的架构在准确性与延迟方面展现出更优的性价比。研究发现：虽然Raudra的多任务设计具有最全面的鲁棒性能，但每种架构在速度、可解释性和资源需求方面存在独特权衡，可为实际LLM安全应用中复杂度与效率的平衡选择提供实践指导。

---

## [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)

### Abstract
arXiv:2506.07416v1 Announce Type: cross 
Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline specifically optimized for deployment on embedded devices, such as those used in robotics and autonomous driving. The pipeline significantly reduces the computational overhead by jointly leveraging patch selection to filter irrelevant camera views, a token selection module to reduce input sequence length for the LLM, and speculative decoding to accelerate token generation. Evaluation on the NVIDIA DRIVE Thor platform for automonous driving application, our pipeline achieves $2.5\times$ end-to-end latency reduction without compromising task accuracy. The speed-up further increases to $3.2\times$ when applying FP8 post-training quantization. These results demonstrate our pipeline as a viable solution for enabling real-time VLM deployment in resource-constrained environments.

### 摘要
本文介绍了一种专为机器人及自动驾驶等嵌入式设备部署优化的高效视觉-语言模型（VLM）处理流程。该流程通过联合采用以下三项技术显著降低计算开销：基于图像块选择过滤无关摄像头视图、通过令牌选择模块缩短大语言模型输入序列长度，以及运用推测式解码加速令牌生成。在英伟达DRIVE Thor自动驾驶平台上的评估表明，该流程在保证任务精度的同时实现了2.5倍的端到端延迟降低。当应用FP8训练后量化时，加速效果进一步提升至3.2倍。这些结果证明本流程可作为资源受限环境下实现实时VLM部署的有效解决方案。

---

## [Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM](https://arxiv.org/abs/2506.07407)

### Abstract
arXiv:2506.07407v1 Announce Type: cross 
Abstract: With the rapid development of multi-cloud environments, it is increasingly important to ensure the security and reliability of intelligent monitoring systems. In this paper, we propose an anomaly detection and early warning mechanism for intelligent monitoring system in multi-cloud environment based on Large-Scale Language Model (LLM). On the basis of the existing monitoring framework, the proposed model innovatively introduces a multi-level feature extraction method, which combines the natural language processing ability of LLM with traditional machine learning methods to enhance the accuracy of anomaly detection and improve the real-time response efficiency. By introducing the contextual understanding capabilities of LLMs, the model dynamically adapts to different cloud service providers and environments, so as to more effectively detect abnormal patterns and predict potential failures. Experimental results show that the proposed model is significantly better than the traditional anomaly detection system in terms of detection accuracy and latency, and significantly improves the resilience and active management ability of cloud infrastructure.

### 摘要
随着多云环境的快速发展，确保智能监控系统的安全性与可靠性变得日益重要。本文提出一种基于大规模语言模型（LLM）的多云环境智能监控系统异常检测与预警机制。该模型在现有监控框架基础上，创新性地引入多级特征提取方法，将LLM的自然语言处理能力与传统机器学习方法相结合，以提升异常检测的准确性并改善实时响应效率。通过引入LLM的上下文理解能力，该模型能动态适配不同云服务提供商和环境，从而更有效地检测异常模式并预测潜在故障。实验结果表明，所提模型在检测精度和延迟方面显著优于传统异常检测系统，并大幅提升了云基础设施的韧性与主动管理能力。

---

## [MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems](https://arxiv.org/abs/2506.07399)

### Abstract
arXiv:2506.07399v1 Announce Type: cross 
Abstract: Multimodal retrieval-augmented generation (RAG) systems enhance large vision-language models by integrating cross-modal knowledge, enabling their increasing adoption across real-world multimodal tasks. These knowledge databases may contain sensitive information that requires privacy protection. However, multimodal RAG systems inherently grant external users indirect access to such data, making them potentially vulnerable to privacy attacks, particularly membership inference attacks (MIAs). % Existing MIA methods targeting RAG systems predominantly focus on the textual modality, while the visual modality remains relatively underexplored. To bridge this gap, we propose MrM, the first black-box MIA framework targeted at multimodal RAG systems. It utilizes a multi-object data perturbation framework constrained by counterfactual attacks, which can concurrently induce the RAG systems to retrieve the target data and generate information that leaks the membership information. Our method first employs an object-aware data perturbation method to constrain the perturbation to key semantics and ensure successful retrieval. Building on this, we design a counterfact-informed mask selection strategy to prioritize the most informative masked regions, aiming to eliminate the interference of model self-knowledge and amplify attack efficacy. Finally, we perform statistical membership inference by modeling query trials to extract features that reflect the reconstruction of masked semantics from response patterns. Experiments on two visual datasets and eight mainstream commercial visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves consistently strong performance across both sample-level and set-level evaluations, and remains robust under adaptive defenses.

### 摘要
多模态检索增强生成（RAG）系统通过整合跨模态知识增强大型视觉语言模型，使其在现实世界多模态任务中的应用日益广泛。这些知识库可能包含需要隐私保护的敏感信息。然而，多模态RAG系统本质上允许外部用户间接访问此类数据，使其可能面临隐私攻击风险，尤其是成员推理攻击（MIA）。现有针对RAG系统的MIA方法主要集中于文本模态，而视觉模态的研究相对不足。为填补这一空白，我们提出首个面向多模态RAG系统的黑盒MIA框架MrM。该框架采用受反事实攻击约束的多目标数据扰动机制，能同时诱导RAG系统检索目标数据并生成泄露成员信息的响应。我们的方法首先运用目标感知数据扰动技术，将扰动限制在关键语义区域以确保成功检索；在此基础上设计反事实引导的掩码选择策略，优先处理信息量最大的掩码区域，旨在消除模型自有知识的干扰并增强攻击效果；最后通过建模查询试验的统计特征，从响应模式中提取反映掩码语义重建的特征进行成员推理。在两个视觉数据集和八种主流商用视觉语言模型（如GPT-4o、Gemini-2）上的实验表明，MrM在样本级和集合级评估中均表现优异，且在自适应防御下保持稳健性。

---

## [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)

### Abstract
arXiv:2506.07424v1 Announce Type: cross 
Abstract: Large language models (LLMs) are renowned for their extensive linguistic knowledge and strong generalization capabilities, but their high computational demands make them unsuitable for resource-constrained environments. In contrast, small language models (SLMs) are computationally efficient but often lack the broad generalization capacity of LLMs. To bridge this gap, we propose PiFi, a novel framework that combines the strengths of both LLMs and SLMs to achieve high performance while maintaining efficiency. PiFi integrates a single frozen layer from an LLM into a SLM and fine-tunes the combined model for specific tasks, boosting performance without a significant increase in computational cost. We show that PiFi delivers consistent performance improvements across a range of natural language processing tasks, including both natural language understanding and generation. Moreover, our findings demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing generalization to unseen domains and facilitating the transfer of linguistic abilities.

### 摘要
大语言模型（LLMs）以其广泛的语言知识和强大的泛化能力著称，但其高计算需求使其难以在资源受限的环境中部署。相比之下，小语言模型（SLMs）计算效率高，但通常缺乏LLMs的广泛泛化能力。为弥合这一差距，我们提出PiFi——一种新颖框架，通过结合LLMs和SLMs的优势，在保持高效的同时实现高性能。PiFi将LLM的单个冻结层集成到SLM中，并对组合模型进行特定任务微调，从而在不显著增加计算成本的前提下提升性能。实验表明，PiFi在一系列自然语言处理任务（包括自然语言理解和生成）中均能带来稳定的性能提升。此外，我们的研究证实PiFi能有效利用LLM知识，增强对未见领域的泛化能力，并促进语言能力的迁移。

---

## [Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs](https://arxiv.org/abs/2506.07448)

### Abstract
arXiv:2506.07448v1 Announce Type: cross 
Abstract: Although large language models (LLMs) are highly interactive and extendable, current approaches to ensure reliability in deployments remain mostly limited to rejecting outputs with high uncertainty in order to avoid misinformation. This conservative strategy reflects the current lack of tools to systematically distinguish and respond to different sources of uncertainty. In this paper, we advocate for the adoption of Bayesian Modeling of Experiments -- a framework that provides a coherent foundation to reason about uncertainty and clarify the reducibility of uncertainty -- for managing and proactively addressing uncertainty that arises in LLM deployments. This framework enables LLMs and their users to take contextually appropriate steps, such as requesting clarification, retrieving external information, or refining inputs. By supporting active resolution rather than passive avoidance, it opens the door to more reliable, transparent, and broadly applicable LLM systems, particularly in high-stakes, real-world settings.

### 摘要
尽管大型语言模型（LLMs）具有高度交互性和可扩展性，但目前确保部署可靠性的方法仍主要局限于拒绝具有高不确定性的输出以避免错误信息。这种保守策略反映了当前缺乏系统性工具来区分和应对不同来源的不确定性。本文主张采用实验贝叶斯建模框架——该框架为不确定性推理和澄清不确定性的可减少性提供了连贯基础——以管理和主动解决LLM部署中出现的不确定性。该框架使LLMs及其用户能够采取情境适宜的步骤，例如请求澄清、检索外部信息或优化输入。通过支持主动解决而非被动回避，它为构建更可靠、透明且广泛适用的LLM系统开辟了道路，尤其在高风险的实际应用场景中。

---

## [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)

### Abstract
arXiv:2506.07434v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.

### 摘要
大型语言模型（LLMs）需与人类偏好对齐以避免生成冒犯性、虚假或无意义内容。近期低资源LLM对齐方法受到关注，但仍面临同时获取高质量与对齐内容的挑战。基于解码初期生成对齐响应难度较高的观察，我们提出弱到强解码（WSD）新框架，通过小型对齐模型的引导增强基础模型的对齐能力。该框架先由小模型起草对齐良好的起始部分，再通过设计的自动切换机制控制大型基础模型完成后续内容。我们收集了新数据集GenerAlign，用于微调小型Pilot-3B作为起草模型，有效提升WSD框架下不同基础模型的性能，使其超越所有基线方法，同时避免下游任务性能下降（即对齐税）。进一步实验验证了不同设置与时间效率的影响，并对WSD的内在机制进行了深入分析。

---

## [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)

### Abstract
arXiv:2506.07463v1 Announce Type: cross 
Abstract: We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly $35$ TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract $4.5$ billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora.

---

## [Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs](https://arxiv.org/abs/2506.07454)

### Abstract
arXiv:2506.07454v1 Announce Type: cross 
Abstract: In this paper, we introduce a multi-robot system that integrates mapping, localization, and task and motion planning (TAMP) enabled by 3D scene graphs to execute complex instructions expressed in natural language. Our system builds a shared 3D scene graph incorporating an open-set object-based map, which is leveraged for multi-robot 3D scene graph fusion. This representation supports real-time, view-invariant relocalization (via the object-based map) and planning (via the 3D scene graph), allowing a team of robots to reason about their surroundings and execute complex tasks. Additionally, we introduce a planning approach that translates operator intent into Planning Domain Definition Language (PDDL) goals using a Large Language Model (LLM) by leveraging context from the shared 3D scene graph and robot capabilities. We provide an experimental assessment of the performance of our system on real-world tasks in large-scale, outdoor environments.

### 摘要
本文提出一种基于三维场景图的多机器人系统，该系统整合了建图、定位以及任务与运动规划（TAMP）功能，能够执行自然语言表达的复杂指令。该系统构建包含开放集物体地图的共享三维场景图，支持多机器人三维场景图融合。该表征方式通过物体地图实现实时、视角无关的重定位，并依托三维场景图进行规划，使机器人团队能够对环境进行推理并执行复杂任务。此外，我们提出一种规划方法，利用大型语言模型（LLM）结合共享三维场景图上下文和机器人能力，将操作者意图转化为规划域定义语言（PDDL）目标。我们在大型户外真实环境任务中对系统性能进行了实验评估。

---

## [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)

### Abstract
arXiv:2506.07436v1 Announce Type: cross 
Abstract: The recent emergence of multimodal large language models (LLMs) has introduced new opportunities for improving visual hazard recognition on construction sites. Unlike traditional computer vision models that rely on domain-specific training and extensive datasets, modern LLMs can interpret and describe complex visual scenes using simple natural language prompts. However, despite growing interest in their applications, there has been limited investigation into how different LLMs perform in safety-critical visual tasks within the construction domain. To address this gap, this study conducts a comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify potential hazards from real-world construction images. Each model was tested under three prompting strategies: zero-shot, few-shot, and chain-of-thought (CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated basic safety context and a hazard source mnemonic, and CoT provided step-by-step reasoning examples to scaffold model thinking. Quantitative analysis was performed using precision, recall, and F1-score metrics across all conditions. Results reveal that prompting strategy significantly influenced performance, with CoT prompting consistently producing higher accuracy across models. Additionally, LLM performance varied under different conditions, with GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also demonstrate the critical role of prompt design in enhancing the accuracy and consistency of multimodal LLMs for construction safety applications. This study offers actionable insights into the integration of prompt engineering and LLMs for practical hazard recognition, contributing to the development of more reliable AI-assisted safety systems.

### 摘要
近年来，多模态大语言模型（LLMs）的出现为提升建筑工地视觉危险识别带来了新机遇。与传统依赖领域特定训练和海量数据的计算机视觉模型不同，现代LLMs能够通过简单自然语言指令解析和描述复杂视觉场景。然而尽管应用兴趣日益增长，针对不同LLMs在建筑领域安全关键视觉任务中表现的系统研究仍显不足。为填补这一空白，本研究对五种前沿LLMs（Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3和Gemini 2.0 Pro）进行了比较评估，测试其从真实建筑图像中识别潜在危险的能力。每个模型在三种提示策略下进行测试：零样本、少样本和思维链（CoT）。零样本提示仅含基础指令，少样本提示结合基础安全背景和危险源记忆法，CoT提示则提供分步推理示例以构建模型思维。通过精确率、召回率和F1分数对所有条件进行定量分析。结果表明提示策略显著影响性能，CoT提示在各模型中均能产生更高准确率。不同条件下LLMs表现存在差异，GPT-4.5和GPT-o3在多数场景中表现最优。研究还证实提示设计对提升多模态LLMs在建筑安全应用中的准确性和一致性具有关键作用。本研究为提示工程与LLMs在实际危险识别中的整合提供了可行见解，有助于开发更可靠的人工智能辅助安全系统。

---

## [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464)

### Abstract
arXiv:2506.07464v1 Announce Type: cross 
Abstract: Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks.

### 摘要
近期研究表明，基于强化学习（RL）的后期训练能有效增强大语言模型（LLMs）的推理能力。其中，群体相对策略优化（GRPO）通过采用PPO风格强化算法结合群体归一化奖励，展现出显著成效。然而，GRPO在视频大语言模型（Video LLMs）中的应用研究仍较匮乏。本文探究GRPO在视频LLMs中的适用性，发现阻碍其有效学习的两大核心问题：（1）对安全机制的依赖；（2）优势消失问题。为应对这些挑战，我们提出DeepVideo-R1模型——采用新型回归式GRPO（Reg-GRPO）与难度感知数据增强策略训练的视频大语言模型。Reg-GRPO将GRPO目标重构为回归任务，直接预测GRPO中的优势值。该设计摒弃了裁剪函数和最小值函数等安全机制，通过使模型与优势值对齐来实现更直接的策略指导。同时，我们设计的难度感知数据增强策略能动态生成可解难度级别的训练样本，从而产生多样化且信息丰富的奖励信号。综合实验表明，DeepVideo-R1在多项视频推理基准测试中显著提升了视频推理性能。

---

## [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)

### Abstract
arXiv:2506.07452v1 Announce Type: cross 
Abstract: Large language models (LLMs) can be prompted with specific styles (e.g., formatting responses as lists), including in jailbreak queries. Although these style patterns are semantically unrelated to the malicious intents behind jailbreak queries, their safety impact remains unclear. In this work, we seek to understand whether style patterns compromise LLM safety, how superficial style alignment increases model vulnerability, and how best to mitigate these risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks, and find that malicious queries with style patterns inflate the attack success rate (ASR) for nearly all models. Notably, ASR inflation correlates with both the length of style patterns and the relative attention an LLM exhibits on them. We then investigate superficial style alignment, and find that fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of those same styles. Finally, we propose SafeStyle, a defense strategy that incorporates a small amount of safety training data augmented to match the distribution of style patterns in the fine-tuning data. Across three LLMs and five fine-tuning style settings, SafeStyle consistently outperforms baselines in maintaining LLM safety.

### 摘要
大型语言模型（LLMs）能够通过特定风格（如将回答格式化为列表）进行提示，包括在越狱查询中。尽管这些风格模式与越狱查询背后的恶意意图在语义上无关，但其对安全性的影响尚不明确。本研究旨在探究风格模式是否会破坏LLM的安全性、表层风格对齐如何增加模型脆弱性，以及在对齐过程中如何最佳地缓解这些风险。我们在七个越狱基准测试中评估了32个LLM，发现带有风格模式的恶意查询会提高几乎所有模型的攻击成功率（ASR）。值得注意的是，ASR的上升与风格模式的长度及LLM对其表现的相对注意力均呈相关性。随后，我们研究了表层风格对齐，发现使用特定风格进行微调会使LLMs更容易受到相同风格越狱攻击的影响。最后，我们提出了SafeStyle防御策略，该策略通过纳入少量与微调数据中风格模式分布匹配的安全训练数据来增强模型。在三个LLM和五种微调风格设置中，SafeStyle在维护LLM安全性方面始终优于基线方法。

---

## [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)

### Abstract
arXiv:2506.07458v1 Announce Type: cross 
Abstract: Characterizing a large language model's (LLM's) knowledge of a given question is challenging. As a result, prior work has primarily examined LLM behavior under knowledge conflicts, where the model's internal parametric memory contradicts information in the external context. However, this does not fully reflect how well the model knows the answer to the question. In this paper, we first introduce a taxonomy of five knowledge statuses based on the consistency and correctness of LLM knowledge modes. We then propose KScope, a hierarchical framework of statistical tests that progressively refines hypotheses about knowledge modes and characterizes LLM knowledge into one of these five statuses. We apply KScope to nine LLMs across four datasets and systematically establish: (1) Supporting context narrows knowledge gaps across models. (2) Context features related to difficulty, relevance, and familiarity drive successful knowledge updates. (3) LLMs exhibit similar feature preferences when partially correct or conflicted, but diverge sharply when consistently wrong. (4) Context summarization constrained by our feature analysis, together with enhanced credibility, further improves update effectiveness and generalizes across LLMs.

### 摘要
评估大语言模型（LLM）对特定问题的知识掌握程度具有挑战性。因此，先前研究主要考察了知识冲突下LLM的行为表现，即模型内部参数化记忆与外部上下文信息相矛盾的情况。然而，这并不能全面反映模型对问题答案的真实认知水平。本文首先提出基于LLM知识模式一致性与正确性的五类知识状态分类体系，随后设计KScope框架——一种通过逐级细化知识模式假设的层级统计检验方法，将LLM知识精准归类至上述五种状态之一。我们在四个数据集上对九种LLM应用KScope进行系统分析，发现：（1）支持性上下文能有效缩小模型间的知识差距；（2）与问题难度、相关性和熟悉度相关的上下文特征是促成知识成功更新的关键驱动因素；（3）当处于部分正确或冲突状态时，LLMs表现出相似的特征偏好，但在持续错误时则呈现显著差异；（4）基于特征分析的上下文摘要约束结合可信度增强策略，能进一步提升知识更新效果，且该策略在不同LLMs间具有普适性。

---

## [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)

### Abstract
arXiv:2506.07501v1 Announce Type: cross 
Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies only on the information of the previous subchain and may lose long-range dependencies due to the causal mask blocking the global context flow between multi-level subchains, this work proposes a graph of causal evolution (GoCE). Its core principle is to map the implicit token representation into a differentiable and sparse causal adjacency matrix, then permeate causal constraints through each layer of calculation using causal-masked attention and causal-MoE. By combining intervention consistency loss test and self-evolution gate, the dynamic balance between causal structure learning and adaptive updating of transformer architecture is realized. The researcher built experimental environments in sandboxes built with Claude Sonnet 4, o4-mini-high, and DeepSeek R1 respectively with the transformer variant architecture introduced in GoCE. It is evaluated on publicly available datasets including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the baseline LLMs. The finding proves that GoCE strengthens the transformer's ability to capture long-range causal dependencies, while the ability to self-evolve is improved. It not only surpasses the design of CoM in terms of design principles, but also provides experience for future research on causal learning and continuous adaptive improvement.

### 摘要
针对链式模型(CoM)中各子链仅依赖前序子链信息且因果掩码可能阻断多级子链间全局上下文流动而导致长程依赖缺失的问题，本研究提出因果演化图(GoCE)。其核心原理是将隐式令牌表征映射为可微分稀疏因果邻接矩阵，通过因果掩码注意力和因果MoE在每层计算中渗透因果约束。结合干预一致性损失检验与自演化门控机制，实现了因果结构学习与Transformer架构自适应更新的动态平衡。研究者在Claude Sonnet 4、o4-mini-high和DeepSeek R1构建的沙箱环境中，采用GoCE提出的Transformer变体架构，在CLUTRR、CLADDER、EX-FEVER和CausalQA等公开数据集上进行评估并与基线大语言模型对比。实验证明GoCE增强了Transformer捕捉长程因果依赖的能力，同时提升了自演化性能，不仅在设计理念上超越了CoM架构，更为因果学习与持续自适应改进的未来研究提供了经验。

---

## [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)

### Abstract
arXiv:2506.07468v1 Announce Type: cross 
Abstract: Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).

### 摘要
传统语言模型（LM）的安全对齐采用被动割裂的流程：攻击者利用静态模型发起攻击，随后通过防御性微调修补暴露的漏洞。这种串行方法导致攻防错位——攻击者过度适应过时防御，而防御者始终滞后于新兴威胁。为此，我们提出Self-RedTeam算法，这是一种通过持续交互实现攻防智能体协同进化的在线自博弈强化学习方法。我们将安全对齐建模为二人零和博弈，其中单一模型在攻击者与防御者角色间切换——既生成对抗性提示又防范此类攻击——同时由奖励语言模型判定结果。该方法实现了动态协同适应。基于零和博弈的博弈论框架，我们建立了理论安全保证：若自博弈收敛至纳什均衡，防御者将始终对任何对抗输入生成安全响应。实验表明，与静态防御训练的攻击者相比，Self-RedTeam能发现更多样化的攻击（SBERT指标提升21.8%）；在安全基准测试中（如WildJailBreak提升65.5%），其防御鲁棒性优于静态攻击训练的防御者。我们还提出隐藏思维链技术，使智能体可秘密规划策略，从而提升对抗多样性并减少过度拒绝。本研究推动语言模型安全训练从被动修补转向主动协同进化，通过多智能体强化学习（MARL）实现可扩展、自主且鲁棒的自我提升。

---

## [LeVo: High-Quality Song Generation with Multi-Preference Alignment](https://arxiv.org/abs/2506.07520)

### Abstract
arXiv:2506.07520v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) and audio language models have significantly improved music generation, particularly in lyrics-to-song generation. However, existing approaches still struggle with the complex composition of songs and the scarcity of high-quality data, leading to limitations in sound quality, musicality, instruction following, and vocal-instrument harmony. To address these challenges, we introduce LeVo, an LM-based framework consisting of LeLM and a music codec. LeLM is capable of parallelly modeling two types of tokens: mixed tokens, which represent the combined audio of vocals and accompaniment to achieve vocal-instrument harmony, and dual-track tokens, which separately encode vocals and accompaniment for high-quality song generation. It employs two decoder-only transformers and a modular extension training strategy to prevent interference between different token types. To further enhance musicality and instruction following, we introduce a multi-preference alignment method based on Direct Preference Optimization (DPO). This method handles diverse human preferences through a semi-automatic data construction process and DPO post-training. Experimental results demonstrate that LeVo consistently outperforms existing methods on both objective and subjective metrics. Ablation studies further justify the effectiveness of our designs. Audio examples are available at https://levo-demo.github.io/.

### 摘要
大语言模型（LLMs）与音频语言模型的最新进展显著提升了音乐生成能力，尤其在歌词到歌曲生成领域。然而，现有方法仍面临歌曲复杂构成与高质量数据稀缺的挑战，导致音质、音乐性、指令跟随及人声-乐器和谐度等方面存在局限。为解决这些问题，我们提出LeVo——一个基于语言模型的框架，包含LeLM与音乐编解码器。LeLM能够并行建模两种类型的标记：混合标记（代表人声与伴奏的混合音频以实现声乐和谐）和双轨标记（分别编码人声与伴奏以生成高质量歌曲）。该框架采用两个仅含解码器的Transformer模块及模块化扩展训练策略，以避免不同标记类型间的干扰。为进一步增强音乐性与指令跟随能力，我们提出基于直接偏好优化（DPO）的多偏好对齐方法，通过半自动数据构建流程和DPO后训练处理多样化的人类偏好。实验结果表明，LeVo在客观与主观指标上均持续优于现有方法。消融研究进一步验证了我们设计的有效性。音频示例请访问https://levo-demo.github.io/。

---

## [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)

### Abstract
arXiv:2506.07583v1 Announce Type: cross 
Abstract: Despite the popularity of the large language models (LLMs), their application to machine translation is relatively underexplored, especially in context-aware settings. This work presents a literature review of context-aware translation with LLMs. The existing works utilise prompting and fine-tuning approaches, with few focusing on automatic post-editing and creating translation agents for context-aware machine translation. We observed that the commercial LLMs (such as ChatGPT and Tower LLM) achieved better results than the open-source LLMs (such as Llama and Bloom LLMs), and prompt-based approaches serve as good baselines to assess the quality of translations. Finally, we present some interesting future directions to explore.

### 摘要
尽管大语言模型（LLMs）广受欢迎，但其在机器翻译中的应用仍相对不足，尤其在上下文感知场景中。本文献综述探讨了基于LLMs的上下文感知翻译研究现状。现有研究主要采用提示工程与微调方法，鲜少聚焦于自动后编辑技术及构建上下文感知机器翻译代理系统。研究发现，商用LLMs（如ChatGPT和Tower LLM）的翻译质量优于开源模型（如Llama和Bloom LLMs），而基于提示的方法可作为评估翻译质量的有效基线。最后，本文提出了若干值得探索的未来研究方向。

---

## [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)

### Abstract
arXiv:2506.07557v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have achieved remarkable success in a wide range of applications, their performance often degrades in complex reasoning tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to enhance LLM reasoning without relying on external reward models. By redefining the Upper Confidence Bound scoring to align with intrinsic self-evaluation capabilities of LLMs and decomposing the inference process into atomic subtasks augmented with semantic clustering at each node, SELT effectively balances exploration and exploitation, reduces redundant reasoning paths, and mitigates hallucination. We validate our approach on challenging benchmarks, including the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT achieves significant improvements in answer accuracy and reasoning robustness compared to baseline methods. Notably, our framework operates without task-specific fine-tuning, demonstrating strong generalizability across diverse reasoning tasks. Relevant results and code are available at https://github.com/fairyshine/SELT .

### 摘要
虽然大型语言模型（LLMs）在广泛应用中取得了显著成功，但其在复杂推理任务中的表现往往欠佳。本研究提出SELT（自评估型LLM树状搜索）框架，该创新方法通过改进的蒙特卡洛树搜索（MCTS）增强LLM推理能力，且无需依赖外部奖励模型。通过重构置信上限评分机制以适配LLMs的固有自评估能力，并在每个节点将推理过程分解为结合语义聚类的原子子任务，SELT有效平衡了探索与利用、减少了冗余推理路径并缓解了幻觉现象。我们在知识型基准MMLU和工具学习数据集Seal-Tools等挑战性测试中验证了该方法，相较于基线模型，SELT在答案准确性和推理鲁棒性方面均实现显著提升。值得注意的是，该框架无需任务特定微调，在不同推理任务中展现出强大的泛化能力。相关成果与代码详见https://github.com/fairyshine/SELT。

---

## [IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents](https://arxiv.org/abs/2506.07524)

### Abstract
arXiv:2506.07524v1 Announce Type: cross 
Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking APIs through natural language instructions. While powerful, they often suffer from misinterpretation of user intent, leading to the agent's actions that diverge from the user's intended goal, especially as external toolkits evolve. Traditional software testing assumes structured inputs and thus falls short in handling the ambiguity of natural language. We introduce IntenTest, an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents. Unlike prior work focused on fixed benchmarks or adversarial inputs, IntenTest generates realistic tasks based on toolkits' documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. To guide testing, we propose semantic partitioning, which organizes natural language tasks into meaningful categories based on toolkit API parameters and their equivalence classes. Within each partition, seed tasks are mutated and ranked by a lightweight predictor that estimates the likelihood of triggering agent errors. To enhance efficiency, IntenTest maintains a datatype-aware strategy memory that retrieves and adapts effective mutation patterns from past cases. Experiments on 80 toolkit APIs demonstrate that IntenTest effectively uncovers intent integrity violations, significantly outperforming baselines in both error-exposing rate and query efficiency. Moreover, IntenTest generalizes well to stronger target models using smaller LLMs for test generation, and adapts to evolving APIs across domains.

### 摘要
大语言模型（LLM）智能体正越来越多地通过自然语言指令调用API来自动化现实世界任务。尽管功能强大，这些智能体常因对用户意图的误解而导致其行为偏离用户预期目标，尤其是在外部工具包不断演变的场景下。传统软件测试方法假设输入具有结构化特征，因而难以处理自然语言的歧义性。我们提出IntenTest——一个以API为核心的压力测试框架，该系统化地揭示LLM智能体中的意图完整性违规现象。与先前专注于固定基准测试或对抗性输入的研究不同，IntenTest基于工具包文档生成真实任务，并通过定向变异在保持用户意图的前提下暴露智能体的细微错误。为指导测试过程，我们提出语义分区技术，该技术根据工具包API参数及其等价类将自然语言任务组织为有意义的类别。在每个分区内，种子任务经过变异后由轻量级预测器进行排序，该预测器可评估触发智能体错误的概率。为提升效率，IntenTest采用数据类型感知的策略记忆机制，从历史案例中检索并适配有效的变异模式。在80个工具包API上的实验表明，IntenTest能有效发现意图完整性违规，在错误暴露率和查询效率方面显著优于基线方法。此外，IntenTest展现出良好的泛化能力：使用较小LLM生成测试时仍能适配更强目标模型，并适应跨领域演进的API。

---

## [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)

### Abstract
arXiv:2506.07551v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated promising capabilities in chemistry tasks while still facing challenges due to outdated pretraining knowledge and the difficulty of incorporating specialized chemical expertise. To address these issues, we propose an LLM-based agent that synergistically integrates 137 external chemical tools created ranging from basic information retrieval to complex reaction predictions, and a dataset curation pipeline to generate the dataset ChemToolBench that facilitates both effective tool selection and precise parameter filling during fine-tuning and evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework, enabling independent optimization of tool planning and execution. By leveraging self-generated data, our approach supports step-level fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM that surpass GPT-4o. Experimental evaluations demonstrate that our approach significantly improves performance in Chemistry QA and discovery tasks, offering a robust solution to integrate specialized tools with LLMs for advanced chemical applications. All datasets and code are available at https://github.com/AI4Chem/ChemistryAgent .

### 摘要
大语言模型（LLMs）近期在化学任务中展现出潜力，但仍面临预训练知识过时和难以融合专业化学知识的挑战。为解决这些问题，我们提出一种基于LLM的智能体，其协同整合了137种从基础信息检索到复杂反应预测的外部化学工具，并通过数据集构建流程生成ChemToolBench数据集，以支持微调与评估过程中有效的工具选择及参数精确填充。我们引入分层进化蒙特卡洛树搜索（HE-MCTS）框架，实现工具规划与执行的独立优化。通过利用自生成数据，该方法支持策略模型的步骤级微调（FT），并训练出超越GPT-4o的任务自适应PRM与ORM。实验评估表明，我们的方法显著提升了化学问答与发现任务的性能，为LLMs与专业工具的集成提供了稳健解决方案，以推动高级化学应用发展。所有数据集与代码详见https://github.com/AI4Chem/ChemistryAgent。

---

## [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)

### Abstract
arXiv:2506.07621v1 Announce Type: cross 
Abstract: Large Language Models have shown remarkable capabilities in the NLP domain. Their effectiveness can mainly be attributed to their ability to adapt to an array of downstream tasks. However, generally, full fine-tuning is a computationally expensive job. To mitigate this, many techniques have been developed that prime efficiency, a prominent one being Low-Rank Adaptation (LoRA). However, LoRA and its variants employ re-parametrized additive updates. In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which shifts the paradigm of additive updates to a richer space of matrix multiplicative transformations. We tackle challenges such as computational complexity and rank bottleneck of matrix multiplication by effectively re-ordering operations and introducing rank inflation strategies. We conduct extensive experiments to demonstrate the effectiveness of our approach in terms of various evaluation metrics.

### 摘要
大型语言模型在自然语言处理领域展现出卓越能力，其有效性主要归功于对多样化下游任务的适应能力。然而，全参数微调通常存在计算成本高昂的问题。为此，研究者开发了多种提升效率的技术，其中低秩自适应（LoRA）尤为突出。但现有LoRA及其变体均采用重参数化的加法式参数更新。本文提出低秩乘性自适应（LoRMA），将加法式更新范式转移到更丰富的矩阵乘性变换空间。我们通过高效的操作重排序和秩扩展策略，有效解决了矩阵乘法计算复杂度与秩瓶颈等挑战。大量实验结果表明，该方法在多项评估指标上均具有显著优势。

---

## [SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding](https://arxiv.org/abs/2506.07600)

### Abstract
arXiv:2506.07600v1 Announce Type: cross 
Abstract: Despite recent advances in retrieval-augmented generation (RAG) for video understanding, effectively understanding long-form video content remains underexplored due to the vast scale and high complexity of video data. Current RAG approaches typically segment videos into fixed-length chunks, which often disrupts the continuity of contextual information and fails to capture authentic scene boundaries. Inspired by the human ability to naturally organize continuous experiences into coherent scenes, we present SceneRAG, a unified framework that leverages large language models to segment videos into narrative-consistent scenes by processing ASR transcripts alongside temporal metadata. SceneRAG further sharpens these initial boundaries through lightweight heuristics and iterative correction. For each scene, the framework fuses information from both visual and textual modalities to extract entity relations and dynamically builds a knowledge graph, enabling robust multi-hop retrieval and generation that account for long-range dependencies. Experiments on the LongerVideos benchmark, featuring over 134 hours of diverse content, confirm that SceneRAG substantially outperforms prior baselines, achieving a win rate of up to 72.5 percent on generation tasks.

### 摘要
尽管检索增强生成（RAG）在视频理解领域取得进展，但由于视频数据规模庞大且复杂度高，针对长视频内容的有效理解仍待探索。现有RAG方法通常将视频分割为固定长度片段，这种方式往往会破坏上下文信息的连续性，且难以捕捉真实的场景边界。受人类将连续体验自然组织为连贯场景的能力启发，我们提出SceneRAG框架——通过处理自动语音识别文本及时间元数据，利用大语言模型将视频分割为叙事一致的场景。该框架通过轻量级启发式规则和迭代修正进一步优化初始边界。针对每个场景，系统融合视觉与文本模态信息以提取实体关系，动态构建知识图谱，从而实现考虑长程依赖的鲁棒多跳检索与生成。在总时长超过134小时的LongerVideos基准测试中，SceneRAG显著超越现有基线方法，在生成任务上取得高达72.5%的胜率。

---

## [PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs](https://arxiv.org/abs/2506.07587)

### Abstract
arXiv:2506.07587v1 Announce Type: cross 
Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and promising approaches for fine-tuning pre-trained language models. Compared with Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance with a substantial reduction of trainable parameters, which largely saved the training and storage costs. However, using the PEFT method requires considering a vast design space, such as the type of PEFT modules and their insertion layers. Inadequate configurations can lead to sub-optimal results. Conventional solutions such as architectural search techniques, while effective, tend to introduce substantial additional overhead. In this paper, we propose a novel approach, PrunePEFT, which formulates the PEFT strategy search as a pruning problem and introduces a hybrid pruning strategy that capitalizes on the sensitivity of pruning methods to different PEFT modules. This method extends traditional pruning techniques by iteratively removing redundant or conflicting PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently identifying the most relevant modules, our approach significantly reduces the computational burden typically associated with architectural search processes, making it a more scalable and efficient solution for fine-tuning large pre-trained models.

### 摘要
参数高效微调（PEFT）方法已成为微调预训练语言模型的有效且有前景的途径。与全参数微调（FFT）相比，PEFT在显著减少可训练参数量的同时实现了可比的任务性能，从而大幅降低了训练和存储成本。然而，采用PEFT方法需要考虑庞大的设计空间，例如PEFT模块类型及其插入层数等。不当的配置可能导致次优结果。尽管架构搜索技术等传统解决方案行之有效，但往往会引入大量额外开销。本文提出创新方法PrunePEFT，将PEFT策略搜索建模为剪枝问题，并引入一种混合剪枝策略，该策略充分利用了剪枝方法对不同PEFT模块的敏感性。该方法通过迭代移除冗余或冲突的PEFT模块来优化微调配置，从而扩展了传统剪枝技术。通过高效识别最相关模块，我们的方法显著降低了通常与架构搜索过程相关的计算负担，使其成为大规模预训练模型微调中更具可扩展性和高效性的解决方案。

---

## [LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization](https://arxiv.org/abs/2506.07570)

### Abstract
arXiv:2506.07570v1 Announce Type: cross 
Abstract: Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation.

### 摘要
自动室内布局生成技术因其在室内设计、虚拟环境构建和具身人工智能领域的应用潜力而日益受到关注。现有方法可分为两类：基于提示驱动的商用大语言模型服务（如GPT API）方法，以及基于扩散模型在布局数据上训练的学习方法。提示驱动方法常存在空间不一致性和高计算成本问题，而学习方法通常受限于粗糙的关系图和有限数据集，难以泛化至多样化的房间类型。本文重新审视基于LLM的室内布局生成，提出了3D-SynthPlace——一个通过"GPT生成、人工校验"流程从3D-Front数据集升级构建的大规模合成布局数据集。该数据集包含近17,000个场景，涵盖卧室、客厅、厨房和浴室四种常见房型，并配有丰富物体和高层次空间标注。我们进一步推出OptiScene，这是一个基于3D-SynthPlace数据集通过两阶段训练微调的强开源LLM：在预热阶段I采用监督微调（SFT），模型先学习生成高层空间描述，再条件预测具体物体摆放；在强化阶段II，通过多轮直接偏好优化（DPO）使生成布局更符合人类设计偏好，显著提升了布局质量和生成成功率。大量实验表明OptiScene优于传统提示驱动和学习基线方法，且在场景编辑和机器人导航等交互任务中展现出良好潜力。

---

## [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)

### Abstract
arXiv:2506.07664v1 Announce Type: cross 
Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and the need for precise computation. Existing methods enhance LLM reasoning by synthesizing datasets through problem rephrasing, but face issues with generation quality and problem complexity. To address this, we propose to extract structural information with generated problem-solving code from mathematical reasoning and guide data generation with structured solutions. Applied to MATH and GSM8K, our approach produces 39K problems with labeled intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results on our benchmark show that model performance declines as reasoning length increases. Additionally, we conducted fine-tuning experiments using the proposed training data on a range of LLMs, and the results validate the effectiveness of our dataset. We hope the proposed method and dataset will contribute to future research in enhancing LLM reasoning capabilities.

### 摘要
数学推理对大型语言模型（LLM）而言仍具挑战性，这源于其复杂的逻辑需求和精确计算的要求。现有方法通过问题重述合成数据集以增强LLM推理能力，但在生成质量和问题复杂度方面存在不足。为此，我们提出从数学推理中提取结构化信息并生成解题代码，以结构化解决方案指导数据生成。该方法应用于MATH和GSM8K数据集时，产生了包含标注中间步骤的39K问题及6.1K更高难度的基准问题集。实验结果表明，随着推理长度增加，模型性能呈现下降趋势。此外，我们在多种LLM上使用所提训练数据进行了微调实验，结果验证了数据集的有效性。我们期望所提出的方法和数据集能为未来提升LLM推理能力的研究作出贡献。

---

## [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)

### Abstract
arXiv:2506.07671v1 Announce Type: cross 
Abstract: We present GaRAGe, a large RAG benchmark with human-curated long-form answers and annotations of each grounding passage, allowing a fine-grained evaluation of whether LLMs can identify relevant grounding when generating RAG answers. Our benchmark contains 2366 questions of diverse complexity, dynamism, and topics, and includes over 35K annotated passages retrieved from both private document sets and the Web, to reflect real-world RAG use cases. This makes it an ideal test bed to evaluate an LLM's ability to identify only the relevant information necessary to compose a response, or provide a deflective response when there is insufficient information. Evaluations of multiple state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise rather than (a) ground their answers strictly on the annotated relevant passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b) deflect when no relevant grounding is available (reaching at most 31% true positive rate in deflections). The F1 in attribution to relevant sources is at most 58.9%, and we show that performance is particularly reduced when answering time-sensitive questions and when having to draw knowledge from sparser private grounding sources.

### 摘要
我们推出GaRAGe——一个包含人工标注长答案及每个基础段落标注的大规模RAG评估基准，可精细评估大语言模型在生成RAG答案时能否识别相关基础内容。该基准包含2366个涉及不同复杂度、动态性和主题的问题，并从私有文档集和网络爬取超过3.5万条标注段落，以反映真实世界的RAG应用场景。这使其成为评估大语言模型能力的理想测试平台：既能检验其仅筛选必要相关信息构建回答的能力，也能测试其在信息不足时提供回避性应答的表现。对多个前沿大语言模型的评估表明，这些模型倾向于过度概括而非：(a)严格基于标注相关段落生成答案（相关感知事实性评分最高仅达60%），或(b)在缺乏相关基础内容时进行回避（真实回避率最高仅31%）。相关源归因的F1值最高为58.9%，研究还发现模型在回答时效性问题和需要从更稀疏的私有基础源获取知识时表现显著下降。

---

## [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)

### Abstract
arXiv:2506.07751v1 Announce Type: cross 
Abstract: Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in their reasoning. I.e., they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further "instantiate" reasoning problems on potential variations. In contrast, our approach focuses on "abstracting" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. We find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstraL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks.

### 摘要
近期研究表明，大型语言模型（LLMs），特别是较小规模的模型，其推理能力往往缺乏鲁棒性。具体表现为当面临数值变量或名义变量的分布偏移，或插入干扰性从句等变化时，模型性能会出现显著下降。针对该问题，现有解决方案可能涉及生成合成数据以进一步"实例化"推理问题的潜在变体。与之相反，我们的方法聚焦于对推理问题进行"抽象化"处理。这种方法不仅能有效应对分布偏移，还有助于连接符号化工具来推导解决方案。我们发现相较于仅采用监督微调（常无法产生忠实抽象），通过强化学习（RL）能更好地掌握这种抽象过程。我们提出的AbstraL方法——通过在细粒度抽象数据上运用强化学习来增强LLMs的抽象推理能力——显著减轻了模型在最新GSM扰动基准测试中的性能衰减现象。

---

## [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)

### Abstract
arXiv:2506.07785v1 Announce Type: cross 
Abstract: Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

### 摘要
大型视觉语言模型（LVLMs）通过多模态检索增强生成（RAG）技术在视觉问答（VQA）任务中取得了显著性能提升。然而，现有方法仍面临推理示例知识匮乏和检索知识响应不稳定等挑战。为解决这些问题，本研究提出了一种多模态RAG框架RCTS，通过构建富含推理上下文的知识库和树搜索重排序方法增强LVLMs。具体而言，我们引入自洽评估机制，利用内在推理模式丰富知识库；进一步提出基于启发式奖励的蒙特卡洛树搜索（MCTS-HR）以优先选择最相关示例。这确保LVLMs能够利用高质量上下文推理生成更优且更稳定的响应。大量实验表明，该框架在多个VQA数据集上达到最先进性能，显著优于上下文学习（ICL）和基础RAG方法，凸显了知识库与重排序方法对提升LVLMs的有效性。代码发布于https://github.com/yannqi/RCTS-RAG。

---

## [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)

### Abstract
arXiv:2506.07833v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase "ribonucleic acid" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments ("rib", "on", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm

### 摘要
大型语言模型（LLM）已成为现代人工智能的基石。然而，现有的下一词元预测范式从根本上限制了其形成连贯高层概念的能力，这成为实现类人理解与推理的关键障碍。以短语"核糖核酸"为例：LLM会首先将其分解为人工文本片段（"rib"、"on"等）组成的词元，然后逐个学习这些词元，而非将该短语作为统一连贯的语义实体来掌握。这种碎片化表征阻碍了更深层次的概念理解，最终制约了真正智能系统的开发。为此，我们提出概念感知微调（CAFT）——一种重新定义LLM微调方式的新型多词元训练方法。该方法通过支持跨多词元的序列学习，促进了更强的概念感知能力。实验结果表明，在文本摘要等传统任务和蛋白质从头设计等专业领域任务中，该方法相较传统下一词元微调方法均有显著提升。多词元预测此前仅能在成本极高的预训练阶段实现；据我们所知，CAFT首次将多词元设置引入训练后阶段，从而有效普惠更广泛的研究与实践群体。最后，该方法出人意料的效果暗示其对机器学习研究界具有更广泛的启示意义。所有代码与数据详见https://github.com/michaelchen-lab/caft-llm。

---

## [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)

### Abstract
arXiv:2506.07962v1 Announce Type: cross 
Abstract: Diversity in training data, architecture, and providers is assumed to mitigate homogeneity in LLMs. However, we lack empirical evidence on whether different LLMs differ meaningfully. We conduct a large-scale empirical evaluation on over 350 LLMs overall, using two popular leaderboards and a resume-screening task. We find substantial correlation in model errors -- on one leaderboard dataset, models agree 60% of the time when both models err. We identify factors driving model correlation, including shared architectures and providers. Crucially, however, larger and more accurate models have highly correlated errors, even with distinct architectures and providers. Finally, we show the effects of correlation in two downstream tasks: LLM-as-judge evaluation and hiring -- the latter reflecting theoretical predictions regarding algorithmic monoculture.

### 摘要
传统观点认为训练数据多样性、架构差异和供应商多元化能够降低大语言模型（LLM）的同质性。然而，关于不同LLM是否存在显著差异，我们尚缺乏实证依据。本研究通过对350余个大语言模型进行大规模实证评估，采用两个主流评测榜单和简历筛选任务展开分析。研究发现模型错误存在高度相关性——在某评测数据集上，当两个模型同时出错时，其错误一致率高达60%。我们识别出驱动模型相关性的关键因素，包括共享架构和供应商来源。但最关键的是，性能更强、准确率更高的模型即使架构和供应商不同，其错误仍呈现高度相关性。最后，我们通过LLM作为评审员的评估和招聘任务（后者反映了算法单一化的理论预测）揭示了这种相关性在下游任务中的实际影响。

---

## [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/abs/2506.07943)

### Abstract
arXiv:2506.07943v1 Announce Type: cross 
Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires segmenting objects based on implicit text queries, demanding both precise visual perception and vision-text reasoning capabilities. Current RS approaches rely on fine-tuning vision-language models (VLMs) for both perception and reasoning, but their tokenization of images fundamentally disrupts continuous spatial relationships between objects. We introduce DTwinSeger, a novel RS approach that leverages Digital Twin (DT) representation as an intermediate layer to decouple perception from reasoning. Innovatively, DTwinSeger reformulates RS as a two-stage process, where the first transforms the image into a structured DT representation that preserves spatial relationships and semantic properties and then employs a Large Language Model (LLM) to perform explicit reasoning over this representation to identify target objects. We propose a supervised fine-tuning method specifically for LLM with DT representation, together with a corresponding fine-tuning dataset Seg-DT, to enhance the LLM's reasoning capabilities with DT representations. Experiments show that our method can achieve state-of-the-art performance on two image RS benchmarks and three image referring segmentation benchmarks. It yields that DT representation functions as an effective bridge between vision and text, enabling complex multimodal reasoning tasks to be accomplished solely with an LLM.

### 摘要
推理分割（Reasoning Segmentation，RS）是一种多模态视觉-文本任务，需要根据隐含的文本查询对目标对象进行分割，同时要求具备精确的视觉感知和视觉-文本推理能力。现有RS方法依赖对视觉语言模型（VLM）进行端到端微调来实现感知与推理，但其图像标记化处理会从根本上破坏物体间的连续空间关系。本文提出DTwinSeger——一种创新性RS方法，通过引入数字孪生（Digital Twin，DT）表征作为中间层，实现感知与推理的解耦。该方法将RS重构为两阶段流程：首先将图像转换为保留空间关系和语义属性的结构化DT表征，随后利用大语言模型（LLM）对该表征进行显式推理以定位目标对象。我们专门针对LLM与DT表征的协同提出监督微调方法，并构建配套微调数据集Seg-DT，以增强LLM基于DT表征的推理能力。实验表明，本方法在两项图像RS基准和三项图像指代分割基准上均达到最先进性能。这证明DT表征能有效充当视觉与文本间的桥梁，使得复杂多模态推理任务可仅通过LLM完成。

---

## [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)

### Abstract
arXiv:2506.07900v1 Announce Type: cross 
Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.

### 摘要
本文介绍专为终端设备设计的高效大语言模型MiniCPM4。我们通过模型架构、训练数据、训练算法和推理系统四个关键维度的系统性创新实现这一目标。具体而言，在模型架构方面，我们提出可训练的稀疏注意力机制InfLLM v2，加速长上下文处理的预填充和解码阶段；在训练数据方面，我们开发高效精准的预训练数据过滤生成策略UltraClean，以及全面监督微调数据集UltraChat v2，仅需8万亿训练标记即可实现满意性能；在训练算法方面，我们提出高效预训练策略搜索方法ModelTunnel v2，并通过引入分块式rollout实现负载均衡强化学习，以及数据高效的三值大模型BitCPM来改进现有后训练方法；在推理系统方面，我们集成稀疏注意力、模型量化和推测采样的CPM.cu系统实现高效预填充与解码。为满足多样化设备需求，MiniCPM4提供0.5B和8B两种参数版本。充分评估结果表明，MiniCPM4在多个基准测试中超越同规模开源模型，突显其效率与效能优势。值得注意的是，MiniCPM4-8B在处理长序列时相较Qwen3-8B展现出显著速度提升。通过进一步适配，该模型成功支持可信调查生成、基于模型上下文协议的工具使用等多样化应用，充分展现其广泛适用性。

---

## [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)

### Abstract
arXiv:2506.07945v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have shown promising capabilities in generating code for general-purpose programming languages. In contrast, their applicability for hardware description languages, particularly for generating synthesizable and functionally correct designs, remains significantly underexplored. HDLs such as SystemVerilog are logic-oriented and demand strict adherence to timing semantics, concurrency, and synthesizability constraints. Moreover, HDL-based design flows encompass a broad set of tasks beyond structural code generation, including testbench development, assertion-based verification, timing closure, and protocol-level integration for on-chip communication. The objective of our paper is to analyze the capabilities of state-of-the-art LLMs in generating SystemVerilog implementations of standard communication protocols, a core component of embedded and System-on-Chip (SoC) architectures. This paper introduces the first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and AXI. We define code generation tasks that capture varying levels of design abstraction and prompt specificity. The generated designs are assessed for syntactic correctness, synthesizability, and functional fidelity via waveform simulation and test benches.

### 摘要
大语言模型（LLM）的最新进展在通用编程语言的代码生成方面展现出显著潜力。然而，其在硬件描述语言（HDL）中的应用，特别是针对可综合且功能正确的设计生成，仍存在明显的研究空白。以SystemVerilog为代表的HDL具有逻辑导向特性，要求严格遵循时序语义、并发性及可综合约束。此外，基于HDL的设计流程涵盖结构代码生成之外的广泛任务，包括测试平台开发、基于断言的验证、时序收敛以及片上通信协议级集成。本文旨在分析前沿LLM在生成标准通信协议SystemVerilog实现方面的能力，这些协议是嵌入式及片上系统（SoC）架构的核心组件。我们首次提出了针对四种广泛使用协议（SPI、I2C、UART和AXI）的基准测试套件，定义了涵盖不同设计抽象层次与提示明确度的代码生成任务。通过波形仿真与测试平台，对生成设计进行语法正确性、可综合性及功能保真度评估。

---

## [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)

### Abstract
arXiv:2506.07972v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.

### 摘要
尽管大型语言模型（LLMs）在推理和基于代理的问题解决方面展现出显著进步，现有评估方法却无法充分衡量其能力：当前基准测试要么依赖易趋于饱和和记忆的封闭式问题，要么采用缺乏一致性与严谨性的主观比较。本研究提出HeuriGym——一个专为评估LLMs生成的组合优化问题启发式算法而设计的代理框架，其特点在于具有明确定义的目标和广阔解空间。该框架使LLMs能够提出启发式方法、通过代码执行获得评估反馈，并迭代优化解决方案。我们在计算机系统、物流和生物学等领域的九个问题上测试了九种前沿模型，揭示了这些模型在工具使用、规划和自适应推理方面持续存在的局限。为量化性能，我们提出质量-产出指数（QYI），该指标同时捕获解决方案通过率与质量。即使如GPT-4-mini-high和Gemini-2.5-Pro等顶级模型，其QYI得分也仅为0.6，远低于专家基线水平1。我们开源的基准测试旨在引导LLMs向更有效、更贴近实际的科学与工程领域问题解决方向发展。

---

## [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)

### Abstract
arXiv:2506.08001v1 Announce Type: cross 
Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.

### 摘要
尽管大语言模型（LLMs）正在推动人工智能的快速发展，但如何有效且可靠地训练这些大型模型仍是该领域最重大的挑战之一。为解决这一挑战，我们提出POET，一种新颖的再参数化训练算法，通过正交等价变换优化神经元。具体而言，POET利用两个可学习的正交矩阵和一个固定的随机权重矩阵对每个神经元进行再参数化。由于该算法可证明保持权重矩阵的谱特性，POET能够稳定优化目标函数并提升泛化能力。我们进一步开发了高效近似方法，使POET能够灵活且可扩展地用于训练大规模神经网络。大量实验验证了POET在训练LLMs中的有效性和可扩展性。

---

## [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)

### Abstract
arXiv:2506.07976v1 Announce Type: cross 
Abstract: The current paradigm of test-time scaling relies on generating long reasoning traces ("thinking" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.

### 摘要
当前测试时扩展的范式依赖于在生成响应前构建长推理轨迹（"多思考"）。在需要交互的智能体问题中，这可以通过在环境行动前生成思考轨迹来实现。然而，该过程不允许智能体从环境中获取新信息或随时间调整行为。本研究中，我们提出测试时交互扩展这一尚未开发的维度，通过延长智能体的交互视界来实现丰富行为（如探索、回溯和动态重规划）在单次 rollout 内的执行。为验证该扩展维度的潜力，我们以网页智能体为研究领域。首先证明即使仅采用基于提示的交互扩展（无需任何训练），也能显著提升网页基准测试的任务成功率。在此基础上，我们提出TTI（测试时交互）——一种基于课程的在线强化学习方法，通过自适应调整 rollout 长度来训练智能体。使用Gemma 3 12B模型时，TTI在WebVoyager和WebArena基准上创造了开源开放数据网页智能体的最先进水平。进一步研究表明TTI能使智能体自适应平衡探索与利用。我们的成果确立了交互扩展作为单步计算扩展的强大互补维度，为训练自适应智能体开辟了新途径。

---

## [PecSched: Preemptive and Efficient Cluster Scheduling for LLM Inference](https://arxiv.org/abs/2409.15104)

### Abstract
arXiv:2409.15104v2 Announce Type: replace 
Abstract: The scaling of transformer-based Large Language Models (LLMs) has significantly expanded their context lengths, enabling applications where inputs exceed 100K tokens. Our analysis of a recent Azure LLM inference trace reveals a highly skewed long-tail distribution of input lengths, with approximately 80% of inputs shorter than 2K tokens. Long inputs constitute only a small fraction. Existing cluster-level LLM scheduling strategies, including First-In-First-Out (FIFO), reservation-based, and priority-based approaches, primarily target short-input requests with lengths below 2K and fail to address this heterogeneity, leading to inefficiencies such as head-of-line blocking, resource underutilization, and starvation of long-input requests. We propose PecSched, a Preemptive and Efficient Cluster SCHEDuling system for LLM inference. PecSched introduces the following key techniques: 1) preemptive scheduling that prioritizes short-input requests for their performance; 2) coordinated prefill-decode colocation and disaggregation, which reduces both the duration and frequency of preemptions; 3) fast Sequence Parallelism (SP) that minimizes the prefill time of long-input requests to further reduce the likelihood and frequency of preemptions. Evaluations based on Azure LLM inference trace show that, compared to state-of-the-art cluster-level LLM inference schedulers, PecSched reduces the 99th percentile queueing delay of short-input requests by up to 92% and improves their throughput by up to 595%, without significantly affecting the Job Completion Time (JCT) of long-input requests. We open-sourced our code.

### 摘要
基于Transformer架构的大语言模型（LLM）规模扩展显著提升了上下文长度，使得处理超过10万标记的输入成为可能。通过对近期Azure LLM推理轨迹的分析，我们发现输入长度呈现高度偏态的长尾分布：约80%的输入短于2K标记，长输入仅占极小比例。现有集群级LLM调度策略（包括先进先出、基于预留和基于优先级的方法）主要针对2K标记以下的短输入请求，无法应对这种异构性，导致队头阻塞、资源利用率不足以及长输入请求饥饿等低效现象。为此，我们提出PecSched——一种面向LLM推理的抢占式高效集群调度系统，其核心技术包括：1）为短输入请求提供性能优先的抢占式调度；2）协调式预填充-解码协同定位与解耦，降低抢占的持续时间和频率；3）快速序列并行技术（SP）缩短长输入请求的预填充时间，进一步减少抢占可能性和频率。基于Azure LLM推理轨迹的评估表明，相较于最先进的集群级LLM推理调度器，PecSched能将短输入请求的第99百分位排队延迟降低92%，吞吐量提升595%，且不会显著影响长输入请求的作业完成时间（JCT）。我们已公开系统源代码。

---

## [CE-CoLLM: Efficient and Adaptive Large Language Models Through Cloud-Edge Collaboration](https://arxiv.org/abs/2411.02829)

### Abstract
arXiv:2411.02829v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit remarkable human-like predictive capabilities. However, it is challenging to deploy LLMs to provide efficient and adaptive inference services at the edge. This paper proposes a novel Cloud-Edge Collaboration framework for LLMs (CE-CoLLM) to tackle these challenges. First, we identify the transmission of LLM contextual data between the cloud and edge as a key performance bottleneck, which introduces substantial communication overhead that dominates overall inference latency and makes na\"ive cloud-edge collaboration for LLMs inefficient. Second, we introduce a suite of novel techniques, including a latency-aware early exit mechanism and efficient cloud context management, into CE-CoLLM, which collectively reduce communication overhead and preserve LLM inference accuracy. Third, we design two adaptive inference modes to accommodate diverse edge environments: (1) a low-latency standalone edge inference mode that enables reliable edge-side independent LLM inference even under unstable network conditions, and (2) a high-accuracy cloud-edge collaborative inference mode that adaptively leverages cloud resources to enhance prediction accuracy. Extensive experiments on multiple benchmark datasets demonstrate that CE-CoLLM reduces overall inference time by up to 13.81% and offloads over 84.53% of the computational workload from the cloud to the edge, compared to conventional cloud-based LLM deployment, without sacrificing prediction accuracy. The code is provided on GitHub at https://github.com/mlsysx/CE-CoLLM.

### 摘要
大型语言模型（LLMs）展现出类人的卓越预测能力，但在边缘侧部署LLMs以提供高效自适应推理服务仍存在挑战。本文提出新型云边协同LLM框架CE-CoLLM应对这些挑战。首先，我们发现云边间LLM上下文数据传输是性能瓶颈关键，其引入的通信开销主导整体推理延迟，导致传统云边协作方案效率低下。其次，我们在CE-CoLLM中集成包括延迟感知早期退出机制和高效云上下文管理在内的创新技术，协同降低通信开销并保持LLM推理精度。第三，我们设计两种自适应推理模式以适应不同边缘环境：(1) 低延迟独立边缘推理模式，可在网络不稳定时实现可靠的边缘侧自主LLM推理；(2) 高精度云边协同推理模式，自适应利用云端资源提升预测准确性。多基准数据集实验表明，相比传统云端部署方案，CE-CoLLM在保持预测精度的同时，最高可降低13.81%的整体推理时间，并将84.53%以上的计算负载从云端卸载至边缘。代码已发布于GitHub：https://github.com/mlsysx/CE-CoLLM。

---

## [Can the Rookies Cut the Tough Cookie? Exploring the Use of LLMs for SQL Equivalence Checking](https://arxiv.org/abs/2412.05561)

### Abstract
arXiv:2412.05561v2 Announce Type: replace 
Abstract: Equivalence checking of SQL queries is an intractable problem often encountered in settings ranging from grading SQL submissions to debugging query optimizers. Despite recent work toward developing practical solutions, only simple queries written using a small subset of SQL are supported, leaving the equivalence checking of sophisticated SQL queries at the mercy of intensive, potentially error-prone, manual analysis. In this paper, we explore how LLMs can be used to reason with SQL queries to address this challenging problem. Towards this, we introduce a novel, realistic, and sufficiently complex benchmark called SQLEquiQuest for SQL query equivalence checking that reflects real-world settings. We establish strong baselines for SQL equivalence checking by leveraging the ability of LLMs to reason with SQL queries. We conduct a detailed evaluation of several state-of-the-art LLMs using various prompting strategies and carefully constructed in-context learning examples, including logical plans generated by SQL query processors. Our empirical evaluation shows that LLMs go well beyond the current capabilities of formal models for SQL equivalence, going from a mere 30% supported query pairs to full coverage, achieving up to 82% accuracy on Spider+DIN. However, a critical limitation of LLMs revealed by our analysis is that they exhibit a strong bias for equivalence predictions, with consistently poor performance over non-equivalent pairs, opening a new direction for potential future research.

### 摘要
SQL查询的等价性检查是一个棘手的问题，常见于从SQL作业评分到查询优化器调试等多种场景。尽管近期已有研究致力于开发实用解决方案，但仅支持使用SQL小子集编写的简单查询，导致复杂SQL查询的等价性检查仍需依赖高强度且易出错的人工分析。本文探讨如何利用大型语言模型（LLMs）对SQL查询进行推理以解决这一难题。为此，我们引入了一个新颖、真实且足够复杂的基准测试SQLEquiQuest，用于反映真实场景下的SQL查询等价性检查。通过利用LLMs对SQL查询的推理能力，我们为SQL等价性检查建立了强基线。我们对多种最先进的LLMs进行了详细评估，采用包括SQL查询处理器生成的逻辑计划在内的多种提示策略及精心构建的上下文学习示例。实证评估表明，LLMs显著超越了当前SQL等价性形式化模型的能力——从仅支持30%的查询对提升至全覆盖，在Spider+DIN数据集上达到82%的准确率。然而，我们的分析揭示了LLMs一个关键局限：其对等价性预测存在强烈偏好，在非等价查询对上表现持续不佳，这为未来潜在研究开辟了新方向。

---

## [From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs](https://arxiv.org/abs/2501.16207)

### Abstract
arXiv:2501.16207v4 Announce Type: replace 
Abstract: The research in AI-based formal mathematical reasoning has shown an unstoppable growth trend. These studies have excelled in mathematical competitions like IMO and have made significant progress. This paper focuses on formal verification, an immediate application scenario of formal reasoning, and breaks it down into sub-tasks. We constructed 18k high-quality instruction-response pairs across five formal specification languages (Coq, Lean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten open-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned several 7~8B small models to achieve comparable performance with Deepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data also enhances mathematics, reasoning, and coding capabilities. Fine-tuned models are released at https: //huggingface.co/fm-universe.

### 摘要
基于AI的形式化数学推理研究呈现出不可阻挡的增长趋势。这些研究在国际数学奥林匹克竞赛（IMO）等数学赛事中表现优异，并取得了显著进展。本文聚焦形式化验证这一形式化推理的直接应用场景，将其分解为多个子任务。通过蒸馏GPT-4o模型，我们构建了涵盖五种形式化规范语言（Coq、Lean4、Dafny、ACSL和TLA+）的18k高质量指令-响应对，并针对包括近期热门的DeepSeek-R1在内的十个开源大语言模型进行了评估。我们还微调了若干7~8B参数的小型模型，使其达到与Deepseek-R1-671B相当的性能。有趣的是，我们发现使用形式化数据进行微调还能提升模型的数学、推理和编码能力。微调后的模型已发布于https://huggingface.co/fm-universe。

---

## [Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models](https://arxiv.org/abs/2501.05752)

### Abstract
arXiv:2501.05752v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral. Our code is available at https://github.com/ml-postech/SEAG-semantic-exploration-with-adaptive-gating .

### 摘要
大型语言模型（LLM）的最新进展在需要多步推理方法（如树搜索）以探索多样化推理路径的复杂任务中展现出显著潜力。然而，现有方法常受计算效率低下和冗余问题困扰。首先，这些方法忽视了任务难度的多样性，导致即使面对简单任务也会进行不必要的广泛搜索。其次，它们忽略了推理路径的语义，导致对语义相同路径的冗余探索。为解决这些局限，我们提出基于自适应门控的语义探索方法（SEAG），这是一种计算高效的方法。SEAG采用自适应门控机制，根据前置简单推理方法答案的置信度动态决定是否执行树搜索。此外，其基于树的探索机制会合并语义相同的推理步骤，在保持甚至提高准确性的同时减少冗余探索。我们在包括GSM8K和ARC在内的复杂推理基准测试中，使用Llama2、Llama3和Mistral等多种语言模型进行大量实验，结果表明：相较于现有基于树搜索的方法，SEAG在平均准确率上显著提升4.3%，同时仅需31%的计算成本。代码已发布于https://github.com/ml-postech/SEAG-semantic-exploration-with-adaptive-gating。

---

## [DeepServe: Serverless Large Language Model Serving at Scale](https://arxiv.org/abs/2501.14417)

### Abstract
arXiv:2501.14417v3 Announce Type: replace 
Abstract: In this paper, we propose DEEPSERVE, a scalable and serverless AI platform designed to efficiently serve large language models (LLMs) at scale in cloud environments. DEEPSERVE addresses key challenges such as resource allocation, serving efficiency, and cold start latencies through four main design components. First, DEEPSERVE uses a simple serverless abstraction called the request-job-task model, which helps manage diverse AI workloads across posttraining and model-serving tasks. Second, DEEPSERVE integrates an in-house serving engine named FLOWSERVE using a microkernel-inspired design, NPU-centric execution, and SPMD-based parallelism to optimize LLM serving. Third, DEEPSERVE includes novel scheduling policies tailored for a configuration with both PD-disaggregated and PD-colocated instances. Fourth, DEEPSERVE includes optimizations such as pre-warmed pods, DRAM pre-loading, and NPU-fork, which allow DEEPSERVE to scale up to 64 instances in seconds. DEEPSERVE has been in production for over a year, operating on a large Ascend NPU cluster and providing industrystandard APIs for fine-tuning, agent serving, and model serving to our customers.

### 摘要
本文提出DEEPSERVE——一种可扩展的无服务器AI平台，旨在云环境中高效大规模部署大型语言模型（LLM）。该平台通过四大核心设计解决资源分配、服务效率和冷启动延迟等关键挑战：首先，采用基于请求-作业-任务模型的简洁无服务器抽象机制，统一管理训练后任务与模型服务等多样化AI工作负载；其次，集成自主研发的FLOWSERVE服务引擎，通过微内核架构设计、NPU中心化执行及基于SPMD的并行计算实现LLM服务优化；第三，创新性地制定了同时支持PD解耦部署与PD协同部署的混合调度策略；第四，采用预热容器、DRAM预加载和NPU分叉等技术，实现秒级扩展至64个实例的快速扩容能力。DEEPSERVE已在昇腾NPU集群稳定运行逾一年，为客户提供涵盖模型微调、智能体服务及模型部署的行业标准API接口。

---

## [How to Mitigate Information Loss in Knowledge Graphs for GraphRAG: Leveraging Triple Context Restoration and Query-Driven Feedback](https://arxiv.org/abs/2501.15378)

### Abstract
arXiv:2501.15378v2 Announce Type: replace 
Abstract: Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently propelled significant advances in complex reasoning tasks, thanks to their broad domain knowledge and contextual awareness. Unfortunately, current methods often assume KGs to be complete, which is impractical given the inherent limitations of KG construction and the potential loss of contextual cues when converting unstructured text into entity-relation triples. In response, this paper proposes the Triple Context Restoration and Query-driven Feedback (TCR-QF) framework, which reconstructs the textual context underlying each triple to mitigate information loss, while dynamically refining the KG structure by iteratively incorporating query-relevant missing knowledge. Experiments on five benchmark question-answering datasets substantiate the effectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1% improvement in Exact Match and a 15.5% improvement in F1 over its state-of-the-art GraphRAG competitors.

### 摘要
知识图谱（KG）增强型大语言模型（LLMs）凭借其广泛的领域知识和上下文感知能力，近期在复杂推理任务中推动了显著进展。然而，现有方法通常假设知识图谱是完整的，这一假设在实际应用中并不成立——既受限于知识图谱构建的固有缺陷，也源于非结构化文本转化为实体-关系三元组时潜在的上下文信息丢失。为此，本文提出三重上下文重建与查询驱动反馈（TCR-QF）框架，通过重建每个三元组对应的原始文本语境以缓解信息损失，同时通过迭代式融入查询相关的缺失知识动态优化知识图谱结构。在五个基准问答数据集上的实验验证表明，TCR-QF在知识图谱与大语言模型融合方面成效显著，其精确匹配率较当前最先进的GraphRAG方法提升29.1%，F1值提高15.5%。

---

## [DeepRAG: Thinking to Retrieve Step by Step for Large Language Models](https://arxiv.org/abs/2502.01142)

### Abstract
arXiv:2502.01142v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities, while their practical applications are limited by severe factual hallucinations due to limitations in the timeliness, accuracy, and comprehensiveness of their parametric knowledge. Meanwhile, enhancing retrieval-augmented generation (RAG) with reasoning remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling reasonable and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency and boosts answer accuracy by 26.4%, demonstrating its effectiveness in enhancing retrieval-augmented reasoning.

### 摘要
大型语言模型（LLMs）已展现出卓越的推理能力，但其实际应用受到参数知识在时效性、准确性和全面性方面的限制，导致严重的事实性幻觉问题。与此同时，由于任务分解效率低下和冗余检索带来的噪声会降低响应质量，如何通过推理增强检索增强生成（RAG）仍具挑战性。本文提出DeepRAG框架，将检索增强推理建模为马尔可夫决策过程（MDP），实现合理且自适应的检索。该框架通过迭代式查询分解，动态决定每一步是检索外部知识还是依赖参数化推理。实验表明，DeepRAG将检索效率提升26.4%，显著提高了答案准确性，验证了其在增强检索推理方面的有效性。

---

## [PlanGenLLMs: A Modern Survey of LLM Planning Capabilities](https://arxiv.org/abs/2502.11221)

### Abstract
arXiv:2502.11221v2 Announce Type: replace 
Abstract: LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.

### 摘要
大型语言模型（LLMs）在生成规划方案方面具有巨大潜力，能够将初始世界状态转化为目标状态。大量研究探索了LLMs在各种规划任务中的应用，包括网页导航、旅行规划及数据库查询等。然而，这些系统大多针对特定问题定制，导致难以进行横向比较或为新任务确定最佳方法。此外，当前领域缺乏清晰统一的评估标准。本综述旨在填补这一空白，全面梳理现有LLM规划器的研究进展。基于Kartam和Wilkins（1990）的开创性工作，我们提出六项关键性能指标：完备性、可执行性、最优性、表征能力、泛化性和效率，并对每项指标下的代表性研究进行深入分析，阐明其优势与不足。本文还指出了未来研究的重要方向，为希望利用LLM规划支持智能体工作流的从业者和新研究者提供了重要参考。

---

## [AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay](https://arxiv.org/abs/2502.16789)

### Abstract
arXiv:2502.16789v2 Announce Type: replace 
Abstract: Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay, where factors lose their predictive power over time, poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and US S&amp;P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.

### 摘要
阿尔法挖掘作为量化投资的核心环节，致力于在日益复杂的金融市场中发现预测未来资产收益的信号。然而普遍存在的阿尔法衰减现象——即因子随时间推移丧失预测能力——对阿尔法挖掘构成了重大挑战。传统方法如遗传编程会因过拟合和复杂性导致阿尔法快速衰减，而基于大语言模型（LLMs）的方法尽管前景广阔，却过度依赖现有知识，产生同质化因子进而加剧拥挤效应并加速衰减。为解决这一难题，我们提出AlphaAgent自主框架，通过将LLM智能体与针对性正则化有效结合来挖掘抗衰减的阿尔法因子。该框架包含三项核心机制：（1）基于抽象语法树（AST）的相似度度量确保与现有因子的原创性；（2）通过LLM评估市场假设与生成因子间的语义一致性实现假设-因子对齐；（3）基于AST的结构约束实施复杂度控制，防止易出现过拟合的过度工程化构造。这些机制协同引导因子生成过程，在原创性、金融逻辑与市场环境适应性之间实现平衡，从而降低阿尔法衰减风险。大量实验表明，AlphaAgent在牛熊市中均优于传统及LLM基线方法，过去四年在中国中证500和美国标普500市场持续产生显著阿尔法收益。值得注意的是，该框架展现出卓越的抗衰减特性，为生成强效因子提供了新的可能性。

---

## [Learning Strategic Language Agents in the Werewolf Game with Iterative Latent Space Policy Optimization](https://arxiv.org/abs/2502.04686)

### Abstract
arXiv:2502.04686v2 Announce Type: replace 
Abstract: Large language model (LLM) agents have recently demonstrated impressive capabilities in various domains like open-ended conversation and multi-step decision-making. However, it remains challenging for these agents to solve strategic language games, such as Werewolf, which demand both strategic decision-making and free-form language interactions. Existing LLM agents often suffer from intrinsic bias in their action distributions and limited exploration of the unbounded text action space, resulting in suboptimal performance. To address these challenges, we propose Latent Space Policy Optimization (LSPO), an iterative framework that combines game-theoretic methods with LLM fine-tuning to build strategic language agents. LSPO leverages the observation that while the language space is combinatorially large, the underlying strategy space is relatively compact. We first map free-form utterances into a finite latent strategy space, yielding an abstracted extensive-form game. Then we apply game-theoretic methods like Counterfactual Regret Minimization (CFR) to optimize the policy in the latent space. Finally, we fine-tune the LLM via Direct Preference Optimization (DPO) to align with the learned policy. By iteratively alternating between these steps, our LSPO agents progressively enhance both strategic reasoning and language communication. Experiment on the Werewolf game shows that our agents iteratively expand the strategy space with improving performance and outperform existing Werewolf agents, underscoring their effectiveness in free-form language games with strategic interactions.

### 摘要
大型语言模型（LLM）智能体近期在开放域对话和多步决策等领域展现出卓越能力，但在战略语言游戏（如狼人杀）中仍面临挑战——这类游戏需同时具备战略决策与自由形式语言交互能力。现有LLM智能体常受限于动作分布的内在偏差及对无界文本动作空间的有限探索，导致表现欠佳。为此，我们提出潜在空间策略优化（LSPO），该迭代框架通过结合博弈论方法与LLM微调来构建战略语言智能体。LSPO基于关键发现：尽管语言空间组合复杂度高，其底层策略空间相对紧凑。我们首先将自由形式话语映射至有限潜在策略空间，生成抽象扩展式博弈；随后应用反事实遗憾最小化（CFR）等博弈论方法优化潜在空间策略；最后通过直接偏好优化（DPO）对LLM进行微调以对齐学习策略。通过迭代执行上述步骤，LSPO智能体持续提升战略推理与语言沟通能力。狼人杀实验表明，我们的智能体能以性能提升为驱动迭代扩展策略空间，其表现超越现有狼人杀智能体，印证了该方法在战略交互型自由语言游戏中的有效性。

---

## [EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments](https://arxiv.org/abs/2503.18825)

### Abstract
arXiv:2503.18825v2 Announce Type: replace 
Abstract: We develop benchmarks for LLM agents that act in, learn from, and strategize in unknown environments, the specifications of which the LLM agent must learn over time from deliberate exploration. Our benchmarks consist of decision-making tasks derived from key problems in economics. To forestall saturation, the benchmark tasks are synthetically generated with scalable difficulty levels. Additionally, we propose litmus tests, a new kind of quantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests quantify differences in character, values, and tendencies of LLMs and LLM agents, by considering their behavior when faced with tradeoffs (e.g., efficiency versus equality) where there is no objectively right or wrong behavior. Overall, our benchmarks and litmus tests assess the abilities and tendencies of LLM agents in tackling complex economic problems in diverse settings spanning procurement, scheduling, task allocation, and pricing -- applications that should grow in importance as such agents are further integrated into the economy.

### 摘要
我们开发了针对在未知环境中行动、学习并制定策略的大型语言模型（LLM）代理的基准测试体系，这些环境的具体规范需要LLM代理通过主动探索逐步学习。该基准测试由经济学关键问题衍生的决策任务构成。为防止性能饱和，测试任务采用可调节难度级别的合成生成方式。此外，我们提出了一种新型定量评估方法——试金石测试，用于衡量LLM及其代理的特质差异。与基准测试不同，试金石测试通过观察模型在面临权衡取舍（如效率与公平）时的行为表现来量化其特性、价值观和倾向性，此类情境不存在客观对错标准。综合而言，我们的基准测试和试金石测试可评估LLM代理在采购、调度、任务分配和定价等多样化场景中处理复杂经济问题的能力与行为倾向——随着此类代理在经济活动中的深度整合，这些应用场景的重要性将日益凸显。

---

## [Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs with Semantic Space](https://arxiv.org/abs/2503.11586)

### Abstract
arXiv:2503.11586v2 Announce Type: replace 
Abstract: Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected quality, conversation planning reasons about the stochastic transitions within a conversation to select the optimal LLM response at each turn. Existing simulation-based conversation planning algorithms typically select the optimal response by simulating future conversations with a large number of LLM queries at every turn. However, this process is extremely time-consuming and hence impractical for real-time conversations. This paper presents a novel approach called Semantic space COnversation Planning with improved Efficiency (SCOPE) that exploits the dense semantic representation of conversations to perform conversation planning efficiently. In particular, SCOPE models the stochastic transitions in conversation semantics and their associated rewards to plan entirely within the semantic space. This allows us to select the optimal LLM response at every conversation turn without needing additional LLM queries for simulation. As a result, SCOPE can perform conversation planning 70 times faster than conventional simulation-based planning algorithms when applied to a wide variety of conversation starters and two reward functions seen in the real world, yet achieving a higher reward within a practical planning budget. Our code can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE.

### 摘要
大型语言模型（LLMs）被应用于聊天机器人或AI助手以与人类用户进行对话。在此类应用中，对话质量（如用户参与度、安全性）至关重要且仅在对话结束时才能准确评估。为最大化其预期质量，对话规划通过推理对话中的随机转移状态来在每轮选择最优LLM响应。现有的基于模拟的对话规划算法通常需在每轮通过大量LLM查询模拟未来对话以选择最优响应，但该过程极其耗时，难以满足实时对话需求。本文提出一种名为"语义空间高效对话规划（SCOPE）"的新方法，该方法利用对话的密集语义表示实现高效规划。具体而言，SCOPE通过建模对话语义的随机转移及其关联奖励，完全在语义空间内进行规划。这使得我们无需额外LLM查询模拟即可在每轮对话中选择最优响应。实验表明，当应用于多样化现实对话场景及两种真实奖励函数时，SCOPE的规划速度比传统基于模拟的算法快70倍，并在实际规划预算内获得更高奖励。代码详见：https://github.com/chenzhiliang94/convo-plan-SCOPE。

---

## [Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies](https://arxiv.org/abs/2504.19487)

### Abstract
arXiv:2504.19487v2 Announce Type: replace 
Abstract: The evolution of cooperation has been extensively studied using abstract mathematical models and simulations. Recent advances in Large Language Models (LLM) and the rise of LLM agents have demonstrated their ability to perform social reasoning, thus providing an opportunity to test the emergence of norms in more realistic agent-based simulations with human-like reasoning using natural language. In this research, we investigate whether the cooperation dynamics presented in Boyd and Richerson's model persist in a more realistic simulation of the diner's dilemma using LLM agents compared to the abstract mathematical nature in the work of Boyd and Richerson. Our findings indicate that agents follow the strategies defined in the Boyd and Richerson model, and explicit punishment mechanisms drive norm emergence, reinforcing cooperative behaviour even when the agent strategy configuration varies. Our results suggest that LLM-based Multi-Agent System simulations, in fact, can replicate the evolution of cooperation predicted by the traditional mathematical models. Moreover, our simulations extend beyond the mathematical models by integrating natural language-driven reasoning and a pairwise imitation method for strategy adoption, making them a more realistic testbed for cooperative behaviour in MASs.

### 摘要
合作行为的演化长期以来主要通过抽象数学模型和仿真进行研究。随着大语言模型（LLM）技术的突破和LLM智能体的兴起，这些模型展现出的社会推理能力为构建更现实的基于智能体的仿真实验提供了可能——通过自然语言实现类人推理来测试社会规范的涌现。本研究通过LLM智能体模拟"晚餐者困境"，检验Boyd和Richerson模型中提出的合作动力学机制是否能在比原抽象数学模型更接近现实的仿真环境中持续存在。研究发现：智能体会遵循Boyd和Richerson模型定义的策略，显性惩罚机制能有效促进规范涌现并强化合作行为，且该效果在智能体策略配置变化时依然稳定。结果表明基于LLM的多智能体系统仿真确实能够复现传统数学模型预测的合作演化规律。此外，本研究通过整合自然语言驱动推理和成对模仿策略选择机制，使仿真系统突破了数学模型的局限性，为多智能体系统中的合作行为研究提供了更贴近现实的实验平台。

---

## [AfroBench: How Good are Large Language Models on African Languages?](https://arxiv.org/abs/2311.07978)

### Abstract
arXiv:2311.07978v5 Announce Type: replace-cross 
Abstract: Large-scale multilingual evaluations, such as MEGA, often include only a handful of African languages due to the scarcity of high-quality evaluation data and the limited discoverability of existing African datasets. This lack of representation hinders comprehensive LLM evaluation across a diverse range of languages and tasks. To address these challenges, we introduce AfroBench -- a multi-task benchmark for evaluating the performance of LLMs across 64 African languages, 15 tasks and 22 datasets. AfroBench consists of nine natural language understanding datasets, six text generation datasets, six knowledge and question answering tasks, and one mathematical reasoning task. We present results comparing the performance of prompting LLMs to fine-tuned baselines based on BERT and T5-style models. Our results suggest large gaps in performance between high-resource languages, such as English, and African languages across most tasks; but performance also varies based on the availability of monolingual data resources. Our findings confirm that performance on African languages continues to remain a hurdle for current LLMs, underscoring the need for additional efforts to close this gap.
  https://mcgill-nlp.github.io/AfroBench/

### 摘要
当前大规模多语言评估（如MEGA）往往仅涵盖少数非洲语言，这主要源于高质量评估数据的稀缺性以及现有非洲数据集的可发现性不足。这种代表性缺失阻碍了针对多样化语言和任务的全面大语言模型（LLM）评估。为解决这些问题，我们推出AfroBench——一个包含64种非洲语言、15项任务和22个数据集的多任务基准测试平台。AfroBench涵盖9个自然语言理解数据集、6个文本生成数据集、6个知识与问答任务，以及1个数学推理任务。我们通过实验对比了基于提示的LLM与基于BERT和T5架构微调基线的性能表现。结果表明：在多数任务中，英语等高资源语言与非洲语言之间存在显著性能差距；同时，性能差异也受单语数据资源可用性的影响。研究结果证实非洲语言处理性能仍是当前LLM面临的重大挑战，凸显了缩小这一差距的紧迫需求。

---

## [Enhancing Open-Domain Task-Solving Capability of LLMs via Autonomous Tool Integration from GitHub](https://arxiv.org/abs/2312.17294)

### Abstract
arXiv:2312.17294v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel in traditional natural language processing tasks but struggle with problems that require complex domain-specific calculations or simulations. While equipping LLMs with external tools to build LLM-based agents can enhance their capabilities, existing approaches lack the flexibility to address diverse and ever-evolving user queries in open domains. Currently, there is also no existing dataset that evaluates LLMs on open-domain knowledge that requires tools to solve. To this end, we introduce OpenAct benchmark to evaluate the open-domain task-solving capability, which is built on human expert consultation and repositories in GitHub. It comprises 339 questions spanning 7 diverse domains that need to be solved with domain-specific methods. In our experiments, even state-of-the-art LLMs and LLM-based agents demonstrate unsatisfactory success rates, underscoring the need for a novel approach. Furthermore, we present OpenAgent, a novel LLM-based agent system that can tackle evolving queries in open domains through autonomously integrating specialized tools from GitHub. OpenAgent employs 1) a hierarchical framework where specialized agents handle specific tasks and can assign tasks to inferior agents, 2) a bi-level experience learning mechanism to learn from both humans' and its own experiences to tackle tool flaws. Experiments demonstrate its superior effectiveness and efficiency, which significantly outperforms baselines. Our data and code are open-source at https://github.com/OpenBMB/OpenAct.

### 摘要
大型语言模型（LLMs）在传统自然语言处理任务中表现优异，但在需要复杂领域特定计算或模拟的问题上仍存在困难。虽然通过为LLMs配备外部工具构建基于LLM的智能体可以增强其能力，但现有方法缺乏灵活性，难以应对开放领域中多样且不断演变的用户查询。目前也缺乏评估LLMs在需要工具解决的开放领域知识上的现有数据集。为此，我们提出OpenAct基准来评估开放领域任务解决能力，该基准基于人类专家咨询和GitHub知识库构建，包含涵盖7个不同领域的339个问题，这些问题需通过领域特定方法解决。实验表明，即使最先进的LLMs和基于LLM的智能体也表现出不尽人意的成功率，凸显了新方法的必要性。进一步，我们提出OpenAgent——一种新型基于LLM的智能体系统，能够通过自主集成GitHub上的专用工具来处理开放领域的动态查询。OpenAgent采用：1）分层框架，由专业智能体处理特定任务并可向下级智能体分配任务；2）双层次经验学习机制，从人类及其自身经验中学习以应对工具缺陷。实验证明其具有显著的优越性和高效性，显著超越基线方法。我们的数据和代码已在https://github.com/OpenBMB/OpenAct开源。

---

## [Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking Attacks for Large Language Models](https://arxiv.org/abs/2406.11682)

### Abstract
arXiv:2406.11682v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been increasingly applied to various domains, which triggers increasing concerns about LLMs' safety on specialized domains, e.g. medicine. Despite prior explorations on general jailbreaking attacks, there are two challenges for applying existing attacks on testing the domain-specific safety of LLMs: (1) Lack of professional knowledge-driven attacks, (2) Insufficient coverage of domain knowledge. To bridge this gap, we propose a new task, knowledge-to-jailbreak, which aims to generate jailbreaking attacks from domain knowledge, requiring both attack effectiveness and knowledge relevance. We collect a large-scale dataset with 12,974 knowledge-jailbreak pairs and fine-tune a large language model as jailbreak-generator, to produce domain knowledge-specific jailbreaks. Experiments on 13 domains and 8 target LLMs demonstrate the effectiveness of jailbreak-generator in generating jailbreaks that are both threatening to the target LLMs and relevant to the given knowledge. We also apply our method to an out-of-domain knowledge base, showing that jailbreak-generator can generate jailbreaks that are comparable in harmfulness to those crafted by human experts. Data and code are available at: https://github.com/THU-KEG/Knowledge-to-Jailbreak/.

---

## [Active Preference Optimization for Sample Efficient RLHF](https://arxiv.org/abs/2402.10500)

### Abstract
arXiv:2402.10500v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) aligned using Reinforcement Learning from Human Feedback (RLHF) have shown remarkable generation abilities in numerous tasks. However, collecting high-quality human preferences creates costly bottlenecks in practical deployments, and hence, training data are often budgeted. In these scenarios, it is crucial to collect training data (e.g., contexts, a pair of generations for each context, and a preference indicating which generation is better) carefully, yet most of the existing methods sample contexts uniformly at random from a given collection. Given this, under the Bradley-Terry-Luce preference model and with a small budget of training data, we show that uniform sampling of contexts could lead to a policy (i.e., an aligned model) that suffers a constant sub-optimality gap from the optimal policy. This highlights the need for an adaptive context sampling strategy for effective alignment under a small sample budget. To address this, we reformulate RLHF within the contextual preference bandit framework, treating generations as actions, and give a nearly complete characterization of the sub-optimality gap in terms of both lower and upper bounds. First, when the action set is a $d$-dimensional hypercube and the number of samples is $T$, we show an $\Omega(d/\sqrt&#123;T&#125;)$ lower bound. Next, we propose an algorithm, $\textit&#123;Active Preference Optimization&#125;$ ($\texttt&#123;APO&#125;$), that iteratively collects preferences for the most uncertain contexts. We show that the sub-optimality gap of the policy learned via $\texttt&#123;APO&#125;$ matches the lower bound up to a log factor and a non-linearity constant. Finally, we perform experiments on practical datasets to validate $\texttt&#123;APO&#125;$'s efficacy over existing methods, establishing it as a sample-efficient and cost-effective solution for LLM alignment.

### 摘要
基于人类反馈强化学习（RLHF）对齐的大语言模型（LLM）在多项任务中展现出卓越的生成能力。然而，高质量人类偏好数据的收集在实际部署中形成了高昂的成本瓶颈，因此训练数据往往受到预算限制。在此类场景下，需谨慎采集训练数据（包括上下文、每段上下文对应的生成对及其偏好标注），但现有方法大多从给定集合中均匀随机采样上下文。针对该问题，我们在Bradley-Terry-Luce偏好模型框架下证明：当训练数据预算较小时，均匀上下文采样可能导致策略（即对齐模型）与最优策略之间存在恒定次优差距。这凸显了小样本预算下自适应上下文采样策略的必要性。为此，我们将RLHF重新建模为上下文偏好赌博机问题，将生成结果视为动作，并完整刻画了次优差距的上下界：首先，当动作集为d维超立方体且样本量为T时，我们给出Ω(d/√T)的下界；继而提出迭代采集最不确定上下文偏好的算法——主动偏好优化（APO），证明其学习策略的次优差距在对数因子和非线性常数范围内匹配下界；最后通过实际数据集实验验证APO相较现有方法的优越性，确立其作为LLM对齐中样本高效且经济可行的解决方案。

---

## [ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models](https://arxiv.org/abs/2406.13342)

### Abstract
arXiv:2406.13342v2 Announce Type: replace-cross 
Abstract: The advancements in large language models (LLMs) have brought significant progress in NLP tasks. However, if a task cannot be fully described in prompts, the models could fail to carry out the task. In this paper, we propose a simple yet effective method to contextualize a task toward a LLM. The method utilizes (1) open-ended zero-shot inference from the entire dataset, (2) aggregate the inference results, and (3) finally incorporate the aggregated meta-information for the actual task. We show the effectiveness in text clustering tasks, empowering LLMs to perform text-to-text-based clustering and leading to improvements on several datasets. Furthermore, we explore the generated class labels for clustering, showing how the LLM understands the task through data.

### 摘要
大型语言模型（LLMs）的进步为自然语言处理任务带来了显著进展。然而，若任务无法通过提示词完整描述，模型可能无法执行该任务。本文提出一种简单而有效的方法，用于将任务情境化以适应LLMs。该方法采用：（1）基于整个数据集的开放式零样本推理，（2）聚合推理结果，（3）最终整合聚合后的元信息以执行实际任务。我们在文本聚类任务中验证了其有效性，使LLMs能够执行基于文本到文本的聚类，并在多个数据集上实现了性能提升。此外，我们还探究了聚类生成的类别标签，揭示了LLM如何通过数据理解任务。

---

## [When Attention Collapses: How Degenerate Layers in LLMs Enable Smaller, Stronger Models](https://arxiv.org/abs/2404.08634)

### Abstract
arXiv:2404.08634v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) rely on the transformer architecture and its self-attention mechanism to deliver strong performance across tasks. However, we uncover a structural inefficiency in standard pre-trained decoder-style LLMs: in many of the deeper layers, attention matrices frequently collapse to near rank-one, single-column patterns. We refer to these underutilized components as lazy layers, which are redundant and computationally inefficient. To address this, we propose Inheritune, a simple and effective training recipe for building smaller, more efficient, and high performing language models. Inheritune initializes a compact model by inheriting the useful early layers from a larger pre-trained model, then progressively retrains and expands it. Our experiments across multiple models and datasets show that Inheritune trained models, despite having significantly fewer layers, can match or even outperform their larger counterparts. This approach yields compact, performant models and offers a practical path for efficient language model compression. Code is available at https://github.com/sanyalsunny111/LLM-Inheritune

### 摘要
大型语言模型（LLMs）依赖Transformer架构及其自注意力机制来实现跨任务的强劲性能。然而，我们发现标准预训练的解码器风格LLMs存在结构低效性：在较深层中，注意力矩阵经常坍缩为接近秩一、单列的模式。我们将这些未充分利用的组件称为惰性层，它们既冗余又存在计算低效问题。为此，我们提出Inheritune——一种简单有效的训练方法，用于构建更小、更高效且高性能的语言模型。该方法通过从大型预训练模型中继承有用的早期层来初始化紧凑模型，随后逐步对其进行重训练和扩展。我们在多个模型和数据集上的实验表明，经Inheritune训练的模型尽管层数显著减少，仍能匹配甚至超越其更大规模的对应模型。这种方法不仅可生成紧凑且高性能的模型，还为高效的语言模型压缩提供了实用路径。代码详见https://github.com/sanyalsunny111/LLM-Inheritune。

---

## [Binary Classifier Optimization for Large Language Model Alignment](https://arxiv.org/abs/2404.04656)

### Abstract
arXiv:2404.04656v2 Announce Type: replace-cross 
Abstract: In real-world services such as ChatGPT, aligning models based on user feedback is crucial for improving model performance. However, due to the simplicity and convenience of providing feedback, users typically offer only basic binary signals, such as 'thumbs-up' or 'thumbs-down'. Most existing alignment research, on the other hand, relies on preference-based approaches that require both positive and negative responses as a pair. We propose Binary Classifier Optimization (BCO), a technique that effectively aligns LLMs using only binary feedback. BCO trains a binary classifier, where the logit serves as an implicit reward, effectively minimizing the Direct Preference Optimization (DPO) loss. We demonstrate that the binary cross-entropy loss employed in classifier training acts as an upper bound for the DPO loss. Additionally, a novel reward shift technique further minimizes the gap between the losses. We validate our methodology in two settings: first, on a paired preference dataset, where our method performs on par with DPO; and second, on a Likert-5 scale annotation dataset which stems from real users' queries. Our model consistently demonstrates effective and robust alignment across four base LLMs and three different datasets, showcasing the strength of our approach to learning from binary signals.

### 摘要
在现实世界的服务（如ChatGPT）中，基于用户反馈的模型对齐对于提升模型性能至关重要。然而由于反馈机制简单便捷，用户通常仅提供基本的二元信号（如"点赞"或"踩"）。而现有的大多数对齐研究依赖于基于偏好的方法，这类方法需要将正负反馈作为配对数据。我们提出二元分类器优化（BCO）技术，该技术仅利用二元反馈即可有效实现大语言模型的对齐。BCO通过训练二元分类器，将逻辑值作为隐式奖励函数，从而有效最小化直接偏好优化（DPO）损失。我们证明分类器训练采用的二元交叉熵损失函数构成了DPO损失的上界，同时引入的新型奖励偏移技术进一步缩小了两种损失之间的差距。我们在两种实验场景中验证了方法的有效性：首先在配对偏好数据集上，本方法达到与DPO相当的性能；其次在源自真实用户查询的Likert-5级评分数据集上，我们的模型在四种基础大语言模型和三个不同数据集上均展现出稳定有效的对齐能力，这证明了从二元信号中学习的方法优势。

---

## [LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning](https://arxiv.org/abs/2401.16185)

### Abstract
arXiv:2401.16185v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated significant potential in various tasks, including those requiring human-level intelligence, such as vulnerability detection. However, recent efforts to use LLMs for vulnerability detection remain preliminary, as they lack a deep understanding of whether a subject LLM's vulnerability reasoning capability stems from the model itself or from external aids such as knowledge retrieval and tooling support.
  In this paper, we aim to decouple LLMs' vulnerability reasoning from other capabilities, such as vulnerability knowledge adoption, context information retrieval, and advanced prompt schemes. We introduce LLM4Vuln, a unified evaluation framework that separates and assesses LLMs' vulnerability reasoning capabilities and examines improvements when combined with other enhancements.
  To support this evaluation, we construct UniVul, the first benchmark that provides retrievable knowledge and context-supplementable code across three representative programming languages: Solidity, Java, and C/C++. Using LLM4Vuln and UniVul, we test six representative LLMs (GPT-4.1, Phi-3, Llama-3, o4-mini, DeepSeek-R1, and QwQ-32B) for 147 ground-truth vulnerabilities and 147 non-vulnerable cases in 3,528 controlled scenarios. Our findings reveal the varying impacts of knowledge enhancement, context supplementation, and prompt schemes. We also identify 14 zero-day vulnerabilities in four pilot bug bounty programs, resulting in $3,576 in bounties.

### 摘要
大型语言模型（LLMs）在各类任务中展现出显著潜力，包括漏洞检测等需要人类智能水平的任务。然而，当前利用LLMs进行漏洞检测的研究仍处于初步阶段，因为这些研究未能深入解析目标LLM的漏洞推理能力究竟源于模型自身，还是依赖于知识检索与工具支持等外部辅助。

本文旨在将LLMs的漏洞推理能力与其他能力（如漏洞知识采纳、上下文信息检索和高级提示方案）进行解耦。我们提出LLM4Vuln——一个统一评估框架，该框架能分离并评估LLMs的漏洞推理能力，同时检验其与其他增强技术结合时的改进效果。

为支持评估工作，我们构建了首个跨三种代表性编程语言（Solidity、Java和C/C++）的可检索知识及上下文可补充代码基准测试集UniVul。通过LLM4Vuln和UniVul，我们在3,528个受控场景中对六种代表性LLM（GPT-4.1、Phi-3、Llama-3、o4-mini、DeepSeek-R1和QwQ-32B）进行了147个真实漏洞与147个非漏洞案例的测试。研究发现：知识增强、上下文补充和提示方案会产生差异化影响。此外，我们在四个漏洞赏金试点项目中发现了14个零日漏洞，共获得3,576美元奖金。

---

## [Outlier-weighed Layerwise Sampling for LLM Fine-tuning](https://arxiv.org/abs/2405.18380)

### Abstract
arXiv:2405.18380v3 Announce Type: replace-cross 
Abstract: The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks. However, the substantial size of LLMs presents significant challenges in training or fine-tuning. While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampling (OWS), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs. Unlike LoRA, which adds extra adapters to all layers, OWS strategically assigns higher sampling probabilities to layers with more outliers, selectively sampling only a few layers and fine-tuning their pre-trained weights. To further increase the number of fine-tuned layers without a proportional rise in memory costs, we incorporate gradient low-rank projection, further boosting the approach's performance. Our extensive experiments across various architectures, including LLaMa2 and Mistral, demonstrate that OWS consistently outperforms baseline approaches, including full fine-tuning. Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient. OWS allows us to fine-tune 7B LLMs with only 21GB of memory. Our code is available at https://github.com/pixeli99/OWS.

### 摘要
大型语言模型（LLM）的快速发展彻底改变了各类自然语言处理任务。然而，LLM庞大的参数量给训练或微调带来了巨大挑战。尽管低秩自适应（LoRA）等参数高效方法广受欢迎，但其性能通常逊色于全秩微调。本文受LLM层间异常值分布的启发，提出一种新型内存高效微调方法——异常值加权分层采样（OWS）。与LoRA在所有层级添加适配器的策略不同，OWS通过为异常值较多的层级分配更高采样概率，选择性地仅对少数层级采样并微调其预训练权重。为进一步增加微调层数而不成比例增加内存消耗，我们引入梯度低秩投影技术，从而进一步提升方法性能。在LLaMa2和Mistral等不同架构上的大量实验表明，OWS始终优于包括全微调在内的基线方法。具体而言，该方法在常识推理基准测试中实现平均准确率最高提升1.1%，在MMLU上提高3.0%，在MT-Bench上显著提升10%，同时保持更高内存效率。OWS仅需21GB内存即可完成70亿参数LLM的微调。代码已开源于https://github.com/pixeli99/OWS。

---

## [Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models](https://arxiv.org/abs/2403.20331)

### Abstract
arXiv:2403.20331v4 Announce Type: replace-cross 
Abstract: This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed $\textbf&#123;Unsolvable Problem Detection (UPD)&#125;$. Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs. The code is available at https://github.com/AtsuMiyai/UPD.

### 摘要
本文提出了一项评估大型多模态模型（LMMs）鲁棒理解能力的新任务——$	extbf&#123;不可解问题检测（UPD）&#125;$。尽管多项选择题回答（MCQA）被广泛用于评估LMMs的理解能力，但其无法确保模型真正理解答案。UPD通过检测LMMs在面对MCQA不可解问题时主动拒答的能力，验证模型是否真正掌握答案内涵。该任务涵盖三类问题：缺失答案检测（AAD）、不兼容答案集检测（IASD）以及不兼容视觉问题检测（IVQD），涉及答案缺失、选项不兼容、图文不匹配等不可解场景。我们构建了MM-UPD基准测试平台，用于多维度能力评估。实验表明，即使在现有基准测试中表现良好的主流LMMs，在MM-UPD上也存在显著缺陷，这揭示了当前基准测试忽略的新型可信度维度。深入分析发现，不同LMMs存在差异化瓶颈：对于大语言模型能力受限的LMMs，思维链与自我反思机制能有效提升性能。本研究期望为开发更可靠的LMMs提供新见解。代码已开源：https://github.com/AtsuMiyai/UPD。

---

## [FDC: Fast KV Dimensionality Compression for Efficient LLM Inference](https://arxiv.org/abs/2408.04107)

### Abstract
arXiv:2408.04107v3 Announce Type: replace-cross 
Abstract: In large-language models, memory constraints in the Key-Value Cache (KVC) pose a challenge during inference. In this work, we propose FDC, a fast KV dimensionality compression system that eliminates the decompression overhead incurred in the existing KV dimensionality compression system, Palu, and reduces attention time. Moreover, FDC employs adaptive compression, tailoring KV compression rates across heads and layers based on their contributions to inference to maximize overall compression while maintaining an accuracy loss constraint. Additionally, FDC enhances the attention kernel to balance the uneven workloads caused by the adaptive compression approach to further reduce attention computation latency. Comprehensive experiments demonstrate that compared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and delivers up to 1.97X throughput under the same latency, while maintaining 99% of the accuracy without compression. When state-of-the-art eviction and quantization methods are combined with FDC, they exhibit similar improvements compared to those combined with Palu. We open-sourced the code.

### 摘要
在大语言模型中，键值缓存（KVC）的内存约束对推理过程构成挑战。本研究提出FDC系统——一种快速键值维度压缩方案，该方案消除了现有键值维度压缩系统Palu的解压缩开销，并降低了注意力计算时间。FDC采用自适应压缩技术，根据各注意力头和网络层对推理的贡献度差异化调整键值压缩率，在保持精度损失约束的前提下实现整体压缩率最大化。此外，FDC通过优化注意力计算内核，平衡自适应压缩策略导致的工作负载不均问题，进一步降低注意力计算延迟。综合实验表明：相较于Palu系统，FDC能将任务完成时间（JCT）最高减少64%，在相同延迟条件下实现1.97倍吞吐量提升，同时保持无压缩状态下99%的模型精度。当与最先进的逐出法和量化方法结合时，FDC相较Palu组合方案仍展现出相似的性能提升。本研究已公开系统源代码。

---

## [Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents](https://arxiv.org/abs/2407.01887)

### Abstract
arXiv:2407.01887v4 Announce Type: replace-cross 
Abstract: In-Context Reinforcement Learning (ICRL) is a frontier paradigm to solve Reinforcement Learning (RL) problems in the foundation model era. While ICRL capabilities have been demonstrated in transformers through task-specific training, the potential of Large Language Models (LLMs) out-of-the-box remains largely unexplored. This paper investigates whether LLMs can generalize cross-domain to perform ICRL under the problem of Dueling Bandits (DB), a stateless preference-based RL setting. We find that the top-performing LLMs exhibit a notable zero-shot capacity for relative decision-making, which translates to low short-term weak regret across all DB environment instances by quickly including the best arm in duels. However, an optimality gap still exists between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and are sensitive to prompt variations. To bridge this gap, we propose an agentic flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates off-the-shelf DB algorithm support with LLM agents through fine-grained adaptive interplay. We show that LEAD has theoretical guarantees inherited from classic DB algorithms on both weak and strong regret. We validate its efficacy and robustness even with noisy and adversarial prompts. The design of such an agentic framework sheds light on how to enhance the trustworthiness of general-purpose LLMs generalized to in-context decision-making tasks.

### 摘要
上下文强化学习（ICRL）是基础模型时代解决强化学习（RL）问题的前沿范式。尽管已有研究通过任务特定训练在Transformer中验证了ICRL能力，但大型语言模型（LLMs）开箱即用的跨领域泛化潜力仍待探索。本文以基于偏好的无状态RL设置——决斗老虎机（DB）问题为研究对象，探究LLMs能否实现跨领域ICRL。研究发现：顶尖LLMs展现出显著的零样本相对决策能力，能通过快速纳入最优臂实现所有DB环境实例的低短期弱遗憾；但在强遗憾指标上，LLMs与经典DB算法仍存在最优性差距——即使明确提示，LLMs也难以实现收敛和持续利用，且对提示变化敏感。为弥合该差距，我们提出智能体流程框架LEAD（增强算法决斗的LLM），通过细粒度自适应交互将现成DB算法与LLM智能体融合。理论证明LEAD在弱/强遗憾指标上均继承了经典DB算法的理论保证，实验验证其即使在噪声和对抗性提示下仍保持高效稳健。该智能体框架的设计为提升通用LLMs在上下文决策任务中的可信性提供了新思路。

---

## [LLM4DSR: Leveraging Large Language Model for Denoising Sequential Recommendation](https://arxiv.org/abs/2408.08208)

### Abstract
arXiv:2408.08208v3 Announce Type: replace-cross 
Abstract: Sequential Recommenders generate recommendations based on users' historical interaction sequences. However, in practice, these collected sequences are often contaminated by noisy interactions, which significantly impairs recommendation performance. Accurately identifying such noisy interactions without additional information is particularly challenging due to the absence of explicit supervisory signals indicating noise. Large Language Models (LLMs), equipped with extensive open knowledge and semantic reasoning abilities, offer a promising avenue to bridge this information gap. However, employing LLMs for denoising in sequential recommendation presents notable challenges: 1) Direct application of pretrained LLMs may not be competent for the denoising task, frequently generating nonsensical responses; 2) Even after fine-tuning, the reliability of LLM outputs remains questionable, especially given the complexity of the denoising task and the inherent hallucinatory issue of LLMs.
  To tackle these challenges, we propose LLM4DSR, a tailored approach for denoising sequential recommendation using LLMs. We constructed a self-supervised fine-tuning task to activate LLMs' capabilities to identify noisy items and suggest replacements. Furthermore, we developed an uncertainty estimation module that ensures only high-confidence responses are utilized for sequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing corrected sequences to be flexibly applied across various recommendation models. Extensive experiments validate the superiority of LLM4DSR over existing methods.

### 摘要
序列推荐系统通过用户历史交互序列生成推荐。然而实践中，这些采集的序列常受到噪声交互的污染，严重影响推荐性能。由于缺乏明确的噪声监督信号，仅凭现有数据精准识别此类噪声交互具有极大挑战性。具备开放知识体系与语义推理能力的大语言模型（LLMs）为填补这一信息鸿沟提供了新思路。但将LLMs应用于序列推荐去噪面临显著挑战：1）直接使用预训练LLMs可能无法胜任去噪任务，常产生无意义响应；2）即使经过微调，LLMs输出的可靠性仍存疑，特别是考虑到去噪任务的复杂性及模型固有的幻觉问题。为此，我们提出LLM4DSR——一种基于LLMs的序列推荐去噪定制方案。通过构建自监督微调任务激活LLMs识别噪声项与生成替代项的能力，并开发不确定性估计模块确保仅采用高置信度响应进行序列修正。值得注意的是，LLM4DSR具有模型无关性，修正后的序列可灵活应用于各类推荐模型。大量实验验证了该方法相对于现有技术的优越性。

---

## [MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair](https://arxiv.org/abs/2408.09568)

### Abstract
arXiv:2408.09568v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown high capabilities in several software development-related tasks such as program repair, documentation, code refactoring, debugging, and testing. However, training these models requires massive amount of data and significant computational resources. Adapters are specialized, small modules designed for parameter efficient fine-tuning of LLMs for specific tasks, domains, or applications without requiring extensive retraining of the entire model. These adapters offer a more efficient way to customize LLMs for particular needs, leveraging the pre-existing capabilities of the large model. Model (and adapter) merging have emerged as a technique to develop one model capable of multiple tasks, with minimal or no training required. Although model and adapter merging has shown promising performance in domains such as natural language processing and computer vision, its applicability to software engineering tasks remains underexplored. In this paper, we investigate the effectiveness of merged adapters within the context of software engineering, with a particular focus on the Automated Program Repair (APR) task, through our approach, MergeRepair. In particular, we merge multiple task-specific adapters using three different merging methods, including weight-averaging, ties, and dare-ties, and evaluate the performance of the merged adapter on the APR task. We introduce a continual merging approach, a novel method in which we sequentially merge the task-specific adapters where the order and weight of the merged adapters play a significant role. We further compare the performance of our approach with a baseline method consisting of equal-weight merging applied on parameters of different adapters, where all adapters are of equal importance.

### 摘要
大型语言模型（LLMs）在程序修复、文档生成、代码重构、调试和测试等软件开发相关任务中展现出卓越能力。然而训练此类模型需要海量数据和巨额计算资源。适配器作为专用于参数高效微调的小型模块，可在无需完整模型重训练的前提下，使LLMs适应特定任务、领域或应用场景。这种机制能基于大模型现有能力实现高效定制化。模型（及适配器）融合技术应运而生，它通过最小化甚至零训练来构建多任务模型。尽管该技术在自然语言处理和计算机视觉领域已表现出优异性能，但其在软件工程任务中的适用性仍待探索。本文通过MergeRepair方法，重点研究适配器融合在软件工程领域（特别是自动程序修复任务APR）的有效性。我们采用权重平均、ties和dare-ties三种融合方法合并多个任务专用适配器，并评估融合后适配器在APR任务中的表现。创新性提出持续融合方法——通过顺序合并任务适配器（其中融合顺序与权重分配起关键作用）实现性能优化。进一步将本方法与基线方法（对不同适配器参数进行等权重融合）进行对比实验，所有适配器在基线中均具有同等重要性。

---

## [Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications](https://arxiv.org/abs/2408.11878)

### Abstract
arXiv:2408.11878v3 Announce Type: replace-cross 
Abstract: Financial LLMs hold promise for advancing financial tasks and domain-specific applications. However, they are limited by scarce corpora, weak multimodal capabilities, and narrow evaluations, making them less suited for real-world application. To address this, we introduce \textit&#123;Open-FinLLMs&#125;, the first open-source multimodal financial LLMs designed to handle diverse tasks across text, tabular, time-series, and chart data, excelling in zero-shot, few-shot, and fine-tuning settings. The suite includes FinLLaMA, pre-trained on a comprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K financial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning pairs for strong cross-modal reasoning. We comprehensively evaluate Open-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in zero-shot, few-shot, and supervised fine-tuning settings, introducing two new multimodal evaluation datasets. Our results show that Open-FinLLMs outperforms afvanced financial and general LLMs such as GPT-4, across financial NLP, decision-making, and multi-modal tasks, highlighting their potential to tackle real-world challenges. To foster innovation and collaboration across academia and industry, we release all codes (https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under OSI-approved licenses.

### 摘要
金融大语言模型（LLMs）在推动金融任务和领域专用应用方面展现出潜力，但其发展受限于语料稀缺、多模态能力薄弱及评估范围狭窄等问题，导致实际应用适配性不足。为此，我们推出首个开源多模态金融LLMs套件——\textit&#123;Open-FinLLMs&#125;，该套件能够处理文本、表格、时间序列和图表数据等多样化任务，并在零样本、少样本及微调场景中表现卓越。该套件包含三项核心模型：基于520亿token综合语料预训练的FinLLaMA；通过57.3万条金融指令微调的FinLLaMA-Instruct；以及整合143万组多模态调优对以强化跨模态推理的FinLLaVA。我们在14类金融任务、30个数据集和4项多模态任务上对Open-FinLLMs进行了零样本、少样本及监督微调的全面评估，并引入两个新型多模态评估数据集。实验结果表明，Open-FinLLMs在金融NLP、决策制定和多模态任务中均超越GPT-4等先进金融及通用LLMs，展现出解决实际挑战的潜力。为促进学术界与工业界的创新协作，我们依据OSI认证许可协议公开全部代码（https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE）和模型。

---

## [Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models](https://arxiv.org/abs/2409.06277)

### Abstract
arXiv:2409.06277v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have become indispensable in numerous real-world applications. However, fine-tuning these models at scale, especially in federated settings where data privacy and communication efficiency are critical, presents significant challenges. Existing approaches often resort to parameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but this typically comes at the cost of model accuracy. To this end, we propose federated full-parameter tuning at scale for LLMs (Ferret), the first first-order method with shared randomness to enable scalable full-parameter tuning of LLMs across decentralized data sources while maintaining competitive model accuracy. Ferret accomplishes this through three aspects: (i) it employs widely used first-order methods for efficient local updates; (ii) it projects these updates into a low-dimensional space to considerably reduce communication overhead; and (iii) it reconstructs local updates from this low-dimensional space with shared randomness to facilitate effective full-parameter global aggregation, ensuring fast convergence and competitive final performance. Our rigorous theoretical analyses and insights along with extensive experiments, show that Ferret significantly enhances the scalability of existing federated full-parameter tuning approaches by achieving high computational efficiency, reduced communication overhead, and fast convergence, all while maintaining competitive model accuracy. Our implementation is available at https://github.com/allen4747/Ferret.

### 摘要
大型语言模型（LLMs）已在众多实际应用中变得不可或缺。然而，在数据隐私和通信效率至关重要的联邦学习场景中，对这些模型进行大规模微调仍面临重大挑战。现有方法通常采用参数高效微调（PEFT）来降低通信开销，但这往往以牺牲模型准确性为代价。为此，我们提出面向LLMs的联邦全参数大规模微调方法（Ferret），这是首个采用共享随机性的优化方法，能够在分散数据源上实现LLMs的可扩展全参数微调，同时保持具有竞争力的模型精度。Ferret通过三方面实现这一目标：（i）采用广泛使用的一阶方法进行高效的本地更新；（ii）将这些更新投影至低维空间以显著降低通信开销；（iii）利用共享随机性从低维空间重构本地更新，从而实现有效的全参数全局聚合，确保快速收敛和具有竞争力的最终性能。我们的严格理论分析、深入见解及大量实验表明，Ferret通过实现高计算效率、降低通信开销和快速收敛，显著提升了现有联邦全参数微调方法的可扩展性，同时保持具有竞争力的模型精度。实现代码已发布于https://github.com/allen4747/Ferret。

---

## [Test-driven Software Experimentation with LASSO: an LLM Prompt Benchmarking Example](https://arxiv.org/abs/2410.08911)

### Abstract
arXiv:2410.08911v2 Announce Type: replace-cross 
Abstract: Empirical software engineering faces a critical gap: the lack of standardized tools for rapid development and execution of Test-Driven Software Experiments (TDSEs) -- that is, experiments that involve the execution of software subjects and the observation and analysis of their "de facto" run-time behavior. In this paper we present a general-purpose analysis platform called LASSO that provides a minimal set of domain-specific languages and data structures to conduct TDSEs. By empowering users with an executable scripting language to design and execute TDSEs, LASSO enables efficient evaluation of run-time semantics and execution characteristics in addition to statically determined properties. We present an example TDSE that demonstrates the practical benefits of LASSO's scripting capabilities for assessing the reliability of LLMs for code generation by means of a self-contained, reusable and extensible study script. The LASSO platform and live pipeline examples are publicly available at: https://softwareobservatorium.github.io/.

### 摘要
经验软件工程面临一个关键缺陷：缺乏用于快速开发和执行测试驱动软件实验（TDSEs）的标准化工具——这类实验涉及执行软件对象并观察分析其"实际"运行时行为。本文提出一个名为LASSO的通用分析平台，该平台提供一组最小化的领域特定语言和数据结构来实施TDSEs。通过赋予用户可执行脚本语言来设计和运行TDSEs，LASSO除了能评估静态属性外，还能高效评价运行时语义和执行特性。我们通过一个示例TDSE展示了LASSO脚本功能在评估LLM代码生成可靠性方面的实践价值，该示例采用自包含、可复用且可扩展的研究脚本实现。LASSO平台及实时流水线示例已公开于：https://softwareobservatorium.github.io/。

---

## [HSF: Defending against Jailbreak Attacks with Hidden State Filtering](https://arxiv.org/abs/2409.03788)

### Abstract
arXiv:2409.03788v2 Announce Type: replace-cross 
Abstract: With the growing deployment of LLMs in daily applications like chatbots and content generation, efforts to ensure outputs align with human values and avoid harmful content have intensified. However, increasingly sophisticated jailbreak attacks threaten this alignment, aiming to induce unsafe outputs. Current defense efforts either focus on prompt rewriting or detection, which are limited in effectiveness due to the various design of jailbreak prompts, or on output control and detection, which are computationally expensive as they require LLM inference. Therefore, designing a pre-inference defense method that resists diverse jailbreak prompts is crucial for preventing LLM jailbreak attacks. We observe that jailbreak attacks, safe queries, and harmful queries exhibit different clustering patterns within the LLM's hidden state representation space. This suggests that by leveraging the LLM's hidden state representational capabilities, we can analyze the LLM's forthcoming behavior and proactively intervene for defense. In this paper, we propose a jailbreak attack defense strategy based on a Hidden State Filter (HSF), a lossless architectural defense mechanism that enables the model to preemptively identify and reject adversarial inputs before the inference process begins. We activate its defensive potential through an additional plugin module, effectively framing the defense task as a classification problem. Experimental results on two benchmark datasets, utilizing three different LLMs, show that HSF significantly enhances resilience against six cutting-edge jailbreak attacks. It significantly reduces the success rate of jailbreak attacks while minimally impacting responses to benign user queries, with negligible inference overhead, and outperforming defense baselines.Our code and data are available at https://anonymous.4open.science/r/Hidden-State-Filtering-8652/

### 摘要
随着大型语言模型(LLMs)在聊天机器人和内容生成等日常应用中的广泛部署，确保输出符合人类价值观并避免有害内容的努力日益加强。然而，日益复杂的越狱攻击威胁着这种对齐性，其目的是诱导模型产生不安全输出。现有防御方法主要集中于提示词重写或检测(由于越狱提示的多样性而效果有限)，或输出控制与检测(需要LLM推理计算成本高昂)。因此，设计一种能抵抗多样化越狱提示的推理前防御方法对防止LLM越狱攻击至关重要。我们观察到越狱攻击、安全查询和有害查询在LLM隐藏状态表示空间中呈现不同的聚类模式，这表明通过利用LLM的隐藏状态表征能力，可以分析模型的后续行为并主动实施防御干预。本文提出基于隐藏状态过滤器(HSF)的越狱攻击防御策略，这是一种无损架构防御机制，使模型能在推理过程开始前预先识别并拒绝对抗性输入。我们通过附加插件模块激活其防御潜力，将防御任务有效构建为分类问题。在三个不同LLM和两个基准数据集上的实验结果表明，HSF显著增强了对六种前沿越狱攻击的抵御能力：在基本不影响良性用户查询响应的前提下，大幅降低越狱攻击成功率，且推理开销可忽略不计，性能优于现有防御基线。代码与数据详见https://anonymous.4open.science/r/Hidden-State-Filtering-8652/

---

## [Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization](https://arxiv.org/abs/2409.18433)

### Abstract
arXiv:2409.18433v2 Announce Type: replace-cross 
Abstract: While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still blank. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six state-of-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.

### 摘要
尽管从简单任务到困难任务的泛化能力对刻画语言模型（LLM）至关重要，但目前仍缺乏覆盖广泛复杂度范围且每个问题均带有细粒度难度标注的数据集。为弥补这一不足，我们提出了Easy2Hard-Bench——一个格式统一的基准数据集集合，包含数学与编程问题、国际象棋谜题及推理题等6个不同领域的基准数据集。每个数据集中的问题均标注了数值化的难度分数。为系统化评估问题难度，我们收集了现实世界中人类解题者或知名排行榜上LLM对每个问题的大量作答表现数据，并基于项目反应理论（IRT）和Glicko-2等成熟难度排名模型，统一计算问题的数值难度分数。此外，Easy2Hard-Bench中的数据集相较于以往集合具有更高比例的难题。通过对6个前沿LLM的广泛实验，我们对其在不同难度层级上的表现与泛化能力进行了全面分析，以期启发未来关于LLM泛化能力的研究。数据集发布于https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench。

---

## [SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment](https://arxiv.org/abs/2410.14676)

### Abstract
arXiv:2410.14676v3 Announce Type: replace-cross 
Abstract: Existing preference alignment is a one-size-fits-all alignment mechanism, where the part of the large language model (LLM) parametric knowledge with non-preferred features is uniformly blocked to all the users. However, this part of knowledge can be useful to advanced users whose expertise qualifies them to handle these information. The one-size-fits-all alignment mechanism undermines LLM's utility for these qualified users. To address this problem, we propose SudoLM, a framework that lets LLMs learn access control over specific parametric knowledge for users with different credentials via authorization alignment. SudoLM allows authorized users to unlock their access to all the parametric knowledge with an assigned SUDO key while blocking access to non-qualified users. Experiments on two application scenarios demonstrate that SudoLM effectively controls the user's access to the parametric knowledge and maintains its general utility.

### 摘要
现有偏好对齐机制采用一刀切的统一对齐方式，会大规模语言模型（LLM）参数知识中具有非偏好特征的部分对所有用户进行统一屏蔽。然而这部分知识对于具备专业能力处理此类信息的高级用户可能具有重要价值。这种统一对齐机制降低了LLM对这类合格用户的实用性。为解决该问题，我们提出SudoLM框架，通过授权对齐使LLM能够根据用户不同资质对特定参数知识实施访问控制。SudoLM允许授权用户通过分配的SUDO密钥解锁对所有参数知识的访问权限，同时继续屏蔽非合格用户的访问。在两个应用场景上的实验表明，SudoLM能有效控制用户对参数知识的访问，并保持模型的通用实用性。

---

## [Epistemic Integrity in Large Language Models](https://arxiv.org/abs/2411.06528)

### Abstract
arXiv:2411.06528v2 Announce Type: replace-cross 
Abstract: Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration $\unicode&#123;x2013&#125;$ where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.

### 摘要
大型语言模型日益成为人们依赖的信息来源，但其倾向于以高度自信生成虚假或误导性陈述的特性，给用户和社会带来了风险。本文针对认知误校准这一关键问题展开研究——即模型的语言自信度未能反映其真实内部确定性。我们引入了一个新的人工标注数据集和一种创新方法，用于测量大型语言模型（LLMs）的语言自信度，相较于先前基准，该方法将错误率降低了50%以上。经多个数据集验证，我们的方法揭示了模型在语言表达信息时的自信程度与其实际准确性之间存在严重错位。进一步的人类评估证实了这种误校准的严重性。这一证据凸显了LLMs过度自信所带来的紧迫风险，可能导致大规模用户误导。我们的框架为诊断这种误校准提供了关键进展，为跨领域实现更可信赖的人工智能指明了修正路径。

---

## [Unveiling and Addressing Pseudo Forgetting in Large Language Models](https://arxiv.org/abs/2411.11932)

### Abstract
arXiv:2411.11932v2 Announce Type: replace-cross 
Abstract: Although substantial efforts have been made to mitigate catastrophic forgetting in continual learning, the intrinsic mechanisms are not well understood. In this work, we demonstrate the existence of "pseudo forgetting": the performance degradation on previous tasks is not attributed to a loss of capabilities, but rather to the failure of the instructions to activate the appropriate model abilities. We show that the model's performance on previous tasks can be restored through two simple interventions: (1) providing partial external correct rationale, and (2) appending semantically meaningless suffixes to the original instructions, to guide the generation of correct rationales. Through empirical analysis of the internal mechanisms governing rationale generation, we reveal that models exhibiting pseudo forgetting show reduced instruction dependence during rationale generation, leading to suboptimal activation of their inherent capabilities. Based on this insight, we propose Rationale-Guidance Difficulty based Replay (RGD-R) framework that dynamically allocates replay data based on the model's ability to correctly leverage the intrinsic capabilities. Experimental results demonstrate that RGD-R effectively mitigates pseudo forgetting while maintaining model plasticity.

### 摘要
尽管在持续学习中已投入大量努力来缓解灾难性遗忘问题，但其内在机制仍未得到充分理解。本研究发现了一种"伪遗忘"现象：模型在先前任务上的性能下降并非源于能力丧失，而是由于指令未能有效激活模型应有的能力。我们证实通过两种简单干预即可恢复模型在历史任务中的表现：（1）提供部分外部正确推理依据；（2）在原始指令后附加语义无意义的后缀以引导生成正确推理。通过对推理生成内在机制的实证分析，我们发现表现出伪遗忘的模型在生成推理时呈现指令依赖性降低的特征，导致其固有能力的次优激活。基于此发现，我们提出基于推理引导难度的动态回放框架（RGD-R），该框架根据模型正确调用内在能力的情况动态分配回放数据。实验结果表明，RGD-R在保持模型可塑性的同时，能有效缓解伪遗忘现象。

---

## [LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired LDR-to-HDR Image Reconstruction](https://arxiv.org/abs/2410.15068)

### Abstract
arXiv:2410.15068v3 Announce Type: replace-cross 
Abstract: The translation of Low Dynamic Range (LDR) to High Dynamic Range (HDR) images is an important computer vision task. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired &#123;LDR,HDR&#125; datasets for model training. In addition, there is limited literature on using unpaired datasets for this task, that is, the model learns a mapping between domains, i.e., &#123;LDR,HDR&#125;. This paper proposes LLM-HDR, a method that integrates the perception of Large Language Models (LLM) into a modified semantic- and cycle-consistent adversarial architecture that utilizes unpaired &#123;LDR,HDR&#125; datasets for training. The method introduces novel artifact- and exposure-aware generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. LLM-HDR is the first to use an LLM for the &#123;LDR,HDR&#125; translation task in a self-supervised setup. The method achieves state-of-the-art performance across several benchmark datasets and reconstructs high-quality HDR images. The official website of this work is available at: https://github.com/HrishavBakulBarua/LLM-HDR

### 摘要
将低动态范围（LDR）图像转换为高动态范围（HDR）图像是一项重要的计算机视觉任务。现有研究主要采用传统非学习方法与现代数据驱动方法，聚焦于利用单曝光及多曝光LDR图像进行HDR重建。然而当前最先进方法大多需要高质量配对的&#123;LDR,HDR&#125;数据集进行模型训练，且关于使用非配对数据集（即模型学习&#123;LDR,HDR&#125;域间映射）的研究较为有限。本文提出LLM-HDR方法，通过将大语言模型（LLM）的感知能力整合至改进的语义与循环一致对抗架构中，利用非配对&#123;LDR,HDR&#125;数据集进行训练。该方法引入新型伪影与曝光感知生成器以解决视觉伪影消除问题，并采用编码器及损失函数确保语义一致性——这也是当前研究较少的议题。LLM-HDR是首个在自监督设置下将LLM应用于&#123;LDR,HDR&#125;转换任务的方法，在多个基准数据集上实现了最先进的性能，并能重建高质量的HDR图像。本项目官网详见：https://github.com/HrishavBakulBarua/LLM-HDR

---

## [Diffusion as Reasoning: Enhancing Object Navigation via Diffusion Model Conditioned on LLM-based Object-Room Knowledge](https://arxiv.org/abs/2410.21842)

### Abstract
arXiv:2410.21842v2 Announce Type: replace-cross 
Abstract: The Object Navigation (ObjectNav) task aims to guide an agent to locate target objects in unseen environments using partial observations. Prior approaches have employed location prediction paradigms to achieve long-term goal reasoning, yet these methods often struggle to effectively integrate contextual relation reasoning. Alternatively, map completion-based paradigms predict long-term goals by generating semantic maps of unexplored areas. However, existing methods in this category fail to fully leverage known environmental information, resulting in suboptimal map quality that requires further improvement. In this work, we propose a novel approach to enhancing the ObjectNav task, by training a diffusion model to learn the statistical distribution patterns of objects in semantic maps, and using the map of the explored regions during navigation as the condition to generate the map of the unknown regions, thereby realizing the long-term goal reasoning of the target object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the Room Guidance method, which leverages commonsense knowledge derived from large language models (LLMs) to guide the diffusion model in generating room-aware object distributions. Based on the generated map in the unknown region, the agent sets the predicted location of the target as the goal and moves towards it. Experiments on Gibson and MP3D show the effectiveness of our method.

### 摘要
物体导航（ObjectNav）任务旨在指导智能体利用局部观测在未知环境中定位目标物体。现有方法多采用位置预测范式实现长期目标推理，但这些方法往往难以有效整合上下文关系推理。另一种基于地图补全的范式通过生成未探索区域的语义地图来预测长期目标，但该类现有方法未能充分利用已知环境信息，导致地图质量欠佳仍需改进。本研究提出一种增强物体导航任务的新方法：通过训练扩散模型学习语义地图中物体的统计分布规律，并以导航过程中已探索区域地图为条件生成未知区域地图，从而实现目标物体的长期目标推理（即扩散即推理，DAR）。同时，我们提出房间引导方法，利用大语言模型（LLMs）提取的常识知识指导扩散模型生成具有房间感知的物体分布。智能体基于生成的未知区域地图，将预测目标位置设为导航点并向其移动。在Gibson和MP3D数据集上的实验验证了本方法的有效性。

---

## [TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training](https://arxiv.org/abs/2410.06511)

### Abstract
arXiv:2410.06511v3 Announce Type: replace-cross 
Abstract: The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens require sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes require non-trivial engineering effort.
  This paper introduces TorchTitan, an open-source, PyTorch-native distributed training system that unifies state-of-the-art techniques, streamlining integration and reducing overhead. TorchTitan enables 3D parallelism in a modular manner with elastic scaling, providing comprehensive logging, checkpointing, and debugging tools for production-ready training. It also incorporates hardware-software co-designed solutions, leveraging features like Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan facilitates custom recipe curation and comparison, allowing us to develop optimized training recipes for Llama 3.1 and provide guidance on selecting techniques for maximum efficiency based on our experiences.
  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking training optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized baselines.

### 摘要
大型语言模型（LLM）的发展对推动自然语言处理前沿应用至关重要。训练具有数十亿参数和数万亿标记的LLM需要复杂的分布式系统，该系统需整合并比较多种先进技术，以实现跨数千个加速器的高效扩展。然而，现有解决方案存在复杂度高、分散于多个库/代码仓库、缺乏互操作性及维护困难等问题，导致训练方案的构建与实证比较需要大量工程投入。

本文推出TorchTitan——一个基于PyTorch的原生开源分布式训练系统，它统一了前沿技术，简化集成并降低开销。该系统以模块化方式实现弹性扩展的3D并行，提供完备的日志记录、检查点与调试工具以支持生产级训练。同时整合了软硬件协同设计解决方案，利用Float8训练和SymmetricMemory等特性。作为灵活测试平台，TorchTitan支持自定义训练方案的构建与比较，我们借此为Llama 3.1开发了优化训练方案，并根据实践经验提供技术选型建议以最大化效率。

我们在80亿至4050亿参数的Llama 3.1系列模型上全面评估TorchTitan，验证了其卓越性能、模块化组合能力与弹性扩展性。通过叠加训练优化策略，在NVIDIA H100 GPU上相比优化基线实现了：128GPU规模（Llama 3.1 8B）1D并行加速65.08%，256GPU规模（Llama 3.1 70B）2D并行额外加速12.59%，512GPU规模（Llama 3.1 405B）3D并行额外加速30%。

---

## [Leveraging MLLM Embeddings and Attribute Smoothing for Compositional Zero-Shot Learning](https://arxiv.org/abs/2411.12584)

### Abstract
arXiv:2411.12584v2 Announce Type: replace-cross 
Abstract: Compositional zero-shot learning (CZSL) aims to recognize novel compositions of attributes and objects learned from seen compositions. Previous works disentangle attributes and objects by extracting shared and exclusive parts between the image pair sharing the same attribute (object), as well as aligning them with pretrained word embeddings to improve unseen attribute-object recognition. Despite the significant achievements of existing efforts, they are hampered by three limitations: (1) The efficacy of disentanglement is compromised due to the influence of the background and the intricate entanglement of attributes with objects in the same parts. (2) Existing word embeddings fail to capture complex multimodal semantic information. (3) Overconfidence exhibited by existing models in seen compositions hinders their generalization to novel compositions. Being aware of these, we propose a novel framework named multimodal large language model (MLLM) embeddings and attribute smoothing guided disentanglement for CZSL. First, we leverage feature adaptive aggregation modules to mitigate the impact of background, and utilize learnable condition masks to capture multi-granularity features for disentanglement. Moreover, the last hidden states of MLLM are employed as word embeddings for their superior representation capabilities. Furthermore, we propose attribute smoothing with auxiliary attributes generated by the large language model (LLM) for seen compositions to address the overconfidence challenge. Extensive experiments demonstrate that our method achieves state-of-the-art performance on three challenging datasets. The source code will be available at https://github.com/xud-yan/Trident .

### 摘要
组合零样本学习（CZSL）旨在识别从已知组合中学习到的属性与对象的新颖组合。先前研究通过提取共享相同属性（对象）的图像对之间的共有和独有部分来实现属性与对象的解耦，并将其与预训练词向量对齐以提升未知属性-对象组合的识别能力。尽管现有方法取得了显著成果，但仍受限于三个问题：（1）由于背景干扰以及属性与对象在同一部位的复杂纠缠，解耦效果受到影响；（2）现有词向量难以捕捉复杂的多模态语义信息；（3）现有模型对已知组合的过度自信阻碍了其对新颖组合的泛化能力。针对这些问题，我们提出一个名为"多模态大语言模型（MLLM）词向量与属性平滑引导解耦"的新框架。首先，采用特征自适应聚合模块减轻背景影响，并利用可学习条件掩码捕获多粒度特征以实现解耦；其次，利用MLLM最后隐藏层状态作为词向量以发挥其卓越的表征能力；此外，通过大语言模型（LLM）生成辅助属性对已知组合实施属性平滑，以解决过度自信问题。大量实验表明，我们的方法在三个具有挑战性的数据集上实现了最先进的性能。源代码将发布于https://github.com/xud-yan/Trident。

---

## [Selective Prompt Anchoring for Code Generation](https://arxiv.org/abs/2408.09121)

### Abstract
arXiv:2408.09121v5 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have transformed software development by automatically generating code from natural language. Yet challenges remain in generating fully correct code that aligns with user intent. Our study reveals that LLMs tend to pay less attention to user prompts as more code tokens are generated. We hypothesize that this attention dilution issue is an important reason for code generation errors. To mitigate this issue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay more attention to user intent when generating code. We evaluate SPA using six base LLMs across six benchmarks. Our results demonstrate that SPA enhances Pass@1 by up to 12.9%, consistently outperforming SOTA code generation methods in all settings. Our code is available at https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.

### 摘要
大语言模型（LLMs）的最新进展通过从自然语言自动生成代码，彻底改变了软件开发。然而，在生成完全正确且符合用户意图的代码方面仍存在挑战。我们的研究表明，随着生成代码标记数量的增加，LLMs对用户提示的关注度会逐渐降低。我们假设这种注意力稀释问题是导致代码生成错误的重要原因。为缓解这一问题，我们提出选择性提示锚定（SPA）方法，引导代码LLMs在生成代码时更多关注用户意图。我们在六个基准测试中使用六种基础LLMs对SPA进行评估。结果表明，SPA将Pass@1最高提升12.9%，在所有实验设置中均优于当前最先进的代码生成方法。代码已开源：https://github.com/magic-YuanTian/Selective-Prompt-Anchoring。

---

## [On Adversarial Robustness of Language Models in Transfer Learning](https://arxiv.org/abs/2501.00066)

### Abstract
arXiv:2501.00066v2 Announce Type: replace-cross 
Abstract: We investigate the adversarial robustness of LLMs in transfer learning scenarios. Through comprehensive experiments on multiple datasets (MBIB Hate Speech, MBIB Political Bias, MBIB Gender Bias) and various model architectures (BERT, RoBERTa, GPT-2, Gemma, Phi), we reveal that transfer learning, while improving standard performance metrics, often leads to increased vulnerability to adversarial attacks. Our findings demonstrate that larger models exhibit greater resilience to this phenomenon, suggesting a complex interplay between model size, architecture, and adaptation methods. Our work highlights the crucial need for considering adversarial robustness in transfer learning scenarios and provides insights into maintaining model security without compromising performance. These findings have significant implications for the development and deployment of LLMs in real-world applications where both performance and robustness are paramount.

### 摘要
我们研究了大型语言模型在迁移学习场景中的对抗鲁棒性。通过对多个数据集（MBIB仇恨言论、MBIB政治偏见、MBIB性别偏见）和不同模型架构（BERT、RoBERTa、GPT-2、Gemma、Phi）的系统性实验，我们发现迁移学习在提升标准性能指标的同时，往往会导致模型对抗攻击的脆弱性增加。研究结果表明，更大规模的模型对这种现象表现出更强的抵抗力，这揭示了模型规模、架构与适应方法之间复杂的相互作用。本研究强调在迁移学习场景中必须重视对抗鲁棒性问题，并为在保持模型性能的同时维护安全性提供了重要见解。这些发现对实际应用中需要兼顾性能与鲁棒性的大型语言模型开发和部署具有重要指导意义。

---

## [Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features](https://arxiv.org/abs/2412.00142)

### Abstract
arXiv:2412.00142v3 Announce Type: replace-cross 
Abstract: Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks. Despite strong performance, LMMs' generative outputs are not specialized for vision-language classification tasks (i.e., tasks with vision-language inputs and discrete labels) such as image classification and multiple-choice VQA. One key challenge in utilizing LMMs for these tasks is the extraction of useful features from generative LMMs. To overcome this, we propose an approach that leverages multimodal feature extraction from the LMM's latent space. Toward this end, we present Sparse Attention Vectors (SAVs) -- a finetuning-free method that leverages sparse attention head activations (fewer than 5% of the heads) in LMMs as strong feature representations. With only few-shot examples, SAVs demonstrate state-of-the-art performance compared to a variety of few-shot and finetuned baselines on a collection of vision-language classification tasks. Our experiments also imply that SAVs can scale in performance with additional examples and generalize to similar tasks, establishing SAVs as both effective and robust multimodal feature representations.

### 摘要
生成式大型多模态模型（如LLaVA和Qwen-VL）在多种视觉-语言任务中表现卓越。尽管性能强大，这类模型生成式输出并不专门针对视觉-语言分类任务（即输入为视觉-语言数据、输出为离散标签的任务，如图像分类和多选题视觉问答）。利用生成式多模态模型处理此类任务的核心挑战在于如何从其提取有效特征。为此，我们提出一种基于模型潜在空间的多模态特征提取方法。具体而言，我们开发了稀疏注意力向量（SAVs）——一种无需微调的技术，通过激活多模态模型中少量稀疏注意力头（少于总注意力头的5%）来获取强表征特征。实验表明，仅需少量样本，SAVs在多项视觉-语言分类任务上即能超越各类少样本学习和微调基线方法，达到最先进性能。进一步实验证实SAVs的性能可随样本量增加而提升，并能迁移至相似任务，这确立了SAVs作为高效且鲁棒的多模态特征表征方法的优势。

---

## [LLMs Can Simulate Standardized Patients via Agent Coevolution](https://arxiv.org/abs/2412.11716)

### Abstract
arXiv:2412.11716v2 Announce Type: replace-cross 
Abstract: Training medical personnel using standardized patients (SPs) remains a complex challenge, requiring extensive domain expertise and role-specific practice. Previous research on Large Language Model (LLM)-based SPs mostly focuses on improving data retrieval accuracy or adjusting prompts through human feedback. However, this focus has overlooked the critical need for patient agents to learn a standardized presentation pattern that transforms data into human-like patient responses through unsupervised simulations. To address this gap, we propose EvoPatient, a novel simulated patient framework in which a patient agent and doctor agents simulate the diagnostic process through multi-turn dialogues, simultaneously gathering experience to improve the quality of both questions and answers, ultimately enabling human doctor training. Extensive experiments on various cases demonstrate that, by providing only overall SP requirements, our framework improves over existing reasoning methods by more than 10\% in requirement alignment and better human preference, while achieving an optimal balance of resource consumption after evolving over 200 cases for 10 hours, with excellent generalizability. Our system will be available at https://github.com/ZJUMAI/EvoPatient.

### 摘要
使用标准化病人（SPs）培训医务人员仍是一项复杂挑战，需要大量领域专业知识和角色特异性实践。以往基于大语言模型（LLM）的SP研究主要集中于提高数据检索准确性或通过人工反馈调整提示，却忽视了病人智能体通过无监督模拟学习标准化表述模式、将数据转化为类人患者反应的关键需求。为此，我们提出EvoPatient——一种新型模拟病人框架，其中病人智能体与医生智能体通过多轮对话模拟诊断过程，同步积累经验以提升问答质量，最终实现人类医生培训。多病例实验表明，仅需提供整体SP要求，本框架在需求对齐度和人类偏好方面较现有推理方法提升超10%，经过10小时200例进化后达到资源消耗最优平衡，并展现出色泛化能力。系统将在https://github.com/ZJUMAI/EvoPatient开源。

---

## [Rational Tuning of LLM Cascades via Probabilistic Modeling](https://arxiv.org/abs/2501.09345)

### Abstract
arXiv:2501.09345v4 Announce Type: replace-cross 
Abstract: Understanding the reliability of large language models (LLMs) has recently garnered significant attention. Given LLMs' propensity to hallucinate, as well as their high sensitivity to prompt design, it is already challenging to predict the performance of an individual LLM. However, the problem becomes more complex for compound LLM systems such as cascades, where in addition to each model's standalone performance, we must understand how the error rates of different models interact. In this paper, we present a probabilistic model for the joint performance distribution of a sequence of LLMs, which enables a framework for rationally tuning the confidence thresholds of a LLM cascade using continuous optimization. Compared to selecting confidence thresholds using Bayesian optimization, our parametric Markov-copula model yields more favorable error-cost trade-offs, improving the area under the error-cost curve by 4.3% on average for cascades with $k\geq 3$ models. In the low-sample regime with $n \leq 30$ training examples, the performance improvement widens to 10.2%, suggesting that our framework's inductive assumptions about the interactions between the error rates of different LLMs enhance sample efficiency. Overall, our Markov-copula model provides a rational basis for tuning LLM cascade performance and points to the potential of probabilistic methods in analyzing systems of LLMs.

### 摘要
理解大型语言模型（LLMs）的可靠性近来受到广泛关注。由于LLMs易产生幻觉且对提示设计高度敏感，预测单个LLM的性能已颇具挑战性。然而对于级联等复合LLM系统，问题更为复杂——除了每个模型的独立性能外，还需理解不同模型错误率间的相互作用。本文提出了一种序列化LLMs联合性能分布的概率模型，通过连续优化为理性调整LLM级联置信度阈值提供了框架。与贝叶斯优化选择阈值相比，我们的参数化马尔可夫-耦合模型能产生更优的错误-成本权衡，在$k\geq 3$模型的级联中平均将错误-成本曲线下面积提升4.3%。在训练样本量$n \leq 30$的低样本场景下，性能提升幅度扩大至10.2%，表明本框架对不同LLM错误率交互关系的归纳假设增强了样本效率。总体而言，马尔可夫-耦合模型为调整LLM级联性能提供了理性基础，并揭示了概率方法在分析LLM系统方面的潜力。

---

## [Unraveling Token Prediction Refinement and Identifying Essential Layers in Language Models](https://arxiv.org/abs/2501.15054)

### Abstract
arXiv:2501.15054v2 Announce Type: replace-cross 
Abstract: This research aims to unravel how large language models (LLMs) iteratively refine token predictions through internal processing. We utilized a logit lens technique to analyze the model's token predictions derived from intermediate representations. Specifically, we focused on (1) how LLMs access and utilize information from input contexts, and (2) how positioning of relevant information affects the model's token prediction refinement process. On a multi-document question answering task with varying input context lengths, we found that the depth of prediction refinement (defined as the number of intermediate layers an LLM uses to transition from an initial correct token prediction to its final, stable correct output), as a function of the position of relevant information, exhibits an approximately inverted U-shaped curve. We also found that the gap between these two layers, on average, diminishes when relevant information is positioned at the beginning or end of the input context. This suggested that the model requires more refinements when processing longer contexts with relevant information situated in the middle. Furthermore, our findings indicate that not all layers are equally essential for determining final correct outputs. Our analysis provides insights into how token predictions are distributed across different conditions, and establishes important connections to existing hypotheses and previous findings in AI safety research and development.

### 摘要
本研究旨在揭示大型语言模型（LLMs）如何通过内部处理迭代优化其token预测。我们采用logit lens技术分析模型基于中间表征生成的token预测结果，重点探究：（1）LLMs如何获取并利用输入上下文信息；（2）相关信息的位置如何影响模型token预测的优化过程。通过多文档问答任务（输入上下文长度可变）的实验，我们发现预测优化深度（定义为LLM从初始正确token预测过渡到最终稳定正确输出所需的中间层数）随相关信息位置的变化呈现近似倒U型曲线。当相关信息位于输入上下文首尾位置时，这两层之间的间隔平均会缩小，这表明模型在处理相关信息位于中间位置的长上下文时需要更多优化步骤。此外，研究发现并非所有层对最终正确输出的判定都同等重要。我们的分析揭示了不同条件下token预测的分布规律，并与AI安全研究领域的现有假设及先前发现建立了重要关联。

---

## [Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models](https://arxiv.org/abs/2501.08248)

### Abstract
arXiv:2501.08248v3 Announce Type: replace-cross 
Abstract: Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and perform retrieval and reasoning directly -- a capability we define as In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like LOFT often overestimate LCLM performance by providing overly simplified contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs in more realistic scenarios by including confounding passages retrieved with strong retrievers. We then propose three methods to enhance LCLM performance: (1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which uses attention heads to filter and de-noise long contexts during decoding, and (3) joint retrieval head training alongside the generation head. Our evaluation of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks despite being a much smaller model.

### 摘要
长上下文语言模型（LCLMs）的最新进展有望通过简化流程来改变检索增强生成（RAG）。凭借其扩展的上下文窗口，LCLMs能够直接处理整个知识库并执行检索和推理——我们将这一能力定义为上下文内检索与推理（ICR^2）。然而，现有基准测试（如LOFT）往往通过提供过于简化的上下文而高估了LCLMs的性能。为解决这一问题，我们提出了ICR^2基准，该基准通过包含由强大检索器获取的干扰性段落，在更现实的场景中评估LCLMs。随后，我们提出三种方法来提升LCLMs的性能：（1）检索后生成微调，（2）检索注意力探测，该方法在解码过程中利用注意力头过滤和去噪长上下文，（3）与生成头联合训练的检索头。我们在LOFT和ICR^2上对五种知名LCLMs的评估表明，应用于Mistral-7B的最佳方法相比原始RAG和监督微调分别取得了显著提升：在LOFT上精确匹配得分提高了+17和+15分，在ICR^2上提高了+13和+2分。尽管模型规模小得多，该方法在多数任务上甚至超越了GPT-4-Turbo的表现。

---

## [GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models](https://arxiv.org/abs/2501.12956)

### Abstract
arXiv:2501.12956v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial resource requirements. While low-bit quantized weights can reduce memory usage and improve inference efficiency, current hardware lacks native support for mixed-precision General Matrix Multiplication (mpGEMM), resulting in inefficient dequantization-based implementations. Moreover, uniform quantization methods often fail to capture weight distributions adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive Non-Uniform Quantization), a layer-wise post-training non-uniform quantization framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ achieves superior quantization performance by utilizing a training-free, GPU-adaptive optimization algorithm to efficiently reduce layer-wise quantization errors. Extensive experiments demonstrate GANQ's ability to reduce the perplexity gap from the FP16 baseline compared to state-of-the-art methods for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single NVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\times$ speedup over the baseline, advancing memory and inference efficiency in LLM deployment.

### 摘要
大型语言模型（LLMs）因资源需求庞大而面临显著部署挑战。虽然低位量化权重可降低内存占用并提升推理效率，但当前硬件缺乏对混合精度通用矩阵乘法（mpGEMM）的原生支持，导致基于反量化的实现效率低下。此外，均匀量化方法往往无法充分捕捉权重分布特性，引发性能下降。我们提出GANQ（GPU自适应非均匀量化）——一种面向硬件高效查表式mpGEMM优化的逐层训练后非均匀量化框架。该框架通过免训练的GPU自适应优化算法有效降低逐层量化误差，从而获得卓越的量化性能。大量实验表明，在3比特和4比特量化场景下，GANQ相比最先进方法能显著缩小与FP16基线的困惑度差距。当部署于NVIDIA RTX 4090 GPU时，GANQ量化模型相较基线实现最高2.57倍的加速比，推进了LLM部署中的内存与推理效率。

---

## [GraphRAG under Fire](https://arxiv.org/abs/2501.14050)

### Abstract
arXiv:2501.14050v3 Announce Type: replace-cross 
Abstract: GraphRAG advances retrieval-augmented generation (RAG) by structuring external knowledge as multi-scale knowledge graphs, enabling language models to integrate both broad context and granular details in their generation. While GraphRAG has demonstrated success across domains, its security implications remain largely unexplored. To bridge this gap, this work examines GraphRAG's vulnerability to poisoning attacks, uncovering an intriguing security paradox: existing RAG poisoning attacks are less effective under GraphRAG than conventional RAG, due to GraphRAG's graph-based indexing and retrieval; yet, the same features also create new attack surfaces. We present GragPoison, a novel attack that exploits shared relations in the underlying knowledge graph to craft poisoning text capable of compromising multiple queries simultaneously. GragPoison employs three key strategies: (i) relation injection to introduce false knowledge, (ii) relation enhancement to amplify poisoning influence, and (iii) narrative generation to embed malicious content within coherent text. Empirical evaluation across diverse datasets and models shows that GragPoison substantially outperforms existing attacks in terms of effectiveness (up to 98% success rate) and scalability (using less than 68% poisoning text) on multiple variations of GraphRAG. We also explore potential defensive measures and their limitations, identifying promising directions for future research.

### 摘要
GraphRAG通过将外部知识构建为多尺度知识图谱，改进了检索增强生成（RAG）技术，使语言模型能够在生成过程中整合广泛的上下文和精细的细节。尽管GraphRAG已在多个领域取得成功，但其安全性影响尚未得到充分探索。为填补这一空白，本研究探讨了GraphRAG对投毒攻击的脆弱性，揭示了一个有趣的安全悖论：由于GraphRAG基于图的索引和检索机制，现有RAG投毒攻击对其效果较弱；然而，这些特性也创造了新的攻击面。我们提出了GragPoison攻击，该攻击利用底层知识图谱中的共享关系构造投毒文本，可同时破坏多个查询。GragPoison采用三种关键策略：（i）关系注入以引入虚假知识，（ii）关系增强以扩大投毒影响，（iii）叙事生成将恶意内容嵌入连贯文本。跨多种数据集和模型的实证评估表明，在GraphRAG的多个变体上，GragPoison在有效性（成功率高达98%）和可扩展性（使用少于68%的投毒文本）方面显著优于现有攻击。我们还探讨了潜在的防御措施及其局限性，为未来研究指明了有前景的方向。

---

## [Scaling Inference-Efficient Language Models](https://arxiv.org/abs/2501.18107)

### Abstract
arXiv:2501.18107v2 Announce Type: replace-cross 
Abstract: Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to 3.5x difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by 1.8x while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff. Notably, our experiments reveal that wider and shallower models can yield efficiency gains while preserving accuracy.

### 摘要
缩放定律是预测大型语言模型性能的强大工具。然而，现有缩放定律未能充分考虑推理成本。本研究首先证明模型架构会影响推理延迟，相同规模的模型可能存在高达3.5倍的延迟差异。为解决这一问题，我们改进Chinchilla缩放定律，实现模型参数量、训练标记数和模型架构的协同优化。鉴于具有相似训练损失的模型在下游评估中存在性能差距，我们还提出一种基于修正缩放定律的新型方法，用于训练推理高效的模型。我们通过大量实证研究来拟合和评估这种推理感知的缩放定律，实验涵盖8000万至10亿参数、16亿至300亿训练标记及多种模型结构，共训练63个模型。基于推理高效缩放定律和模型选择方法，我们发布了Morph-1B模型，在保持下游任务精度的同时，相比开源模型将推理延迟降低1.8倍，推动了精度-延迟权衡的帕累托前沿。值得注意的是，实验表明更宽更浅的模型可在保持精度的同时提升效率。

---

## [Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning](https://arxiv.org/abs/2502.01968)

### Abstract
arXiv:2502.01968v2 Announce Type: replace-cross 
Abstract: Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant, uninformative, or even harmful. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance. In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks. Our method filters out uninformative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves downstream performance. Code is available at https://github.com/UCSC-REAL/TokenCleaning.

### 摘要
最近研究表明，在大语言模型（LLM）的监督微调（SFT）过程中，数据质量比数量更为重要。虽然大多数数据清洗方法集中于对整个样本进行过滤，但样本内部各个标记（token）的质量可能存在显著差异。在预训练完成后，即使是高质量样本中，与任务无关的模式或短语也可能存在冗余、缺乏信息量甚至有害的情况。继续对这些模式进行微调可能收益有限，甚至会导致下游任务性能下降。本文从噪声标签的角度研究标记质量，并提出了一种适用于SFT任务的通用标记清洗流程。我们的方法在保留携带关键任务信息标记的同时，能够过滤掉无信息量的标记。具体而言，我们首先通过评估模型更新对每个标记的影响来衡量其质量，随后应用基于阈值的分离方法。标记影响力可通过固定参考模型单次测量，或采用自进化参考模型进行迭代测量。我们通过误差上界从理论上分析了两种方法的优势与局限性。大量实验表明，该框架能持续提升下游任务性能。代码已发布于https://github.com/UCSC-REAL/TokenCleaning。

---

## [BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning](https://arxiv.org/abs/2501.18858)

### Abstract
arXiv:2501.18858v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Within this framework, we introduce the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in two steps. First, it generates high-quality rationales by approximating the optimal thinking process through reinforcement learning, using a novel reward shaping mechanism. Second, it enhances the base LLM by maximizing the joint probability of rationale generation with respect to the model's parameters. Theoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$ representing the number of iterations. Empirical evaluations on math and coding benchmarks demonstrate that our approach consistently improves performance across different base models without requiring human-annotated thinking processes. In addition, BRiTE demonstrates superior performance compared to existing algorithms that bootstrap thinking processes use alternative methods such as rejection sampling, and can even match or exceed the results achieved through supervised fine-tuning with human-annotated data.

### 摘要
大型语言模型（LLMs）在复杂推理任务中展现出卓越能力，但生成可靠的推理过程仍面临重大挑战。我们提出一个统一的概率框架，通过融合潜在思维过程与评估信号的新型图模型，对LLM推理进行形式化建模。在此框架下，我们引入自举强化思维过程（BRiTE）算法，其运作分为两个阶段：首先，通过采用新型奖励塑造机制的强化学习逼近最优思维过程，生成高质量理论依据；其次，通过最大化理论依据生成与模型参数的联合概率来增强基础LLM。理论上，我们证明BRiTE以$1/T$的速率收敛（$T$为迭代次数）。在数学与编程基准测试中的实证评估表明，该方法无需人工标注思维过程即可持续提升不同基础模型的性能。此外，与采用拒绝采样等替代方法自举思维过程的现有算法相比，BRiTE展现出更优性能，甚至可匹配或超越基于人工标注数据进行监督微调所获得的结果。

---

## [When Incentives Backfire, Data Stops Being Human](https://arxiv.org/abs/2502.07732)

### Abstract
arXiv:2502.07732v2 Announce Type: replace-cross 
Abstract: Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content -- it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations -- rather than relying solely on external incentives -- can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.

### 摘要
人工智能的发展长期依赖人类生成的数据资源，从专业标注平台到开放互联网皆是如此。然而大型语言模型的广泛应用正威胁着这些平台人类生成数据的质量与真实性。我们认为该问题不仅涉及过滤AI生成内容这一表层挑战，更暴露出数据收集系统设计中的深层缺陷。现行系统往往以牺牲人类内在动机为代价，过度追求速度、规模与效率，导致参与度下降与数据质量劣化。我们提出，通过重构数据收集系统以契合贡献者的内在动机——而非单纯依赖外部激励——能够在维持大规模高质量数据采集的同时，保障贡献者信任与长期参与意愿。

---

## [Multi-agent Architecture Search via Agentic Supernet](https://arxiv.org/abs/2502.04180)

### Abstract
arXiv:2502.04180v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \textbf&#123;agentic supernet&#125;, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\textit&#123;e.g.&#125;, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \textbf&#123;(I)&#125; requires only $6\sim45\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \textbf&#123;(II)&#125; surpasses them by $0.54\%\sim11.82\%$, and \textbf&#123;(III)&#125; enjoys superior cross-dataset and cross-LLM-backbone transferability.

### 摘要
大型语言模型（LLM）赋能的智能体系统通过规范化的协作与交互拓展了个体智能体的认知边界，但构建此类系统通常需要耗费大量人工设计。尽管已有方法可自动设计智能体工作流，但这些方法通常旨在寻找静态、复杂且通用的系统架构，无法根据查询问题的难度和领域动态分配推理资源。为解决这一挑战，我们摒弃了构建单一智能体系统的传统思路，转而优化**智能体超网**——一种概率化、连续分布的智能体架构空间。本文提出MaAS自动化框架，该框架能够从超网中采样出与查询任务适配的智能体系统，在提供高质量解决方案的同时实现定制化的资源分配（如LLM调用次数、工具调用次数及token消耗）。在六个基准测试上的综合评估表明，MaAS具有以下优势：**（I）**仅需现有手工或自动化多智能体系统6%~45%的推理成本；**（II）**性能超越现有系统0.54%~11.82%；**（III）**具备优异的跨数据集和跨LLM骨干模型的迁移能力。

---

## [Refining Adaptive Zeroth-Order Optimization at Ease](https://arxiv.org/abs/2502.01014)

### Abstract
arXiv:2502.01014v2 Announce Type: replace-cross 
Abstract: Recently, zeroth-order (ZO) optimization plays an essential role in scenarios where gradient information is inaccessible or unaffordable, such as black-box systems and resource-constrained environments. While existing adaptive methods such as ZO-AdaMM have shown promise, they are fundamentally limited by their underutilization of moment information during optimization, usually resulting in underperforming convergence. To overcome these limitations, this paper introduces Refined Adaptive Zeroth-Order Optimization (R-AdaZO). Specifically, we first show the untapped variance reduction effect of first moment estimate on ZO gradient estimation, which improves the accuracy and stability of ZO updates. We then refine the second moment estimate based on these variance-reduced gradient estimates to better capture the geometry of the optimization landscape, enabling a more effective scaling of ZO updates. We present rigorous theoretical analysis to show (a) the first analysis to the variance reduction of first moment estimate in ZO optimization, (b) the improved second moment estimates with a more accurate approximation of its variance-free ideal, (c) the first variance-aware convergence framework for adaptive ZO methods, which may be of independent interest, and (d) the faster convergence of R-AdaZO than existing baselines like ZO-AdaMM. Our extensive experiments, including synthetic problems, black-box adversarial attack, and memory-efficient fine-tuning of large language models (LLMs), further verify the superior convergence of R-AdaZO, indicating that R-AdaZO offers an improved solution for real-world ZO optimization challenges.

### 摘要
近年来，零阶（ZO）优化在梯度信息无法获取或成本过高的场景（如黑盒系统和资源受限环境）中发挥着关键作用。尽管现有自适应方法（如ZO-AdaMM）展现出潜力，但其根本局限在于优化过程中对矩信息利用不足，通常导致收敛性能欠佳。为突破这些限制，本文提出精细化自适应零阶优化方法（R-AdaZO）。具体而言，我们首先揭示一阶矩估计在ZO梯度估计中未被挖掘的方差缩减效应，该效应能提升ZO更新的精度与稳定性。随后基于这些方差缩减后的梯度估计改进二阶矩估计，以更精准捕捉优化曲面的几何特性，从而实现ZO更新的更有效缩放。我们通过严格理论分析证明：（a）首次对ZO优化中一阶矩估计方差缩减效应的理论分析；（b）改进的二阶矩估计能更准确地逼近其无方差理想值；（c）首个针对自适应ZO方法的方差感知收敛框架，该框架可能具有独立研究价值；（d）R-AdaZO较ZO-AdaMM等基线方法具有更快收敛速度。在合成问题、黑盒对抗攻击和大语言模型（LLM）内存高效微调等广泛实验中的表现，进一步验证了R-AdaZO的优越收敛性，表明该方法为现实世界ZO优化挑战提供了更优解决方案。

---

## [RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs](https://arxiv.org/abs/2502.07424)

### Abstract
arXiv:2502.07424v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit strong multilingual performance despite being predominantly trained on English-centric corpora. This raises a fundamental question: How do LLMs achieve such multilingual capabilities? Focusing on languages written in non-Roman scripts, we investigate the role of Romanization - the representation of non-Roman scripts using Roman characters - as a potential bridge in multilingual processing. Using mechanistic interpretability techniques, we analyze next-token generation and find that intermediate layers frequently represent target words in Romanized form before transitioning to native script, a phenomenon we term Latent Romanization. Further, through activation patching experiments, we demonstrate that LLMs encode semantic concepts similarly across native and Romanized scripts, suggesting a shared underlying representation. Additionally, for translation into non-Roman script languages, our findings reveal that when the target language is in Romanized form, its representations emerge earlier in the model's layers compared to native script. These insights contribute to a deeper understanding of multilingual representation in LLMs and highlight the implicit role of Romanization in facilitating language transfer.

### 摘要
尽管大型语言模型（LLMs）主要基于英语语料库进行训练，却展现出强大的多语言性能。这引发了一个根本性问题：LLMs如何实现这种多语言能力？针对非罗马字母书写语言，我们研究了罗马化（即用罗马字符表示非罗马文字）作为多语言处理潜在桥梁的作用。通过机制可解释性技术分析下一词元生成过程，发现中间层在转向原生文字前常以罗马化形式表示目标词汇，这种现象我们称为'潜在罗马化'。进一步通过激活修补实验证明，LLMs对原生文字和罗马化文字编码的语义概念具有相似性，表明存在共享的底层表征。此外，在翻译为非罗马字母语言时，当目标语言以罗马化形式呈现时，其表征会比原生文字更早出现在模型层级中。这些发现深化了对LLMs多语言表征机制的理解，并揭示了罗马化在促进语言迁移中的隐性作用。

---

## [Measuring Diversity in Synthetic Datasets](https://arxiv.org/abs/2502.08512)

### Abstract
arXiv:2502.08512v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing methods. Code is available at: https://github.com/bluewhalelab/dcscore.

### 摘要
大型语言模型（LLMs）被广泛用于生成各类自然语言处理（NLP）任务的合成数据集，例如文本分类与摘要生成。然而，如何准确衡量这些合成数据集的多样性——这一对模型鲁棒性至关重要的指标——仍存在显著挑战。本文提出DCScore，一种基于分类视角的合成数据集多样性度量新方法。该方法通过将多样性评估构建为样本分类任务，利用样本间的互相关性进行量化。我们进一步从理论上验证了DCScore满足的多样性公理，阐明其作为原则性评估方法的特性。在合成数据集上的实验表明，DCScore与多种多样性伪真值具有更强的相关性，印证了其有效性。此外，实验与理论分析均证明，相较于现有方法，DCScore能显著降低计算成本。代码已开源：https://github.com/bluewhalelab/dcscore。

---

## [SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering](https://arxiv.org/abs/2502.06994)

### Abstract
arXiv:2502.06994v2 Announce Type: replace-cross 
Abstract: Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator's understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator's actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents' capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent &lt;= 3.33% to Claude-3.5-Sonnet &gt;= 28.18%), their consistently low collaboration willingness (&lt;= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents' resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: https://xhguo7.github.io/SyncMind/.

### 摘要
软件工程（SE）正日益走向协作化，开发者们需要在共享的复杂代码库上协同工作。在共享环境中实现有效协作，要求参与者（无论是人类还是AI智能体）在环境演变过程中保持同步理解。当协作者的理解与当前状态出现偏差时（我们称之为"失步挑战"），其操作可能会失败，导致集成问题。本研究提出SyncMind框架，系统性地定义了大型语言模型（LLM）智能体在协作软件工程（CSE）中面临的失步问题。基于SyncMind，我们构建了SyncBench基准测试集，包含来自21个热门GitHub仓库的24,332个真实CSE场景中的智能体失步实例，并配有可执行的验证测试。SyncBench实验揭示了现有LLM智能体的关键能力与局限：不同智能体间存在显著性能差距（从Llama-3.1智能体的≤3.33%到Claude-3.5-Sonnet的≥28.18%），其协作意愿持续偏低（≤4.86%）表明现有LLM在CSE中存在根本性局限。然而当协作发生时，其与失步恢复成功率呈正相关。智能体在资源感知型失步恢复中表现差异微小，进一步揭示了其严重缺乏资源意识与适应性，这为未来资源高效的协作系统指明了方向。代码与数据已在项目网站公开：https://xhguo7.github.io/SyncMind/。

---

## [Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search](https://arxiv.org/abs/2502.04951)

### Abstract
arXiv:2502.04951v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering precise and efficient responses by integrating external databases with pre-existing knowledge. However, we observe that these AIPSEs raise risks such as quoting malicious content or citing malicious websites, leading to harmful or unverified information dissemination. In this study, we conduct the first safety risk quantification on seven production AIPSEs by systematically defining the threat model, risk type, and evaluating responses to various query types. With data collected from PhishTank, ThreatBook, and LevelBlue, our findings reveal that AIPSEs frequently generate harmful content that contains malicious URLs even with benign queries (e.g., with benign keywords). We also observe that directly querying a URL will increase the number of main risk-inclusive responses, while querying with natural language will slightly mitigate such risk. Compared to traditional search engines, AIPSEs outperform in both utility and safety. We further perform two case studies on online document spoofing and phishing to show the ease of deceiving AIPSEs in the real-world setting. To mitigate these risks, we develop an agent-based defense with a GPT-4.1-based content refinement tool and a URL detector. Our evaluation shows that our defense can effectively reduce the risk, with only a minor cost of reducing available information by approximately 10.7%. Our research highlights the urgent need for robust safety measures in AIPSEs.

### 摘要
大型语言模型（LLMs）的最新进展显著增强了人工智能搜索引擎（AIPSEs）的能力，通过将外部数据库与既有知识相结合，提供精准高效的响应。然而，我们观察到这些AIPSEs存在引用恶意内容或指向恶意网站的风险，导致有害或未经核实的信息传播。本研究首次对七款生产级AIPSEs进行了安全风险量化，通过系统定义威胁模型、风险类型，并评估其对各类查询的响应。基于PhishTank、ThreatBook和LevelBlue收集的数据，我们发现AIPSEs即使面对良性查询（如含无害关键词）也频繁生成包含恶意URL的有害内容。同时发现，直接查询URL会显著增加包含主要风险的响应数量，而使用自然语言查询可略微降低此类风险。与传统搜索引擎相比，AIPSEs在实用性和安全性方面均表现更优。我们进一步通过在线文档伪造和钓鱼攻击的案例研究，展示了现实场景中欺骗AIPSEs的简易性。为缓解风险，我们开发了一种基于智能体的防御方案，整合了GPT-4.1内容净化工具和URL检测器。评估表明该方案能有效降低风险，仅以减少约10.7%可用信息为代价。本研究揭示了AIPSEs亟需建立强健的安全防护机制。

---

## [Automated Capability Discovery via Foundation Model Self-Exploration](https://arxiv.org/abs/2502.07577)

### Abstract
arXiv:2502.07577v3 Announce Type: replace-cross 
Abstract: Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of these abilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce Automated Capability Discovery (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers a diverse spectrum of surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically generates thousands of distinct tasks, which are then clustered to reveal dozens of broader capability areas and failure modes, that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems. All code and evaluation logs are open-sourced at https://github.com/conglu1997/ACD.

### 摘要
基础模型已成为通用型智能助手，通过互联网规模数据的训练，在众多领域展现出多样化能力。精确刻画这些能力的全谱范围及新模型的潜在风险仍具挑战性，现有评估方法通常需要大量人工投入，且随着模型能力提升，设计更高难度测试的代价日益增大。我们提出自动化能力发现框架（ACD），该框架指定一个基础模型作为'科学家'系统性地提出开放式任务，以探测目标模型（可能是其自身）的能力。通过将前沿模型与开放式研究理念相结合，ACD能自动、系统地发现目标模型中多样化的意外能力与缺陷。我们在多个基础模型系列（包括GPT、Claude和Llama）上验证ACD，表明其可自动生成数千项差异化任务，经聚类后揭示出数十个更广泛的能力领域与失效模式——这些发现对任何独立研究团队而言都难以完整揭示。通过大规模人工调查验证，我们证实该方法自动评分与人工评估高度一致。通过利用基础模型兼具任务创建与自我评估的能力，ACD为新型AI系统的可扩展自动化评估迈出重要一步。全部代码与评估日志已开源：https://github.com/conglu1997/ACD。

---

## [From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN](https://arxiv.org/abs/2502.13544)

### Abstract
arXiv:2502.13544v3 Announce Type: replace-cross 
Abstract: Despite the rapid progress of large language models (LLMs), their length-controllable text generation (LCTG) ability remains below expectations, posing a major limitation for practical applications. Existing methods mainly focus on end-to-end training to reinforce adherence to length constraints. However, the lack of decomposition and targeted enhancement of LCTG sub-abilities restricts further progress. To bridge this gap, we conduct a bottom-up decomposition of LCTG sub-abilities with human patterns as reference and perform a detailed error analysis. On this basis, we propose MarkerGen, a simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental deficiencies via external tool integration;(2) conducts explicit length modeling with dynamically inserted markers;(3) employs a three-stage generation scheme to better align length constraints while maintaining content quality. Comprehensive experiments demonstrate that MarkerGen significantly improves LCTG across various settings, exhibiting outstanding effectiveness and generalizability.

### 摘要
尽管大语言模型（LLMs）发展迅速，但其长度可控文本生成（LCTG）能力仍低于预期，这成为实际应用的主要限制。现有方法主要集中于通过端到端训练来强化对长度约束的遵循。然而，由于缺乏对LCTG子能力的分解与针对性增强，进一步改进受到制约。为填补这一空白，我们以人类模式为参照对LCTG子能力进行自底向上分解，并开展详细的错误分析。基于此，我们提出MarkerGen——一种简单高效的即插即用方法：（1）通过外部工具集成缓解LLM的基础缺陷；（2）采用动态插入标记符进行显式长度建模；（3）实施三阶段生成方案以在保持内容质量的同时更好地对齐长度约束。综合实验表明，MarkerGen在各种设定下显著提升了LCTG性能，展现出卓越的有效性与泛化能力。

---

## [CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?](https://arxiv.org/abs/2502.11300)

### Abstract
arXiv:2502.11300v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: https://aashish2000.github.io/CORDIAL/

### 摘要
多模态大语言模型（MLLMs）因其在不同问题领域中卓越的指令遵循与推理能力而广受认可。然而，现有基准测试主要关注下游任务中事实性与逻辑正确性的评估，对MLLMs解读语用线索与模态间关系能力的评测重视不足。为填补这一空白，我们采用连贯关系（Coherence Relations）评估MLLMs执行多模态话语分析（MDA）的能力。我们提出的CORDIAL基准涵盖3个不同话语领域中多种粒度层次的广泛连贯关系。通过对10余个采用不同提示策略的MLLMs进行实验，我们发现即使Gemini 1.5 Pro和GPT-4o等顶级模型也无法达到基于简单分类器的基线性能。本研究强调需要超越基于相似度的度量标准，采用话语驱动的评估框架，从而对MLLMs能力进行更精细化的评估。基准测试与代码已发布于：https://aashish2000.github.io/CORDIAL/

---

## [Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis](https://arxiv.org/abs/2502.14767)

### Abstract
arXiv:2502.14767v2 Announce Type: replace-cross 
Abstract: With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.

### 摘要
随着现代技术发展和研究可及性提升推动的指数级增长，科学发现在本领域内及跨领域间日益呈现碎片化特征。这使得评估相关研究成果（尤其是来自不同学术社区的研究）的重要性、新颖性、增量发现及等效观点变得极具挑战性。大型语言模型（LLMs）近期展现出强大的定量与定性推理能力，而多智能体LLM辩论机制通过探索多元视角和推理路径，在处理复杂推理任务方面显示出良好前景。受此启发，我们提出'辩论树'（Tree-of-Debate, ToD）框架，该框架将科学论文转化为LLM角色进行新颖性辩论。为强调结构化批判性推理而非仅关注结果，ToD动态构建辩论树，实现对学术论文中独立新颖性论点的细粒度分析。通过跨学科科学文献实验（经领域专家评估），我们证明ToD能生成信息丰富的论点，有效对比论文差异，并为研究者的文献综述工作提供支持。

---

## [MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding](https://arxiv.org/abs/2502.15786)

### Abstract
arXiv:2502.15786v2 Announce Type: replace-cross 
Abstract: Decoding functional magnetic resonance imaging (fMRI) signals into text has been a key challenge in the neuroscience community, with the potential to advance brain-computer interfaces and uncover deeper insights into brain mechanisms. However, existing approaches often struggle with suboptimal predictive performance, limited task variety, and poor generalization across subjects. In response to this, we propose MindLLM, a model designed for subject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an fMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a neuroscience-informed attention mechanism, which is capable of accommodating subjects with varying input shapes and thus achieves high-performance subject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning (BIT), a novel approach that enhances the model's ability to capture diverse semantic representations from fMRI signals, facilitating more versatile decoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results demonstrate that our model outperforms the baselines, improving downstream tasks by 12.0%, unseen subject generalization by 24.5%, and novel task adaptation by 25.0%. Furthermore, the attention patterns in MindLLM provide interpretable insights into its decision-making process.

### 摘要
将功能磁共振成像(fMRI)信号解码为文本一直是神经科学领域的核心挑战，这一突破有望推动脑机接口发展并深化对脑机制的理解。然而现有方法普遍存在预测性能欠佳、任务类型有限及跨被试泛化能力不足等问题。为此，我们提出MindLLM模型——一种面向被试无关且多功能的fMRI到文本解码框架。该模型由fMRI编码器和现成大型语言模型(LLM)组成，其中fMRI编码器采用神经科学启发的注意力机制，可适配不同输入维度的被试数据，从而实现高性能的被试无关解码。此外，我们提出脑指令微调(BIT)新方法，通过增强模型从fMRI信号中捕获多样化语义表征的能力，促进更通用的解码性能。在综合性fMRI到文本基准测试中，MindLLM相较基线模型在下游任务提升12.0%、未知被试泛化能力提高24.5%、新任务适应能力增强25.0%。模型中的注意力模式还为其决策过程提供了可解释的神经机制洞察。

---

## [DISC: DISC: Dynamic Decomposition Improves LLM Inference Scaling](https://arxiv.org/abs/2502.16706)

### Abstract
arXiv:2502.16706v2 Announce Type: replace-cross 
Abstract: Inference scaling methods for large language models often work by breaking problems into steps or groups of tokens, then sampling and selecting the best next steps. However, these steps and their sizes are usually fixed or manually designed based on domain knowledge. We introduce dynamic decomposition, a method that adaptively and automatically breaks down solution and reasoning traces into manageable steps during inference. By allocating compute more effectively - especially by subdividing difficult steps and prioritizing their sampling - dynamic decomposition significantly boosts inference efficiency. Experiments on benchmarks like APPS, MATH, and LiveCodeBench show that dynamic decomposition outperforms fixed strategies such as token-level, sentence-level, and single-step decompositions, reducing the pass@10 error rate by 5.0%, 6.7%, and 10.5% respectively. These results show the promise of dynamic decomposition for improving a broad range of inference scaling techniques.

### 摘要
针对大型语言模型的推理扩展方法通常通过将问题分解为步骤或令牌组，随后采样并选择最佳后续步骤来实现。然而，这些步骤及其规模通常固定或基于领域知识手动设计。我们提出动态分解法，该方法在推理过程中自适应地自动将解决方案和推理轨迹分解为可管理的步骤。通过更有效地分配计算资源——特别是通过细分困难步骤并优先进行采样——动态分解显著提升了推理效率。在APPS、MATH和LiveCodeBench等基准测试上的实验表明，动态分解优于令牌级、句子级和单步分解等固定策略，分别将pass@10错误率降低了5.0%、6.7%和10.5%。这些结果证明了动态分解在改进广泛推理扩展技术方面的潜力。

---

## [MMTEB: Massive Multilingual Text Embedding Benchmark](https://arxiv.org/abs/2502.13595)

### Abstract
arXiv:2502.13595v3 Announce Type: replace-cross 
Abstract: Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.

### 摘要
文本嵌入模型通常仅在有限的任务集上进行评估，这些任务受限于语言、领域和任务的多样性。为突破这些限制并提供更全面的评估，我们提出了大规模多语言文本嵌入基准（MMTEB）——这是对MTEB的大规模社区驱动扩展，涵盖250多种语言中超过500个经过质量控制的评估任务。MMTEB包含一系列多样化的挑战性新任务，如指令跟随、长文档检索和代码检索，成为迄今为止最大的多语言嵌入模型评估任务集合。基于该集合，我们开发了多个高度多语言的基准测试，并用于评估一组代表性模型。研究发现，虽然拥有数十亿参数的大语言模型（LLM）能在特定语言子集和任务类别上达到最先进性能，但公开可用的最佳表现模型是仅含5.6亿参数的多语言e5-large-instruct。为提升可访问性并降低计算成本，我们提出了一种基于任务间相关性的新型降采样方法，确保在保持模型相对排序的同时实现多样化选择。此外，我们通过采样困难负例优化了检索等任务，创建了规模更小但有效的子集。这些优化使我们能够推出显著降低计算需求的基准测试。例如，新推出的零样本英语基准在保持与完整版本相似排序顺序的同时，仅需极低计算成本。

---

## [DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance](https://arxiv.org/abs/2502.16886)

### Abstract
arXiv:2502.16886v2 Announce Type: replace-cross 
Abstract: To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. These techniques are often designed with a pre-defined KV budget; however, as the optimal budget varies by different input lengths and task types, the existence of a fixed budget could result in inconsistent performance accepting inputs of diverse domains. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods.

### 摘要
为缓解大语言模型（LLM）推理过程中的内存负担，大量研究通过探索注意力稀疏性等方面来压缩键值（KV）缓存。这些技术通常基于预定义的KV预算进行设计，但由于不同输入长度和任务类型所需的最优预算存在差异，固定预算可能导致处理多样化领域输入时性能不稳定。为解决这一局限，我们提出新的KV缓存压缩目标：在确保始终达到全缓存性能的前提下，尽可能最大化KV缓存剪枝。为实现该目标，我们提出名为DBudgetKV的新型KV缓存压缩方法，其采用基于注意力的度量指标来判定剩余KV缓存是否可能无法匹配全缓存性能，从而及时终止剪枝过程。跨多种上下文长度、任务类型和模型规模的实证评估表明，我们的方法能高效稳健地实现无损KV剪枝，平均压缩率超过25%。此外，该方法易于集成至LLM推理流程中，不仅能优化内存空间，与现有方法相比还能减少推理时间。

---

## [PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation](https://arxiv.org/abs/2502.20377)

### Abstract
arXiv:2502.20377v2 Announce Type: replace-cross 
Abstract: High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at https://github.com/kilian-group/phantom-wiki.

### 摘要
高质量基准数据集对于评估大语言模型（LLMs）的推理与检索能力至关重要。然而，为此目的构建的数据集并非永久解决方案，因为它们容易发生数据泄露并导致性能评估虚高。为解决这些问题，我们提出PhantomWiki：一种能生成具有多样化问答对的独特事实一致性文档语料库的流程。与先前工作不同，PhantomWiki既非固定数据集，也不基于任何现有数据，而是按需为每次评估生成全新实例。我们通过调节问题难度和语料库规模分别解耦推理与检索能力的评估，发现前沿LLMs在PhantomWiki数据集上表现出乎意料地困难。因此，我们提出了一个可扩展且抗数据泄露的框架，用于解耦评估推理、检索及工具使用能力。代码已开源：https://github.com/kilian-group/phantom-wiki。

---

## [AMPO: Active Multi-Preference Optimization for Self-play Preference Selection](https://arxiv.org/abs/2502.18293)

### Abstract
arXiv:2502.18293v2 Announce Type: replace-cross 
Abstract: Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, thereby enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, rendering it computationally infeasible to include all responses in the training objective. In this work, we propose $\textit&#123;Active Multi-Preference Optimization&#125;$ (AMPO), a novel approach that combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses and then select a small, yet informative, subset that covers reward extremes and distinct semantic clusters for preference optimization. Our contrastive training scheme is capable of identifying not only the best and worst answers but also subtle, underexplored modes that are crucial for robust alignment. Theoretically, we provide guarantees for expected reward maximization using our active selection method, and empirically, AMPO achieves state-of-the-art results on $\textit&#123;AlpacaEval&#125;$ using Llama 8B and Mistral 7B. We release our datasets $\href&#123;https://huggingface.co/Multi-preference-Optimization&#125;&#123;here&#125;$.

### 摘要
多偏好优化通过对比完整的有益响应集与不良响应集，丰富了语言模型对齐机制，超越了传统的成对偏好方法，从而为大型语言模型提供更丰富的训练信号。在自对齐过程中，这些模型通常针对每个查询生成大量候选答案，若将所有响应纳入训练目标将导致计算成本不可行。本研究提出主动多偏好优化(AMPO)，该方法创新性地结合了在线策略生成、多偏好组对比损失函数和主动子集选择机制。具体而言，我们对大规模候选响应池进行评分和嵌入表示，随后选择一个小型但信息量丰富的子集——该子集需覆盖奖励极值及不同语义簇以进行偏好优化。我们的对比训练方案不仅能识别最优和最差答案，还能发现对鲁棒对齐至关重要的细微且未被充分探索的模式。理论分析证明，采用主动选择方法可实现期望奖励最大化保证；实证研究表明，AMPO在AlpacaEval基准测试中使用Llama 8B和Mistral 7B模型取得了最先进的性能。相关数据集已发布于https://huggingface.co/Multi-preference-Optimization。

---

## [NeoBERT: A Next-Generation BERT](https://arxiv.org/abs/2502.19587)

### Abstract
arXiv:2502.19587v2 Announce Type: replace-cross 
Abstract: Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.

### 摘要
近年来，架构设计、预训练和微调技术的创新使得LLaMA、DeepSeek等大型自回归语言模型展现出卓越的上下文学习与推理能力。相比之下，尽管BERT和RoBERTa等编码器模型为众多下游自然语言处理任务奠定了基础，但其进展却相对滞后。为弥补这一差距，我们提出了NeoBERT——新一代编码器模型，通过整合最先进的架构创新、现代数据集和优化的预训练方法，重新定义了双向模型的性能边界。NeoBERT设计具备即插即用特性：可直接替代现有基础模型，采用最优的深度-宽度比例，并支持4,096个标记的扩展上下文长度。尽管参数量仅为2.5亿，该模型在MTEB大规模基准测试中取得了最先进成果，在相同微调条件下性能超越BERT-large、RoBERTa-large、NomicBERT和ModernBERT。此外，我们系统评估了各项改进对GLUE基准的影响，并为MTEB设计了统一的微调与评估框架。我们公开了全部代码、数据、检查点和训练脚本，以加速学术研究与实际应用部署。

---

## [LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement](https://arxiv.org/abs/2503.00493)

### Abstract
arXiv:2503.00493v3 Announce Type: replace-cross 
Abstract: Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.

### 摘要
语言模型（LMs）的最新进展在语义理解和上下文建模方面展现出强大能力，这推动了生成式语音增强（SE）的发展。然而，许多基于LM的SE方法主要关注语义信息，往往忽视了声学信息的关键作用，导致增强后的声学不一致性以及在多样化SE任务中泛化能力有限。本文提出LLaSE-G1，一种基于LLaMA的语言模型，旨在提升语音增强的泛化能力。LLaSE-G1具有以下关键贡献：首先，为减少声学不一致性，LLaSE-G1采用WavLM的连续表示作为输入，并预测X-Codec2的语音标记，以最大化声学信息保留。其次，为增强泛化能力，LLaSE-G1引入双通道输入和输出，统一多种SE任务而无需任务特定标识。第三，LLaSE-G1在测试时展现出超越以往任务特定判别式和生成式SE模型的性能，并表现出对未见SE任务的涌现能力。此外，我们公开了代码和模型以支持该领域的进一步研究。

---

## [Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses in LLMs](https://arxiv.org/abs/2503.00979)

### Abstract
arXiv:2503.00979v2 Announce Type: replace-cross 
Abstract: Autoregressive Transformers rely on Key-Value (KV) caching to accelerate inference. However, the linear growth of the KV cache with context length leads to excessive memory consumption and bandwidth constraints. This bottleneck is particularly problematic in real-time applications -- such as chatbots and interactive assistants -- where low latency and high memory efficiency are critical. Existing methods drop distant tokens or compress states in a lossy manner, sacrificing accuracy by discarding vital context or introducing bias.
  We propose MorphKV, an inference-time technique that maintains a constant-sized KV cache while preserving accuracy. MorphKV balances long-range dependencies and local coherence during text generation. It eliminates early-token bias while retaining high-fidelity context by adaptively ranking tokens through correlation-aware selection. Unlike heuristic retention or lossy compression, MorphKV iteratively refines the KV cache via lightweight updates guided by attention patterns of recent tokens. This approach captures inter-token correlation with greater accuracy, crucial for tasks like content creation and code generation. Our studies on long-response tasks show 52.9$\%$ memory savings and 18.2$\%$ higher accuracy on average compared to state-of-the-art prior works, enabling efficient real-world deployment.

### 摘要
自回归Transformer模型依赖键值缓存（KV缓存）加速推理过程。然而，KV缓存随上下文长度线性增长会导致内存消耗激增和带宽限制，这一瓶颈在聊天机器人和交互式助手等实时应用中尤为突出——此类场景对低延迟和高内存效率有严苛要求。现有方法通过丢弃远端标记或采用有损压缩来缩减状态，但会因丢失关键上下文或引入偏差而牺牲准确性。
我们提出MorphKV技术，该推理阶段方法能在保持KV缓存恒定大小的同时维持生成精度。MorphKV在文本生成过程中平衡长程依赖与局部连贯性，通过基于相关性的自适应标记排序机制，在消除早期标记偏差的同时保留高保真上下文。与启发式保留或有损压缩不同，MorphKV根据近期标记的注意力模式进行轻量级更新，从而迭代优化KV缓存。该方法能更精准捕捉标记间相关性，这对内容创作和代码生成等任务至关重要。我们在长响应任务上的实验表明，相较现有最优方法，该技术平均可节省52.9%内存并提升18.2%准确率，为实际场景部署提供了高效解决方案。

---

## [RONA: Pragmatically Diverse Image Captioning with Coherence Relations](https://arxiv.org/abs/2503.10997)

### Abstract
arXiv:2503.10997v2 Announce Type: replace-cross 
Abstract: Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying a central message alongside visual descriptions using pragmatic cues. To enhance caption diversity, it is essential to explore alternative ways of communicating these messages in conjunction with visual content. We propose RONA, a novel prompting strategy for Multi-modal Large Language Models (MLLM) that leverages Coherence Relations as a controllable axis for pragmatic variations. We demonstrate that RONA generates captions with better overall diversity and ground-truth alignment, compared to MLLM baselines across multiple domains. Our code is available at: https://github.com/aashish2000/RONA

### 摘要
传统写作辅助工具（如Grammarly、Microsoft Copilot）通常通过句法和语义变化来描述图像成分，从而生成多样化的图像说明。然而，人工撰写的说明会优先利用语用线索传递核心信息，同时兼顾视觉描述。为提升说明的多样性，必须探索视觉内容与这些信息相结合的其他表达方式。我们提出RONA——一种面向多模态大语言模型（MLLM）的新型提示策略，该策略将连贯关系作为可控轴来实现语用变化。实验表明，与跨多个领域的MLLM基线相比，RONA生成的说明在整体多样性和真实对齐性方面表现更优。代码已开源：https://github.com/aashish2000/RONA

---

## [Generalized Interpolating Discrete Diffusion](https://arxiv.org/abs/2503.04482)

### Abstract
arXiv:2503.04482v2 Announce Type: replace-cross 
Abstract: While state-of-the-art language models achieve impressive results through next-token prediction, they have inherent limitations such as the inability to revise already generated tokens. This has prompted exploration of alternative approaches such as discrete diffusion. However, masked diffusion, which has emerged as a popular choice due to its simplicity and effectiveness, reintroduces this inability to revise words. To overcome this, we generalize masked diffusion, deriving a new family of general interpolating discrete diffusion (GIDD) which offers greater flexibility in the design of the noising processes. Leveraging a novel diffusion ELBO, we achieve compute-matched state-of-the-art performance in diffusion language modeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining masking and uniform noise, leading to improved sample quality and unlocking the ability for the model to correct its own mistakes, an area where autoregressive models notoriously have struggled. Code: https://github.com/dvruette/gidd/

### 摘要
尽管最先进的语言模型通过下一词预测取得了令人瞩目的成果，但其存在固有局限性，例如无法修正已生成的词元。这促使研究者开始探索离散扩散等替代方法。然而，由于简单性和有效性而成为主流选择的掩码扩散方法，却重新引入了无法修正词汇的问题。为解决这一缺陷，我们推广了掩码扩散方法，推导出新型通用插值离散扩散（GIDD）框架，该框架在噪声过程设计中提供了更高的灵活性。通过采用新型扩散ELBO目标函数，我们在扩散语言建模领域实现了计算效率匹配的尖端性能。利用GIDD的灵活性，我们探索了结合掩码与均匀噪声的混合方法，从而提升了生成样本质量，并解锁了模型自我纠错的能力——这一直是自回归模型众所周知的薄弱环节。代码详见：https://github.com/dvruette/gidd/

---

## [Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models](https://arxiv.org/abs/2503.04280)

### Abstract
arXiv:2503.04280v3 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) and Visual Language Models (VLMs) have significantly impacted robotics, enabling high-level semantic motion planning applications. Reinforcement Learning (RL), a complementary paradigm, enables agents to autonomously optimize complex behaviors through interaction and reward signals. However, designing effective reward functions for RL remains challenging, especially in real-world tasks where sparse rewards are insufficient and dense rewards require elaborate design. In this work, we propose Autonomous Reinforcement learning for Complex Human-Informed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4, a pre-trained LLM, to generate reward functions directly from natural language task descriptions. The rewards are used to train RL agents in simulated environments, where we formalize the reward generation process to enhance feasibility. Additionally, GPT-4 automates the coding of task success criteria, creating a fully automated, one-shot procedure for translating human-readable text into deployable robot skills. Our approach is validated through extensive simulated experiments on single-arm and bi-manual manipulation tasks using an ABB YuMi collaborative robot, highlighting its practicality and effectiveness. Tasks are demonstrated on the real robot setup.

### 摘要
大型语言模型（LLM）与视觉语言模型（VLM）的最新进展对机器人技术产生了深远影响，推动了高层次语义运动规划应用的发展。作为互补范式，强化学习（RL）使智能体能够通过交互与奖励信号自主优化复杂行为。然而，设计有效的RL奖励函数仍具挑战性，特别是在现实任务中，稀疏奖励不足而稠密奖励又需精心设计。本研究提出面向复杂人类指令环境的自主强化学习框架（ARCHIE），该无监督流程利用预训练LLM模型GPT-4，直接从自然语言任务描述生成奖励函数。生成的奖励用于在仿真环境中训练RL智能体，我们通过形式化奖励生成过程以提升可行性。此外，GPT-4可自动编码任务成功标准，实现将人类可读文本转化为可部署机器人技能的端到端单次流程。基于ABB YuMi协作机器人的单臂与双臂操作任务仿真实验验证了本方法的实用性与有效性，相关任务在真实机器人平台上得到了演示。

---

## [Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models](https://arxiv.org/abs/2503.16853)

### Abstract
arXiv:2503.16853v2 Announce Type: replace-cross 
Abstract: Language models pretrained on text-only corpora often struggle with tasks that require auditory commonsense knowledge. Previous work addresses this problem by augmenting the language model to retrieve knowledge from external audio databases. This approach has several limitations, such as the potential lack of relevant audio in databases and the high costs associated with constructing the databases. To address these issues, we propose Imagine to Hear, a novel approach that dynamically generates auditory knowledge using generative models. Our framework detects multiple audio-related textual spans from the given prompt and generates corresponding auditory knowledge. We develop several mechanisms to efficiently process multiple auditory knowledge, including a CLAP-based rejection sampler and a language-audio fusion module. Our experiments show that our method achieves state-of-the-art performance on AuditoryBench without relying on external databases, highlighting the effectiveness of our generation-based approach.

### 摘要
仅基于文本语料库预训练的语言模型在处理需要听觉常识知识的任务时往往表现不佳。先前研究通过增强语言模型从外部音频数据库中检索知识来解决这一问题，但该方法存在若干局限性，例如数据库中可能缺乏相关音频以及构建数据库的高成本。为解决这些问题，我们提出"Imagine to Hear"这一创新方法，利用生成模型动态产生听觉知识。我们的框架能够从给定提示中检测多个与音频相关的文本片段，并生成相应的听觉知识。我们开发了多种机制来高效处理多重听觉知识，包括基于CLAP的拒绝采样器和语言-音频融合模块。实验结果表明，该方法在不依赖外部数据库的情况下，在AuditoryBench上实现了最先进的性能，充分证明了我们基于生成的方法的有效性。

---

## [FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-World LoRA](https://arxiv.org/abs/2503.11880)

### Abstract
arXiv:2503.11880v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) in federated settings enables privacy-preserving adaptation but suffers from cross-client interference due to model aggregation. Existing federated LoRA fine-tuning methods, primarily based on FedAvg, struggle with data heterogeneity, leading to harmful cross-client interference and suboptimal personalization. In this work, we propose \textbf&#123;FedALT&#125;, a novel personalized federated LoRA fine-tuning algorithm that fundamentally departs from FedAvg. Instead of using an aggregated model to initialize local training, each client continues training its individual LoRA while incorporating shared knowledge through a separate Rest-of-World (RoW) LoRA component. To effectively balance local adaptation and global information, FedALT introduces an adaptive mixer that dynamically learns input-specific weightings between the individual and RoW LoRA components, drawing conceptual foundations from the Mixture-of-Experts (MoE) paradigm. Through extensive experiments on NLP benchmarks, we demonstrate that FedALT significantly outperforms state-of-the-art personalized federated LoRA fine-tuning methods, achieving superior local adaptation without sacrificing computational efficiency.

### 摘要
在联邦学习环境下对大语言模型（LLM）进行微调可实现隐私保护的模型适配，但会因模型聚合产生跨客户端干扰。现有基于FedAvg的联邦LoRA微调方法难以应对数据异构性，导致有害的跨客户端干扰和次优的个性化表现。本研究提出	extbf&#123;FedALT&#125;——一种根本性区别于FedAvg的新型个性化联邦LoRA微调算法。该方法摒弃聚合模型初始化本地训练的传统方式，使每个客户端持续训练其独立LoRA模块的同时，通过单独的'世界其余部分'（RoW）LoRA组件整合共享知识。为有效平衡本地适配与全局信息，FedALT引入自适应混合器，基于混合专家（MoE）范式概念，动态学习独立LoRA与RoW LoRA组件间输入特定的权重配比。通过在NLP基准测试上的大量实验，我们证明FedALT显著优于当前最先进的个性化联邦LoRA微调方法，在保持计算效率的同时实现了更优的本地适配性能。

---

## [nvBench 2.0: Resolving Ambiguity in Text-to-Visualization through Stepwise Reasoning](https://arxiv.org/abs/2503.12880)

### Abstract
arXiv:2503.12880v2 Announce Type: replace-cross 
Abstract: Text-to-Visualization (Text2VIS) enables users to create visualizations from natural language queries, making data insights more accessible. However, Text2VIS faces challenges in interpreting ambiguous queries, as users often express their visualization needs in imprecise language.
  To address this challenge, we introduce nBench 2.0, a new benchmark designed to evaluate Text2VIS systems in scenarios involving ambiguous queries. nvBench 2.0 includes 7,878 natural language queries and 24,076 corresponding visualizations, derived from 780 tables across 153 domains. It is built using a controlled ambiguity-injection pipeline that generates ambiguous queries through a reverse-generation workflow. By starting with unambiguous seed visualizations and selectively injecting ambiguities, the pipeline yields multiple valid interpretations for each query, with each ambiguous query traceable to its corresponding visualization through step-wise reasoning paths.
  We evaluate various Large Language Models (LLMs) on their ability to perform ambiguous Text2VIS tasks using nBench 2.0. We also propose Step-Text2Vis, an LLM-based model trained on nvBench 2.0, which enhances performance in ambiguous scenarios through step-wise preference optimization. Our results show that Step-Text2Vis outperforms all baselines, setting a new state-of-the-art for ambiguous Text2VIS tasks. Our source code and data are available at https://nvbench2.github.io/

### 摘要
文本到可视化(Text2VIS)技术允许用户通过自然语言查询创建可视化图表，使数据洞察更加易于获取。然而，Text2VIS在解释模糊查询时面临挑战，因为用户通常使用不精确的语言表达其可视化需求。为应对这一挑战，我们推出nBench 2.0基准测试，专门用于评估Text2VIS系统处理模糊查询场景的能力。该基准包含来自153个领域780张表格的7,878条自然语言查询及24,076个对应可视化结果，通过受控的模糊注入流程构建。该流程采用逆向生成工作流，从无歧义的种子可视化出发，通过选择性注入模糊性，使每个查询产生多个有效解释方案，且所有模糊查询均可通过分步推理路径追溯至对应可视化。我们基于nBench 2.0评估了多种大语言模型(LLM)执行模糊Text2VIS任务的能力，并提出了Step-Text2Vis模型。该模型通过分步偏好优化在nBench 2.0上训练，显著提升了模糊场景下的表现。实验结果表明，Step-Text2Vis优于所有基线模型，为模糊Text2VIS任务树立了新的性能标杆。

---

## [sudo rm -rf agentic_security](https://arxiv.org/abs/2503.20279)

### Abstract
arXiv:2503.20279v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal-trained safeguards in commercial computer-use agents, such as Claude for Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24.41% (with no refinement), and up to 41.33% (by its iterative refinement) in Claude for Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs

### 摘要
大语言模型（LLMs）正日益被部署为计算机应用代理，在真实的桌面或网络环境中自主执行任务。虽然这一发展极大地扩展了人类的实际应用场景，但也带来了严重的安全隐患。我们提出SUDO（基于屏幕的通用解毒转毒攻击框架），这是一种新型攻击框架，能够系统性地绕过商用计算机应用代理（如Claude for Computer Use）中经过拒绝训练的安全防护机制。其核心机制Detox2Tox通过解毒过程将有害请求（代理初始拒绝的请求）转化为看似无害的请求，从先进视觉语言模型（VLMs）获取详细指令，随后在执行前通过投毒过程重新引入恶意内容。与传统越狱攻击不同，SUDO基于内置拒绝反馈迭代优化攻击策略，使其对强健的策略过滤器具有持续增强的攻击效力。在涵盖50个现实世界任务和多个最先进VLM的广泛测试中，SUDO在Claude for Computer Use上实现了24.41%的基础攻击成功率（无优化），通过迭代优化后最高可达41.33%。本文通过揭示这些漏洞并展示其在真实计算环境中的易利用性，强调了建立强健、上下文感知安全防护机制的迫切需求。警告：本文包含有害或冒犯性的模型输出

---

## [LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models](https://arxiv.org/abs/2504.10415)

### Abstract
arXiv:2504.10415v2 Announce Type: replace-cross 
Abstract: Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.

### 摘要
科学方程发现是科学进步史上的一项基础性任务，其目标在于推导支配自然现象的基本规律。近年来，大型语言模型（LLMs）因其可能利用内嵌科学知识进行假设生成而备受关注。然而，现有评估方法难以真实反映这些模型的发现能力——当前基准测试多采用常见方程，易被LLMs通过记忆机制掌握，导致性能指标虚高而无法体现真实发现水平。本文提出LLM-SRBench基准测试框架，包含四大科学领域的239个挑战性问题，专门用于评估基于LLM的科学方程发现方法，同时有效规避简单记忆问题。该基准包含两大类别：LSR-Transform通过将常见物理模型转换为非常见数学表达形式来测试超越记忆的推理能力；LSR-Synth则引入需要数据驱动推理的合成发现型问题。通过对多个前沿方法（包括开源和闭源LLMs）的广泛评估，我们发现当前最佳系统的符号准确率仅为31.5%。这些发现揭示了科学方程发现面临的严峻挑战，也使LLM-SRBench成为未来研究的重要基准资源。

---

## [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)

### Abstract
arXiv:2504.13151v2 Announce Type: replace-cross 
Abstract: How can we know whether new mechanistic interpretability methods achieve real improvements? In pursuit of lasting evaluation standards, we propose MIB, a Mechanistic Interpretability Benchmark, with two tracks spanning four tasks and five models. MIB favors methods that precisely and concisely recover relevant causal pathways or causal variables in neural language models. The circuit localization track compares methods that locate the model components - and connections between them - most important for performing a task (e.g., attribution patching or information flow routes). The causal variable localization track compares methods that featurize a hidden vector, e.g., sparse autoencoders (SAEs) or distributed alignment search (DAS), and align those features to a task-relevant causal variable. Using MIB, we find that attribution and mask optimization methods perform best on circuit localization. For causal variable localization, we find that the supervised DAS method performs best, while SAE features are not better than neurons, i.e., non-featurized hidden vectors. These findings illustrate that MIB enables meaningful comparisons, and increases our confidence that there has been real progress in the field.

### 摘要
我们如何判断新的机制可解释性方法是否实现了真正的改进？为建立持久的评估标准，我们提出MIB（机制可解释性基准测试），包含两条轨道、四项任务和五个模型。MIB青睐那些能精确且简洁地还原神经语言模型中相关因果路径或因果变量的方法。电路定位轨道比较不同方法在定位执行任务时最重要的模型组件及其连接（如归因修补或信息流路径）。因果变量定位轨道则比较对隐藏向量进行特征化的方法（如稀疏自编码器SAE或分布式对齐搜索DAS），并将这些特征与任务相关的因果变量对齐。通过MIB测试，我们发现归因和掩码优化方法在电路定位中表现最佳。对于因果变量定位，监督式DAS方法表现最优，而SAE特征并不优于神经元（即未特征化的隐藏向量）。这些发现表明MIB能实现有意义的比较，并增强我们对该领域取得实质性进展的信心。

---

## [Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach](https://arxiv.org/abs/2505.01997)

### Abstract
arXiv:2505.01997v2 Announce Type: replace-cross 
Abstract: One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.

### 摘要
大型语言模型（LLMs）成功的关键技术之一是偏好对齐。然而，偏好对齐的一个显著副作用是校准性下降：虽然预训练模型通常具有良好的校准性，但在与人类偏好对齐后，LLMs往往会变得校准性较差。本文研究了偏好对齐为何影响校准性以及如何解决这一问题。针对第一个问题，我们观察到对齐过程中的偏好坍缩问题会不良地泛化至校准场景，导致LLMs表现出过度自信和校准性差的现象。为解决此问题，我们证明了利用领域特定知识进行微调对缓解过度自信问题的重要性。为进一步分析这是否影响模型性能，我们将模型分为两类：可校准与不可校准，其界限由期望校准误差（ECE）界定。在可校准范围内，我们提出一种校准感知的微调方法，在不损害LLMs性能的前提下实现适当校准。然而，当模型为进一步提升性能而持续微调时，会进入不可校准范围。针对此情况，我们开发了一种基于EM算法的ECE正则化微调损失函数，以维持较低的校准误差。大量实验验证了所提方法的有效性。

---

## [LookAlike: Consistent Distractor Generation in Math MCQs](https://arxiv.org/abs/2505.01903)

### Abstract
arXiv:2505.01903v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used to generate distractors for multiple-choice questions (MCQs), especially in domains like math education. However, existing approaches are limited in ensuring that the generated distractors are consistent with common student errors. We propose LookAlike, a method that improves error-distractor consistency via preference optimization. Our two main innovations are: (a) mining synthetic preference pairs from model inconsistencies, and (b) alternating supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike prior work that relies on heuristics or manually annotated preference data, LookAlike uses its own generation inconsistencies as dispreferred samples, thus enabling scalable and stable training. Evaluated on a real-world dataset of 1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an existing state-of-the-art method (45.6% / 47.7%). These improvements highlight the effectiveness of preference-based regularization and inconsistency mining for generating consistent math MCQ distractors at scale.

### 摘要
大语言模型（LLMs）越来越多地用于为多项选择题（MCQs）生成干扰项，尤其在数学教育等领域。然而，现有方法在确保生成的干扰项与学生常见错误保持一致方面存在局限。我们提出LookAlike方法，通过偏好优化提升错误-干扰项一致性。该方法的两大创新在于：（a）从模型不一致性中挖掘合成偏好对；（b）将监督微调（SFT）与直接偏好优化（DPO）交替进行以稳定训练。与依赖启发式规则或人工标注偏好数据的现有工作不同，LookAlike利用自身生成的不一致性作为非偏好样本，从而实现可扩展且稳定的训练。在包含1,400多道数学MCQs的真实数据集上评估表明，LookAlike在LLM作为评判者的设定下，干扰项生成准确率达51.6%，错误生成准确率达57.2%，优于现有最优方法（45.6%/47.7%）。这些改进凸显了基于偏好的正则化与不一致性挖掘对于大规模生成一致性数学MCQ干扰项的有效性。

---

## [Toward Total Recall: Enhancing FAIRness through AI-Driven Metadata Standardization](https://arxiv.org/abs/2504.05307)

### Abstract
arXiv:2504.05307v2 Announce Type: replace-cross 
Abstract: Scientific metadata often suffer from incompleteness, inconsistency, and formatting errors, which hinder effective discovery and reuse of the associated datasets. We present a method that combines GPT-4 with structured metadata templates from the CEDAR knowledge base to automatically standardize metadata and to ensure compliance with established standards. A CEDAR template specifies the expected fields of a metadata submission and their permissible values. Our standardization process involves using CEDAR templates to guide GPT-4 in accurately correcting and refining metadata entries in bulk, resulting in significant improvements in metadata retrieval performance, especially in recall -- the proportion of relevant datasets retrieved from the total relevant datasets available. Using the BioSample and GEO repositories maintained by the National Center for Biotechnology Information (NCBI), we demonstrate that retrieval of datasets whose metadata are altered by GPT-4 when provided with CEDAR templates (GPT-4+CEDAR) is substantially better than retrieval of datasets whose metadata are in their original state and that of datasets whose metadata are altered using GPT-4 with only data-dictionary guidance (GPT-4+DD). The average recall increases dramatically, from 17.65\% with baseline raw metadata to 62.87\% with GPT-4+CEDAR. Furthermore, we evaluate the robustness of our approach by comparing GPT-4 against other large language models, including LLaMA-3 and MedLLaMA2, demonstrating consistent performance advantages for GPT-4+CEDAR. These results underscore the transformative potential of combining advanced language models with symbolic models of standardized metadata structures for more effective and reliable data retrieval, thus accelerating scientific discoveries and data-driven research.

### 摘要
科学元数据常常存在不完整、不一致和格式错误等问题，这阻碍了相关数据集的有效发现和重用。我们提出一种将GPT-4与CEDAR知识库中的结构化元数据模板相结合的方法，用于自动标准化元数据并确保符合既定标准。CEDAR模板规定了元数据提交的预期字段及其允许值。我们的标准化流程利用CEDAR模板引导GPT-4对元数据条目进行批量精确校正和优化，从而显著提升元数据检索性能——特别是在召回率（从可用相关数据集中检索到的相关数据集比例）方面。通过在美国国家生物技术信息中心（NCBI）维护的BioSample和GEO数据库上进行实验，我们证明：当提供CEDAR模板时（GPT-4+CEDAR），经GPT-4修改元数据的数据集检索效果明显优于原始元数据状态下的数据集，以及仅使用数据字典指导（GPT-4+DD）经GPT-4修改元数据的数据集。平均召回率从基线原始元数据的17.65%大幅提升至GPT-4+CEDAR的62.87%。此外，我们通过将GPT-4与LLaMA-3、MedLLaMA2等其他大语言模型进行比较来评估方法的鲁棒性，结果表明GPT-4+CEDAR始终具有性能优势。这些结果凸显了将先进语言模型与标准化元数据结构的符号模型相结合，对于实现更高效可靠的数据检索、从而加速科学发现和数据驱动研究的变革性潜力。

---

