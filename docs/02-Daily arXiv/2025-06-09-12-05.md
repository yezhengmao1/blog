# 2025-06-09-12-05

## [Beyond the Buzz: A Pragmatic Take on Inference Disaggregation](https://arxiv.org/abs/2506.05508)

### Abstract
arXiv:2506.05508v1 Announce Type: new 
Abstract: As inference scales to multi-node deployments, disaggregation - splitting inference into distinct phases - offers a promising path to improving the throughput-interactivity Pareto frontier. Despite growing enthusiasm and a surge of open-source efforts, practical deployment of disaggregated serving remains limited due to the complexity of the optimization search space and system-level coordination. In this paper, we present the first systematic study of disaggregated inference at scale, evaluating hundreds of thousands of design points across diverse workloads and hardware configurations. We find that disaggregation is most effective for prefill-heavy traffic patterns and larger models. Our results highlight the critical role of dynamic rate matching and elastic scaling in achieving Pareto-optimal performance. Our findings offer actionable insights for efficient disaggregated deployments to navigate the trade-off between system throughput and interactivity.

### 摘要
随着推理任务扩展到多节点部署场景，解耦技术——将推理过程划分为不同阶段——为提升吞吐量与交互性的帕累托前沿提供了可行路径。尽管业界热情高涨且开源项目激增，但由于优化搜索空间的复杂性和系统级协调难度，解耦服务的实际部署仍受限。本文首次对大规模解耦推理进行了系统研究，通过评估数十万个跨不同工作负载和硬件配置的设计点，发现解耦技术对预填充密集型流量模式和大规模模型最为有效。研究结果揭示了动态速率匹配与弹性扩缩容在实现帕累托最优性能中的关键作用。本研究成果为高效部署解耦系统提供了可操作的见解，以权衡系统吞吐量与交互性之间的关系。

---

## [Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars](https://arxiv.org/abs/2506.05555)

### Abstract
arXiv:2506.05555v1 Announce Type: new 
Abstract: Collective risk social dilemmas (CRSD) highlight a trade-off between individual preferences and the need for all to contribute toward achieving a group objective. Problems such as climate change are in this category, and so it is critical to understand their social underpinnings. However, rigorous CRSD methodology often demands large-scale human experiments but it is difficult to guarantee sufficient power and heterogeneity over socio-demographic factors. Generative AI offers a potential complementary approach to address thisproblem. By replacing human participants with large language models (LLM), it allows for a scalable empirical framework. This paper focuses on the validity of this approach and whether it is feasible to represent a large-scale human-like experiment with sufficient diversity using LLM. In particular, where previous literature has focused on political surveys, virtual towns and classical game-theoretic examples, we focus on a complex CRSD used in the institutional economics and sustainability literature known as Port of Mars

### 摘要
集体风险社会困境(CRSD)揭示了个人偏好与为实现集体目标所需共同贡献之间的权衡。诸如气候变化等问题即属此类，因此理解其社会基础至关重要。然而，严格的CRSD研究方法通常需要大规模人类实验，但难以保证在社会人口因素方面具有足够的统计功效和异质性。生成式人工智能为这一问题提供了潜在的补充解决方案。通过用大语言模型(LLM)替代人类参与者，该方法可构建一个可扩展的实证研究框架。本文重点探讨该方法的有效性，以及使用LLM是否能够充分模拟具有足够多样性的大规模类人实验。特别值得注意的是，现有文献多聚焦于政治调查、虚拟城镇和经典博弈论案例，而本研究则针对制度经济学与可持续性文献中被称为"火星港"的复杂CRSD展开分析。

---

## [Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists](https://arxiv.org/abs/2506.05616)

### Abstract
arXiv:2506.05616v1 Announce Type: new 
Abstract: We aim at designing language agents with greater autonomy for crystal materials discovery. While most of existing studies restrict the agents to perform specific tasks within predefined workflows, we aim to automate workflow planning given high-level goals and scientist intuition. To this end, we propose Materials Agent unifying Planning, Physics, and Scientists, known as MAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a Scientific Mediator. The Workflow Planner uses large language models (LLMs) to generate structured and multi-step workflows. The Tool Code Generator synthesizes executable Python code for various tasks, including invoking a force field foundation model that encodes physics. The Scientific Mediator coordinates communications, facilitates scientist feedback, and ensures robustness through error reflection and recovery. By unifying planning, physics, and scientists, MAPPS enables flexible and reliable materials discovery with greater autonomy, achieving a five-fold improvement in stability, uniqueness, and novelty rates compared with prior generative models when evaluated on the MP-20 data. We provide extensive experiments across diverse tasks to show that MAPPS is a promising framework for autonomous materials discovery.

### 摘要
我们致力于设计具有更高自主性的语言智能体以实现晶体材料发现。现有研究大多将智能体限制在预定义工作流中执行特定任务，而本研究旨在根据高层目标与科学家直觉实现工作流规划的自动化。为此，我们提出了统一规划、物理与科学家的材料智能体MAPPS。该框架由工作流规划器、工具代码生成器和科学协调器组成：工作流规划器利用大语言模型生成结构化多步骤工作流；工具代码生成器合成可执行Python代码以完成各类任务，包括调用编码物理原理的力场基础模型；科学协调器负责通信协调、整合科学家反馈，并通过错误反思与恢复机制确保系统鲁棒性。通过整合规划、物理与科学家三方要素，MAPPS实现了灵活可靠的高自主性材料发现，在MP-20数据评估中稳定性、独特性与新颖性指标较现有生成模型提升五倍。多任务实验表明，MAPPS是自主材料发现领域极具前景的框架。

---

## [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)

### Abstract
arXiv:2506.05754v1 Announce Type: new 
Abstract: Constrained decoding enables Language Models (LMs) to produce samples that provably satisfy hard constraints. However, existing constrained-decoding approaches often distort the underlying model distribution, a limitation that is especially problematic in applications like program fuzzing, where one wants to generate diverse and valid program inputs for testing purposes. We propose a new constrained sampling framework based on Markov Chain Monte Carlo (MCMC) that simultaneously satisfies three core desiderata: constraint satisfying (every sample satisfies the constraint), monotonically converging (the sampling process converges to the true conditional distribution), and efficient (high-quality samples emerge in few steps). Our method constructs a proposal distribution over valid outputs and applies a Metropolis-Hastings acceptance criterion based on the LM's likelihood, ensuring principled and efficient exploration of the constrained space. Empirically, our sampler outperforms existing methods on both synthetic benchmarks and real-world program fuzzing tasks.

### 摘要
约束解码技术能够使语言模型生成严格满足硬约束条件的样本。然而，现有约束解码方法往往会扭曲底层模型分布，这一局限在程序模糊测试等应用中尤为突出——此类应用需要生成多样化且有效的程序输入用于测试。我们提出了一种基于马尔可夫链蒙特卡洛（MCMC）的新型约束采样框架，同时满足三个核心要求：约束满足性（每个样本均符合约束条件）、单调收敛性（采样过程收敛至真实条件分布）以及高效性（少量步骤即可生成高质量样本）。该方法通过构建有效输出的提议分布，并基于语言模型似然度应用Metropolis-Hastings接受准则，实现了对约束空间的原理性高效探索。实验表明，我们的采样器在合成基准测试和现实程序模糊测试任务中均优于现有方法。

---

## [Towards Next-Generation Intelligent Maintenance: Collaborative Fusion of Large and Small Models](https://arxiv.org/abs/2506.05854)

### Abstract
arXiv:2506.05854v1 Announce Type: new 
Abstract: With the rapid advancement of intelligent technologies, collaborative frameworks integrating large and small models have emerged as a promising approach for enhancing industrial maintenance. However, several challenges persist, including limited domain adaptability, insufficient real-time performance and reliability, high integration complexity, and difficulties in knowledge representation and fusion. To address these issues, an intelligent maintenance framework for industrial scenarios is proposed. This framework adopts a five-layer architecture and integrates the precise computational capabilities of domain-specific small models with the cognitive reasoning, knowledge integration, and interactive functionalities of large language models. The objective is to achieve more accurate, intelligent, and efficient maintenance in industrial applications. Two realistic implementations, involving the maintenance of telecommunication equipment rooms and the intelligent servicing of energy storage power stations, demonstrate that the framework significantly enhances maintenance efficiency.

### 摘要
随着智能技术的快速发展，整合大模型与小模型的协作框架已成为提升工业维护效能的重要途径。然而该领域仍存在诸多挑战，包括领域适应性有限、实时性与可靠性不足、系统集成复杂度高，以及知识表征与融合困难等问题。针对这些挑战，本研究提出一种面向工业场景的智能维护框架。该框架采用五层架构设计，通过融合领域专用小模型的精确计算能力与大语言模型的认知推理、知识整合及交互功能，旨在实现工业应用中更精准、智能且高效的维护作业。基于通信机房运维和储能电站智能服务两个实际案例的验证表明，该框架能显著提升维护效率。

---

## [Training-Free Query Optimization via LLM-Based Plan Similarity](https://arxiv.org/abs/2506.05853)

### Abstract
arXiv:2506.05853v1 Announce Type: new 
Abstract: Large language model (LLM) embeddings offer a promising new avenue for database query optimization. In this paper, we explore how pre-trained execution plan embeddings can guide SQL query execution without the need for additional model training. We introduce LLM-PM (LLM-based Plan Mapping), a framework that embeds the default execution plan of a query, finds its k nearest neighbors among previously executed plans, and recommends database hintsets based on neighborhood voting. A lightweight consistency check validates the selected hint, while a fallback mechanism searches the full hint space when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM achieves an average speed-up of 21% query latency reduction. This work highlights the potential of LLM-powered embeddings to deliver practical improvements in query performance and opens new directions for training-free, embedding-based optimizer guidance systems.

### 摘要
大语言模型（LLM）嵌入为数据库查询优化提供了新的研究方向。本文探讨了如何利用预训练的执行计划嵌入来指导SQL查询执行，而无需额外模型训练。我们提出LLM-PM（基于LLM的计划映射）框架，该框架通过嵌入查询的默认执行计划，在历史执行计划中寻找其k个最近邻，并基于邻域投票推荐数据库提示集。轻量级一致性检查验证所选提示的有效性，同时设置回退机制在需要时搜索完整提示空间。基于OpenGauss在JOB-CEB基准测试中的评估表明，LLM-PM平均实现查询延迟降低21%的加速效果。本研究揭示了LLM驱动的嵌入技术在提升查询性能方面的实用价值，并为免训练的基于嵌入的优化器引导系统开辟了新方向。

---

## [Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/abs/2506.05744)

### Abstract
arXiv:2506.05744v1 Announce Type: new 
Abstract: Recent large-scale reasoning models have achieved state-of-the-art performance on challenging mathematical benchmarks, yet the internal mechanisms underlying their success remain poorly understood. In this work, we introduce the notion of a reasoning graph, extracted by clustering hidden-state representations at each reasoning step, and systematically analyze three key graph-theoretic properties: cyclicity, diameter, and small-world index, across multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly more recurrent cycles (about 5 per sample), substantially larger graph diameters, and pronounced small-world characteristics (about 6x) compared to their base counterparts. Notably, these structural advantages grow with task difficulty and model capacity, with cycle detection peaking at the 14B scale and exploration diameter maximized in the 32B variant, correlating positively with accuracy. Furthermore, we show that supervised fine-tuning on an improved dataset systematically expands reasoning graph diameters in tandem with performance gains, offering concrete guidelines for dataset design aimed at boosting reasoning capabilities. By bridging theoretical insights into reasoning graph structures with practical recommendations for data construction, our work advances both the interpretability and the efficacy of large reasoning models.

### 摘要
近期的大规模推理模型在具有挑战性的数学基准测试中取得了最先进的性能，但其成功背后的内部机制仍鲜为人知。本研究引入推理图的概念——通过聚类每个推理步骤的隐藏状态表示提取而得，并系统分析了三项关键图论特性（循环性、直径和小世界指数）在多个任务（GSM8K、MATH500、AIME 2024）中的表现。研究发现，相较于基础模型，经蒸馏的推理模型（如DeepSeek-R1-Distill-Qwen-32B）表现出更显著的循环结构（每个样本约5个周期）、更大的图直径以及更突出的小世界特征（约6倍）。值得注意的是，这些结构优势随任务难度和模型容量增长而增强：周期检测在140亿参数规模达到峰值，探索直径在320亿参数版本最大化，且与准确率呈正相关。进一步研究表明，在改进数据集上的监督微调会系统性地扩展推理图直径并同步提升性能，这为旨在增强推理能力的数据集设计提供了具体指导。通过将推理图结构的理论洞见与数据构建的实践建议相结合，本研究推动了大模型推理能力的可解释性与有效性发展。

---

## [Preference Learning for AI Alignment: a Causal Perspective](https://arxiv.org/abs/2506.05967)

### Abstract
arXiv:2506.05967v1 Announce Type: new 
Abstract: Reward modelling from preference data is a crucial step in aligning large language models (LLMs) with human values, requiring robust generalisation to novel prompt-response pairs. In this work, we propose to frame this problem in a causal paradigm, providing the rich toolbox of causality to identify the persistent challenges, such as causal misidentification, preference heterogeneity, and confounding due to user-specific factors. Inheriting from the literature of causal inference, we identify key assumptions necessary for reliable generalisation and contrast them with common data collection practices. We illustrate failure modes of naive reward models and demonstrate how causally-inspired approaches can improve model robustness. Finally, we outline desiderata for future research and practices, advocating targeted interventions to address inherent limitations of observational data.

### 摘要
基于偏好数据的奖励建模是将大语言模型（LLM）与人类价值观对齐的关键步骤，其需要对新提示-响应对具有鲁棒的泛化能力。本研究提出将该问题置于因果范式下，利用因果关系的丰富工具集来识别持续性挑战，如因果误识别、偏好异质性以及用户特定因素导致的混杂效应。继承因果推断文献的方法，我们明确了实现可靠泛化所需的关键假设，并将其与常见数据收集实践进行对比。通过展示朴素奖励模型的失效模式，我们论证了因果启发方法如何提升模型鲁棒性。最后，我们提出了未来研究和实践的理想方向，主张通过针对性干预来解决观测数据固有的局限性。

---

## [Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems](https://arxiv.org/abs/2506.05370)

### Abstract
arXiv:2506.05370v1 Announce Type: new 
Abstract: A critical challenge remains unresolved as generative AI systems are quickly implemented in various organizational settings. Despite significant advances in memory components such as RAG, vector stores, and LLM agents, these systems still have substantial memory limitations. Gen AI workflows rarely store or reflect on the full context in which decisions are made. This leads to repeated errors and a general lack of clarity. This paper introduces Contextual Memory Intelligence (CMI) as a new foundational paradigm for building intelligent systems. It repositions memory as an adaptive infrastructure necessary for longitudinal coherence, explainability, and responsible decision-making rather than passive data. Drawing on cognitive science, organizational theory, human-computer interaction, and AI governance, CMI formalizes the structured capture, inference, and regeneration of context as a fundamental system capability. The Insight Layer is presented in this paper to operationalize this vision. This modular architecture uses human-in-the-loop reflection, drift detection, and rationale preservation to incorporate contextual memory into systems. The paper argues that CMI allows systems to reason with data, history, judgment, and changing context, thereby addressing a foundational blind spot in current AI architectures and governance efforts. A framework for creating intelligent systems that are effective, reflective, auditable, and socially responsible is presented through CMI. This enhances human-AI collaboration, generative AI design, and the resilience of the institutions.

### 摘要
随着生成式人工智能系统在各组织机构中的快速部署，一个关键挑战仍未得到解决。尽管RAG、向量数据库和LLM代理等记忆组件取得了重大进展，这些系统仍存在显著的内存局限性。生成式AI工作流很少存储或反思决策制定的完整上下文，导致错误重复发生且整体透明度不足。本文提出"情境记忆智能"(CMI)作为构建智能系统的新基础范式，将记忆重新定位为实现纵向连贯性、可解释性和负责任决策所需的适应性基础设施，而非被动数据。CMI融合认知科学、组织理论、人机交互和AI治理等学科，将情境的结构化捕获、推理与再生形式化为系统的基础能力。本文提出的"洞察层"架构通过人在环反思、漂移检测和原理保存等模块化设计，实现了情境记忆的系统集成。研究表明，CMI使系统能够结合数据、历史记录、判断力和动态情境进行推理，从而弥补当前AI架构与治理实践中的根本盲区。通过CMI框架，我们提出了一种构建高效能、可反思、可审计且具有社会责任的智能系统方案，这将增强人机协作、改进生成式AI设计，并提升社会机构的韧性。

---

## [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)

### Abstract
arXiv:2506.05587v1 Announce Type: new 
Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis. Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.

### 摘要
表格及基于表格的应用场景在许多重要现实应用中发挥着关键作用，例如电子表格、数据库和计算笔记本等领域，这些传统上需要数据工程师、数据分析师和数据库管理员等专业用户操作。尽管大型语言模型（LLMs）在处理表格方面已取得显著进展（如电子表格和数据库辅助场景），但对此类能力的全面基准测试仍显不足。与日益增长的NLP基准测试相比，表格相关任务的评估研究稀缺且局限，主要集中在自然语言转SQL和表格问答等任务，忽视了专业用户面临的更广泛现实任务谱系。这一局限阻碍了我们在该重要领域对模型进展的理解。

本研究提出MMTU——一个包含25类现实表格任务、超过3万道问题的大规模基准测试，旨在全面评估模型在专家级别上理解、推理和操作真实表格的能力。这些任务源自数十年来计算机科学对表格数据的研究成果，重点关注专业用户面临的复杂表格任务。我们发现MMTU需要结合表格理解、推理和编程等技能，这对当前前沿模型仍具挑战性：即使如OpenAI o4-mini和DeepSeek R1等前沿推理模型也仅达到约60%的准确率，表明存在显著改进空间。我们通过MMTU评估揭示了关键发现，希望该基准能推动结构化数据处理与分析基础模型的研发进展。相关代码与数据详见https://github.com/MMTU-Benchmark/MMTU 和 https://huggingface.co/datasets/MMTU-benchmark/MMTU。

---

## [SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models](https://arxiv.org/abs/2506.05745)

### Abstract
arXiv:2506.05745v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) excel at complex reasoning tasks but typically generate lengthy sequential chains-of-thought, resulting in long inference times before arriving at the final answer. To address this challenge, we introduce SPRINT, a novel post-training and inference-time framework designed to enable LRMs to dynamically identify and exploit opportunities for parallelization during their reasoning process. SPRINT incorporates an innovative data curation pipeline that reorganizes natural language reasoning trajectories into structured rounds of long-horizon planning and parallel execution. By fine-tuning LRMs on a small amount of such curated data, the models learn to dynamically identify independent subtasks within extended reasoning processes and effectively execute them in parallel. Through extensive evaluations, we show that the models fine-tuned with the SPRINT framework match the performance of reasoning models on complex domains such as mathematics while generating up to ~39% fewer sequential tokens on problems requiring more than 8000 output tokens. Finally, we observe consistent results transferred to two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65% reduction in average sequential tokens for longer reasoning trajectories, while achieving the performance of the fine-tuned reasoning model.

### 摘要
大型推理模型（LRMs）在复杂推理任务中表现出色，但通常生成冗长的序列化思维链，导致在得出最终答案前需要较长的推理时间。为解决这一挑战，我们提出了SPRINT框架，这是一种新颖的训练后及推理时框架，旨在使LRMs在推理过程中动态识别并利用并行化机会。SPRINT采用创新的数据整理流程，将自然语言推理轨迹重组为结构化轮次的长程规划与并行执行。通过对少量此类整理数据进行微调，LRMs能够学会在扩展推理过程中动态识别独立子任务并有效并行执行。通过广泛评估，我们发现经SPRINT框架微调的模型在数学等复杂领域与推理模型的性能相当，同时在需要超过8000个输出标记的问题上，生成的序列标记数量减少了约39%。最后，我们观察到该框架在GPQA和Countdown两个分布外任务上具有一致的迁移效果，对于较长推理轨迹，平均序列标记分别减少高达45%和65%，同时保持了微调后推理模型的性能。

---

## [Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs](https://arxiv.org/abs/2506.05887)

### Abstract
arXiv:2506.05887v1 Announce Type: new 
Abstract: The growing application of artificial intelligence in sensitive domains has intensified the demand for systems that are not only accurate but also explainable and trustworthy. Although explainable AI (XAI) methods have proliferated, many do not consider the diverse audiences that interact with AI systems: from developers and domain experts to end-users and society. This paper addresses how trust in AI is influenced by the design and delivery of explanations and proposes a multilevel framework that aligns explanations with the epistemic, contextual, and ethical expectations of different stakeholders. The framework consists of three layers: algorithmic and domain-based, human-centered, and social explainability. We highlight the emerging role of Large Language Models (LLMs) in enhancing the social layer by generating accessible, natural language explanations. Through illustrative case studies, we demonstrate how this approach facilitates technical fidelity, user engagement, and societal accountability, reframing XAI as a dynamic, trust-building process.

### 摘要
随着人工智能在敏感领域的应用日益广泛，人们对系统不仅要求准确性，更要求其具备可解释性与可信赖性。尽管可解释人工智能（XAI）方法层出不穷，但多数未能充分考虑与AI系统交互的多元受众——从开发人员、领域专家到终端用户乃至社会大众。本文探讨了AI信任如何受到解释设计与呈现方式的影响，并提出一个多层次框架，使解释能够契合不同利益相关者在认知、情境和伦理层面的期望。该框架包含三个层级：算法与领域基础层、以人为本层和社会可解释层。我们重点分析了大型语言模型（LLMs）在增强社会层方面的新兴作用，其通过生成易于理解的自然语言解释来实现这一目标。通过典型案例分析，我们展示了该方法如何促进技术保真度、用户参与度和社会问责制，从而将XAI重新定义为动态的信任构建过程。

---

## [CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents](https://arxiv.org/abs/2506.05981)

### Abstract
arXiv:2506.05981v1 Announce Type: new 
Abstract: Modeling urban crime is an important yet challenging task that requires understanding the subtle visual, social, and cultural cues embedded in urban environments. Previous work has predominantly focused on rule-based agent-based modeling (ABM) and deep learning methods. ABMs offer interpretability of internal mechanisms but exhibit limited predictive accuracy.In contrast, deep learning methods are often effective in prediction but are less interpretable and require extensive training data. Moreover, both lines of work lack the cognitive flexibility to adapt to changing environments. Leveraging the capabilities of large language models (LLMs), we propose CrimeMind, a novel LLM-driven ABM framework for simulating urban crime within a multi-modal urban context.A key innovation of our design is the integration of the Routine Activity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to process rich multi-modal urban features and reason about criminal behavior.However, RAT requires LLM agents to infer subtle cues in evaluating environmental safety as part of assessing guardianship, which can be challenging for LLMs. To address this, we collect a small-scale human-annotated dataset and align CrimeMind's perception with human judgment via a training-free textual gradient method.Experiments across four major U.S. cities demonstrate that CrimeMind outperforms both traditional ABMs and deep learning baselines in crime hotspot prediction and spatial distribution accuracy, achieving up to a 24% improvement over the strongest baseline.Furthermore, we conduct counterfactual simulations of external incidents and policy interventions and it successfully captures the expected changes in crime patterns, demonstrating its ability to reflect counterfactual scenarios.Overall, CrimeMind enables fine-grained modeling of individual behaviors and facilitates evaluation of real-world interventions.

### 摘要
建模城市犯罪是一项重要但具有挑战性的任务，需要理解城市环境中蕴含的微妙视觉、社会和文化线索。先前研究主要集中于基于规则的智能体建模（ABM）和深度学习方法。ABM虽能提供内部机制的可解释性，但预测精度有限；而深度学习方法虽预测效果较好，却缺乏可解释性且需要大量训练数据。此外，这两种方法均缺乏适应环境变化的认知灵活性。利用大语言模型（LLM）的能力，我们提出CrimeMind——一个新颖的LLM驱动ABM框架，用于多模态城市环境下的犯罪模拟。该设计的核心创新是将'日常活动理论'（RAT）整合至CrimeMind的智能体工作流，使其能够处理丰富的多模态城市特征并推理犯罪行为。然而RAT要求LLM智能体在评估环境安全性时推断细微线索（作为监护评估环节），这对LLM具有挑战性。为此，我们收集了小规模人工标注数据集，并通过免训练的文本梯度方法将CrimeMind的感知与人类判断对齐。在美国四大城市的实验中，CrimeMind在犯罪热点预测和空间分布准确性上均优于传统ABM和深度学习基线，较最强基线提升达24%。此外，我们对外部事件和政策干预进行反事实模拟，成功捕捉到犯罪模式的预期变化，证明其反映反事实场景的能力。总体而言，CrimeMind实现了个体行为的细粒度建模，并为现实世界干预措施的评估提供了支持。

---

## [A Red Teaming Roadmap Towards System-Level Safety](https://arxiv.org/abs/2506.05376)

### Abstract
arXiv:2506.05376v1 Announce Type: cross 
Abstract: Large Language Model (LLM) safeguards, which implement request refusals, have become a widely adopted mitigation strategy against misuse. At the intersection of adversarial machine learning and AI safety, safeguard red teaming has effectively identified critical vulnerabilities in state-of-the-art refusal-trained LLMs. However, in our view the many conference submissions on LLM red teaming do not, in aggregate, prioritize the right research problems. First, testing against clear product safety specifications should take a higher priority than abstract social biases or ethical principles. Second, red teaming should prioritize realistic threat models that represent the expanding risk landscape and what real attackers might do. Finally, we contend that system-level safety is a necessary step to move red teaming research forward, as AI models present new threats as well as affordances for threat mitigation (e.g., detection and banning of malicious users) once placed in a deployment context. Adopting these priorities will be necessary in order for red teaming research to adequately address the slate of new threats that rapid AI advances present today and will present in the very near future.

### 摘要
大型语言模型（LLM）的安全防护机制通过实施请求拒绝策略，已成为当前广泛采用的防滥用缓解方案。在对抗性机器学习与AI安全的交叉领域，安全防护红队测试已有效识别出最先进拒答训练LLM中的关键漏洞。然而我们认为，当前大量关于LLM红队测试的会议投稿总体上未能聚焦正确的研究问题。首先，针对明确产品安全规范的测试应优先于抽象的社会偏见或伦理原则。其次，红队测试应聚焦代表不断扩展的风险格局及真实攻击者行为的现实威胁模型。最后，我们主张系统级安全是推动红队测试研究发展的必要步骤，因为AI模型在部署环境中既会带来新威胁，同时也提供威胁缓解的新途径（例如检测并封禁恶意用户）。红队测试研究必须采纳这些优先级，才能有效应对当前AI技术迅猛发展带来的新型威胁，以及即将在近期出现的更复杂挑战。

---

## [CP-Bench: Evaluating Large Language Models for Constraint Modelling](https://arxiv.org/abs/2506.06052)

### Abstract
arXiv:2506.06052v1 Announce Type: new 
Abstract: Combinatorial problems are present in a wide range of industries. Constraint Programming (CP) is a well-suited problem-solving paradigm, but its core process, namely constraint modelling, is a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) as modelling assistants, transforming combinatorial problem descriptions to executable constraint models, similar to coding assistants. However, the existing evaluation datasets for constraint modelling are often limited to small, homogeneous, or domain-specific instances, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing CP-Bench, a novel benchmark dataset that includes a diverse set of well-known combinatorial problem classes sourced from the CP community, structured explicitly for evaluating LLM-driven CP modelling. With this dataset, and given the variety of constraint modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax: the high-level MiniZinc language and Python-based CPMpy library, and the lower-level Python interface of the OR-Tools CP-SAT solver. In order to enhance the ability of LLMs to produce valid constraint models, we systematically evaluate the use of prompt-based and inference-time compute methods adapted from existing LLM-based code generation research. Our results underscore the modelling convenience provided by Python-based frameworks, as well as the effectiveness of documentation-rich system prompts, which, augmented with repeated sampling and self-verification, achieve further improvements, reaching up to 70\% accuracy on this new, highly challenging benchmark.

### 摘要
组合优化问题广泛存在于众多行业中。约束规划（CP）是一种非常适合的求解范式，但其核心过程——约束建模——已成为阻碍其广泛应用的瓶颈。为缓解这一问题，近期研究探索将大型语言模型（LLMs）作为建模助手，将组合问题描述转换为可执行的约束模型，类似于代码助手的功能。然而，现有约束建模评估数据集通常局限于小型、同质化或特定领域实例，未能涵盖现实场景的多样性。本研究通过引入CP-Bench填补这一空白，该新型基准数据集包含源自CP社区的多样化经典组合问题类别，专为评估LLM驱动的约束建模而设计。基于该数据集，并考虑到约束建模框架的多样性，我们比较评估了LLMs在三种不同建模系统中的建模能力：高层次MiniZinc语言、基于Python的CPMpy库，以及OR-Tools CP-SAT求解器的底层Python接口。为提升LLMs生成有效约束模型的能力，我们系统评估了基于提示的方法和推理时计算方法的运用，这些方法改编自现有基于LLM的代码生成研究。研究结果凸显了基于Python框架的建模便利性，以及富含文档说明的系统提示的有效性——当结合重复采样和自我验证技术时，可进一步提升性能，在这个极具挑战性的新基准上达到70%的准确率。

---

## [Can ChatGPT Perform Image Splicing Detection? A Preliminary Study](https://arxiv.org/abs/2506.05358)

### Abstract
arXiv:2506.05358v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning across text and image modalities, showing promise in a variety of complex vision-language tasks. In this preliminary study, we investigate the out-of-the-box capabilities of GPT-4V in the domain of image forensics, specifically, in detecting image splicing manipulations. Without any task-specific fine-tuning, we evaluate GPT-4V using three prompting strategies: Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a curated subset of the CASIA v2.0 splicing dataset.
  Our results show that GPT-4V achieves competitive detection performance in zero-shot settings (more than 85% accuracy), with CoT prompting yielding the most balanced trade-off across authentic and spliced images. Qualitative analysis further reveals that the model not only detects low-level visual artifacts but also draws upon real-world contextual knowledge such as object scale, semantic consistency, and architectural facts, to identify implausible composites. While GPT-4V lags behind specialized state-of-the-art splicing detection models, its generalizability, interpretability, and encyclopedic reasoning highlight its potential as a flexible tool in image forensics.

### 摘要
诸如GPT-4V等多模态大语言模型（MLLMs）能够跨越文本和图像模态进行推理，在多种复杂的视觉-语言任务中展现出潜力。在这项初步研究中，我们探究了GPT-4V在图像取证领域（特别是检测图像拼接篡改）的开箱即用能力。未经任何任务特定微调，我们采用三种提示策略——零样本（ZS）、少样本（FS）和思维链（CoT）——在CASIA v2.0拼接数据集的精选子集上评估GPT-4V。结果表明，GPT-4V在零样本设置下达到具有竞争力的检测性能（准确率超过85%），其中CoT提示在真实图像与拼接图像间实现了最均衡的权衡。定性分析进一步揭示，该模型不仅能检测低层视觉伪影，还能利用现实世界上下文知识（如物体尺度、语义一致性和建筑学事实）识别不合逻辑的合成图像。尽管GPT-4V落后于专用的最先进拼接检测模型，但其泛化性、可解释性和百科全书式推理能力，凸显了其作为图像取证灵活工具的潜力。

---

## [Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs](https://arxiv.org/abs/2506.05387)

### Abstract
arXiv:2506.05387v1 Announce Type: cross 
Abstract: This chapter explores advancements in decoding strategies for large language models (LLMs), focusing on enhancing the Locally Typical Sampling (LTS) algorithm. Traditional decoding methods, such as top-k and nucleus sampling, often struggle to balance fluency, diversity, and coherence in text generation. To address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS) is proposed as an improved version of LTS, incorporating dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments. ASTS ensures contextually coherent and diverse text generation while maintaining computational efficiency. Its performance is evaluated across multiple benchmarks, including story generation and abstractive summarization, using metrics such as perplexity, MAUVE, and diversity scores. Experimental results demonstrate that ASTS outperforms existing sampling techniques by reducing repetition, enhancing semantic alignment, and improving fluency.

### 摘要
本章探讨了大型语言模型（LLM）解码策略的进展，重点改进了局部典型采样（LTS）算法。传统解码方法（如top-k和核采样）在文本生成中往往难以平衡流畅性、多样性和连贯性。为解决这些问题，本文提出自适应语义感知典型性采样（ASTS）作为LTS的改进版本，该方法融合了动态熵阈值、多目标评分和奖励-惩罚调整机制。ASTS在保持计算效率的同时，能生成上下文连贯且多样化的文本。通过在故事生成和抽象摘要等多个基准测试中使用困惑度、MAUVE指标和多样性分数进行评估，实验结果表明：ASTS通过减少重复、增强语义对齐和提升流畅性，性能优于现有采样技术。

---

## [AD-EE: Early Exiting for Fast and Reliable Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2506.05404)

### Abstract
arXiv:2506.05404v1 Announce Type: cross 
Abstract: With the rapid advancement of autonomous driving, deploying Vision-Language Models (VLMs) to enhance perception and decision-making has become increasingly common. However, the real-time application of VLMs is hindered by high latency and computational overhead, limiting their effectiveness in time-critical driving scenarios. This challenge is particularly evident when VLMs exhibit over-inference, continuing to process unnecessary layers even after confident predictions have been reached. To address this inefficiency, we propose AD-EE, an Early Exit framework that incorporates domain characteristics of autonomous driving and leverages causal inference to identify optimal exit layers. We evaluate our method on large-scale real-world autonomous driving datasets, including Waymo and the corner-case-focused CODA, as well as on a real vehicle running the Autoware Universe platform. Extensive experiments across multiple VLMs show that our method significantly reduces latency, with maximum improvements reaching up to 57.58%, and enhances object detection accuracy, with maximum gains of up to 44%.

### 摘要
随着自动驾驶技术的快速发展，部署视觉语言模型（VLMs）以增强感知和决策能力已成为普遍做法。然而，高延迟和计算开销阻碍了VLM在实时场景中的应用，限制了其在时间敏感的驾驶情境中的有效性。当VLM出现过度推理现象（即在已获得确信预测后仍继续处理不必要的网络层）时，这一问题尤为突出。为解决这一效率缺陷，我们提出AD-EE框架：一种融合自动驾驶领域特性并利用因果推理确定最优退出层的早退机制。我们在包括Waymo和专注于极端案例的CODA等大规模真实自动驾驶数据集上，以及运行Autoware Universe平台的实际车辆上评估了该方法。跨多个VLM的广泛实验表明，我们的方法显著降低了延迟（最大改进达57.58%），同时提升了目标检测精度（最高提升达44%）。

---

## [Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment](https://arxiv.org/abs/2506.05384)

### Abstract
arXiv:2506.05384v1 Announce Type: cross 
Abstract: Recent studies demonstrate that multimodal large language models (MLLMs) can proficiently evaluate visual quality through interpretable assessments. However, existing approaches typically treat quality scoring and reasoning descriptions as separate tasks with disjoint optimization objectives, leading to a trade-off: models adept at quality reasoning descriptions struggle with precise score regression, while score-focused models lack interpretability. This limitation hinders the full potential of MLLMs in visual quality assessment, where accuracy and interpretability should be mutually reinforcing. To address this, we propose a unified two-stage training framework comprising a cold-start stage and a reinforcement learning-based fine-tuning stage. Specifically, in the first stage, we distill high-quality data from a teacher model through expert-designed prompts, initializing reasoning capabilities via cross-entropy loss supervision. In the second stage, we introduce a novel reward with Group Relative Policy Optimization (GRPO) to jointly optimize scoring accuracy and reasoning consistency. We designate the models derived from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain datasets. Furthermore, Q-Ponder significantly outperforms description-based SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in description accuracy and reasonableness, demonstrating the generalization potential over diverse tasks.

### 摘要
近期研究表明，多模态大语言模型（MLLMs）能够通过可解释的评估熟练判断视觉质量。然而现有方法通常将质量评分与推理描述视为具有分离优化目标的独立任务，导致两难困境：擅长质量推理描述的模型难以实现精确分数回归，而专注评分的模型则缺乏可解释性。这种局限阻碍了MLLMs在视觉质量评估中本应相互促进的准确性与可解释性的充分发挥。为此，我们提出包含冷启动阶段和基于强化学习的微调阶段的统一两阶段训练框架。具体而言，第一阶段通过专家设计的提示从教师模型蒸馏高质量数据，利用交叉熵损失监督初始化推理能力；第二阶段引入新型奖励机制结合群组相对策略优化（GRPO）联合优化评分准确性与推理一致性。我们将这两个阶段得到的模型分别命名为Q-Ponder-CI和Q-Ponder。大量实验表明，Q-Ponder在质量评分回归基准测试中达到最先进（SOTA）水平，在跨领域数据集上SRCC指标最高提升6.5%。此外，Q-Ponder在描述准确性与合理性方面显著优于包括其教师模型Qwen-2.5-VL-72B在内的基于描述的SOTA模型，展现出跨任务的泛化潜力。

---

## [Designing DSIC Mechanisms for Data Sharing in the Era of Large Language Models](https://arxiv.org/abs/2506.05379)

### Abstract
arXiv:2506.05379v1 Announce Type: cross 
Abstract: Training large language models (LLMs) requires vast amounts of high-quality data from institutions that face legal, privacy, and strategic constraints. Existing data procurement methods often rely on unverifiable trust or ignore heterogeneous provider costs. We introduce a mechanism-design framework for truthful, trust-minimized data sharing that ensures dominant-strategy incentive compatibility (DSIC), individual rationality, and weak budget balance, while rewarding data based on both quality and learning utility. We formalize a model where providers privately know their data cost and quality, and value arises solely from the data's contribution to model performance. Based on this, we propose the Quality-Weighted Marginal-Incentive Auction (Q-MIA), which ranks providers using a virtual cost metric and uses Myerson-style payments to ensure DSIC and budget feasibility. To support settings with limited liquidity or long-term incentives, we introduce the Marginal Utility Token (MUT), which allocates future rights based on marginal contributions. We unify these in Mixed-MIA, a hybrid mechanism balancing upfront payments and deferred rewards. All mechanisms support verifiable, privacy-preserving implementation. Theoretically and empirically, they outperform volume-based and trust-based baselines, eliciting higher-quality data under budget constraints while remaining robust to misreporting and collusion. This establishes a principled foundation for sustainable and fair data markets for future LLMs.

### 摘要
训练大型语言模型（LLM）需要来自受法律、隐私和战略约束机构的大量高质量数据。现有数据获取方法通常依赖不可验证的信任或忽略异构提供者成本。我们提出一种机制设计框架，用于实现真实可信、最小化信任的数据共享，该框架确保占优策略激励相容（DSIC）、个体理性及弱预算平衡，同时根据数据质量和学习效用进行奖励。我们建立了一个形式化模型，其中提供者私有地知晓其数据成本和质量，而价值仅源于数据对模型性能的贡献。基于此，我们提出质量加权边际激励拍卖（Q-MIA），该机制通过虚拟成本指标对提供者进行排序，并采用迈尔森式支付以确保DSIC和预算可行性。针对流动性受限或需长期激励的场景，我们引入边际效用代币（MUT），根据边际贡献分配未来权益。我们将这些机制统一为混合MIA机制，平衡预付款与递延奖励。所有机制均支持可验证且保护隐私的实现。理论与实证表明，这些机制优于基于数据量或信任的基线方法，在预算约束下能获取更高质量数据，同时对虚假报告和共谋保持稳健。这为未来LLM的可持续公平数据市场奠定了理论基础。

---

## [Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes](https://arxiv.org/abs/2506.05386)

### Abstract
arXiv:2506.05386v1 Announce Type: cross 
Abstract: Clinical note generation aims to automatically produce free-text summaries of a patient's condition and diagnostic process, with discharge instructions being a representative long-form example. While recent large language model (LLM)-based methods pre-trained on general clinical corpora show promise in clinical text generation, they fall short in producing long-form notes from limited patient information. In this paper, we propose R2AG, the first reinforced retriever for long-form discharge instruction generation based on pre-admission data. R2AG is trained with reinforcement learning to retrieve reasoning paths from a medical knowledge graph, providing explicit semantic guidance to the LLM. To bridge the information gap, we propose Group-Based Retriever Optimization (GRO) which improves retrieval quality with group-relative rewards, encouraging reasoning leaps for deeper inference by the LLM. Comprehensive experiments on the MIMIC-IV-Note dataset show that R2AG outperforms baselines in both clinical efficacy and natural language generation metrics. Further analysis reveals that R2AG fills semantic gaps in sparse input scenarios, and retrieved reasoning paths help LLMs avoid clinical misinterpretation by focusing on key evidence and following coherent reasoning.

### 摘要
临床记录生成旨在自动生成患者病情及诊疗过程的自由文本摘要，其中出院指导是典型的长文本范例。尽管当前基于大型语言模型（LLM）的方法通过通用临床语料库预训练在临床文本生成中展现出潜力，但其基于有限患者信息生成长文本记录的能力仍显不足。本文提出R2AG——首个基于入院前数据的强化检索器，用于长文本出院指导生成。R2AG通过强化学习训练，从医学知识图谱中检索推理路径，为LLM提供显式语义指导。为弥补信息鸿沟，我们提出基于群体的检索器优化（GRO）方法，通过群体相对奖励提升检索质量，促使LLM通过推理跃迁实现更深层推断。在MIMIC-IV-Note数据集上的综合实验表明，R2AG在临床效度和自然语言生成指标上均优于基线模型。进一步分析揭示，R2AG能在稀疏输入场景下填补语义空白，且检索到的推理路径通过聚焦关键证据并遵循连贯推理，帮助LLM避免临床误判。

---

## [Speaking images. A novel framework for the automated self-description of artworks](https://arxiv.org/abs/2506.05368)

### Abstract
arXiv:2506.05368v1 Announce Type: cross 
Abstract: Recent breakthroughs in generative AI have opened the door to new research perspectives in the domain of art and cultural heritage, where a large number of artifacts have been digitized. There is a need for innovation to ease the access and highlight the content of digital collections. Such innovations develop into creative explorations of the digital image in relation to its malleability and contemporary interpretation, in confrontation to the original historical object. Based on the concept of the autonomous image, we propose a new framework towards the production of self-explaining cultural artifacts using open-source large-language, face detection, text-to-speech and audio-to-animation models. The goal is to start from a digitized artwork and to automatically assemble a short video of the latter where the main character animates to explain its content. The whole process questions cultural biases encapsulated in large-language models, the potential of digital images and deepfakes of artworks for educational purposes, along with concerns of the field of art history regarding such creative diversions.

### 摘要
生成式人工智能的最新突破为艺术与文化遗产领域开辟了新的研究视角，该领域已有大量文物完成数字化。当前亟需通过创新手段提升数字藏品的可及性并凸显其内容价值。这类创新体现为对数字图像可塑性及其当代诠释的创造性探索，并与原始历史文物形成观照。基于自主图像概念，我们提出一个新框架，利用开源大语言模型、面部检测、文本转语音及音频驱动动画模型，实现自阐释文化产物的生成。该框架以数字化艺术品为起点，自动合成短视频使画中主角动态解说作品内容。整个过程引发了对大语言模型中文化偏见的反思，探讨了数字图像及艺术品深度伪造在教育应用中的潜力，同时回应了艺术史领域对此类创造性转译的关切。

---

## [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)

### Abstract
arXiv:2506.05413v1 Announce Type: cross 
Abstract: We present SmoothRot, a novel post-training quantization technique to enhance the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot addresses the critical challenge of massive activation outliers, by integrating channel-wise scaling with Hadamard transformations. Our technique effectively transforms extreme outliers into quantization-friendly activations, significantly improving quantization accuracy. Experiments conducted on popular LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot consistently reduces the performance gap between quantized and FP16 models by approximately 10-30\% across language generation and zero-shot reasoning tasks, without introducing additional inference latency. Code is available at https://github.com/czakop/smoothrot.

### 摘要
我们提出SmoothRot，这是一种新颖的训练后量化技术，用于提升大语言模型(LLM)中4位量化的效率。该技术通过将通道级缩放与Hadamard变换相结合，有效解决了大规模激活异常值这一关键挑战。我们的方法能将极端异常值转化为适合量化的激活状态，从而显著提高量化精度。在主流大语言模型(LLaMA2 7B、LLaMA3.1 8B和Mistral 7B)上的实验表明，SmoothRot在语言生成和零样本推理任务中，持续将量化模型与FP16模型之间的性能差距缩小约10-30%，且不会引入额外的推理延迟。代码已开源：https://github.com/czakop/smoothrot。

---

## [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/abs/2506.06254)

### Abstract
arXiv:2506.06254v1 Announce Type: new 
Abstract: Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users' varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.

### 摘要
大型语言模型（LLM）赋能的智能体近期已成为一种先进范式，在众多领域和任务中展现出卓越能力。尽管潜力巨大，现有LLM智能体通常采用通用化方案，缺乏响应用户多样化需求与偏好的灵活性。这一局限促使我们开发PersonaAgent——首个专为多维度个性化任务设计的个性化LLM智能体框架。具体而言，PersonaAgent整合了两个互补组件：包含情景记忆与语义记忆机制的个性化记忆模块；支持智能体执行用户定制化工具操作的个性化行动模块。其核心在于将用户画像（定义为每位用户的独特系统提示）作为中介：通过个性化记忆的洞察调控智能体行为，同时这些行为产生的结果又反向优化记忆。基于该框架，我们提出测试时用户偏好对齐策略，通过模拟最近n次交互来优化用户画像提示，并借助模拟响应与真实响应间的文本损失反馈实现实时偏好校准。实验评估表明，PersonaAgent不仅显著优于基线方法实现行动空间的有效个性化，更能适应测试阶段实际应用的扩展需求。这些结果验证了本方法在提供定制化、动态用户体验方面的可行性与潜力。

---

## [SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing](https://arxiv.org/abs/2506.05414)

### Abstract
arXiv:2506.05414v1 Announce Type: cross 
Abstract: 3D spatial reasoning in dynamic, audio-visual environments is a cornerstone of human cognition yet remains largely unexplored by existing Audio-Visual Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic scenes with synchronized spatial audio. SAVVY-Bench is comprised of thousands of relationships involving static and moving objects, and requires fine-grained temporal grounding, consistent 3D localization, and multi-modal annotation. To tackle this challenge, we propose SAVVY, a novel training-free reasoning pipeline that consists of two stages: (i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as other audio-visual methods to track the trajectories of key objects related to the query using both visual and spatial audio cues, and (ii) Dynamic Global Map Construction, which aggregates multi-modal queried object trajectories and converts them into a unified global dynamic map. Using the constructed map, a final QA answer is obtained through a coordinate transformation that aligns the global map with the queried viewpoint. Empirical evaluation demonstrates that SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.

### 摘要
动态视听环境中的三维空间推理是人类认知的基石，但现有视听大语言模型（AV-LLM）和基准测试主要集中于静态或二维场景，对此领域仍鲜有探索。我们提出了SAVVY-Bench，这是首个针对动态场景中同步空间音频的三维空间推理基准测试。SAVVY-Bench包含数千个涉及静态和移动对象的关系，要求细粒度的时间定位、一致的三维空间标定以及多模态标注。为应对这一挑战，我们提出了SAVVY，一种无需训练的新型推理流程，包含两个阶段：（i）自我中心空间轨迹估计，利用AV-LLM及其他视听方法，通过视觉和空间音频线索追踪与查询相关的关键对象轨迹；（ii）动态全局地图构建，聚合多模态查询对象轨迹并将其转换为统一的全局动态地图。通过构建的地图，最终通过坐标变换将全局地图与查询视点对齐，从而获得问答答案。实证评估表明，SAVVY显著提升了最先进AV-LLM的性能，为动态三维空间推理在AV-LLM中的应用设立了新标准和新阶段。

---

## [FERRET: Private Deep Learning Faster And Better Than DPSGD](https://arxiv.org/abs/2506.05416)

### Abstract
arXiv:2506.05416v1 Announce Type: cross 
Abstract: We revisit 1-bit gradient compression through the lens of mutual-information differential privacy (MI-DP). Building on signSGD, we propose FERRET--Fast and Effective Restricted Release for Ethical Training--which transmits at most one sign bit per parameter group with Bernoulli masking.
  Theory: We prove each fired group leaks at most ln 2 nats; after subsampling with rate s, the total privacy loss of G groups trained for T steps with firing probability p is epsilon = G * T * s * p * ln 2. Thus FERRET achieves MI-DP for epsilon in [0.1, 2] without additive noise.
  Practice: We evaluate three granularities--FERRET-MAX (finest), FERRET-EIGHTH (medium), and FERRET-2 (coarsest)--on five LLMs (137M-1.8B parameters) against DPSGD and Non-DP baselines. All methods trained for 1, 3, and 5 epochs.
  Utility: Across all settings, FERRET-MAX/EIGHTH beat DPSGD's perplexity. At epsilon=0.5, 5 epochs: FERRET-EIGHTH achieves 3.98 perplexity vs DPSGD's 11.61 (2.9x better), within 23% of Non-DP (3.25).
  Privacy: MI-AUC stays at chance for FERRET-MAX/EIGHTH (~0.51), matching DPSGD vs Non-DP's 0.76-0.99. FERRET-2 shows higher leakage (~0.55) due to lower headroom.
  Efficiency: Stricter budgets fire fewer signs, so FERRET uses 19-33% of DPSGD's training time and only 34-36% of Non-DP training time.
  Take-away: Sign-based MI-DP gets closer to achieving all three qualities of the privacy, utility, performance trilemma: FERRET trains up to 5x faster, achieves 3x lower perplexity compared to DPSGD and 1.2x greater than Non-DP, all while providing formal, mathematically provable privacy guarantees using zero additive noise. The results also show that, in certain instances, masked 1-bit updates can match non-private training utility while safeguarding data.

### 摘要
我们通过互信息差分隐私（MI-DP）的视角重新审视1比特梯度压缩技术。基于signSGD框架，提出FERRET——快速有效的伦理训练受限释放机制——该方法通过伯努利掩码为每个参数组传输至多一个符号位。理论贡献：证明每个激活组最多泄露ln 2纳特的隐私信息；在采样率s下，G个组经过T步训练且激活概率为p时，总隐私损失ε=G*T*s*p*ln 2。因此FERRET在ε∈[0.1,2]范围内无需添加噪声即可实现MI-DP。

---

## [Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction](https://arxiv.org/abs/2506.05428)

### Abstract
arXiv:2506.05428v1 Announce Type: cross 
Abstract: Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by a trade-off between immediacy--making fast predictions from a single baseline sMRI--and accuracy--leveraging longitudinal scans to capture disease progression. We propose MCI-Diff, a diffusion-based framework that synthesizes clinically plausible future sMRI representations directly from baseline data, achieving both real-time risk assessment and high predictive performance. First, a multi-task sequence reconstruction strategy trains a shared denoising network on interpolation and extrapolation tasks to handle irregular follow-up sampling and learn robust latent trajectories. Second, an LLM-driven "linguistic compass" is introduced for clinical plausibility sampling: generated feature candidates are quantized, tokenized, and scored by a fine-tuned language model conditioned on expected structural biomarkers, guiding autoregressive generation toward realistic disease patterns. Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms state-of-the-art baselines, improving early conversion accuracy by 5-12%.

### 摘要
轻度认知障碍（MCI）转化的早期预测面临即时性与准确性之间的权衡——前者需基于单次基线结构磁共振成像（sMRI）快速预测，后者则依赖纵向扫描以捕捉疾病进展。我们提出MCI-Diff框架，通过扩散模型直接从基线数据合成临床合理的未来sMRI表征，实现实时风险评估与高预测性能。首先，采用多任务序列重建策略，在插值和外推任务上训练共享去噪网络，以处理不规则随访采样并学习稳健的潜在轨迹。其次，引入LLM驱动的"语言罗盘"实现临床合理性采样：生成的特征候选经量化、标记化后，由基于预期结构生物标志物微调的语言模型评分，引导自回归生成过程朝向真实疾病模式。在ADNI和AIBL队列上的实验表明，MCI-Diff优于现有最优基线方法，将早期转化预测准确率提升5-12%。

---

## [SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning](https://arxiv.org/abs/2506.05425)

### Abstract
arXiv:2506.05425v1 Announce Type: cross 
Abstract: The rich and multifaceted nature of human social interaction, encompassing multimodal cues, unobservable relations and mental states, and dynamical behavior, presents a formidable challenge for artificial intelligence. To advance research in this area, we introduce SIV-Bench, a novel video benchmark for rigorously evaluating the capabilities of Multimodal Large Language Models (MLLMs) across Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). SIV-Bench features 2,792 video clips and 8,792 meticulously generated question-answer pairs derived from a human-LLM collaborative pipeline. It is originally collected from TikTok and YouTube, covering a wide range of video genres, presentation styles, and linguistic and cultural backgrounds. It also includes a dedicated setup for analyzing the impact of different textual cues-original on-screen text, added dialogue, or no text. Our comprehensive experiments on leading MLLMs reveal that while models adeptly handle SSU, they significantly struggle with SSR and SDP, where Relation Inference (RI) is an acute bottleneck, as further examined in our analysis. Our study also confirms the critical role of transcribed dialogue in aiding comprehension of complex social interactions. By systematically identifying current MLLMs' strengths and limitations, SIV-Bench offers crucial insights to steer the development of more socially intelligent AI. The dataset and code are available at https://kfq20.github.io/sivbench/.

### 摘要
人类社交互动具有丰富多元的特性，涵盖多模态线索、不可观测的关系与心理状态以及动态行为模式，这对人工智能构成了重大挑战。为推进该领域研究，我们提出SIV-Bench这一新型视频基准测试，用于系统评估多模态大语言模型（MLLMs）在社交场景理解（SSU）、社交状态推理（SSR）和社交动态预测（SDP）三个维度的能力。该基准包含2,792个视频片段及通过人机协作流程生成的8,792个精细标注问答对，原始素材采集自TikTok和YouTube平台，覆盖多样化的视频类型、呈现风格以及语言文化背景，并专门设置了分析不同文本线索（原始屏幕文字、添加对话或无文本）影响的实验模块。针对主流MLLMs的全面实验表明：模型虽能胜任SSU任务，但在SSR和SDP方面表现显著欠佳，其中关系推理（RI）是突出瓶颈问题（分析部分将深入探讨）。研究同时证实转录对话对理解复杂社交互动的关键作用。通过系统揭示当前MLLMs的优势与局限，SIV-Bench为开发更具社交智能的AI提供了重要指引。数据集与代码已发布于https://kfq20.github.io/sivbench/。

---

## [Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety](https://arxiv.org/abs/2506.05451)

### Abstract
arXiv:2506.05451v1 Announce Type: cross 
Abstract: As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but such connections with safety are often overlooked in prior surveys. We present the first survey that bridges this gap, introducing a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them. Our novel taxonomy, organized by LLM workflow stages, summarizes nearly 70 works at their intersections. We conclude with open challenges and future directions. This timely survey helps researchers and practitioners navigate key advancements for safer, more interpretable LLMs.

### 摘要
随着大语言模型（LLMs）在现实世界中的广泛应用，理解并减少其不安全行为变得至关重要。解释技术能够揭示不安全输出的成因并指导安全改进，但此类与安全的关联在既往综述中常被忽视。本综述首次弥合这一空白，提出了一个统一框架，将聚焦安全的解释方法、其指导的安全增强措施以及实施工具三者有机结合。我们按LLM工作流程阶段构建的新型分类法，系统梳理了近70项相关研究的交叉点。最后探讨了开放挑战与未来方向。这项及时的研究有助于学者和从业者把握关键进展，推动构建更安全、更可解释的大语言模型。

---

## [MLLM-CL: Continual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2506.05453)

### Abstract
arXiv:2506.05453v1 Announce Type: cross 
Abstract: Recent Multimodal Large Language Models (MLLMs) excel in vision-language understanding but face challenges in adapting to dynamic real-world scenarios that require continuous integration of new knowledge and skills. While continual learning (CL) offers a potential solution, existing benchmarks and methods suffer from critical limitations. In this paper, we introduce MLLM-CL, a novel benchmark encompassing domain and ability continual learning, where the former focuses on independently and identically distributed (IID) evaluation across evolving mainstream domains, whereas the latter evaluates on non-IID scenarios with emerging model ability. Methodologically, we propose preventing catastrophic interference through parameter isolation, along with an MLLM-based routing mechanism. Extensive experiments demonstrate that our approach can integrate domain-specific knowledge and functional abilities with minimal forgetting, significantly outperforming existing methods.

### 摘要
当前多模态大语言模型（MLLMs）在视觉语言理解方面表现出色，但在适应需要持续整合新知识与技能的动态现实场景时仍面临挑战。虽然持续学习（CL）提供了潜在解决方案，但现有基准与方法存在显著局限性。本文提出MLLM-CL这一新型基准，涵盖领域持续学习与能力持续学习：前者关注主流演进领域中的独立同分布（IID）评估，后者则针对新兴模型能力的非IID场景进行评测。在方法论层面，我们通过参数隔离结合基于MLLM的路由机制来防止灾难性干扰。大量实验表明，该方法能以最小遗忘度整合领域特定知识与功能能力，显著优于现有方法。

---

## [Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward](https://arxiv.org/abs/2506.05433)

### Abstract
arXiv:2506.05433v1 Announce Type: cross 
Abstract: Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at https://github.com/johncaged/PrefixGrouper

### 摘要
群组相对策略优化（GRPO）通过从共享相同输入前缀的候选输出之间的相对比较中计算梯度，从而增强策略学习。尽管该方法效果显著，但在处理长共享前缀时，GRPO会引入大量计算开销，因为每个群组成员都需要对前缀进行冗余编码。这种低效性成为长上下文学习场景中的主要可扩展性瓶颈。我们提出Prefix Grouper算法，这是一种高效的GRPO训练方法，通过共享前缀前向策略消除冗余前缀计算。具体而言，该方法将自注意力机制重构为两部分，使得共享前缀仅需编码一次，同时保持完整的可微性以及与端到端训练的兼容性。我们通过理论和实验证据证明，Prefix Grouper在训练效果上等同于标准GRPO：它产生完全相同的前向输出和反向梯度，确保优化动态和最终策略性能保持不变。实验结果表明，Prefix Grouper在显著降低训练计算成本（尤其是长前缀场景）的同时，能够获得一致的结果。所提方法完全即插即用：与现有基于GRPO的架构兼容，可作为直接替换无缝集成至当前训练流程，无需结构调整且仅需对输入构建和注意力计算进行最小改动。Prefix Grouper使得在相同计算预算下能够使用更大群组规模，从而提升GRPO在更复杂任务和更大模型上的可扩展性。代码已发布于https://github.com/johncaged/PrefixGrouper。

---

## [PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling](https://arxiv.org/abs/2506.05432)

### Abstract
arXiv:2506.05432v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.

### 摘要
大型语言模型（LLMs）因其庞大的参数量级在边缘部署中面临重大挑战。向量量化（VQ）作为一种基于聚类的量化方法，凭借其极低位宽（可低至2比特）和可观精度成为主流解决方案。由于向量在数学和物理学中是兼具方向与大小的量，现有VQ工作通常采用耦合方式进行量化。然而本研究发现，方向对量化的敏感度显著高于幅度：例如对LLaMA-2-7B权重向量方向与幅度分别聚类时，零样本任务精度下降分别为46.5%与2.3%，且该差距随聚类中心减少进一步扩大。此外，当前VQ工作中衡量向量相似性的常用指标欧氏距离更侧重减小幅度误差，这一特性与上述发现相悖，必然导致更大量化误差。为此，本文提出极坐标解耦向量量化（PCDVQ），该高效框架包含两个核心模块：1）极坐标解耦（PCD），将向量转换为极坐标表示后对方向与幅度参数独立量化；2）分布对齐码本构建（DACC），根据源分布优化方向与幅度码本。实验表明PCDVQ在2比特量化级别上零样本准确率至少超越基线方法1.5%，为高精度强压缩LLMs建立了新范式。

---

## [Zeroth-Order Optimization Finds Flat Minima](https://arxiv.org/abs/2506.05454)

### Abstract
arXiv:2506.05454v1 Announce Type: cross 
Abstract: Zeroth-order methods are extensively used in machine learning applications where gradients are infeasible or expensive to compute, such as black-box attacks, reinforcement learning, and language model fine-tuning. Existing optimization theory focuses on convergence to an arbitrary stationary point, but less is known on the implicit regularization that provides a fine-grained characterization on which particular solutions are finally reached. We show that zeroth-order optimization with the standard two-point estimator favors solutions with small trace of Hessian, which is widely used in previous work to distinguish between sharp and flat minima. We further provide convergence rates of zeroth-order optimization to approximate flat minima for convex and sufficiently smooth functions, where flat minima are defined as the minimizers that achieve the smallest trace of Hessian among all optimal solutions. Experiments on binary classification tasks with convex losses and language model fine-tuning support our theoretical findings.

### 摘要
零阶方法在梯度不可行或计算成本高昂的机器学习应用中广泛使用，例如黑盒攻击、强化学习和语言模型微调。现有的优化理论主要关注收敛到任意稳定点，但对于隐式正则化的研究较少，而隐式正则化能够精细刻画最终到达的具体解。我们证明，使用标准两点估计器的零阶优化倾向于选择具有较小Hessian矩阵迹的解，这一指标在先前工作中被广泛用于区分尖锐和平坦极小值。我们进一步给出了零阶优化在凸且充分光滑函数上收敛到近似平坦极小值的收敛速率，其中平坦极小值定义为在所有最优解中Hessian矩阵迹最小的极小值。在凸损失函数的二元分类任务和语言模型微调上的实验支持了我们的理论发现。

---

## [StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models](https://arxiv.org/abs/2506.05502)

### Abstract
arXiv:2506.05502v1 Announce Type: cross 
Abstract: Watermarking for large language models (LLMs) offers a promising approach to identifying AI-generated text. Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification. We present StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance data, such as userID, TimeStamp, and modelID, within LLM-generated text. This enhances fast traceability without requiring access to the language model's API or prompts. We derive a lower bound on the number of tokens necessary for watermark detection at a fixed equal error rate, which provides insights on how to enhance the capacity. Comprehensive empirical evaluations across diverse tasks highlight the stealthiness, detectability, and resilience of StealthInk, establishing it as an effective solution for LLM watermarking applications.

### 摘要
针对大语言模型（LLMs）的水印技术为识别AI生成文本提供了一种可行方案。然而现有方法要么会改变LLM生成文本的原始分布，要么仅限于嵌入零比特信息——仅支持水印检测而无法实现身份识别。本文提出StealthInk，一种保持文本原始分布特性的隐蔽多比特水印方案，可在LLM生成文本中嵌入用户ID、时间戳和模型ID等溯源数据。该方法无需访问语言模型的API或提示词即可实现快速溯源。我们推导出在固定等错误率下水印检测所需令牌数量的下界，这为提升水印容量提供了理论依据。跨多种任务的综合实验评估表明，StealthInk在隐蔽性、可检测性和鲁棒性方面表现优异，成为LLM水印应用的有效解决方案。

---

## [Structured Labeling Enables Faster Vision-Language Models for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.05442)

### Abstract
arXiv:2506.05442v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) offer a promising approach to end-to-end autonomous driving due to their human-like reasoning capabilities. However, troublesome gaps remains between current VLMs and real-world autonomous driving applications. One major limitation is that existing datasets with loosely formatted language descriptions are not machine-friendly and may introduce redundancy. Additionally, high computational cost and massive scale of VLMs hinder the inference speed and real-world deployment. To bridge the gap, this paper introduces a structured and concise benchmark dataset, NuScenes-S, which is derived from the NuScenes dataset and contains machine-friendly structured representations. Moreover, we present FastDrive, a compact VLM baseline with 0.9B parameters. In contrast to existing VLMs with over 7B parameters and unstructured language processing(e.g., LLaVA-1.5), FastDrive understands structured and concise descriptions and generates machine-friendly driving decisions with high efficiency. Extensive experiments show that FastDrive achieves competitive performance on structured dataset, with approximately 20% accuracy improvement on decision-making tasks, while surpassing massive parameter baseline in inference speed with over 10x speedup. Additionally, ablation studies further focus on the impact of scene annotations (e.g., weather, time of day) on decision-making tasks, demonstrating their importance on decision-making tasks in autonomous driving.

### 摘要
视觉语言模型（VLMs）因其类人推理能力，为端到端自动驾驶提供了有前景的解决方案。然而，当前VLM与实际自动驾驶应用之间仍存在显著差距。主要局限在于现有数据集采用松散格式的语言描述，不仅缺乏机器友好性，还可能引入冗余信息。此外，VLM的高计算成本和大规模参数量严重制约了推理速度与实际部署。为弥补这一差距，本文提出了结构化且简洁的基准数据集NuScenes-S（基于NuScenes数据集构建，包含机器友好的结构化表征），并推出紧凑型VLM基线模型FastDrive（参数量仅0.9B）。相较于参数量超7B的非结构化语言处理VLM（如LLaVA-1.5），FastDrive能高效理解结构化简洁描述并生成机器友好的驾驶决策。大量实验表明，FastDrive在结构化数据集上达到具有竞争力的性能，决策任务准确率提升约20%，同时推理速度超越大规模参数基线模型，加速超10倍。消融实验进一步揭示了场景标注（如天气、时段）对决策任务的影响，证实其对自动驾驶决策任务的重要性。

---

## [Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning](https://arxiv.org/abs/2506.05447)

### Abstract
arXiv:2506.05447v1 Announce Type: cross 
Abstract: This work aims to understand how scaling improves language models, specifically in terms of training dynamics. We find that language models undergo loss deceleration early in training; an abrupt slowdown in the rate of loss improvement, resulting in piecewise linear behaviour of the loss curve in log-log space. Scaling up the model mitigates this transition by (1) decreasing the loss at which deceleration occurs, and (2) improving the log-log rate of loss improvement after deceleration. We attribute loss deceleration to a type of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL, per-example gradients become systematically opposed, leading to destructive interference in per-example changes in loss. As a result, improving loss on one subset of examples degrades it on another, bottlenecking overall progress. Loss deceleration and ZSL provide new insights into the training dynamics underlying language model scaling laws, and could potentially be targeted directly to improve language models independent of scale. We make our code and artefacts available at: https://github.com/mirandrom/zsl

### 摘要
本研究旨在探究规模扩展如何提升语言模型性能，尤其关注训练动态的变化。我们发现语言模型在训练早期会出现损失减速现象——即损失改善速率突然放缓，导致对数空间中的损失曲线呈现分段线性特征。扩大模型规模可通过两种方式缓解这一转变：(1) 降低损失减速发生时的损失值，(2) 提升减速后损失改善的对数速率。我们将损失减速归因于一种称为"零和学习"(ZSL)的退化训练动态。在ZSL状态下，样本梯度呈现系统性对抗，导致样本损失变化产生破坏性干涉，使得优化某样本子集的损失会损害其他子集的损失，从而形成整体进步的瓶颈。损失减速与ZSL现象为理解语言模型缩放定律背后的训练动态提供了新视角，未来或可直接针对这些现象进行优化，实现不依赖规模扩展的语言模型改进。代码与实验材料已开源：https://github.com/mirandrom/zsl

---

## [Sentinel: SOTA model to protect against prompt injections](https://arxiv.org/abs/2506.05446)

### Abstract
arXiv:2506.05446v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly powerful but remain vulnerable to prompt injection attacks, where malicious inputs cause the model to deviate from its intended instructions. This paper introduces Sentinel, a novel detection model, qualifire/prompt-injection-sentinel, based on the \answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced features and fine-tuning on an extensive and diverse dataset comprising a few open-source and private collections, Sentinel achieves state-of-the-art performance. This dataset amalgamates varied attack types, from role-playing and instruction hijacking to attempts to generate biased content, alongside a broad spectrum of benign instructions, with private datasets specifically targeting nuanced error correction and real-world misclassifications. On a comprehensive, unseen internal test set, Sentinel demonstrates an average accuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on public benchmarks, it consistently outperforms strong baselines like protectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's architecture, its meticulous dataset curation, its training methodology, and a thorough evaluation, highlighting its superior detection capabilities.

### 摘要
大语言模型（LLMs）虽日益强大，但仍易受提示注入攻击的影响，恶意输入会导致模型偏离其既定指令。本文提出Sentinel，一种基于\answerdotai/ModernBERT-large架构的新型检测模型qualifire/prompt-injection-sentinel。通过利用ModernBERT的先进特性，并在包含多个开源和私有数据集的广泛多样数据集上进行微调，Sentinel实现了最先进的性能。该数据集融合了多种攻击类型，包括角色扮演、指令劫持和生成偏见内容的尝试，以及涵盖广泛良性指令的样本，其中私有数据集专门针对细微错误修正和现实世界误分类。在一个全面的未见内部测试集上，Sentinel展现出0.987的平均准确率和0.980的F1分数。此外，在公共基准测试中，其表现始终优于protectai/deberta-v3-base-prompt-injection-v2等强基线。本研究详细阐述了Sentinel的架构、精细的数据集构建方法、训练流程及全面评估，凸显了其卓越的检测能力。

---

## [Conformal Prediction Adaptive to Unknown Subpopulation Shifts](https://arxiv.org/abs/2506.05583)

### Abstract
arXiv:2506.05583v1 Announce Type: cross 
Abstract: Conformal prediction is widely used to equip black-box machine learning models with uncertainty quantification enjoying formal coverage guarantees. However, these guarantees typically break down in the presence of distribution shifts, where the data distribution at test time differs from the training (or calibration-time) distribution. In this work, we address subpopulation shifts, where the test environment exhibits an unknown and differing mixture of subpopulations compared to the calibration data. We propose new methods that provably adapt conformal prediction to such shifts, ensuring valid coverage without requiring explicit knowledge of subpopulation structure. Our algorithms scale to high-dimensional settings and perform effectively in realistic machine learning tasks. Extensive experiments on vision (with vision transformers) and language (with large language models) benchmarks demonstrate that our methods reliably maintain coverage and controls risk in scenarios where standard conformal prediction fails.

### 摘要
共形预测被广泛用于为黑盒机器学习模型提供具有形式化覆盖保证的不确定性量化。然而，当存在分布偏移时，即测试时的数据分布与训练（或校准时的）分布不同时，这些保证通常会失效。本研究针对子群体偏移问题，即测试环境呈现出与校准数据相比未知且不同的子群体混合比例。我们提出了新方法，可证明地将共形预测适配至此类偏移，在无需明确知晓子群体结构的情况下确保有效覆盖。我们的算法可扩展至高维场景，并在现实机器学习任务中高效运行。通过对视觉（使用视觉变换器）和语言（使用大语言模型）基准的广泛实验表明，在标准共形预测失效的场景下，我们的方法能可靠地维持覆盖范围并控制风险。

---

## [Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models](https://arxiv.org/abs/2506.05497)

### Abstract
arXiv:2506.05497v1 Announce Type: cross 
Abstract: Uncertainty quantification (UQ) is essential for safe deployment of generative AI models such as large language models (LLMs), especially in high stakes applications. Conformal prediction (CP) offers a principled uncertainty quantification framework, but classical methods focus on regression and classification, relying on geometric distances or softmax scores: tools that presuppose structured outputs. We depart from this paradigm by studying CP in a query only setting, where prediction sets must be constructed solely from finite queries to a black box generative model, introducing a new trade off between coverage, test time query budget, and informativeness. We introduce Conformal Prediction with Query Oracle (CPQ), a framework characterizing the optimal interplay between these objectives. Our finite sample algorithm is built on two core principles: one governs the optimal query policy, and the other defines the optimal mapping from queried samples to prediction sets. Remarkably, both are rooted in the classical missing mass problem in statistics. Specifically, the optimal query policy depends on the rate of decay, or the derivative, of the missing mass, for which we develop a novel estimator. Meanwhile, the optimal mapping hinges on the missing mass itself, which we estimate using Good Turing estimators. We then turn our focus to implementing our method for language models, where outputs are vast, variable, and often under specified. Fine grained experiments on three real world open ended tasks and two LLMs, show CPQ applicability to any black box LLM and highlight: (1) individual contribution of each principle to CPQ performance, and (2) CPQ ability to yield significantly more informative prediction sets than existing conformal methods for language uncertainty quantification.

### 摘要
不确定性量化（UQ）对于大型语言模型（LLM）等生成式人工智能模型的安全部署至关重要，尤其是在高风险应用中。保形预测（CP）提供了一个原则性的不确定性量化框架，但传统方法侧重于回归和分类，依赖于几何距离或softmax分数：这些工具预设了结构化输出。我们通过研究仅查询设置下的CP来突破这一范式，其中预测集必须仅通过对黑盒生成模型的有限查询构建，从而在覆盖率、测试时查询预算和信息量之间引入新的权衡。我们提出了带查询预言机的保形预测（CPQ），该框架刻画了这些目标之间的最优交互关系。我们的有限样本算法基于两个核心原则：一是管理最优查询策略，二是定义从查询样本到预测集的最优映射。值得注意的是，两者均源于统计学中的经典缺失质量问題。具体而言，最优查询策略取决于缺失质量的衰减率或其导数，为此我们开发了一种新颖的估计器。同时，最优映射则依赖于缺失质量本身，我们使用古德-图灵估计器进行估计。随后我们将重点转向在语言模型中实施该方法，其输出规模庞大、多变且常未明确指定。针对三个现实世界开放式任务和两种LLM的细粒度实验表明，CPQ适用于任何黑盒LLM，并突出显示：（1）每条原则对CPQ性能的独立贡献；（2）与现有语言不确定性量化的保形方法相比，CPQ能够产生信息量显著更丰富的预测集。

---

## [Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning](https://arxiv.org/abs/2506.05568)

### Abstract
arXiv:2506.05568v1 Announce Type: cross 
Abstract: Large language models (LLMs) have not yet effectively leveraged the vast amounts of edge-device data, and federated learning (FL) offers a promising paradigm to collaboratively fine-tune LLMs without transferring private edge data to the cloud. To operate within the computation and communication constraints of edge devices, recent literature on federated fine-tuning of LLMs proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient methods. However, LoRA-based methods suffer from accuracy degradation in FL settings, primarily because of data and computational heterogeneity across clients. We propose \textsc&#123;Ravan&#125;, an adaptive multi-head LoRA method that balances parameter efficiency and model expressivity by reparameterizing the weight updates as the sum of multiple LoRA heads $s_i\textbf&#123;B&#125;_i\textbf&#123;H&#125;_i\textbf&#123;A&#125;_i$ in which only the core matrices $\textbf&#123;H&#125;_i$ and their lightweight scaling factors $s_i$ are trained. These trainable scaling factors let the optimization focus on the most useful heads, recovering a higher-rank approximation of the full update without increasing the number of communicated parameters since clients upload $s_i\textbf&#123;H&#125;_i$ directly. Experiments on vision and language benchmarks show that \textsc&#123;Ravan&#125; improves test accuracy by 2-8\% over prior parameter-efficient baselines, making it a robust and scalable solution for federated fine-tuning of LLMs.

### 摘要
大型语言模型（LLMs）尚未有效利用海量边缘设备数据，而联邦学习（FL）为在不将私有边缘数据传输至云端的情况下协同微调LLMs提供了可行范式。为适应边缘设备的计算与通信限制，近期关于LLMs联邦微调的研究提出采用低秩自适应（LoRA）等参数高效方法。然而基于LoRA的方法在FL环境中存在精度下降问题，主要源于客户端间的数据与计算异质性。我们提出\textsc&#123;Ravan&#125;——一种自适应多头LoRA方法，通过将权重更新重参数化为多个LoRA头$s_i\textbf&#123;B&#125;_i\textbf&#123;H&#125;_i\textbf&#123;A&#125;_i$的叠加（仅训练核心矩阵$\textbf&#123;H&#125;_i$及其轻量级缩放因子$s_i$），在参数效率与模型表达能力间取得平衡。这些可训练缩放因子使优化聚焦于最有用的头部，由于客户端直接上传$s_i\textbf&#123;H&#125;_i$且不增加通信参数量，从而恢复了完整更新的更高秩近似。在视觉与语言基准测试中，\textsc&#123;Ravan&#125;相较现有参数高效基线实现了2-8\%的测试准确率提升，成为LLMs联邦微调的鲁棒可扩展解决方案。

---

## [ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation](https://arxiv.org/abs/2506.05566)

### Abstract
arXiv:2506.05566v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled near-human performance on software coding benchmarks, but their effectiveness in RTL code generation remains limited due to the scarcity of high-quality training data. While prior efforts have fine-tuned LLMs for RTL tasks, they do not fundamentally overcome the data bottleneck and lack support for test-time scaling due to their non-reasoning nature. In this work, we introduce ScaleRTL, the first reasoning LLM for RTL coding that scales up both high-quality reasoning data and test-time compute. Specifically, we curate a diverse set of long chain-of-thought reasoning traces averaging 56K tokens each, resulting in a dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a general-purpose reasoning model on this corpus yields ScaleRTL that is capable of deep RTL reasoning. Subsequently, we further enhance the performance of ScaleRTL through a novel test-time scaling strategy that extends the reasoning process via iteratively reflecting on and self-correcting previous reasoning steps. Experimental results show that ScaleRTL achieves state-of-the-art performance on VerilogEval and RTLLM, outperforming 18 competitive baselines by up to 18.4% on VerilogEval and 12.7% on RTLLM.

### 摘要
尽管大语言模型（LLMs）在软件编码基准测试中已实现接近人类的表现，但由于高质量训练数据的稀缺，其在寄存器传输级（RTL）代码生成中的有效性仍受限。虽然先前研究通过微调LLMs来适应RTL任务，但这些方法未能从根本上突破数据瓶颈，且因其非推理特性而缺乏测试时扩展支持。本研究提出ScaleRTL——首个面向RTL编码的推理型LLM，可同步扩展高质量推理数据与测试时计算资源。具体而言，我们构建了平均每条含5.6万标记的多样化长链思维推理轨迹数据集，最终形成包含35亿标记的RTL知识库。基于该语料库对通用推理模型进行微调，最终获得具备深度RTL推理能力的ScaleRTL。进一步地，我们通过创新的测试时扩展策略提升模型性能，该策略通过迭代式反思与自我修正先前推理步骤来延伸推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM基准上均达到最先进性能，较18个竞争基线模型分别最高提升18.4%（VerilogEval）和12.7%（RTLLM）。

---

## [Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data](https://arxiv.org/abs/2506.05542)

### Abstract
arXiv:2506.05542v1 Announce Type: cross 
Abstract: The adoption of machine learning (ML) and deep learning methods has revolutionized molecular medicine by driving breakthroughs in genomics, transcriptomics, drug discovery, and biological systems modeling. The increasing quantity, multimodality, and heterogeneity of biological datasets demand automated methods that can produce generalizable predictive models. Recent developments in large language model-based agents have shown promise for automating end-to-end ML experimentation on structured benchmarks. However, when applied to heterogeneous computational biology datasets, these methods struggle with generalization and success rates. Here, we introduce Agentomics-ML, a fully autonomous agent-based system designed to produce a classification model and the necessary files for reproducible training and inference. Our method follows predefined steps of an ML experimentation process, repeatedly interacting with the file system through Bash to complete individual steps. Once an ML model is produced, training and validation metrics provide scalar feedback to a reflection step to identify issues such as overfitting. This step then creates verbal feedback for future iterations, suggesting adjustments to steps such as data representation, model architecture, and hyperparameter choices. We have evaluated Agentomics-ML on several established genomic and transcriptomic benchmark datasets and show that it outperforms existing state-of-the-art agent-based methods in both generalization and success rates. While state-of-the-art models built by domain experts still lead in absolute performance on the majority of the computational biology datasets used in this work, Agentomics-ML narrows the gap for fully autonomous systems and achieves state-of-the-art performance on one of the used benchmark datasets. The code is available at https://github.com/BioGeMT/Agentomics-ML.

### 摘要
机器学习（ML）与深度学习方法的采用，通过推动基因组学、转录组学、药物发现和生物系统建模等领域的突破性进展，彻底改变了分子医学领域。生物数据集的日益增长的数量、多模态性和异质性，亟需能够构建通用化预测模型的自动化方法。基于大语言模型的智能体最新进展，已展现出在结构化基准测试上实现端到端ML实验自动化的潜力。然而，当应用于异构计算生物学数据集时，这些方法在泛化能力和成功率方面表现欠佳。本文提出Agentomics-ML——一个完全自主的基于智能体的系统，旨在生成分类模型及可复现训练与推理所需的必要文件。该方法遵循ML实验流程的预定义步骤，通过Bash命令与文件系统反复交互以完成各独立步骤。当ML模型生成后，训练与验证指标将为反思步骤提供标量反馈，用以识别过拟合等问题。该反思步骤随后生成文本反馈以指导后续迭代，建议调整数据表征、模型架构和超参数选择等环节。我们在多个成熟的基因组和转录组基准数据集上评估了Agentomics-ML，结果表明其在泛化能力和成功率方面均优于现有最先进的基于智能体的方法。尽管领域专家构建的最先进模型在本研究使用的大多数计算生物学数据集上仍保持绝对性能优势，但Agentomics-ML显著缩小了全自动系统与人工模型的差距，并在其中一个基准数据集上达到了最先进性能。代码详见https://github.com/BioGeMT/Agentomics-ML。

---

## [SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs](https://arxiv.org/abs/2506.05598)

### Abstract
arXiv:2506.05598v1 Announce Type: cross 
Abstract: Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.

### 摘要
近期关于大语言模型（LLMs）多元化对齐的倡导鼓励模型适应用户的多样化偏好。然而，现有的大多数个性化奖励模型研究严重依赖额外身份信息，如人口统计细节或预定义的偏好类别。为此，我们提出SynthesizeMe方法，通过用户交互生成合成用户角色以构建个性化奖励模型。SynthesizeMe首先生成并验证解释用户偏好的推理过程，随后基于该推理归纳合成用户角色，最后筛选出信息丰富的历史用户交互数据，从而为特定用户构建个性化提示。实验表明，使用SynthesizeMe生成的提示将Chatbot Arena平台上LLM作为评判者的个性化准确率提升4.4%。在PersonalRewardBench（一个基于854名Chatbot Arena和PRISM用户与聊天机器人交互的新建分层数据集）上，结合SynthesizeMe生成的提示与奖励模型取得了最优性能。

---

## [SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code](https://arxiv.org/abs/2506.05692)

### Abstract
arXiv:2506.05692v1 Announce Type: cross 
Abstract: The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce \benchmark, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on \benchmark, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon.

---

## [RKEFino1: A Regulation Knowledge-Enhanced Large Language Model](https://arxiv.org/abs/2506.05700)

### Abstract
arXiv:2506.05700v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.

### 摘要
大型语言模型（LLM）的最新进展为金融应用带来了巨大潜力，但也在数字监管报告（DRR）中引发了准确性与合规性的关键挑战。为解决这些问题，我们提出RKEFino1模型——一种基于Fino1框架、通过XBRL、CDM和MOF领域知识微调的监管知识增强型金融推理模型。我们构建了两类问答任务（基于知识的推理与数学推理），并创新性地设计了覆盖文本与表格中金融实体的数值命名实体识别任务。实验结果表明RKEFino1在合规敏感型金融任务中具有显著的有效性与泛化能力。该模型已在Hugging Face平台开源发布。

---

## [Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework](https://arxiv.org/abs/2506.05623)

### Abstract
arXiv:2506.05623v1 Announce Type: cross 
Abstract: Infrastructure-as-Code (IaC) generation holds significant promise for automating cloud infrastructure provisioning. Recent advances in Large Language Models (LLMs) present a promising opportunity to democratize IaC development by generating deployable infrastructure templates from natural language descriptions, but current evaluation focuses on syntactic correctness while ignoring deployability, the fatal measure of IaC template utility. We address this gap through two contributions: (1) IaCGen, an LLM-based deployability-centric framework that uses iterative feedback mechanism to generate IaC templates, and (2) DPIaC-Eval, a deployability-centric IaC template benchmark consists of 153 real-world scenarios that can evaluate syntax, deployment, user intent, and security. Our evaluation reveals that state-of-the-art LLMs initially performed poorly, with Claude-3.5 and Claude-3.7 achieving only 30.2% and 26.8% deployment success on the first attempt respectively. However, IaCGen transforms this performance dramatically: all evaluated models reach over 90% passItr@25, with Claude-3.5 and Claude-3.7 achieving 98% success rate. Despite these improvements, critical challenges remain in user intent alignment (25.2% accuracy) and security compliance (8.4% pass rate), highlighting areas requiring continued research. Our work provides the first comprehensive assessment of deployability-centric IaC template generation and establishes a foundation for future research.

### 摘要
基础设施即代码（IaC）生成技术在自动化云基础设施配置领域具有重要应用前景。近期大语言模型（LLM）的进展为IaC开发的民主化带来了新机遇——能够根据自然语言描述生成可部署的基础设施模板，但现有评估仅关注语法正确性而忽略了部署可行性这一衡量IaC模板实用性的关键指标。我们通过两项贡献填补这一空白：（1）IaCGen——基于LLM的以部署可行性为核心的框架，采用迭代反馈机制生成IaC模板；（2）DPIaC-Eval——包含153个真实场景的部署导向型IaC模板基准，可评估语法、部署、用户意图和安全性。评估表明，最先进的LLM初始表现欠佳：Claude-3.5和Claude-3.7首次尝试的部署成功率分别仅为30.2%和26.8%。但IaCGen显著改善了这一状况：所有评估模型在25次迭代内通过率均超90%，其中Claude-3.5和Claude-3.7达到98%成功率。尽管取得这些进展，在用户意图对齐（25.2%准确率）和安全合规性（8.4%通过率）方面仍存在重大挑战，这些领域需要持续研究。本研究首次提供了以部署可行性为核心的IaC模板生成全面评估，为未来研究奠定了基础。

---

## [FedShield-LLM: A Secure and Scalable Federated Fine-Tuned Large Language Model](https://arxiv.org/abs/2506.05640)

### Abstract
arXiv:2506.05640v1 Announce Type: cross 
Abstract: Federated Learning (FL) offers a decentralized framework for training and fine-tuning Large Language Models (LLMs) by leveraging computational resources across organizations while keeping sensitive data on local devices. It addresses privacy and security concerns while navigating challenges associated with the substantial computational demands of LLMs, which can be prohibitive for small and medium-sized organizations. FL supports the development of task-specific LLMs for cross-silo applications through fine-tuning but remains vulnerable to inference attacks, such as membership inference and gradient inversion, which threaten data privacy. Prior studies have utilized Differential Privacy (DP) in LLM fine-tuning, which, despite being effective at preserving privacy, can degrade model performance. To overcome these challenges, we propose a novel method, FedShield-LLM, that uses pruning with Fully Homomorphic Encryption (FHE) for Low-Rank Adaptation (LoRA) parameters, enabling secure computations on encrypted model updates while mitigating the attack surface by deactivating less important LoRA parameters. Furthermore, optimized federated algorithms for cross-silo environments enhance scalability and efficiency. Parameter-efficient fine-tuning techniques like LoRA substantially reduce computational and communication overhead, making FL feasible for resource-constrained clients. Experimental results show that the proposed method outperforms existing methods while maintaining robust privacy protection, enabling organizations to collaboratively train secure and efficient LLMs.
  The code and data are available at, https://github.com/solidlabnetwork/fedshield-llm

### 摘要
联邦学习（FL）通过利用跨组织的计算资源，同时将敏感数据保留在本地设备上，为大规模语言模型（LLM）的训练与微调提供了一种去中心化框架。该方法在应对LLM巨大计算需求（对中小型组织构成障碍）相关挑战的同时，解决了隐私与安全问题。FL支持通过微调开发面向跨机构应用的专用LLM，但仍易受成员推理和梯度反转等推断攻击威胁，这些攻击会危及数据隐私。先前研究已在LLM微调中应用差分隐私（DP），虽能有效保护隐私，但会降低模型性能。为克服这些挑战，我们提出FedShield-LLM新方法，该方法结合全同态加密（FHE）对低秩自适应（LoRA）参数进行剪枝，既能对加密模型更新执行安全计算，又可通过停用次要LoRA参数来缩小攻击面。此外，针对跨机构环境优化的联邦算法提升了可扩展性与效率。诸如LoRA等参数高效微调技术显著降低了计算与通信开销，使资源受限客户端实施FL成为可能。实验结果表明，所提方法在保持强隐私保护的同时性能优于现有方案，助力各组织协同训练安全高效的LLM。

---

## [To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt](https://arxiv.org/abs/2506.05739)

### Abstract
arXiv:2506.05739v1 Announce Type: cross 
Abstract: LLM agents are widely used as agents for customer support, content generation, and code assistance. However, they are vulnerable to prompt injection attacks, where adversarial inputs manipulate the model's behavior. Traditional defenses like input sanitization, guard models, and guardrails are either cumbersome or ineffective. In this paper, we propose a novel, lightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which protects against prompt injection with near-zero overhead. The approach is based on the insight that prompt injection requires guessing and breaking the structure of the system prompt. By dynamically varying the structure of system prompts, PPA prevents attackers from predicting the prompt structure, thereby enhancing security without compromising performance. We conducted experiments to evaluate the effectiveness of PPA against existing attacks and compared it with other defense methods.

### 摘要
大语言模型（LLM）代理广泛应用于客户支持、内容生成和代码辅助等场景。然而，这类代理易受提示注入攻击的影响，即对抗性输入会操纵模型行为。传统防御措施如输入净化、防护模型和防护栏等，要么繁琐低效，要么收效甚微。本文提出一种名为"多态提示组装"（PPA）的新型轻量级防御机制，能以近乎零开销防御提示注入攻击。该方法基于以下洞见：提示注入攻击需猜测并破坏系统提示的结构。通过动态变换系统提示结构，PPA使攻击者无法预测提示结构，从而在不影响性能的前提下提升安全性。我们通过实验评估了PPA对现有攻击的防御效果，并与其他防御方法进行了对比。

---

## [dots.llm1 Technical Report](https://arxiv.org/abs/2506.05767)

### Abstract
arXiv:2506.05767v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints at every one trillion tokens, providing valuable insights into the learning dynamics of large language models.

### 摘要
混合专家（MoE）模型通过仅为每个输入标记激活部分参数，已成为高效扩展语言模型的重要范式。本报告介绍dots.llm1——一个大规模MoE模型，该模型从总计1420亿参数中激活140亿参数，在保持与最先进模型相当性能的同时显著降低训练和推理成本。通过精心设计的高效数据处理流程，dots.llm1在预训练11.2万亿高质量标记并进行后训练充分释放潜力后，性能可比肩Qwen2.5-72B。值得注意的是，预训练阶段未使用任何合成数据。为促进后续研究，我们开源了每万亿标记间隔的训练检查点，为大型语言模型的学习动态提供了宝贵洞见。

---

## [Large Language Models are Good Relational Learners](https://arxiv.org/abs/2506.05725)

### Abstract
arXiv:2506.05725v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various domains, yet their application to relational deep learning (RDL) remains underexplored. Existing approaches adapt LLMs by traversing relational links between entities in a database and converting the structured data into flat text documents. Still, this text-based serialization disregards critical relational structures, introduces redundancy, and often exceeds standard LLM context lengths. We introduce Rel-LLM, a novel architecture that utilizes a graph neural network (GNN)- based encoder to generate structured relational prompts for LLMs within a retrieval-augmented generation (RAG) framework. Unlike traditional text-based serialization approaches, our method preserves the inherent relational structure of databases while enabling LLMs to effectively process and reason over complex entity relationships. Specifically, the GNN encoder extracts a local subgraph around an entity to build feature representations that contain relevant entity relationships and temporal dependencies. These representations are transformed into structured prompts using a denormalization process, effectively allowing the LLM to reason over relational structures. Through extensive experiments, we demonstrate that Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and efficient approach to integrating LLMs with structured data sources. Code is available at https://github.com/smiles724/Rel-LLM.

### 摘要
大语言模型（LLMs）已在多个领域展现出卓越能力，但其在关系深度学习（RDL）中的应用仍待深入探索。现有方法通过遍历数据库中实体间的关系链接，将结构化数据转换为扁平文本文档来适配LLMs。然而，这种基于文本的序列化方式忽略了关键的关系结构，引入了冗余信息，且常超出标准LLM的上下文长度限制。我们提出Rel-LLM——一种新颖的架构，该架构利用基于图神经网络（GNN）的编码器，在检索增强生成（RAG）框架内为LLMs生成结构化关系提示。与传统基于文本的序列化方法不同，我们的方法在保持数据库固有关系结构的同时，使LLMs能够有效处理并推理复杂的实体关系。具体而言，GNN编码器提取实体周围的局部子图，构建包含相关实体关系和时序依赖的特征表示。这些表示通过反规范化过程转化为结构化提示，使LLM能够对关系结构进行有效推理。大量实验表明，Rel-LLM在关键RDL任务上优于现有方法，为LLMs与结构化数据源的集成提供了可扩展且高效的解决方案。代码详见https://github.com/smiles724/Rel-LLM。

---

## [Heartcare Suite: Multi-dimensional Understanding of ECG with Raw Multi-lead Signal Modeling](https://arxiv.org/abs/2506.05831)

### Abstract
arXiv:2506.05831v1 Announce Type: cross 
Abstract: We present Heartcare Suite, a multimodal comprehensive framework for finegrained electrocardiogram (ECG) understanding. It comprises three key components: (i) Heartcare-220K, a high-quality, structured, and comprehensive multimodal ECG dataset covering essential tasks such as disease diagnosis, waveform morphology analysis, and rhythm interpretation. (ii) Heartcare-Bench, a systematic and multi-dimensional benchmark designed to evaluate diagnostic intelligence and guide the optimization of Medical Multimodal Large Language Models (Med-MLLMs) in ECG scenarios. and (iii) HeartcareGPT with a tailored tokenizer Bidirectional ECG Abstract Tokenization (Beat), which compresses raw multi-lead signals into semantically rich discrete tokens via duallevel vector quantization and query-guided bidirectional diffusion mechanism. Built upon Heartcare-220K, HeartcareGPT achieves strong generalization and SoTA performance across multiple clinically meaningful tasks. Extensive experiments demonstrate that Heartcare Suite is highly effective in advancing ECGspecific multimodal understanding and evaluation. Our project is available at https://github.com/Wznnnnn/Heartcare-Suite .

---

## [FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging](https://arxiv.org/abs/2506.05828)

### Abstract
arXiv:2506.05828v1 Announce Type: cross 
Abstract: We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs' financial reasoning capabilities through refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs' performance (e.g., 83.2% $\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks.

---

## [Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance](https://arxiv.org/abs/2506.05748)

### Abstract
arXiv:2506.05748v1 Announce Type: cross 
Abstract: Reward-model training is the cost bottleneck in modern Reinforcement Learning Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters and an offline preference-tuning phase. In the proposed method, a frozen, instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling it to serve as a complete substitute for the previously used heavyweight evaluation models. The plug-and-play judge achieves 96.2% accuracy on RewardBench, outperforming specialized reward networks ranging from 27B to 70B parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K utilizing online PPO. Thorough ablations indicate that (i) six in context demonstrations deliver the majority of the zero-to-few-shot improvements (+2pp), and (ii) the LoRA effectively addresses the remaining disparity, particularly in the safety and adversarial Chat-Hard segments. The proposed model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic HH-RLHF, to examine interpretability, accompanied by human generated justifications. GPT-4 scoring indicates that our LoRA judge attains approximately = 9/10 in similarity to human explanations, while zero-shot judges score around =5/10. These results indicate that the combination of prompt engineering and tiny LoRA produces a cost effective, transparent, and easily adjustable reward function, removing the offline phase while achieving new state-of-the-art outcomes for both static evaluation and online RLHF.

### 摘要
奖励模型训练是现代强化学习人类反馈（RLHF）流程中的成本瓶颈，通常需要数百亿参数和离线偏好调优阶段。本方法仅通过单行JSON评估标准与秩16的LoRA适配器（仅影响模型0.8%参数）增强冻结的指令调优7B大语言模型，使其能完全替代先前使用的重型评估模型。该即插即用评判器在RewardBench上达到96.2%准确率，优于27B至70B参数的专用奖励网络。此外，该方法使7B行动器通过在线PPO在GSM-8K上实现92%精确匹配准确率，超越得分61.8%的顶级70B DPO基线。全面消融实验表明：（1）六个上下文示例贡献了零样本到少样本改进的主要部分（+2个百分点）；（2）LoRA有效弥补了剩余差距，尤其在安全性和对抗性Chat-Hard环节。研究提出HH-Rationales数据集——来自Anthropic HH-RLHF的10,000对子集，并辅以人工生成的理由说明用于可解释性分析。GPT-4评估显示我们的LoRA评判器与人类解释相似度达≈9/10，而零样本评判器仅≈5/10。这些结果表明提示工程与微型LoRA的结合能产生高性价比、透明且易调整的奖励函数，在消除离线阶段的同时，为静态评估和在线RLHF实现了新的最先进成果。

---

## [Research on Personalized Financial Product Recommendation by Integrating Large Language Models and Graph Neural Networks](https://arxiv.org/abs/2506.05873)

### Abstract
arXiv:2506.05873v1 Announce Type: cross 
Abstract: With the rapid growth of fintech, personalized financial product recommendations have become increasingly important. Traditional methods like collaborative filtering or content-based models often fail to capture users' latent preferences and complex relationships. We propose a hybrid framework integrating large language models (LLMs) and graph neural networks (GNNs). A pre-trained LLM encodes text data (e.g., user reviews) into rich feature vectors, while a heterogeneous user-product graph models interactions and social ties. Through a tailored message-passing mechanism, text and graph information are fused within the GNN to jointly optimize embeddings. Experiments on public and real-world financial datasets show our model outperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong interpretability. This work offers new insights for personalized financial recommendations and cross-modal fusion in broader recommendation tasks.

### 摘要
随着金融科技的快速发展，个性化金融产品推荐变得日益重要。传统方法如协同过滤或基于内容的模型往往难以捕捉用户的潜在偏好和复杂关系。我们提出了一种融合大型语言模型（LLMs）与图神经网络（GNNs）的混合框架：预训练LLM将文本数据（如用户评论）编码为丰富特征向量，而异构用户-产品图则建模交互关系与社会联系。通过定制的消息传递机制，文本信息与图结构数据在GNN中进行融合以联合优化嵌入表示。在公开和真实金融数据集上的实验表明，我们的模型在准确率、召回率和NDCG指标上均优于独立LLM或GNN方法，并具备强可解释性。该研究为个性化金融推荐及更广泛推荐任务中的跨模态融合提供了新思路。

---

## [BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures](https://arxiv.org/abs/2506.05871)

### Abstract
arXiv:2506.05871v1 Announce Type: cross 
Abstract: Serving large language models (LLMs) to millions of users requires efficient resource allocation and parallelism strategies. It is a labor intensive trial-and-error process to find such a strategy. We present BestServe, a novel framework for ranking serving strategies by estimating goodput under various operating scenarios. Supporting both collocated and disaggregated architectures, BestServe leverages an inference simulator built on an adapted roofline model and CPU-GPU dispatch dynamics. Our framework determines the optimal strategy in minutes on a single standard CPU, eliminating the need for costly benchmarking, while achieving predictions within a $20\%$ error margin. It appeals to be practical for rapid deployment planning because of its lightweight design and strong extensibility.

### 摘要
向数百万用户提供大型语言模型（LLM）服务需要高效的资源分配与并行策略。当前寻找此类策略的过程需要密集的人工试错。我们提出BestServe框架，该创新系统通过评估不同运行场景下的有效吞吐量来对服务策略进行排序。BestServe同时支持共置与解耦架构，其核心是基于改进屋顶线模型和CPU-GPU调度动态构建的推理模拟器。我们的框架在单颗标准CPU上仅需数分钟即可确定最优策略，无需昂贵基准测试，且预测误差率控制在20%以内。凭借轻量化设计和强大扩展性，该框架为快速部署规划提供了实用解决方案。

---

## [Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models](https://arxiv.org/abs/2506.05850)

### Abstract
arXiv:2506.05850v1 Announce Type: cross 
Abstract: We identify \textbf&#123;Cross-lingual Collapse&#125;, a systematic drift in which the chain-of-thought (CoT) of a multilingual language model reverts to its dominant pre-training language even when the prompt is expressed in a different language. Recent large language models (LLMs) with reinforcement learning with verifiable reward (RLVR) have achieved strong logical reasoning performances by exposing their intermediate reasoning traces, giving rise to large reasoning models (LRMs). However, the mechanism behind multilingual reasoning in LRMs is not yet fully explored. To investigate the issue, we fine-tune multilingual LRMs with Group-Relative Policy Optimization (GRPO) on translated versions of the GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese, Korean, and Ukrainian. During training, we monitor both task accuracy and language consistency of the reasoning chains. Our experiments reveal three key findings: (i) GRPO rapidly amplifies pre-training language imbalances, leading to the erosion of low-resource languages within just a few hundred updates; (ii) language consistency reward mitigates this drift but does so at the expense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting language collapse is severely damaging and largely irreversible, as subsequent fine-tuning struggles to steer the model back toward its original target-language reasoning capabilities. Together, these findings point to a remarkable conclusion: \textit&#123;not all languages are trained equally for reasoning&#125;. Furthermore, our paper sheds light on the roles of reward shaping, data difficulty, and pre-training priors in eliciting multilingual reasoning.

### 摘要
我们发现了**跨语言坍缩**现象，即多语言模型的思维链（CoT）会系统性地漂移回其预训练主导语言，即使提示词使用其他语言表达。近期通过可验证奖励强化学习（RLVR）训练的大语言模型（LLM）通过展示中间推理轨迹，催生出大型推理模型（LRM）并实现了强大的逻辑推理能力。然而，LRM的多语言推理机制尚未得到充分探索。为研究该问题，我们在GSM$8$K和SimpleRL-Zoo数据集的汉语、韩语和乌克兰语翻译版本上，采用组相对策略优化（GRPO）对多语言LRM进行微调。训练过程中同时监测任务准确率和推理链的语言一致性。实验揭示三个关键发现：（i）GRPO会迅速放大预训练语言不平衡性，仅需数百次更新即可导致低资源语言能力退化；（ii）语言一致性奖励能缓解这种漂移，但会使准确率下降近5-10个百分点；（iii）由此产生的语言坍缩具有严重破坏性且基本不可逆，后续微调难以使模型恢复原有目标语言推理能力。这些发现共同指向一个显著结论：**并非所有语言在推理训练中都被平等对待**。此外，本研究揭示了奖励塑造、数据难度和预训练先验在激发多语言推理中的作用。

---

## [HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios](https://arxiv.org/abs/2506.05883)

### Abstract
arXiv:2506.05883v1 Announce Type: cross 
Abstract: We present HaoMo Vision-Language Model (HMVLM), an end-to-end driving framework that implements the slow branch of a cognitively inspired fast-slow architecture. A fast controller outputs low-level steering, throttle, and brake commands, while a slow planner-a large vision-language model-generates high-level intents such as "yield to pedestrian" or "merge after the truck" without compromising latency. HMVLM introduces three upgrades: (1) selective five-view prompting with an embedded 4s history of ego kinematics, (2) multi-stage chain-of-thought (CoT) prompting that enforces a Scene Understanding -&gt; Driving Decision -&gt; Trajectory Inference reasoning flow, and (3) spline-based trajectory post-processing that removes late-stage jitter and sharp turns. Trained on the Waymo Open Dataset, these upgrades enable HMVLM to achieve a Rater Feedback Score (RFS) of 7.7367, securing 2nd place in the 2025 Waymo Vision-based End-to-End (E2E) Driving Challenge and surpassing the public baseline by 2.77%.

### 摘要
我们提出豪摩视觉语言模型（HMVLM），这是一种实现认知启发式快慢架构中慢速分支的端到端驾驶框架。快速控制器输出低级别的转向、油门和刹车指令，而慢速规划器——一个大型视觉语言模型——在不增加延迟的情况下生成高级意图，如"礼让行人"或"在卡车后并道"。HMVLM引入三项升级：(1) 选择性五视角提示机制，嵌入了4秒的自车运动历史；(2) 多阶段思维链（CoT）提示策略，强制遵循"场景理解→驾驶决策→轨迹推断"的推理流程；(3) 基于样条的轨迹后处理技术，可消除后期抖动和急转弯。基于Waymo开放数据集训练后，这些升级使HMVLM获得7.7367的评分员反馈分数（RFS），在2025年Waymo基于视觉的端到端（E2E）驾驶挑战赛中位列第二，较公开基线提升2.77%。

---

## [DynamicMind: A Tri-Mode Thinking System for Large Language Models](https://arxiv.org/abs/2506.05936)

### Abstract
arXiv:2506.05936v1 Announce Type: cross 
Abstract: Modern large language models (LLMs) often struggle to dynamically adapt their reasoning depth to varying task complexities, leading to suboptimal performance or inefficient resource utilization. To address this, we introduce DynamicMind, a novel tri-mode thinking system. DynamicMind empowers LLMs to autonomously select between Fast, Normal, and Slow thinking modes for zero-shot question answering (ZSQA) tasks through cognitive-inspired prompt engineering. Our framework's core innovations include: (1) expanding the established dual-process framework of fast and slow thinking into a tri-mode thinking system involving a normal thinking mode to preserve the intrinsic capabilities of LLM; (2) proposing the Thinking Density metric, which aligns computational resource allocation with problem complexity; and (3) developing the Thinking Mode Capacity (TMC) dataset and a lightweight Mind Router to predict the optimal thinking mode. Extensive experiments across diverse mathematical, commonsense, and scientific QA benchmarks demonstrate that DynamicMind achieves superior ZSQA capabilities while establishing an effective trade-off between performance and computational efficiency.

### 摘要
现代大型语言模型（LLM）往往难以动态调整其推理深度以适应不同任务复杂度，导致性能欠佳或计算资源利用率低下。为此，我们提出DynamicMind——一种创新的三模态思维系统。该框架通过认知启发的提示工程，使LLM能在零样本问答任务（ZSQA）中自主选择快速、常规与慢速三种思维模式。本研究的核心创新包括：（1）将传统的双进程思维框架扩展为包含常规思维模式的三模态系统，以保留LLM的固有能力；（2）提出"思维密度"指标，使计算资源分配与问题复杂度相匹配；（3）构建思维模式容量（TMC）数据集并开发轻量级思维路由器来预测最优思维模式。在数学、常识推理和科学问答等多个基准测试上的实验表明，DynamicMind在保持卓越ZSQA能力的同时，成功实现了性能与计算效率的有效平衡。

---

## [Small Models, Big Support: A Local LLM Framework for Teacher-Centric Content Creation and Assessment using RAG and CAG](https://arxiv.org/abs/2506.05925)

### Abstract
arXiv:2506.05925v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) are increasingly utilized as student-facing educational aids, their potential to directly support educators, particularly through locally deployable and customizable open-source solutions, remains significantly underexplored. Many existing educational solutions rely on cloud-based infrastructure or proprietary tools, which are costly and may raise privacy concerns. Regulated industries with limited budgets require affordable, self-hosted solutions. We introduce an end-to-end, open-source framework leveraging small (3B-7B parameters), locally deployed LLMs for customized teaching material generation and assessment. Our system uniquely incorporates an interactive loop crucial for effective small-model refinement, and an auxiliary LLM verifier to mitigate jailbreaking risks, enhancing output reliability and safety. Utilizing Retrieval and Context Augmented Generation (RAG/CAG), it produces factually accurate, customized pedagogically-styled content. Deployed on-premises for data privacy and validated through an evaluation pipeline and a college physics pilot, our findings show that carefully engineered small LLM systems can offer robust, affordable, practical, and safe educator support, achieving utility comparable to larger models for targeted tasks.

### 摘要
虽然大型语言模型（LLMs）作为面向学生的教育辅助工具正日益普及，但其直接支持教育工作者的潜力——特别是通过可本地部署、可定制的开源解决方案——仍远未得到充分探索。现有教育解决方案多依赖云基础设施或专有工具，成本高昂且可能引发隐私问题。预算有限的受监管行业需要经济实惠的自主托管方案。我们提出一个端到端的开源框架，利用小型（30亿-70亿参数）本地部署的LLM实现定制化教学材料生成与评估。该系统创新性地整合了小型模型优化所必需的人机交互闭环，以及通过辅助LLM验证器降低越狱风险，从而提升输出可靠性与安全性。借助检索增强生成（RAG）和上下文增强生成（CAG）技术，该系统能生成事实准确、符合教学风格的定制化内容。基于本地部署保障数据隐私，并通过评估流程和大学物理课程试点验证，研究表明：经过精心设计的小型LLM系统能够为教育工作者提供稳定、经济、实用且安全的支持，在特定任务上达到与大型模型相当的效用水平。

---

## [MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.05928)

### Abstract
arXiv:2506.05928v1 Announce Type: cross 
Abstract: Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ \emph&#123;homogeneous&#125; MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a \emph&#123;heterogeneous&#125; \textbf&#123;Mixture-of-Adapters (MoA)&#125; approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: \textbf&#123;(i)&#125; \textit&#123;Soft MoA&#125; achieves fine-grained integration by performing a weighted fusion of all expert outputs; \textbf&#123;(ii)&#125; \textit&#123;Sparse MoA&#125; activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.

### 摘要
近期研究通过整合低秩自适应（LoRA）与专家混合（MoE）技术，进一步提升大语言模型（LLM）应用中参数高效微调（PEFT）方法的性能。现有方法采用由结构及容量相似或相同的LoRA专家构成的同质化MoE-LoRA架构，但这些方案常面临表征崩溃与专家负载失衡问题，制约了LLM的潜力发挥。为解决这些挑战，我们提出一种异质化的适配器混合（MoA）方法。该方法通过动态整合具有多样化结构的PEFT适配器专家，利用其互补的表征能力促进专家专业化，从而增强预训练知识向下游任务的有效迁移。MoA支持两种变体：(i) 软性MoA通过对所有专家输出进行加权融合实现细粒度整合；(ii) 稀疏MoA根据贡献度稀疏激活适配器专家，在性能损失可忽略的前提下达成目标。实验结果表明，异质化MoA在性能与参数效率上均优于同质化MoE-LoRA方法。项目代码详见https://github.com/DCDmllm/MoA。

---

## [IntentionESC: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems](https://arxiv.org/abs/2506.05947)

### Abstract
arXiv:2506.05947v1 Announce Type: cross 
Abstract: In emotional support conversations, unclear intentions can lead supporters to employ inappropriate strategies, inadvertently imposing their expectations or solutions on the seeker. Clearly defined intentions are essential for guiding both the supporter's motivations and the overall emotional support process. In this paper, we propose the Intention-centered Emotional Support Conversation (IntentionESC) framework, which defines the possible intentions of supporters in emotional support conversations, identifies key emotional state aspects for inferring these intentions, and maps them to appropriate support strategies. While Large Language Models (LLMs) excel in text generating, they fundamentally operate as probabilistic models trained on extensive datasets, lacking a true understanding of human thought processes and intentions. To address this limitation, we introduce the Intention Centric Chain-of-Thought (ICECoT) mechanism. ICECoT enables LLMs to mimic human reasoning by analyzing emotional states, inferring intentions, and selecting suitable support strategies, thereby generating more effective emotional support responses. To train the model with ICECoT and integrate expert knowledge, we design an automated annotation pipeline that produces high-quality training data. Furthermore, we develop a comprehensive evaluation scheme to assess emotional support efficacy and conduct extensive experiments to validate our framework. Our data and code are available at https://github.com/43zxj/IntentionESC_ICECoT.

### 摘要
在情感支持对话中，意图不明确可能导致支持者采用不当策略，无意间将自身期望或解决方案强加于求助者。明确定义的意图对于引导支持者动机和整体情感支持过程至关重要。本文提出以意图为核心的情感支持对话框架（IntentionESC），该框架界定了情感支持对话中支持者的潜在意图，识别推断这些意图所需的关键情感状态维度，并将其映射至相应的支持策略。尽管大语言模型（LLMs）在文本生成方面表现优异，但其本质是基于海量数据训练的概率模型，缺乏对人类思维过程与意图的真正理解。为克服这一局限，我们提出意图中心思维链（ICECoT）机制。ICECoT使大语言模型能够通过分析情感状态、推断意图并选择适宜支持策略来模拟人类推理，从而生成更有效的情感支持回应。为训练具备ICECoT能力的模型并整合专家知识，我们设计了自动标注流程以生成高质量训练数据。此外，开发了综合评估方案来衡量情感支持效能，并通过大量实验验证框架有效性。相关数据与代码已开源：https://github.com/43zxj/IntentionESC_ICECoT。

---

## [Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.05970)

### Abstract
arXiv:2506.05970v1 Announce Type: cross 
Abstract: Recent studies have shown that Theory of Mind (ToM) in large language models (LLMs) has not reached human-level performance yet. Since fine-tuning LLMs on ToM datasets often degrades their generalization, several inference-time methods have been proposed to enhance ToM in LLMs. However, existing inference-time methods for ToM are specialized for inferring beliefs from contexts involving changes in the world state. In this study, we present a new inference-time method for ToM, Shoes-of-Others (SoO) prefixing, which makes fewer assumptions about contexts and is applicable to broader scenarios. SoO prefixing simply specifies the beginning of LLM outputs with ``Let's put ourselves in A's shoes.'', where A denotes the target character's name. We evaluate SoO prefixing on two benchmarks that assess ToM in conversational and narrative contexts without changes in the world state and find that it consistently improves ToM across five categories of mental states. Our analysis suggests that SoO prefixing elicits faithful thoughts, thereby improving the ToM performance.

### 摘要
近期研究表明，大型语言模型（LLMs）的心理理论（ToM）能力尚未达到人类水平。由于在ToM数据集上微调LLMs往往会削弱其泛化能力，研究者已提出若干推理阶段的方法来增强LLMs的ToM能力。然而，现有ToM推理方法专用于从涉及世界状态变化的语境中推断信念。本研究提出一种新型ToM推理方法——'他人之履'（SoO）前缀法，该方法对语境假设更少且适用于更广泛场景。SoO前缀法仅需在LLM输出起始处添加'让我们站在A的角度思考'，其中A表示目标角色名称。我们在两个评估对话和叙事语境中（无世界状态变化）ToM能力的基准测试上验证了该方法，发现其能持续提升五类心理状态的ToM表现。分析表明，SoO前缀法能激发忠实思维，从而提升ToM性能。

---

## [Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router](https://arxiv.org/abs/2506.05901)

### Abstract
arXiv:2506.05901v1 Announce Type: cross 
Abstract: Multi-step reasoning has proven essential for enhancing the problem-solving capabilities of Large Language Models (LLMs) by decomposing complex tasks into intermediate steps, either explicitly or implicitly. Extending the reasoning chain at test time through deeper thought processes or broader exploration, can furthur improve performance, but often incurs substantial costs due to the explosion in token usage. Yet, many reasoning steps are relatively simple and can be handled by more efficient smaller-scale language models (SLMs). This motivates hybrid approaches that allocate subtasks across models of varying capacities. However, realizing such collaboration requires accurate task decomposition and difficulty-aware subtask allocation, which is challenging. To address this, we propose R2-Reasoner, a novel framework that enables collaborative reasoning across heterogeneous LLMs by dynamically routing sub-tasks based on estimated complexity. At the core of our framework is a Reinforced Model Router, composed of a task decomposer and a subtask allocator. The task decomposer segments complex input queries into logically ordered subtasks, while the subtask allocator assigns each subtask to the most appropriate model, ranging from lightweight SLMs to powerful LLMs, balancing accuracy and efficiency. To train this router, we introduce a staged pipeline that combines supervised fine-tuning on task-specific datasets with Group Relative Policy Optimization algorithm, enabling self-supervised refinement through iterative reinforcement learning. Extensive experiments across four challenging benchmarks demonstrate that R2-Reasoner reduces API costs by 86.85% while maintaining or surpassing baseline accuracy. Our framework paves the way for more cost-effective and adaptive LLM reasoning. The code is open-source at https://anonymous.4open.science/r/R2_Reasoner .

### 摘要
多步推理已被证明对增强大型语言模型（LLMs）的问题解决能力至关重要，其通过将复杂任务分解为显式或隐式的中间步骤来实现。在测试时通过更深入的思考过程或更广泛的探索来扩展推理链可以进一步提升性能，但由于令牌使用量的激增，通常会带来高昂成本。然而，许多推理步骤相对简单，可由更高效的小规模语言模型（SLMs）处理。这促使了采用混合方法，将子任务分配给不同能力的模型。但实现这种协作需要精确的任务分解和难度感知的子任务分配，具有挑战性。为此，我们提出R2-Reasoner，这是一个通过动态路由基于估计复杂度的子任务来实现异构LLMs协作推理的新框架。我们框架的核心是强化模型路由器，由任务分解器和子任务分配器组成。任务分解器将复杂输入查询分割为逻辑有序的子任务，而子任务分配器将每个子任务分配给从轻量级SLMs到强大LLMs的最合适模型，以平衡准确性和效率。为训练该路由器，我们引入了一个分阶段流程，结合了针对特定任务数据集的监督微调和组相对策略优化算法，通过迭代强化学习实现自监督优化。在四个具有挑战性的基准测试上的大量实验表明，R2-Reasoner在保持或超越基线准确性的同时，将API成本降低了86.85%。我们的框架为更具成本效益和自适应性的LLM推理开辟了道路。代码开源在https://anonymous.4open.science/r/R2_Reasoner。

---

## [Audio-Aware Large Language Models as Judges for Speaking Styles](https://arxiv.org/abs/2506.05984)

### Abstract
arXiv:2506.05984v1 Announce Type: cross 
Abstract: Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.

### 摘要
音频感知大语言模型（ALLMs）能够理解音频输入中的文本与非文本信息。本文探索将ALLMs作为自动评估工具用于演讲风格的评判。我们采用ALLM评判器对语音语言模型（SLMs）在两项任务中的生成内容进行评估：语音风格指令跟随与角色扮演。所考察的演讲风格要素包括情感、音量、语速、词语重音、音高控制及非语言元素。研究使用四个口语模型（SLMs）完成上述任务，并通过人类评估员与ALLMs对模型输出进行评判。通过对比GPT-4o-audio和Gemini-2.5-pro两种ALLM评判器与人类评估结果，我们发现Gemini与人类评判者的一致性程度可媲美人类评估员之间的一致性。这些积极结果表明ALLMs可作为评估SLMs的有效工具。实验结果同时揭示，当前SLMs（包括GPT-4o-audio）在语音风格控制与自然对话生成方面仍存在改进空间。

---

## [Leveraging Generative AI for Enhancing Automated Assessment in Programming Education Contests](https://arxiv.org/abs/2506.05990)

### Abstract
arXiv:2506.05990v1 Announce Type: cross 
Abstract: Competitive programming contests play a crucial role in cultivating computational thinking and algorithmic skills among learners. However, generating comprehensive test cases to effectively assess programming solutions remains resource-intensive and challenging for educators. This paper introduces an innovative NLP-driven method leveraging generative AI (large language models) to automate the creation of high-quality test cases for competitive programming assessments. We extensively evaluated our approach on diverse datasets, including 25 years of Romanian Informatics Olympiad (OJI) data for 5th graders, recent competitions hosted on the Kilonova.ro platform, and the International Informatics Olympiad in Teams (IIOT). Our results demonstrate that AI-generated test cases substantially enhanced assessments, notably identifying previously undetected errors in 67% of the OJI 5th grade programming problems. These improvements underscore the complementary educational value of our technique in formative assessment contexts. By openly sharing our prompts, translated datasets, and methodologies, we offer practical NLP-based tools that educators and contest organizers can readily integrate to enhance assessment quality, reduce workload, and deepen insights into learner performance.

### 摘要
编程竞赛在培养学习者计算思维和算法能力方面具有关键作用。然而，为有效评估编程解决方案而生成全面测试用例的过程，对教育工作者而言仍存在资源密集和难度高的挑战。本文提出一种创新的自然语言处理方法，利用生成式人工智能（大语言模型）实现竞赛编程评估中高质量测试用例的自动生成。我们在多类数据集上进行了广泛评估，包括罗马尼亚信息学奥赛（OJI）五年级组25年数据、Kilonova.ro平台近期赛事以及国际团队信息学奥赛（IIOT）。研究结果表明，AI生成的测试用例显著提升了评估效果，尤其在OJI五年级编程题目中识别出67%以往未检测到的错误。这些改进印证了本技术在形成性评估场景中的互补教育价值。通过公开分享提示模板、翻译数据集和方法论，我们为教育工作者和赛事组织者提供了可即用的自然语言处理工具，以提升评估质量、减轻工作负担并深化对学习者表现的洞察。

---

## [Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning](https://arxiv.org/abs/2506.05977)

### Abstract
arXiv:2506.05977v1 Announce Type: cross 
Abstract: Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as a promising solution for adapting models to distributed data environments while ensuring data privacy.
  Existing FedFT methods predominantly utilize parameter-efficient fine-tuning (PEFT) techniques to reduce communication and computation overhead.
  However, they often fail to adequately address the catastrophic forgetting, a critical challenge arising from continual adaptation in distributed environments. The traditional centralized fine-tuning methods, which are not designed for the heterogeneous and privacy-constrained nature of federated environments, struggle to mitigate this issue effectively. Moreover, the challenge is further exacerbated by significant variation in data distributions and device capabilities across clients, which leads to intensified forgetting and degraded model generalization. To tackle these issues, we propose FedBE, a novel FedFT framework that integrates an adaptive transformer block expansion mechanism with a dynamic trainable-block allocation strategy. Specifically, FedBE expands trainable blocks within the model architecture, structurally separating newly learned task-specific knowledge from the original pre-trained representations. Additionally, FedBE dynamically assigns these trainable blocks to clients based on their data distributions and computational capabilities. This enables the framework to better accommodate heterogeneous federated environments and enhances the generalization ability of the model.Extensive experiments show that compared with existing federated fine-tuning methods, FedBE achieves 12-74% higher accuracy retention on general tasks after fine-tuning and a model convergence acceleration ratio of 1.9-3.1x without degrading the accuracy of downstream tasks.

### 摘要
联邦微调（FedFT）已成为在分布式数据环境中适配大语言模型（LLM）同时确保数据隐私的有力解决方案。现有FedFT方法主要采用参数高效微调（PEFT）技术以降低通信与计算开销，但往往未能充分解决灾难性遗忘这一分布式环境持续适应中的关键挑战。传统集中式微调方法并非为联邦环境的异构性和隐私约束设计，难以有效缓解该问题。此外，客户端间数据分布与设备能力的显著差异进一步加剧了这一挑战，导致遗忘现象加重且模型泛化能力下降。为此，我们提出FedBE框架，通过自适应Transformer块扩展机制与动态可训练块分配策略相结合的新型FedFT方案。具体而言，FedBE在模型架构内扩展可训练块，从结构上分离新学习的任务特定知识与原始预训练表征；同时基于客户端数据分布与计算能力动态分配这些可训练块。该框架能更好适应异构联邦环境，并提升模型泛化能力。大量实验表明，相比现有联邦微调方法，FedBE在微调后通用任务上实现12-74%的准确率保留提升，模型收敛速度加快1.9-3.1倍，且下游任务准确率无衰减。

---

## [Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models](https://arxiv.org/abs/2506.06008)

### Abstract
arXiv:2506.06008v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) technique has proven effective in improving the performance of large language models (LLMs) on complex reasoning tasks. However, the performance gains are inconsistent across different tasks, and the underlying mechanism remains a long-standing research question. In this work, we make a preliminary observation that the monotonicity of token probability distributions may be correlated with the gains achieved through CoT reasoning. Leveraging this insight, we propose two indicators based on the token probability distribution to assess CoT effectiveness across different tasks. By combining instance-level indicators with logistic regression model, we introduce Dynamic CoT, a method that dynamically select between CoT and direct answer. Furthermore, we extend Dynamic CoT to closed-source models by transferring decision strategies learned from open-source models. Our indicators for assessing CoT effectiveness achieve an accuracy of 89.2\%, and Dynamic CoT reduces token consumption by more than 35\% while maintaining high accuracy. Overall, our work offers a novel perspective on the underlying mechanisms of CoT reasoning and provides a framework for its more efficient deployment.

### 摘要
链式思考（CoT）技术已被证实能有效提升大语言模型（LLMs）在复杂推理任务中的表现。然而，不同任务间的性能增益存在差异，其内在机制仍是长期悬而未决的研究问题。本研究通过初步观察发现，标记概率分布的单调性可能与CoT推理所获得的增益存在关联。基于这一发现，我们提出两种基于标记概率分布的指标，用于评估不同任务中CoT的有效性。通过将实例级指标与逻辑回归模型相结合，我们提出了动态CoT方法——该技术能动态选择采用CoT或直接回答策略。进一步地，我们通过迁移开源模型习得的决策策略，将动态CoT扩展至闭源模型应用场景。实验表明，我们的CoT有效性评估指标准确率达89.2%，动态CoT在保持高精度的同时降低了超过35%的标记消耗。本研究不仅为CoT推理的内在机制提供了新视角，更为其高效部署建立了系统性框架。

---

## [When to Trust Context: Self-Reflective Debates for Context Reliability](https://arxiv.org/abs/2506.06020)

### Abstract
arXiv:2506.06020v1 Announce Type: cross 
Abstract: Large language models frequently encounter conflicts between their parametric knowledge and contextual input, often resulting in factual inconsistencies or hallucinations. We propose Self-Reflective Debate for Contextual Reliability (SR-DCR), a lightweight framework that integrates token-level self-confidence with an asymmetric multi-agent debate to adjudicate such conflicts. A critic, deprived of context, challenges a defender who argues from the given passage; a judge model evaluates the debate and determines the context's reliability. The final answer is selected by combining the verdict with model confidence. Experiments on the ClashEval benchmark demonstrate that SR-DCR consistently enhances robustness to misleading context while maintaining accuracy on trustworthy inputs, outperforming both classical debate and confidence-only baselines with minimal computational overhead. The code is available at https://github.com/smiles724/Self-Reflective-Debates.

### 摘要
大语言模型经常面临参数化知识与上下文输入之间的冲突，往往导致事实不一致或幻觉现象。我们提出基于自反思辩论的上下文可靠性框架（SR-DCR），该轻量级框架通过整合词元级自置信度与非对称多智能体辩论机制来裁决此类冲突。该框架包含一个无上下文访问的批评者，其挑战基于给定段落论证的辩护者；由法官模型评估辩论过程并判定上下文的可靠性。最终答案通过结合裁决结果与模型置信度进行选择。在ClashEval基准测试中，SR-DCR在保持可信输入准确性的同时，持续提升对误导性上下文的鲁棒性，以最小计算开销优于传统辩论和纯置信度基线方法。代码详见https://github.com/smiles724/Self-Reflective-Debates。

---

## [Unlocking Recursive Thinking of LLMs: Alignment via Refinement](https://arxiv.org/abs/2506.06009)

### Abstract
arXiv:2506.06009v1 Announce Type: cross 
Abstract: The OpenAI o1-series models have demonstrated that leveraging long-form Chain of Thought (CoT) can substantially enhance performance. However, the recursive thinking capabilities of Large Language Models (LLMs) remain limited, particularly in the absence of expert-curated data for distillation. In this paper, we propose \textbf&#123;AvR&#125;: \textbf&#123;Alignment via Refinement&#125;, a novel method aimed at unlocking the potential of LLMs for recursive reasoning through long-form CoT. AvR introduces a refinement process that integrates criticism and improvement actions, guided by differentiable learning techniques to optimize \textbf&#123;refinement-aware rewards&#125;. As a result, the synthesized multi-round data can be organized as a long refinement thought, further enabling test-time scaling. Experimental results show that AvR significantly outperforms conventional preference optimization methods. Notably, with only 3k synthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct model by over 20\% in win rate on AlpacaEval 2.0. Our code is available at Github (https://github.com/Banner-Z/AvR.git).

### 摘要
OpenAI o1系列模型已证明利用长链思维（CoT）能显著提升性能。然而，大型语言模型（LLMs）的递归思维能力仍然有限，特别是在缺乏专家提炼数据的情况下。本文提出	extbf&#123;AvR&#125;：	extbf&#123;通过改进实现对齐&#125;，这是一种旨在通过长链CoT释放LLMs递归推理潜力的新方法。AvR引入了一个改进过程，整合了批评与改进动作，并采用可微分学习技术指导优化	extbf&#123;改进感知奖励&#125;。由此合成的多轮数据可组织为长改进思维链，进一步实现测试时扩展。实验结果表明，AvR显著优于传统偏好优化方法。值得注意的是，仅用3k合成样本，我们的方法就将LLaMA-3-8B-Instruct模型在AlpacaEval 2.0上的胜率性能提升了20%以上。代码已发布于Github（https://github.com/Banner-Z/AvR.git）。

---

## [Hey, That's My Data! Label-Only Dataset Inference in Large Language Models](https://arxiv.org/abs/2506.06057)

### Abstract
arXiv:2506.06057v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing by excelling at interpreting, reasoning about, and generating human language. However, their reliance on large-scale, often proprietary datasets poses a critical challenge: unauthorized usage of such data can lead to copyright infringement and significant financial harm. Existing dataset-inference methods typically depend on log probabilities to detect suspicious training material, yet many leading LLMs have begun withholding or obfuscating these signals. This reality underscores the pressing need for label-only approaches capable of identifying dataset membership without relying on internal model logits.
  We address this gap by introducing CatShift, a label-only dataset-inference framework that capitalizes on catastrophic forgetting: the tendency of an LLM to overwrite previously learned knowledge when exposed to new data. If a suspicious dataset was previously seen by the model, fine-tuning on a portion of it triggers a pronounced post-tuning shift in the model's outputs; conversely, truly novel data elicits more modest changes. By comparing the model's output shifts for a suspicious dataset against those for a known non-member validation set, we statistically determine whether the suspicious set is likely to have been part of the model's original training corpus. Extensive experiments on both open-source and API-based LLMs validate CatShift's effectiveness in logit-inaccessible settings, offering a robust and practical solution for safeguarding proprietary data.

### 摘要
大型语言模型（LLMs）通过卓越的人类语言理解、推理和生成能力，彻底改变了自然语言处理领域。然而，其依赖大规模且通常为专有数据集的特点带来了严峻挑战：未经授权使用此类数据可能导致版权侵权及重大经济损失。现有数据集推断方法通常依赖对数概率检测可疑训练材料，但许多主流LLMs已开始隐藏或混淆这些信号。这一现实凸显了对标签专用方法的迫切需求——该方法需在不依赖模型内部对数的情况下识别数据集成员资格。

我们通过提出CatShift填补了这一空白，该标签专用数据集推断框架利用灾难性遗忘现象：即LLMs接触新数据时倾向于覆盖先前习得知识。若可疑数据集曾被模型训练使用，对其部分数据进行微调会引发模型输出的显著后调优偏移；反之，真正新颖的数据仅引发适度变化。通过比较模型对可疑数据集与已知非成员验证集的输出偏移程度，我们可统计推断可疑集是否可能属于模型原始训练语料。在开源和基于API的LLMs上进行的大量实验验证了CatShift在无法获取对数场景下的有效性，为保护专有数据提供了稳健实用的解决方案。

---

## [Building Models of Neurological Language](https://arxiv.org/abs/2506.06208)

### Abstract
arXiv:2506.06208v1 Announce Type: cross 
Abstract: This report documents the development and evaluation of domain-specific language models for neurology. Initially focused on building a bespoke model, the project adapted to rapid advances in open-source and commercial medical LLMs, shifting toward leveraging retrieval-augmented generation (RAG) and representational models for secure, local deployment. Key contributions include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived data), tools for multi-word expression extraction, and graph-based analyses of medical terminology. The project also produced scripts and Docker containers for local hosting. Performance metrics and graph community results are reported, with future possible work open for multimodal models using open-source architectures like phi-4.

---

## [Text-to-LoRA: Instant Transformer Adaption](https://arxiv.org/abs/2506.06105)

### Abstract
arXiv:2506.06105v1 Announce Type: cross 
Abstract: While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyper-parameter choices. To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting Large Language Models on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements. Our code is available at https://github.com/SakanaAI/text-to-lora

### 摘要
虽然基础模型为快速内容创作提供了通用工具，但其通常需要针对特定任务进行适配。传统方法涉及精心策划数据集并反复微调底层模型。微调技术虽能使基础模型适应诸多新应用场景，但需要昂贵且耗时的训练过程，并对超参数选择极为敏感。为突破这些限制，我们提出Text-to-LoRA（T2L）模型，该模型仅需目标任务的自然语言描述即可实时适配大语言模型。T2L是一种经过训练的超网络，能以单次低成本前向传播构建LoRA适配器。通过在9个预训练LoRA适配器（GSM8K、Arc等）套件上进行训练后，我们证明临时重建的LoRA实例在相应测试集上可达到任务专用适配器的性能水平。此外，T2L能压缩数百个LoRA实例，并对完全未见过的任务实现零样本泛化。该方法为基础模型专业化的大众化迈出重要一步，仅需极小计算资源即可实现基于语言的适配。代码已开源：https://github.com/SakanaAI/text-to-lora

---

## [The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/abs/2506.06166)

### Abstract
arXiv:2506.06166v1 Announce Type: cross 
Abstract: The training and deployment of large language models (LLMs) create a feedback loop with human users: models learn human beliefs from data, reinforce these beliefs with generated content, reabsorb the reinforced beliefs, and feed them back to users again and again. This dynamic resembles an echo chamber. We hypothesize that this feedback loop entrenches the existing values and beliefs of users, leading to a loss of diversity and potentially the lock-in of false beliefs. We formalize this hypothesis and test it empirically with agent-based LLM simulations and real-world GPT usage data. Analysis reveals sudden but sustained drops in diversity after the release of new GPT iterations, consistent with the hypothesized human-AI feedback loop. Code and data available at https://thelockinhypothesis.com

### 摘要
大型语言模型（LLMs）的训练与部署形成了与人类用户的反馈循环：模型从数据中学习人类信念，通过生成内容强化这些信念，重新吸收被强化的信念，并反复将其反馈给用户。这种动态类似于回声室效应。我们假设该反馈循环会固化用户现有的价值观与信念，导致多样性丧失并可能锁定错误信念。我们通过形式化该假设，并基于LLM智能体模拟和真实世界GPT使用数据进行实证检验。分析表明，新版GPT发布后会出现突然但持续的多样性下降，这与假设的人机反馈循环机制一致。代码与数据详见https://thelockinhypothesis.com

---

## [Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness](https://arxiv.org/abs/2506.06112)

### Abstract
arXiv:2506.06112v1 Announce Type: cross 
Abstract: Growing concerns over data privacy and security highlight the importance of machine unlearning--removing specific data influences from trained models without full retraining. Techniques like Membership Inference Attacks (MIAs) are widely used to externally assess successful unlearning. However, existing methods face two key limitations: (1) maximizing MIA effectiveness (e.g., via online attacks) requires prohibitive computational resources, often exceeding retraining costs; (2) MIAs, designed for binary inclusion tests, struggle to capture granular changes in approximate unlearning. To address these challenges, we propose the Interpolated Approximate Measurement (IAM), a framework natively designed for unlearning inference. IAM quantifies sample-level unlearning completeness by interpolating the model's generalization-fitting behavior gap on queried samples. IAM achieves strong performance in binary inclusion tests for exact unlearning and high correlation for approximate unlearning--scalable to LLMs using just one pre-trained shadow model. We theoretically analyze how IAM's scoring mechanism maintains performance efficiently. We then apply IAM to recent approximate unlearning algorithms, revealing general risks of both over-unlearning and under-unlearning, underscoring the need for stronger safeguards in approximate unlearning systems. The code is available at https://github.com/Happy2Git/Unlearning_Inference_IAM.

### 摘要
数据隐私与安全问题的日益凸显，使得机器遗忘技术——即无需完整重训练即可消除特定数据对已训练模型影响的方法——显得尤为重要。成员推理攻击（MIA）等技术被广泛用于从外部评估遗忘效果，但现有方法存在两大局限：（1）最大化MIA有效性（如通过在线攻击）需消耗过高计算资源，往往超过重训练成本；（2）针对二元包含测试设计的MIA难以捕捉近似遗忘中的细微变化。为此，我们提出插值近似测量框架（IAM），该框架专为遗忘推理设计，通过插值模型在查询样本上的泛化-拟合行为间隙来量化样本级遗忘完整性。IAM在精确遗忘的二元包含测试中表现优异，在近似遗忘场景下亦呈现高相关性——仅需一个预训练影子模型即可扩展至大语言模型。我们从理论上分析了IAM评分机制如何高效维持性能，并将其应用于近期近似遗忘算法，揭示了过遗忘与欠遗忘的普遍风险，表明近似遗忘系统需要更强有力的安全保障措施。代码详见https://github.com/Happy2Git/Unlearning_Inference_IAM。

---

## [Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.06060)

### Abstract
arXiv:2506.06060v1 Announce Type: cross 
Abstract: Federated fine-tuning of large language models (FedLLMs) presents a promising approach for achieving strong model performance while preserving data privacy in sensitive domains. However, the inherent memorization ability of LLMs makes them vulnerable to training data extraction attacks. To investigate this risk, we introduce simple yet effective extraction attack algorithms specifically designed for FedLLMs. In contrast to prior "verbatim" extraction attacks, which assume access to fragments from all training data, our approach operates under a more realistic threat model, where the attacker only has access to a single client's data and aims to extract previously unseen personally identifiable information (PII) from other clients. This requires leveraging contextual prefixes held by the attacker to generalize across clients. To evaluate the effectiveness of our approaches, we propose two rigorous metrics-coverage rate and efficiency-and extend a real-world legal dataset with PII annotations aligned with CPIS, GDPR, and CCPA standards, achieving 89.9% human-verified precision. Experimental results show that our method can extract up to 56.57% of victim-exclusive PII, with "Address," "Birthday," and "Name" being the most vulnerable categories. Our findings underscore the pressing need for robust defense strategies and contribute a new benchmark and evaluation framework for future research in privacy-preserving federated learning.

### 摘要
大型语言模型（LLMs）的联邦微调（FedLLMs）为在敏感领域实现强大模型性能同时保护数据隐私提供了一种前景广阔的方法。然而，LLMs固有的记忆能力使其容易遭受训练数据提取攻击。为探究这一风险，我们针对FedLLMs设计了一套简单而有效的提取攻击算法。与以往假设攻击者可获取所有训练数据片段的"逐字"提取攻击不同，我们的方法基于更现实的威胁模型：攻击者仅能访问单个客户端数据，却试图从其他客户端提取未公开的个人身份信息（PII）。这要求攻击者利用其持有的上下文前缀实现跨客户端泛化。为评估方法有效性，我们提出覆盖率与效率两项严谨指标，并扩展了一个包含CPIS、GDPR和CCPA标准PII标注的真实法律数据集，达到89.9%的人工验证精度。实验结果表明，我们的方法最高可提取56.57%的受害者专属PII，其中"地址"、"生日"和"姓名"是最易泄露的类别。这些发现凸显了对强效防御策略的迫切需求，并为隐私保护联邦学习的未来研究贡献了新的基准与评估框架。

---

## [Can Theoretical Physics Research Benefit from Language Agents?](https://arxiv.org/abs/2506.06214)

### Abstract
arXiv:2506.06214v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are rapidly advancing across diverse domains, yet their application in theoretical physics research is not yet mature. This position paper argues that LLM agents can potentially help accelerate theoretical, computational, and applied physics when properly integrated with domain knowledge and toolbox. We analyze current LLM capabilities for physics -- from mathematical reasoning to code generation -- identifying critical gaps in physical intuition, constraint satisfaction, and reliable reasoning. We envision future physics-specialized LLMs that could handle multimodal data, propose testable hypotheses, and design experiments. Realizing this vision requires addressing fundamental challenges: ensuring physical consistency, and developing robust verification methods. We call for collaborative efforts between physics and AI communities to help advance scientific discovery in physics.

### 摘要
大语言模型（LLMs）正在多个领域快速发展，但其在理论物理研究中的应用尚未成熟。本立场文件认为，当LLM智能体与领域知识和工具箱恰当结合时，有望加速理论物理、计算物理和应用物理的发展。我们分析了当前LLM在物理领域的能力——从数学推理到代码生成——并指出了其在物理直觉、约束满足和可靠推理方面的关键不足。我们展望未来可能出现专用于物理的LLM，它们能够处理多模态数据、提出可检验的假设并设计实验。实现这一愿景需要解决基础性挑战：确保物理一致性，以及开发稳健的验证方法。我们呼吁物理学界与人工智能领域开展协作，共同推动物理学的科学发现。

---

## [Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.06151)

### Abstract
arXiv:2506.06151v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by retrieving relevant documents from external corpora before generating responses. This approach significantly expands LLM capabilities by leveraging vast, up-to-date external knowledge. However, this reliance on external knowledge makes RAG systems vulnerable to corpus poisoning attacks that manipulate generated outputs via poisoned document injection. Existing poisoning attack strategies typically treat the retrieval and generation stages as disjointed, limiting their effectiveness. We propose Joint-GCG, the first framework to unify gradient-based attacks across both retriever and generator models through three innovations: (1) Cross-Vocabulary Projection for aligning embedding spaces, (2) Gradient Tokenization Alignment for synchronizing token-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically balancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves at most 25% and an average of 5% higher attack success rate than previous methods across multiple retrievers and generators. While optimized under a white-box assumption, the generated poisons show unprecedented transferability to unseen models. Joint-GCG's innovative unification of gradient-based attacks across retrieval and generation stages fundamentally reshapes our understanding of vulnerabilities within RAG systems. Our code is available at https://github.com/NicerWang/Joint-GCG.

### 摘要
检索增强生成（RAG）系统通过从外部语料库检索相关文档后再生成响应，从而增强大语言模型（LLM）的能力。该方法通过利用海量且最新的外部知识，显著扩展了LLM的功能。然而，这种对外部知识的依赖使得RAG系统容易受到通过注入污染文档来操纵生成输出的语料库投毒攻击。现有的投毒攻击策略通常将检索和生成阶段视为分离的，限制了攻击效果。我们提出Joint-GCG，这是首个通过三项创新将基于梯度的攻击统一应用于检索器和生成器模型的框架：（1）跨词汇投影用于对齐嵌入空间，（2）梯度标记对齐用于同步标记级梯度信号，（3）自适应加权融合用于动态平衡攻击目标。评估表明，Joint-GCG在多种检索器和生成器上相比现有方法最高可提升25%的攻击成功率，平均提升5%。尽管在白盒假设下优化，生成的毒化样本对未见模型展现出前所未有的可迁移性。Joint-GCG创新性地统一了检索和生成阶段的基于梯度攻击，从根本上重塑了我们对RAG系统漏洞的理解。代码发布于https://github.com/NicerWang/Joint-GCG。

---

## [Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning](https://arxiv.org/abs/2506.06205)

### Abstract
arXiv:2506.06205v1 Announce Type: cross 
Abstract: Modern robot navigation systems encounter difficulties in diverse and complex indoor environments. Traditional approaches rely on multiple modules with small models or rule-based systems and thus lack adaptability to new environments. To address this, we developed Astra, a comprehensive dual-model architecture, Astra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a multimodal LLM, processes vision and language inputs to perform self and goal localization using a hybrid topological-semantic graph as the global map, and outperforms traditional visual place recognition methods. Astra-Local, a multitask network, handles local path planning and odometry estimation. Its 4D spatial-temporal encoder, trained through self-supervised learning, generates robust 4D features for downstream tasks. The planning head utilizes flow matching and a novel masked ESDF loss to minimize collision risks for generating local trajectories, and the odometry head integrates multi-sensor inputs via a transformer encoder to predict the relative pose of the robot. Deployed on real in-house mobile robots, Astra achieves high end-to-end mission success rate across diverse indoor environments.

### 摘要
现代机器人导航系统在多样化和复杂的室内环境中面临诸多挑战。传统方法依赖多个小型模型模块或基于规则的系统，因而缺乏对新环境的适应性。为此，我们开发了Astra——一种用于移动机器人导航的综合双模型架构，包含Astra-Global和Astra-Local两大模块。Astra-Global作为多模态大语言模型，通过处理视觉与语言输入，利用混合拓扑-语义图作为全局地图实现自主定位与目标定位，其性能优于传统视觉位置识别方法。Astra-Local作为多任务网络，负责局部路径规划与里程计估计。其通过自监督学习训练的4D时空编码器可为下游任务生成鲁棒的4D特征：规划头采用流匹配技术和新型掩码ESDF损失函数来最小化碰撞风险以生成局部轨迹；里程计头通过Transformer编码器融合多传感器输入来预测机器人相对位姿。在实际部署的室内移动机器人上，Astra系统在多样化室内环境中展现出极高的端到端任务成功率。

---

## [semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces](https://arxiv.org/abs/2506.06169)

### Abstract
arXiv:2506.06169v1 Announce Type: cross 
Abstract: We introduce semantic-features, an extensible, easy-to-use library based on Chronis et al. (2023) for studying contextualized word embeddings of LMs by projecting them into interpretable spaces. We apply this tool in an experiment where we measure the contextual effect of the choice of dative construction (prepositional or double object) on the semantic interpretation of utterances (Bresnan, 2007). Specifically, we test whether "London" in "I sent London the letter." is more likely to be interpreted as an animate referent (e.g., as the name of a person) than in "I sent the letter to London." To this end, we devise a dataset of 450 sentence pairs, one in each dative construction, with recipients being ambiguous with respect to person-hood vs. place-hood. By applying semantic-features, we show that the contextualized word embeddings of three masked language models show the expected sensitivities. This leaves us optimistic about the usefulness of our tool.

### 摘要
我们推出semantic-features这一基于Chronis等人(2023)研究的可扩展、易用工具库，用于通过将语言模型的语境化词向量投射至可解释空间进行研究。我们将该工具应用于一项实验，测量与格结构选择（介词型或双宾语型）对语句语义解释的语境效应(Bresnan, 2007)。具体而言，我们检验在"我给伦敦寄了信"中"伦敦"比"我把信寄到伦敦"更可能被解释为有生指称（如人名）的现象。为此，我们构建了包含450对句子的数据集，每对采用不同与格结构，其中接收者在"人物-地点"属性上具有歧义。通过应用semantic-features，我们发现三个掩码语言模型的语境化词向量均显示出预期的敏感性差异。这使我们对本工具的应用前景持乐观态度。

---

## [PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts](https://arxiv.org/abs/2506.06211)

### Abstract
arXiv:2506.06211v1 Announce Type: cross 
Abstract: Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined problem definitions. In contrast to conventional reasoning benchmarks consisting of tasks with clear instructions, puzzlehunts require models to discover the underlying problem structure from multimodal evidence and iterative reasoning, mirroring real-world domains such as scientific discovery, exploratory data analysis, or investigative problem-solving. Despite recent progress in foundation models, their performance on such open-ended settings remains largely untested. In this paper, we introduce PuzzleWorld, a large-scale benchmark of 667 puzzlehunt-style problems designed to assess step-by-step, open-ended, and creative multimodal reasoning. Each puzzle is annotated with the final solution, detailed reasoning traces, and cognitive skill labels, enabling holistic benchmarking and fine-grained diagnostic analysis. Most state-of-the-art models achieve only 1-2% final answer accuracy, with the best model solving only 14% of puzzles and reaching 40% stepwise accuracy. To demonstrate the value of our reasoning annotations, we show that fine-tuning a small model on reasoning traces improves stepwise reasoning from 4% to 11%, while training on final answers alone degrades performance to near zero. Our error analysis reveals that current models exhibit myopic reasoning, are bottlenecked by the limitations of language-based inference, and lack sketching capabilities crucial for visual and spatial reasoning. We release PuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on building more general, open-ended, and creative reasoning systems.

### 摘要
谜题猎是一种缺乏明确定义问题的复杂多步骤谜题类型。与由清晰指令任务组成的传统推理基准不同，谜题猎要求模型从多模态证据和迭代推理中发现潜在问题结构，这反映了科学发现、探索性数据分析或调查性问题解决等现实领域。尽管基础模型近期取得进展，但其在这种开放式场景下的表现仍基本未经测试。本文提出PuzzleWorld——一个包含667个谜题猎风格问题的大规模基准，旨在评估逐步式、开放式和创造性的多模态推理。每个谜题均标注最终解法、详细推理轨迹和认知技能标签，支持整体基准测试和细粒度诊断分析。当前最先进模型的最终答案准确率仅为1-2%，最优模型仅解决14%的谜题并达到40%的步骤准确率。为证明推理标注的价值，我们展示通过在推理轨迹上微调小型模型，可将逐步推理准确率从4%提升至11%，而仅使用最终答案训练会使性能降至接近零。误差分析表明，当前模型存在短视推理、受限于基于语言的推断瓶颈，且缺乏视觉与空间推理所需的关键草图能力。我们在https://github.com/MIT-MI/PuzzleWorld发布PuzzleWorld，以支持未来构建更通用、开放和创造性推理系统的研究。

---

## [DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code Generation](https://arxiv.org/abs/2506.06251)

### Abstract
arXiv:2506.06251v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in automated front-end engineering, e.g., generating UI code from visual designs. However, existing front-end UI code generation benchmarks have the following limitations: (1) While framework-based development becomes predominant in modern front-end programming, current benchmarks fail to incorporate mainstream development frameworks. (2) Existing evaluations focus solely on the UI code generation task, whereas practical UI development involves several iterations, including refining editing, and repairing issues. (3) Current benchmarks employ unidimensional evaluation, lacking investigation into influencing factors like task difficulty, input context variations, and in-depth code-level analysis. To bridge these gaps, we introduce DesignBench, a multi-framework, multi-task evaluation benchmark for assessing MLLMs' capabilities in automated front-end engineering. DesignBench encompasses three widely-used UI frameworks (React, Vue, and Angular) alongside vanilla HTML/CSS, and evaluates on three essential front-end tasks (generation, edit, and repair) in real-world development workflows. DesignBench contains 900 webpage samples spanning over 11 topics, 9 edit types, and 6 issue categories, enabling detailed analysis of MLLM performance across multiple dimensions. Our systematic evaluation reveals critical insights into MLLMs' framework-specific limitations, task-related bottlenecks, and performance variations under different conditions, providing guidance for future research in automated front-end development. Our code and data are available at https://github.com/WebPAI/DesignBench.

### 摘要
多模态大语言模型（MLLMs）在自动化前端工程领域展现出卓越能力，例如根据视觉设计生成用户界面代码。然而，现有前端UI代码生成基准测试存在以下局限：（1）尽管基于框架的开发已成为现代前端编程主流，当前基准测试却未纳入主流开发框架；（2）现有评估仅关注UI代码生成任务，而实际UI开发需经历多次迭代，包括精细化编辑与问题修复；（3）现行基准采用单维度评估，缺乏对任务难度、输入上下文变化及代码级深度分析等影响因素的考察。为弥补这些不足，我们提出DesignBench——一个用于评估MLLMs自动化前端工程能力的多框架、多任务基准测试。该基准涵盖三种主流UI框架（React、Vue和Angular）及原生HTML/CSS，针对真实开发流程中的三项核心任务（生成、编辑与修复）进行评估。DesignBench包含900个网页样本，覆盖11个主题类别、9种编辑类型和6类问题，支持从多维度对MLLM性能进行细粒度分析。系统化评估揭示了MLLMs在框架特定限制、任务相关瓶颈及不同条件下的性能变异等关键发现，为自动化前端开发的未来研究提供指导。代码与数据详见https://github.com/WebPAI/DesignBench。

---

## [Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models](https://arxiv.org/abs/2506.06242)

### Abstract
arXiv:2506.06242v1 Announce Type: cross 
Abstract: Recent advancements in multimodal large language models have driven breakthroughs in visual question answering. Yet, a critical gap persists, `conceptualization'-the ability to recognize and reason about the same concept despite variations in visual form, a basic ability of human reasoning. To address this challenge, we introduce the Visual Graph Arena (VGA), a dataset featuring six graph-based tasks designed to evaluate and improve AI systems' capacity for visual abstraction. VGA uses diverse graph layouts (e.g., Kamada-Kawai vs. planar) to test reasoning independent of visual form. Experiments with state-of-the-art vision models and multimodal LLMs reveal a striking divide: humans achieved near-perfect accuracy across tasks, while models totally failed on isomorphism detection and showed limited success in path/cycle tasks. We further identify behavioral anomalies suggesting pseudo-intelligent pattern matching rather than genuine understanding. These findings underscore fundamental limitations in current AI models for visual understanding. By isolating the challenge of representation-invariant reasoning, the VGA provides a framework to drive progress toward human-like conceptualization in AI visual models. The Visual Graph Arena is available at: \href&#123;https://vga.csail.mit.edu/&#125;&#123;vga.csail.mit.edu&#125;

### 摘要
多模态大语言模型的最新进展推动了视觉问答领域的突破。然而，一个关键能力仍存在缺失——'概念化'，即尽管视觉形态存在差异仍能识别并推理相同概念的能力，这是人类推理的基本能力。为应对这一挑战，我们提出视觉图竞技场（VGA），该数据集包含六项基于图的任务，旨在评估并提升AI系统进行视觉抽象的能力。VGA通过多样化图形布局（如Kamada-Kawai布局与平面布局）来测试与视觉形态无关的推理能力。对最先进视觉模型和多模态大语言模型的实验揭示了显著差距：人类在所有任务中接近完美准确率，而模型在同构检测任务中完全失败，在路径/环路任务中表现有限。我们进一步发现行为异常，表明模型采用伪智能模式匹配而非真正理解。这些发现揭示了当前AI模型在视觉理解方面的根本局限。通过分离表征不变推理的挑战，VGA为推进AI视觉模型实现类人概念化提供了框架。视觉图竞技场可通过以下网址访问：\href&#123;https://vga.csail.mit.edu/&#125;&#123;vga.csail.mit.edu&#125;

---

## [Distillation Robustifies Unlearning](https://arxiv.org/abs/2506.06278)

### Abstract
arXiv:2506.06278v1 Announce Type: cross 
Abstract: Current LLM unlearning methods are not robust: they can be reverted easily with a few steps of finetuning. This is true even for the idealized unlearning method of training to imitate an oracle model that was never exposed to unwanted information, suggesting that output-based finetuning is insufficient to achieve robust unlearning. In a similar vein, we find that training a randomly initialized student to imitate an unlearned model transfers desired behaviors while leaving undesired capabilities behind. In other words, distillation robustifies unlearning. Building on this insight, we propose Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an unlearned model into a partially noised copy of itself. UNDO introduces a tunable tradeoff between compute cost and robustness, establishing a new Pareto frontier on synthetic language and arithmetic tasks. At its strongest setting, UNDO matches the robustness of a model retrained from scratch with perfect data filtering while using only 60-80% of the compute and requiring only 0.01% of the pretraining data to be labeled. We also show that UNDO robustifies unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP) benchmark. Since distillation is widely used in practice, incorporating an unlearning step beforehand offers a convenient path to robust capability removal.

### 摘要
当前的大型语言模型（LLM）遗忘方法缺乏鲁棒性：仅需几步微调即可轻易恢复被遗忘内容。即使采用理想化的遗忘方法——通过训练模仿从未接触过有害信息的预言机模型，这一现象依然存在，这表明基于输出的微调不足以实现鲁棒遗忘。类似地，我们发现将随机初始化的学生模型训练为模仿已遗忘模型时，期望行为会被传递，而不良能力则被剔除。换言之，蒸馏过程能够强化遗忘效果。基于此发现，我们提出"输出端噪声遗忘蒸馏"（UNDO）方法，这种可扩展技术通过将已遗忘模型蒸馏至其自身添加部分噪声的副本来实现遗忘。UNDO在计算成本与鲁棒性之间建立了可调节的权衡关系，在合成语言和算术任务上构建了新的帕累托前沿。在最强配置下，UNDO能达到与完美数据过滤后从头训练模型相当的鲁棒性，仅需60-80%的计算资源，且仅需标注0.01%的预训练数据。我们进一步证明UNDO能在更现实的"大规模杀伤性武器代理"（WMDP）基准测试中增强遗忘鲁棒性。鉴于蒸馏技术已广泛应用，在流程中前置遗忘步骤为实现鲁棒的能力剔除提供了便捷途径。

---

## [Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias](https://arxiv.org/abs/2506.06280)

### Abstract
arXiv:2506.06280v1 Announce Type: cross 
Abstract: Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight matrices has been an active area of research in recent years. At a high level, eigenspectrum analysis of DNNs involves measuring the heavytailness of the empirical spectral densities (ESD) of weight matrices. It provides insight into how well a model is trained and can guide decisions on assigning better layer-wise training hyperparameters. In this paper, we address a challenge associated with such eigenspectrum methods: the impact of the aspect ratio of weight matrices on estimated heavytailness metrics. We demonstrate that matrices of varying sizes (and aspect ratios) introduce a non-negligible bias in estimating heavytailness metrics, leading to inaccurate model diagnosis and layer-wise hyperparameter assignment. To overcome this challenge, we propose FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the weight matrices by subsampling submatrices with a fixed aspect ratio. Instead of measuring the heavytailness of the original ESD, we measure the average ESD of these subsampled submatrices. We show that measuring the heavytailness of these submatrices with the fixed aspect ratio can effectively mitigate the aspect ratio bias. We validate our approach across various optimization techniques and application domains that involve eigenspectrum analysis of weights, including image classification in computer vision (CV) models, scientific machine learning (SciML) model training, and large language model (LLM) pruning. Our results show that despite its simplicity, FARMS uniformly improves the accuracy of eigenspectrum analysis while enabling more effective layer-wise hyperparameter assignment in these application domains. In one of the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model by 17.3% when compared with the state-of-the-art method.

### 摘要
近年来，通过权重矩阵的特征谱诊断深度神经网络（DNNs）已成为研究热点。从高层次看，DNNs的特征谱分析涉及测量权重矩阵经验谱密度（ESD）的重尾程度，其能够揭示模型的训练效果，并指导分层训练超参数的优化。本文解决了此类特征谱方法面临的一个挑战：权重矩阵纵横比对重尾度估计指标的影响。我们证明，不同尺寸（及纵横比）的矩阵会引入不可忽视的重尾度估计偏差，导致模型诊断和分层超参数分配不准确。为解决这一问题，我们提出FARMS（固定纵横比矩阵子采样）方法，该方法通过采样固定纵横比的子矩阵对权重矩阵进行归一化。我们不再测量原始ESD的重尾度，而是测量这些子矩阵的平均ESD。研究表明，测量固定纵横比子矩阵的重尾度可有效缓解纵横比偏差。我们在涉及权重特征谱分析的各种优化技术和应用领域验证了该方法，包括计算机视觉（CV）模型的图像分类、科学机器学习（SciML）模型训练以及大语言模型（LLM）剪枝。结果表明，尽管方法简单，FARMS在这些应用领域中一致提高了特征谱分析的准确性，同时实现了更有效的分层超参数分配。在一项LLM剪枝实验中，与最先进方法相比，FARMS将LLaMA-7B模型的困惑度降低了17.3%。

---

## [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/abs/2506.06266)

### Abstract
arXiv:2506.06266v1 Announce Type: cross 
Abstract: Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.

### 摘要
大型语言模型常被用于基于大规模文本语料库（如代码库、法律文件或聊天记录）的查询应答，其方法是将整个语料库置于上下文窗口并利用上下文学习（ICL）。尽管当前模型支持100K-1M标记的上下文，但这种设置的服务成本高昂，因为KV缓存的内存消耗与输入长度成比例增长。我们探索了一种替代方案：针对每个语料库离线训练较小的KV缓存。在推理时加载这个经过训练的KV缓存（我们称之为Cartridge）并解码响应。关键在于，Cartridge的训练成本可在引用同一语料库的所有查询中分摊。然而，我们发现直接采用语料库的下一标记预测来训练Cartridge的方法无法与ICL竞争。为此，我们提出"自我学习"训练方案：通过生成关于语料库的合成对话，采用上下文蒸馏目标训练Cartridge。实验表明，经自我学习训练的Cartridge能复现ICL功能，同时显著降低服务成本。在挑战性长上下文基准测试中，自我学习训练的Cartridge在内存消耗减少38.6倍、吞吐量提升26.4倍的同时，性能与ICL相当。该方法还能扩展模型的有效上下文长度（例如在MTOB上从128k标记提升至484k标记），且令人惊讶的是，所得Cartridge可在推理时直接组合使用而无需重新训练。

---

## [Position: Theory of Mind Benchmarks are Broken for Large Language Models](https://arxiv.org/abs/2412.19726)

### Abstract
arXiv:2412.19726v3 Announce Type: replace 
Abstract: Our paper argues that the majority of theory of mind benchmarks are broken because of their inability to directly test how large language models (LLMs) adapt to new partners. This problem stems from the fact that theory of mind benchmarks for LLMs are overwhelmingly inspired by the methods used to test theory of mind in humans and fall victim to a fallacy of attributing human-like qualities to AI agents. We expect that humans will engage in a consistent reasoning process across various questions about a situation, but this is known to not be the case for current LLMs. Most theory of mind benchmarks only measure what we call literal theory of mind: the ability to predict the behavior of others. However, this type of metric is only informative when agents exhibit self-consistent reasoning. Thus, we introduce the concept of functional theory of mind: the ability to adapt to agents in-context following a rational response to their behavior. We find that many open source LLMs are capable of displaying strong literal theory of mind capabilities, but seem to struggle with functional theory of mind -- even with exceedingly simple partner policies. Simply put, strong literal theory of mind performance does not necessarily imply strong functional theory of mind performance or vice versa. Achieving functional theory of mind, particularly over long interaction horizons with a partner, is a significant challenge deserving a prominent role in any meaningful LLM theory of mind evaluation.

---

## [Structure Guided Large Language Model for SQL Generation](https://arxiv.org/abs/2402.13284)

### Abstract
arXiv:2402.13284v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have shown promise in bridging the gap between natural language queries and database management systems, enabling users to interact with databases without the background of SQL. However, LLMs often struggle to comprehend complex database structures and accurately interpret user intentions. Decomposition-based methods have been proposed to enhance the performance of LLMs on complex tasks, but decomposing SQL generation into subtasks is non-trivial due to the declarative structure of SQL syntax and the intricate connections between query concepts and database elements. In this paper, we propose a novel Structure GUided text-to-SQL framework~(SGU-SQL) that incorporates syntax-based prompting to enhance the SQL generation capabilities of LLMs. Specifically, SGU-SQL establishes structure-aware links between user queries and database schema and decomposes the complex generation task using syntax-based prompting to enable more accurate LLM-based SQL generation. Extensive experiments on two benchmark datasets demonstrate that SGU-SQL consistently outperforms state-of-the-art text-to-SQL models.

### 摘要
大型语言模型（LLMs）的最新进展在弥合自然语言查询与数据库管理系统之间的鸿沟方面展现出潜力，使得不具备SQL背景的用户能够与数据库交互。然而，LLMs往往难以理解复杂的数据库结构并准确解读用户意图。尽管已有研究提出基于任务分解的方法来提升LLMs处理复杂任务的表现，但由于SQL语法的声明式结构以及查询概念与数据库元素间错综复杂的关联，将SQL生成分解为子任务具有显著挑战性。本文提出一种新颖的结构引导文本到SQL框架SGU-SQL，该框架通过引入基于语法的提示机制来增强LLMs的SQL生成能力。具体而言，SGU-SQL在用户查询与数据库模式之间建立结构感知的关联，并利用基于语法的提示对复杂生成任务进行分解，从而实现更精准的基于LLM的SQL生成。在两个基准数据集上的大量实验表明，SGU-SQL始终优于当前最先进的文本到SQL模型。

---

## [Is Cognition Consistent with Perception? Assessing and Mitigating Multimodal Knowledge Conflicts in Document Understanding](https://arxiv.org/abs/2411.07722)

### Abstract
arXiv:2411.07722v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities in document understanding, a rapidly growing research area with significant industrial demand. As a multimodal task, document understanding requires models to possess both perceptual and cognitive abilities. However, due to different types of annotation noise in training, current MLLMs often face conflicts between perception and cognition. Taking a document VQA task (cognition) as an example, an MLLM might generate answers that do not match the corresponding visual content identified by its OCR (perception). This conflict suggests that the MLLM might struggle to establish an intrinsic connection between the information it "sees" and what it "understands". Such conflicts challenge the intuitive notion that cognition is consistent with perception, hindering the performance and explainability of MLLMs. In this paper, we define the conflicts between cognition and perception as Cognition and Perception (C&amp;P) knowledge conflicts, a form of multimodal knowledge conflict, and systematically assess them with a focus on document understanding. Our analysis reveals that even GPT-4o, a leading MLLM, achieves only 75.26% C&amp;P consistency. To mitigate the C&amp;P knowledge conflicts, we propose a novel method called Multimodal Knowledge Consistency Fine-tuning. Our method reduces C&amp;P knowledge conflicts across all tested MLLMs and enhances their performance in both cognitive and perceptual tasks. All data we construct will be publicly available.

### 摘要
多模态大语言模型（MLLMs）在文档理解领域展现出卓越能力，这一快速发展的研究方向具有显著的工业需求。作为多模态任务，文档理解要求模型同时具备感知与认知能力。然而由于训练中存在的各类标注噪声，当前MLLMs常面临感知与认知的冲突。以文档问答任务（认知）为例，模型可能生成与其OCR系统识别（感知）的视觉内容不匹配的答案。这种冲突表明MLLMs难以在"所见"与"所理解"信息间建立内在联系，挑战了认知应与感知一致的直觉观念，制约了模型性能与可解释性。本文首次将此类冲突定义为认知与感知（C&amp;P）知识冲突——多模态知识冲突的一种形式，并围绕文档理解展开系统评估。分析表明，即便是领先的GPT-4o模型，其C&amp;P一致性仅达75.26%。为缓解此类冲突，我们提出多模态知识一致性微调方法。该方法有效降低了所有测试模型的C&amp;P知识冲突，同步提升了其在认知与感知任务中的表现。本研究构建的所有数据集将公开提供。

---

## [Agents for self-driving laboratories applied to quantum computing](https://arxiv.org/abs/2412.07978)

### Abstract
arXiv:2412.07978v2 Announce Type: replace 
Abstract: Fully automated self-driving laboratories are promising to enable high-throughput and large-scale scientific discovery by reducing repetitive labour. However, effective automation requires deep integration of laboratory knowledge, which is often unstructured, multimodal, and difficult to incorporate into current AI systems. This paper introduces the k-agents framework, designed to support experimentalists in organizing laboratory knowledge and automating experiments with agents. Our framework employs large language model-based agents to encapsulate laboratory knowledge including available laboratory operations and methods for analyzing experiment results. To automate experiments, we introduce execution agents that break multi-step experimental procedures into agent-based state machines, interact with other agents to execute each step and analyze the experiment results. The analyzed results are then utilized to drive state transitions, enabling closed-loop feedback control. To demonstrate its capabilities, we applied the agents to calibrate and operate a superconducting quantum processor, where they autonomously planned and executed experiments for hours, successfully producing and characterizing entangled quantum states at the level achieved by human scientists. Our knowledge-based agent system opens up new possibilities for managing laboratory knowledge and accelerating scientific discovery.

### 摘要
全自动化自主实验室有望通过减少重复性劳动实现高通量和大规模科学发现。然而，有效自动化需要深度整合实验室知识，这些知识通常是非结构化、多模态且难以融入当前人工智能系统的。本文提出k-agents框架，旨在协助实验人员组织实验室知识并通过智能体实现实验自动化。该框架采用基于大语言模型的智能体来封装实验室知识，包括可用实验操作和实验结果分析方法。为实现实验自动化，我们引入执行智能体，将多步骤实验流程分解为基于智能体的状态机，通过与其他智能体交互执行各步骤并分析实验结果。分析结果随后用于驱动状态转换，实现闭环反馈控制。为验证其能力，我们将智能体应用于超导量子处理器的校准与操作，这些智能体自主规划并执行数小时实验，成功制备并表征了达到人类科学家水平的纠缠量子态。我们的知识驱动智能体系统为管理实验室知识和加速科学发现开辟了新途径。

---

## [Do Large Language Models Reason Causally Like Us? Even Better?](https://arxiv.org/abs/2502.10215)

### Abstract
arXiv:2502.10215v2 Announce Type: replace 
Abstract: Causal reasoning is a core component of intelligence. Large language models (LLMs) have shown impressive capabilities in generating human-like text, raising questions about whether their responses reflect true understanding or statistical patterns. We compared causal reasoning in humans and four LLMs using tasks based on collider graphs, rating the likelihood of a query variable occurring given evidence from other variables. LLMs' causal inferences ranged from often nonsensical (GPT-3.5) to human-like to often more normatively aligned than those of humans (GPT-4o, Gemini-Pro, and Claude). Computational model fitting showed that one reason for GPT-4o, Gemini-Pro, and Claude's superior performance is they didn't exhibit the "associative bias" that plagues human causal reasoning. Nevertheless, even these LLMs did not fully capture subtler reasoning patterns associated with collider graphs, such as "explaining away".

### 摘要
因果推理是智能的核心组成部分。大型语言模型（LLMs）在生成类人文本方面展现出卓越能力，这引发了对它们响应究竟反映真实理解还是统计模式的质疑。我们通过基于碰撞图的任务比较了人类与四种LLMs的因果推理能力，评估在给定其他变量证据时查询变量发生的可能性。LLMs的因果推理表现各异：从经常无逻辑（GPT-3.5）到类人水平，甚至时常比人类更符合规范标准（GPT-4o、Gemini-Pro和Claude）。计算模型拟合表明，GPT-4o、Gemini-Pro和Claude性能优越的一个原因在于它们未表现出困扰人类因果推理的"关联性偏差"。然而，即使这些先进模型也未能完全捕捉与碰撞图相关的更微妙推理模式，例如"解释消除"效应。

---

## [Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires](https://arxiv.org/abs/2503.00566)

### Abstract
arXiv:2503.00566v2 Announce Type: replace 
Abstract: The Los Angeles wildfires of January 2025 caused more than 250 billion dollars in damage and lasted for nearly an entire month before containment. Following our previous work, the Digital Twin Building, we modify and leverage the multi-agent large language model framework as well as the cloud-mapping integration to study the air quality during the Los Angeles wildfires. Recent advances in large language models have allowed for out-of-the-box automated large-scale data analysis. We use a multi-agent large language system comprised of an Instructor agent and Worker agents. Upon receiving the users' instructions, the Instructor agent retrieves the data from the cloud platform and produces instruction prompts to the Worker agents. The Worker agents then analyze the data and provide summaries. The summaries are finally input back into the Instructor agent, which then provides the final data analysis. We test this system's capability for data-based policy recommendation by assessing our Instructor-Worker LLM system's health recommendations based on air quality during the Los Angeles wildfires.

### 摘要
基于我们先前提出的数字孪生建筑研究，本研究改进并利用多智能体大语言模型框架及云图集成技术，对洛杉矶野火期间的空气质量进行分析。大语言模型的最新进展使得开箱即用的大规模自动化数据分析成为可能。我们采用由指导型智能体和工作型智能体构成的多智能体大语言系统：当接收用户指令后，指导型智能体从云平台检索数据并生成指令提示给工作型智能体；工作型智能体执行数据分析并提交摘要；最终这些摘要被反馈至指导型智能体以生成最终分析结果。通过评估我们基于指导-工作型大语言模型系统在洛杉矶野火期间根据空气质量数据提出的健康建议，测试了该系统在数据驱动型政策推荐方面的能力。

---

## [Investigating Non-Transitivity in LLM-as-a-Judge](https://arxiv.org/abs/2502.14074)

### Abstract
arXiv:2502.14074v3 Announce Type: replace 
Abstract: Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0% -&gt; 96.4% and 82.1% -&gt; 86.3% respectively). To address the computational cost of round-robin tournaments, we propose Swiss-Wise Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency.

### 摘要
基于大语言模型（LLM）的自动评估方法正逐渐成为评估基于LLM的智能体指令遵循能力的标准工具。该范式中最常见的方法——与基线模型的成对比较——关键依赖于偏好传递性假设。然而，这一假设的有效性在很大程度上仍未得到验证。本研究探讨了AlpacaEval框架中非传递性偏好的存在，并分析了其对模型排名的影响。我们发现LLM评估者表现出非传递性偏好，导致排名结果对基线模型的选择具有敏感性。为缓解此问题，我们证明循环赛结合Bradley-Terry偏好模型可生成更可靠的排名。值得注意的是，该方法与Chatbot Arena的斯皮尔曼相关系数（95.0%→96.4%）和肯德尔相关系数（82.1%→86.3%）均有所提升。针对循环赛的计算成本问题，我们提出瑞士制迭代配对（Swim）锦标赛，通过动态匹配策略在保持计算效率的同时获取循环赛的优势。

---

## [DebFlow: Automating Agent Creation via Agent Debate](https://arxiv.org/abs/2503.23781)

### Abstract
arXiv:2503.23781v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated strong potential and impressive performance in automating the generation and optimization of workflows. However, existing approaches are marked by limited reasoning capabilities, high computational demands, and significant resource requirements. To address these issues, we propose DebFlow, a framework that employs a debate mechanism to optimize workflows and integrates reflexion to improve based on previous experiences. We evaluated our method across six benchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approach achieved a 3\% average performance improvement over the latest baselines, demonstrating its effectiveness in diverse problem domains. In particular, during training, our framework reduces resource consumption by 37\% compared to the state-of-the-art baselines. Additionally, we performed ablation studies. Removing the Debate component resulted in a 4\% performance drop across two benchmark datasets, significantly greater than the 2\% drop observed when the Reflection component was removed. These findings strongly demonstrate the critical role of Debate in enhancing framework performance, while also highlighting the auxiliary contribution of reflexion to overall optimization.

### 摘要
大型语言模型（LLMs）在自动化工作流生成与优化方面展现出强大潜力与卓越性能，但现有方法存在推理能力有限、计算需求高、资源消耗大等问题。为此，我们提出DebFlow框架，该框架通过辩论机制优化工作流，并整合反思机制以实现基于历史经验的改进。我们在HotpotQA、MATH和ALFWorld等六个基准数据集上评估了该方法，其性能较最新基线平均提升3%，证明了其在多问题领域的有效性。特别值得注意的是，训练过程中本框架资源消耗较最先进基线降低37%。消融研究表明：移除辩论组件导致两个基准数据集性能下降4%，显著大于移除反思组件时观察到的2%降幅。这些发现有力证明了辩论机制对提升框架性能的关键作用，同时揭示了反思机制对整体优化的辅助贡献。

---

## [Artificial Intelligence in Creative Industries: Advances Prior to 2025](https://arxiv.org/abs/2501.02725)

### Abstract
arXiv:2501.02725v3 Announce Type: replace 
Abstract: The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries, enabling more innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores these technological shifts, with particular focus on how those that have emerged since our previous review in 2022 have expanded creative opportunities and improved efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss the integration of AI into post-production workflows, which has significantly accelerated and improved traditional processes. Once content has been created, it must be delivered to its audiences the media industry is facing the demands of increased communication traffic due to creative content. We therefore include a discussion of how AI is beginning to transform the way we represent and compress media content. We highlight the trend toward unified AI frameworks capable of addressing and integrating multiple creative tasks, and we underscore the importance of human insight to drive the creative process and oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges and to maximize its benefits while addressing the associated risks.

### 摘要
人工智能（AI）尤其是生成式AI与大语言模型（LLM）的快速发展，已对创意产业产生深远影响：推动更具创新性的内容创作、优化工作流程并降低创意工具的使用门槛。本文系统梳理了这些技术变革，重点分析了自2022年综述以来新兴技术如何拓展创作可能性并提升效率。技术进步显著增强了文本到图像、文本到视频及多模态生成技术的性能——LLM领域的关键突破为对话式AI树立了新基准，图像生成器的演进则彻底改变了内容创作范式。我们同时探讨了AI融入后期制作流程后对传统工序的提质增效作用。在内容创作完成后，媒体行业还面临着创意内容激增带来的传播流量压力，因此本文亦论述AI如何重塑媒体内容的表征与压缩方式。研究揭示了创意任务整合化的发展趋势，即统一AI框架可协同处理多项创作任务，同时强调人类洞察力对驱动创作过程的核心价值，以及监管机制对规避AI生成谬误的必要性。最后，我们展望了AI在创意领域的未来潜力，指出在应对新兴挑战的同时，必须通过风险管控以实现效益最大化。

---

## [BoA: Attention-aware Post-training Quantization without Backpropagation](https://arxiv.org/abs/2406.13474)

### Abstract
arXiv:2406.13474v3 Announce Type: replace-cross 
Abstract: Post-training quantization (PTQ) is a promising solution for deploying large language models (LLMs) on resource-constrained devices. Early methods developed for small-scale networks, such as ResNet, rely on gradient-based optimization, which becomes impractical for hyper-scale LLMs with billions of parameters. While recently proposed backpropagation-free or transformation-based methods alleviate this issue, they ignore inter-layer interactions or use the naive nearest-rounding-based quantized weight assignment to save the heavy computational cost of weight optimization. In this paper, we introduce a novel backpropagation-free PTQ algorithm that optimizes quantized weights by considering inter-layer dependencies. The key innovation is the development of attention-aware Hessian matrices that capture inter-layer interactions within the attention module. Extensive experiments demonstrate that our approach not only outperforms existing weight quantization methods but also shows good synergy with conventional methods to suppress activation outliers, leading to state-of-the-art weight-activation quantization performance. The code will be available at https://github.com/SamsungLabs/BoA.

### 摘要
训练后量化（PTQ）是一种在资源受限设备上部署大语言模型（LLM）的有效方案。早期针对小规模网络（如ResNet）开发的方法依赖于基于梯度的优化，这在处理具有数十亿参数的超大规模LLM时变得不切实际。虽然最近提出的免反向传播或基于变换的方法缓解了这一问题，但它们忽略了层间相互作用，或采用简单的最近舍入量化权重分配策略以节省权重优化的高昂计算成本。本文提出一种新颖的免反向传播PTQ算法，该算法通过考虑层间依赖性来优化量化权重。关键创新在于开发了注意力感知的Hessian矩阵，该矩阵能够捕捉注意力模块内的层间交互作用。大量实验表明，我们的方法不仅优于现有权重量化技术，还能与传统方法良好协同以抑制激活异常值，从而实现最先进的权重-激活量化性能。代码将在https://github.com/SamsungLabs/BoA发布。

---

## [Multi-Agent Collaboration via Cross-Team Orchestration](https://arxiv.org/abs/2406.08979)

### Abstract
arXiv:2406.08979v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have significantly impacted various domains, especially through organized LLM-driven autonomous agents. A representative scenario is in software development, where agents can collaborate in a team like humans, following predefined phases to complete sub-tasks sequentially. However, for an agent team, each phase yields only one possible outcome. This results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space. Consequently leading to suboptimal results or extensive trial and error. To address this, we introduce Cross-Team Orchestration (Croto), a scalable multi-team framework that enables orchestrated teams to jointly propose various task-oriented solutions and interact with their insights in a self-independence while cross-team collaboration environment for superior solutions generation. Experiments reveal a notable increase in software quality compared to state-of-the-art baselines. We further tested our framework on story generation tasks, which demonstrated a promising generalization ability of our framework in other domains. The code and data is available at https://github.com/OpenBMB/ChatDev/tree/macnet

### 摘要
大型语言模型（LLMs）已对多个领域产生显著影响，尤其通过组织化的LLM驱动自主智能体实现。一个典型应用场景是软件开发，其中智能体可像人类一样在团队中协作，遵循预定义阶段顺序完成子任务。然而对于智能体团队而言，每个阶段仅产生一种可能结果，这导致仅能完成单一开发链条，丧失了在解空间中探索多种潜在决策路径的机会，从而产生次优结果或需要大量试错。为此，我们提出跨团队协同框架（Croto），该可扩展的多团队架构使协同团队能共同提出多种任务导向解决方案，并在保持自主性的跨团队协作环境中通过观点交互生成更优解。实验表明相较最先进基线方法，该框架显著提升了软件质量。我们进一步在故事生成任务上测试本框架，结果展现出该框架在其他领域良好的泛化能力。代码与数据详见https://github.com/OpenBMB/ChatDev/tree/macnet

---

## [The Structure of Financial Equity Research Reports -- Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4](https://arxiv.org/abs/2407.18327)

### Abstract
arXiv:2407.18327v2 Announce Type: replace-cross 
Abstract: This research dissects financial equity research reports (ERRs) by mapping their content into categories. There is insufficient empirical analysis of the questions answered in ERRs. In particular, it is not understood how frequently certain information appears, what information is considered essential, and what information requires human judgment to distill into an ERR. The study analyzes 72 ERRs sentence-by-sentence, classifying their 4940 sentences into 169 unique question archetypes. We did not predefine the questions but derived them solely from the statements in the ERRs. This approach provides an unbiased view of the content of the observed ERRs. Subsequently, we used public corporate reports to classify the questions' potential for automation. Answers were labeled "text-extractable" if the answers to the question were accessible in corporate reports. 78.7% of the questions in ERRs can be automated. Those automatable question consist of 48.2% text-extractable (suited to processing by large language models, LLMs) and 30.5% database-extractable questions. Only 21.3% of questions require human judgment to answer. We empirically validate using Llama-3-70B and GPT-4-turbo-2024-04-09 that recent advances in language generation and information extraction enable the automation of approximately 80% of the statements in ERRs. Surprisingly, the models complement each other's strengths and weaknesses well. The research confirms that the current writing process of ERRs can likely benefit from additional automation, improving quality and efficiency. The research thus allows us to quantify the potential impacts of introducing large language models in the ERR writing process. The full question list, including the archetypes and their frequency, will be made available online after peer review.

### 摘要
本研究通过内容分类对金融股权研究报告（ERRs）进行剖析。目前对ERRs中所解答问题的实证分析不足，尤其是特定信息出现的频率、哪些信息被视为核心要素，以及哪些信息需要人工判断才能提炼至ERR中等问题尚未明晰。该研究逐句分析了72份ERRs，将其中的4940个句子归类为169个独特的问题原型。我们未预先设定问题类型，而是完全基于ERRs中的陈述进行推导，这种方法为观察到的ERRs内容提供了无偏见的视角。随后，我们利用公开企业报告对这些问题的自动化潜力进行分类：若问题答案可直接从企业报告中提取，则标记为"文本可提取"。结果显示ERRs中78.7%的问题可实现自动化，其中48.2%属于适合大语言模型（LLMs）处理的文本可提取问题，30.5%为数据库可提取问题，仅21.3%的问题需依赖人工判断作答。我们使用Llama-3-70B和GPT-4-turbo-2024-04-09进行实证验证，证实语言生成与信息提取技术的最新进展可使约80%的ERR陈述实现自动化。值得注意的是，这两种模型能有效互补彼此的优劣。研究证实当前ERRs撰写流程有望通过增加自动化来提升质量与效率，从而量化了大语言模型引入ERR撰写流程的潜在影响。完整问题列表（含原型及其出现频率）将在同行评议后在线公开。

---

## [PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org/abs/2410.08811)

### Abstract
arXiv:2410.08811v2 Announce Type: replace-cross 
Abstract: Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 21 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.

### 摘要
偏好学习是当前大型语言模型对齐的核心组件，但这一过程可能面临数据投毒攻击的威胁。为解决这一问题，我们推出PoisonBench——一个用于评估大型语言模型在偏好学习过程中对数据投毒脆弱性的基准测试框架。数据投毒攻击可通过操纵模型响应来植入隐蔽的恶意内容或偏见，导致模型在看似正常运行的情况下生成有害或非预期输出。我们在八种现实场景中部署了两种不同类型的攻击，对21个常用模型进行了评估。研究发现存在三个值得关注的趋势：(1) 参数规模扩大并不能天然增强模型对投毒攻击的抵抗能力；(2) 攻击效果与数据投毒比例之间存在对数线性关系；(3) 数据投毒的影响可泛化至未包含在污染数据中的外推触发词。这些结果揭示了当前偏好学习技术的缺陷，凸显了针对恶意模型和数据操纵行为开发更强防御机制的紧迫性。

---

## [The Impact of Inference Acceleration on Bias of LLMs](https://arxiv.org/abs/2410.22118)

### Abstract
arXiv:2410.22118v3 Announce Type: replace-cross 
Abstract: Last few years have seen unprecedented advances in capabilities of Large Language Models (LLMs). These advancements promise to benefit a vast array of application domains. However, due to their immense size, performing inference with LLMs is both costly and slow. Consequently, a plethora of recent work has proposed strategies to enhance inference efficiency, e.g., quantization, pruning, and caching. These acceleration strategies reduce the inference cost and latency, often by several factors, while maintaining much of the predictive performance measured via common benchmarks. In this work, we explore another critical aspect of LLM performance: demographic bias in model generations due to inference acceleration optimizations. Using a wide range of metrics, we probe bias in model outputs from a number of angles. Analysis of outputs before and after inference acceleration shows significant change in bias. Worryingly, these bias effects are complex and unpredictable. A combination of an acceleration strategy and bias type may show little bias change in one model but may lead to a large effect in another. Our results highlight a need for in-depth and case-by-case evaluation of model bias after it has been modified to accelerate inference.

### 摘要
近年来，大型语言模型（LLMs）的能力取得了前所未有的突破。这些进步有望惠及众多应用领域。然而，由于其庞大的规模，LLMs的推理过程既昂贵又缓慢。因此，近期大量研究提出了提升推理效率的策略，如量化、剪枝和缓存等。这些加速策略通常能在保持基准测试预测性能的同时，将推理成本和延迟降低数倍。本研究探讨了LLM性能的另一个关键维度：推理加速优化导致的模型生成结果中的群体偏见。通过多种度量指标，我们从不同角度分析了模型输出中的偏见现象。对比加速前后的输出结果表明，偏见程度发生了显著变化。令人担忧的是，这些偏见效应具有复杂性和不可预测性——某种加速策略与偏见类型的组合可能在一个模型中仅产生微小变化，而在另一个模型中却引发显著影响。我们的研究结果强调，在对模型进行推理加速优化后，需要针对具体案例开展深入的偏见评估。

---

## [AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML](https://arxiv.org/abs/2410.02958)

### Abstract
arXiv:2410.02958v2 Announce Type: replace-cross 
Abstract: Automated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort. Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes AutoML-Agent, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. AutoML-Agent takes user's task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into sub-tasks (e.g., data preprocessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed results and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that AutoML-Agent achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.

### 摘要
自动化机器学习（AutoML）通过自动化开发流程中的任务（如最优模型搜索和超参数调优）来加速人工智能发展。现有AutoML系统通常需要技术专家设置复杂工具，这一过程耗时且需大量人力。因此，近期研究开始利用大语言模型（LLM）减轻此类负担，并通过自然语言界面提升AutoML框架的易用性，使非专业用户也能构建数据驱动解决方案。然而，这些方法通常仅针对AI开发流程中的特定环节设计，未能有效利用LLM的固有能力。本文提出AutoML-Agent——一个专为全流程AutoML（从数据检索到模型部署）设计的多智能体框架。该系统接收用户任务描述，协调专业化LLM智能体间的协作，最终交付可部署模型。与现有工作不同，我们引入检索增强的规划策略以增强探索性，从而搜索更优方案，而非制定单一计划。同时，我们将每个计划分解为子任务（如数据预处理和神经网络设计），每个子任务由我们通过提示构建的专用智能体并行执行，从而提高搜索效率。此外，我们提出多阶段验证机制来核查执行结果，并指导代码生成LLM实现成功解决方案。基于14个数据集在7项下游任务中的大量实验表明，AutoML-Agent在实现全流程自动化方面具有更高成功率，所构建系统在多个领域均表现优异。

---

## [ProofAug: Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis](https://arxiv.org/abs/2501.18310)

### Abstract
arXiv:2501.18310v2 Announce Type: replace-cross 
Abstract: The synergy between deep learning models and traditional automation tools, such as built-in tactics of the proof assistant and off-the-shelf automated theorem provers, plays a crucial role in developing robust and efficient neural theorem provers(NTPs). However, for proof synthesis with LLMs, previous work applies automation tools either only when explicitly invoked by the model or at a single granularity level, failing to fully exploit their power. To solve this issue, we propose ProofAug, a procedure that equips LLMs with automation methods at various granularities through fine-grained structure analysis of model-generated proof proposals. ProofAug also serves as a versatile plug-and-play module that seamlessly integrates with any tree-search algorithm, enabling our construction of an efficient recursive proving (ERP) module to further enhance performance. The superiority of our method is validated on the miniF2F benchmark using the open-source deepseek-math-7b-base model and the Isabelle proof assistant. Notably, by additionally employing a mixed prompting strategy, we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9% for the original version) with 2100 queries to the model per problem (In contrast, the previous SOTA in Isabelle, Subgoal-XL, only achieves 56.1% using 16384 queries per problem). We also implement a Lean 4 version of ProofAug that can improve the pass@1 performance of Kimina-Prover-Preview-Distill-1.5B from 44.3% to 50.4% on miniF2F-test. Our code is available at https://github.com/haoxiongliu/ProofAug.

---

## [The Synergy of LLMs &amp; RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data](https://arxiv.org/abs/2412.06877)

### Abstract
arXiv:2412.06877v2 Announce Type: replace-cross 
Abstract: Developing autonomous agents capable of performing complex, multi-step decision-making tasks specified in natural language remains a significant challenge, particularly in realistic settings where labeled data is scarce and real-time experimentation is impractical. Existing reinforcement learning (RL) approaches often struggle to generalize to unseen goals and states, limiting their applicability. In this paper, we introduce TEDUO, a novel training pipeline for offline language-conditioned policy learning in symbolic environments. Unlike conventional methods, TEDUO operates on readily available, unlabeled datasets and addresses the challenge of generalization to previously unseen goals and states. Our approach harnesses large language models (LLMs) in a dual capacity: first, as automatization tools augmenting offline datasets with richer annotations, and second, as generalizable instruction-following agents. Empirical results demonstrate that TEDUO achieves data-efficient learning of robust language-conditioned policies, accomplishing tasks beyond the reach of conventional RL frameworks or out-of-the-box LLMs alone.

### 摘要
开发能够执行自然语言指定的复杂多步决策任务的自主代理仍面临重大挑战，尤其在标注数据稀缺且实时实验不切实际的现实场景中。现有强化学习（RL）方法通常难以泛化至未见过的目标与状态，限制了其适用性。本文提出TEDUO——一种面向符号化环境的离线语言条件策略学习新型训练框架。与传统方法不同，TEDUO直接利用现成的未标注数据集，并解决了向未知目标与状态泛化的难题。我们的方法通过双重机制利用大语言模型（LLMs）：首先作为自动化工具为离线数据集生成更丰富的标注，其次作为可泛化的指令遵循代理。实验结果表明，TEDUO能以较高数据效率学习到鲁棒的语言条件策略，完成传统RL框架或单独LLMs无法实现的任务。

---

## [Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Audio-Language Models](https://arxiv.org/abs/2411.14842)

### Abstract
arXiv:2411.14842v2 Announce Type: replace-cross 
Abstract: Adversarial audio attacks pose a significant threat to the growing use of large audio-language models (LALMs) in voice-based human-machine interactions. While existing research focused on model-specific adversarial methods, real-world applications demand a more generalizable and universal approach to audio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks (CAA) benchmark including four distinct types of audio attacks, which aims to explore the vulnerabilities of LALMs to these audio attacks in conversational scenarios. To evaluate the robustness of LALMs, we propose three evaluation strategies: Standard Evaluation, utilizing traditional metrics to quantify model performance under attacks; GPT-4o-Based Evaluation, which simulates real-world conversational complexities; and Human Evaluation, offering insights into user perception and trust. We evaluate six state-of-the-art LALMs with voice interaction capabilities, including Gemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on the CAA benchmark. Our comprehensive analysis reveals the impact of four types of audio attacks on the performance of these models, demonstrating that GPT-4o exhibits the highest level of resilience. Our data can be accessed via the following link: \href&#123;https://github.com/crystraldo/CAA&#125;&#123;CAA&#125;.

### 摘要
对抗性音频攻击对基于语音的人机交互中大型音频语言模型（LALMs）的广泛应用构成了严重威胁。尽管现有研究主要关注针对特定模型的对抗方法，但实际应用需要更具普适性和通用性的音频对抗攻击方案。本文提出包含四种不同类型音频攻击的Chat-Audio Attacks（CAA）基准测试，旨在探究LALMs在对话场景中对这些音频攻击的脆弱性。为评估LALMs的鲁棒性，我们提出三种评估策略：标准评估——采用传统指标量化模型在攻击下的性能表现；基于GPT-4o的评估——模拟真实对话场景的复杂性；以及人类评估——获取用户感知与信任度的深入洞察。我们在CAA基准上使用三种不同评估方法对六种具备语音交互能力的先进LALMs（包括Gemini-1.5-Pro、GPT-4o等）进行了测试。综合分析揭示了四类音频攻击对这些模型性能的影响，结果表明GPT-4o展现出最强的抗攻击能力。相关数据可通过以下链接获取：\href&#123;https://github.com/crystraldo/CAA&#125;&#123;CAA&#125;。

---

## [Reasoning Through Execution: Unifying Process and Outcome Rewards for Code Generation](https://arxiv.org/abs/2412.15118)

### Abstract
arXiv:2412.15118v2 Announce Type: replace-cross 
Abstract: Large Language Models excel at code generation yet struggle with complex programming tasks that demand sophisticated reasoning. To bridge this gap, traditional process supervision relies on learned reward models requiring costly training data and suffering from reward misalignment, while outcome supervision fails for complex tasks needing coordinated intermediate steps. We introduce Outcome Refining Process Supervision, which unifies process and outcome supervision by leveraging executable verification: a tree-structured search framework generates strategic alternatives, profiles execution metrics, and scores candidates via self-critique mechanisms that integrate runtime feedback with reasoning. Experiments across 5 models and 3 benchmarks show consistent gains, with 26.9% higher correctness and 42.2% improved code efficiency. The results demonstrate that ORPS enables LLMs to overcome local optima in code generation, suggesting a promising direction for combining verifiable outcomes with structured reasoning to tackle complex challenges. We open-source at: https://github.com/zhuohaoyu/ORPS

### 摘要
大型语言模型在代码生成方面表现出色，但在需要复杂推理的编程任务中仍存在困难。为弥补这一缺陷，传统过程监督依赖于需要昂贵训练数据的奖励模型，且存在奖励失准问题；而结果监督则无法应对需要协调中间步骤的复杂任务。我们提出结果精化过程监督方法，通过可执行验证统一过程与结果监督：该树状结构搜索框架能生成策略性备选方案，分析执行指标，并通过整合运行时反馈与推理的自批判机制对候选方案进行评分。在5种模型和3个基准测试中的实验显示，该方法带来26.9%的正确率提升和42.2%的代码效率改进。结果表明，ORPS能使LLM克服代码生成中的局部最优问题，为结合可验证结果与结构化推理以应对复杂挑战提供了新方向。项目已开源：https://github.com/zhuohaoyu/ORPS

---

## [Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency](https://arxiv.org/abs/2411.16525)

### Abstract
arXiv:2411.16525v2 Announce Type: replace-cross 
Abstract: We investigate the statistical and computational limits of prompt tuning for transformer-based foundation models. Our key contributions are prompt tuning on \emph&#123;single-head&#125; transformers with only a \emph&#123;single&#125; self-attention layer: (i) is universal, and (ii) supports efficient (even almost-linear time) algorithms under the Strong Exponential Time Hypothesis (SETH). Statistically, we prove that prompt tuning on such simplest possible transformers are universal approximators for sequence-to-sequence Lipschitz functions. In addition, we provide an exponential-in-$dL$ and -in-$(1/\epsilon)$ lower bound on the required soft-prompt tokens for prompt tuning to memorize any dataset with 1-layer, 1-head transformers. Computationally, we identify a phase transition in the efficiency of prompt tuning, determined by the norm of the \emph&#123;soft-prompt-induced&#125; keys and queries, and provide an upper bound criterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for prompt tuning exists under SETH. Within this criterion, we showcase our theory by proving the existence of almost-linear time prompt tuning inference algorithms. These fundamental limits provide important necessary conditions for designing expressive and efficient prompt tuning methods for practitioners.

### 摘要
我们研究了基于Transformer基础模型的提示调优在统计与计算层面的极限。主要贡献在于针对仅含单层自注意力机制的单头Transformer模型证明：(i) 提示调优具有普适性；(ii) 在强指数时间假设(SETH)下支持高效（甚至近线性时间）算法。统计层面，我们证实此类极简Transformer架构的提示调优可作为序列到序列Lipschitz函数的通用逼近器。此外，针对单层单头Transformer，我们给出了提示调优记忆任意数据集所需软提示标记数的$dL$和$(1/\epsilon)$指数级下界。计算层面，我们发现提示调优效率存在由软提示诱导键值对范数决定的相变现象，并给出上界判据。超越该判据时，SETH条件下不存在次二次方（高效）提示调优算法；在此判据内，我们通过证明近线性时间提示调优推理算法的存在性验证理论。这些基础极限为设计兼具表达力与效率的实用提示调优方法提供了重要必要条件。

---

## [pLDDT-Predictor: High-speed Protein Screening Using Transformer and ESM2](https://arxiv.org/abs/2410.21283)

### Abstract
arXiv:2410.21283v3 Announce Type: replace-cross 
Abstract: Recent advancements in protein structure prediction, particularly AlphaFold2, have revolutionized structural biology by achieving near-experimental accuracy ($\text&#123;average RMSD&#125; &lt; 1.5\text&#123;\AA&#125;$). However, the computational demands of these models (approximately 30 minutes per protein on an RTX 4090) significantly limit their application in high-throughput protein screening. While large language models like ESM (Evolutionary Scale Modeling) have shown promise in extracting structural information directly from protein sequences, rapid assessment of protein structure quality for large-scale analyses remains a major challenge.
  We introduce pLDDT-Predictor, a high-speed protein screening tool that achieves a $250,000\times$ speedup compared to AlphaFold2 by leveraging pre-trained ESM2 protein embeddings and a Transformer architecture. Our model predicts AlphaFold2's pLDDT (predicted Local Distance Difference Test) scores with a Pearson correlation of 0.7891 and processes proteins in just 0.007 seconds on average. Using a comprehensive dataset of 1.5 million diverse protein sequences (ranging from 50 to 2048 amino acids), we demonstrate that pLDDT-Predictor accurately classifies high-confidence structures (pLDDT $&gt;$ 70) with 91.2\% accuracy and achieves an MSE of 84.8142 compared to AlphaFold2's predictions.
  The source code and pre-trained models are freely available at https://github.com/jw-chae/pLDDT_Predictor, enabling the research community to perform rapid, large-scale protein structure quality assessments.

### 摘要
蛋白质结构预测领域的最新进展，特别是AlphaFold2，通过实现接近实验精度的预测（平均均方根偏差&lt;1.5Å）引发了结构生物学的革命。然而这些模型的计算需求（RTX 4090显卡上每个蛋白质约需30分钟）极大限制了其在高通量蛋白质筛选中的应用。虽然ESM（进化尺度建模）等大型语言模型已展现出直接从蛋白质序列提取结构信息的潜力，但如何快速评估大规模分析中的蛋白质结构质量仍是重大挑战。

我们开发了pLDDT-Predictor这一高速蛋白质筛选工具，通过利用预训练的ESM2蛋白质嵌入和Transformer架构，实现了相比AlphaFold2 250,000倍的加速。该模型预测AlphaFold2的pLDDT（局部距离差异测试）分数时达到0.7891的皮尔逊相关系数，平均每个蛋白质仅需0.007秒处理时间。基于包含150万个多样化蛋白质序列（长度50至2048个氨基酸不等）的综合数据集，我们证明pLDDT-Predictor能以91.2%的准确率分类高置信度结构（pLDDT&gt;70），与AlphaFold2预测结果相比的均方误差为84.8142。

源代码和预训练模型已公开于https://github.com/jw-chae/pLDDT_Predictor，供研究社区快速开展大规模蛋白质结构质量评估。

---

## [CoopetitiveV: Leveraging LLM-powered Coopetitive Multi-Agent Prompting for High-quality Verilog Generation](https://arxiv.org/abs/2412.11014)

### Abstract
arXiv:2412.11014v2 Announce Type: replace-cross 
Abstract: Recent advances in agentic LLMs have demonstrated great capabilities in Verilog code generation. However, existing approaches either use LLM-assisted single-agent prompting or cooperation-only multi-agent learning, which will lead to: (i) Degeneration issue for single-agent learning: characterized by diminished error detection and correction capabilities; (ii) Error propagation in cooperation-only multi-agent learning: erroneous information from the former agent will be propagated to the latter through prompts, which can make the latter agents generate buggy code. In this paper, we propose an LLM-based coopetitive multi-agent prompting framework, in which the agents cannot collaborate with each other to form the generation pipeline, but also create a healthy competitive mechanism to improve the generating quality. Our experimental results show that the coopetitive multi-agent framework can effectively mitigate the degeneration risk and reduce the error propagation while improving code error correction capabilities, resulting in higher quality Verilog code generation. The effectiveness of our approach is validated through extensive experiments. On VerilogEval Machine and Human dataset, CoopetitiveV+GPT-4 achieves 99.2% and 99.1% pass@10 scores, respectively. While on RTLLM, CoopetitiveV+GPT-4 obtains 100% syntax and 99.9% functionality pass@5 scores.

### 摘要
智能体大语言模型（LLM）的最新进展在Verilog代码生成方面展现出强大能力。然而现有方法要么采用LLM辅助的单智能体提示，要么仅使用协作式多智能体学习，这将导致：（i）单智能体学习的退化问题：表现为错误检测与纠正能力下降；（ii）纯协作多智能体学习的错误传播：前序智能体的错误信息会通过提示传播至后续智能体，导致后者生成缺陷代码。本文提出基于LLM的竞合多智能体提示框架，其中智能体既能协作构建生成管道，又能建立良性竞争机制以提升生成质量。实验结果表明，该竞合多智能体框架可有效缓解退化风险、减少错误传播并增强代码纠错能力，从而实现更高质量的Verilog代码生成。通过大量实验验证了方法的有效性：在VerilogEval机器与人类数据集上，CoopetitiveV+GPT-4分别达到99.2%和99.1%的pass@10分数；在RTLLM数据集上，CoopetitiveV+GPT-4获得100%语法通过率和99.9%功能通过率的pass@5分数。

---

## [The Complexity of Learning Sparse Superposed Features with Feedback](https://arxiv.org/abs/2502.05407)

### Abstract
arXiv:2502.05407v3 Announce Type: replace-cross 
Abstract: The success of deep networks is crucially attributed to their ability to capture latent features within a representation space. In this work, we investigate whether the underlying learned features of a model can be efficiently retrieved through feedback from an agent, such as a large language model (LLM), in the form of relative \textit&#123;triplet comparisons&#125;. These features may represent various constructs, including dictionaries in LLMs or a covariance matrix of Mahalanobis distances. We analyze the feedback complexity associated with learning a feature matrix in sparse settings. Our results establish tight bounds when the agent is permitted to construct activations and demonstrate strong upper bounds in sparse scenarios when the agent's feedback is limited to distributional information. We validate our theoretical findings through experiments on two distinct applications: feature recovery from Recursive Feature Machines and dictionary extraction from sparse autoencoders trained on Large Language Models.

### 摘要
深度网络的成功关键归因于其捕捉表征空间中潜在特征的能力。本研究探讨是否能够通过智能体（如大型语言模型LLM）以相对三元组比较的形式反馈，高效检索模型底层学习到的特征。这些特征可能代表多种结构，包括LLM中的字典或马氏距离的协方差矩阵。我们分析了稀疏场景下学习特征矩阵所需的反馈复杂度，在允许智能体构建激活的情况下建立了紧致界，并在智能体反馈仅限于分布信息时证明了稀疏场景下的强上界。通过两类应用实验验证了理论发现：从递归特征机中恢复特征，以及从大型语言模型训练的稀疏自编码器中提取字典。

---

## [FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting](https://arxiv.org/abs/2501.16029)

### Abstract
arXiv:2501.16029v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are rapidly transforming the landscape of digital content creation. However, the prevalent black-box Application Programming Interface (API) access to many LLMs introduces significant challenges in accountability, governance, and security. LLM fingerprinting, which aims to identify the source model by analyzing statistical and stylistic features of generated text, offers a potential solution. Current progress in this area is hindered by a lack of dedicated datasets and the need for efficient, practical methods that are robust against adversarial manipulations. To address these challenges, we introduce FD-Dataset, a comprehensive bilingual fingerprinting benchmark comprising 90,000 text samples from 20 famous proprietary and open-source LLMs. Furthermore, we present FDLLM, a novel fingerprinting method that leverages parameter-efficient Low-Rank Adaptation (LoRA) to fine-tune a foundation model. This approach enables LoRA to extract deep, persistent features that characterize each source LLM. Through our analysis, we find that LoRA adaptation promotes the aggregation of outputs from the same LLM in representation space while enhancing the separation between different LLMs. This mechanism explains why LoRA proves particularly effective for LLM fingerprinting. Extensive empirical evaluations on FD-Dataset demonstrate FDLLM's superiority, achieving a Macro F1 score 22.1% higher than the strongest baseline. FDLLM also exhibits strong generalization to newly released models, achieving an average accuracy of 95% on unseen models. Notably, FDLLM remains consistently robust under various adversarial attacks, including polishing, translation, and synonym substitution. Experimental results show that FDLLM reduces the average attack success rate from 49.2% (LM-D) to 23.9%.

### 摘要
大型语言模型（LLMs）正在迅速改变数字内容创作的格局。然而，当前多数LLM通过黑箱应用程序接口（API）提供服务，这在问责制、治理和安全性方面带来了重大挑战。LLM指纹识别技术通过分析生成文本的统计与风格特征来溯源模型，为此提供了潜在解决方案。该领域的发展目前受限于专用数据集的缺乏，以及需要高效、实用且能抵御对抗性操作的方法。针对这些挑战，我们提出了FD-Dataset——一个包含20个知名开源与专有LLM生成的90,000个文本样本的双语指纹识别基准数据集。此外，我们开发了FDLLM方法，利用参数高效的低秩自适应（LoRA）技术对基础模型进行微调，使LoRA能提取表征各源LLM的深层持久特征。分析表明，LoRA自适应能促进同一LLM输出在表征空间的聚集，同时增强不同LLM间的分离度，这解释了LoRA对LLM指纹识别的特殊有效性。在FD-Dataset上的大量实验证明，FDLLM以22.1%的宏观F1分数优势超越最强基线方法，对新发布模型也展现出95%的平均准确率。值得注意的是，FDLLM在面对文本润色、翻译和同义词替换等对抗攻击时保持稳定，将平均攻击成功率从49.2%（LM-D）降至23.9%。

---

## [Peri-LN: Revisiting Normalization Layer in the Transformer Architecture](https://arxiv.org/abs/2502.02732)

### Abstract
arXiv:2502.02732v3 Announce Type: replace-cross 
Abstract: Selecting a layer normalization (LN) strategy that stabilizes training and speeds convergence in Transformers remains difficult, even for today's large language models (LLM). We present a comprehensive analytical foundation for understanding how different LN strategies influence training dynamics in large-scale Transformers. Until recently, Pre-LN and Post-LN have long dominated practices despite their limitations in large-scale training. However, several open-source models have recently begun silently adopting a third strategy without much explanation. This strategy places normalization layer peripherally around sublayers, a design we term Peri-LN. While Peri-LN has demonstrated promising performance, its precise mechanisms and benefits remain almost unexplored. Our in-depth analysis delineates the distinct behaviors of LN strategies, showing how each placement shapes activation variance and gradient propagation. To validate our theoretical insight, we conduct extensive experiments on Transformers up to $3.2$B parameters, showing that Peri-LN consistently achieves more balanced variance growth, steadier gradient flow, and convergence stability. Our results suggest that Peri-LN warrants broader consideration for large-scale Transformer architectures, providing renewed insights into the optimal placement of LN.

### 摘要
在Transformer模型中选择一种能稳定训练并加速收敛的层归一化（LN）策略仍然具有挑战性，即使对于当今的大型语言模型（LLM）也是如此。我们提出了一个全面的分析框架，用于理解不同LN策略如何影响大规模Transformer的训练动态。长期以来，尽管Pre-LN和Post-LN在大规模训练中存在局限性，但它们一直主导着实践。然而，近期一些开源模型开始悄然采用第三种策略而未作过多解释。该策略将归一化层置于子层周围，我们称之为Peri-LN。尽管Peri-LN已展现出优异的性能，但其具体机制和优势几乎未被探索。我们的深入分析揭示了不同LN策略的独特行为，阐明了每种布局如何影响激活方差和梯度传播。为验证理论见解，我们在参数规模高达32亿的Transformer上进行了大量实验，结果表明Peri-LN始终能实现更平衡的方差增长、更稳定的梯度流动和收敛性。这些发现表明，Peri-LN值得在大规模Transformer架构中得到更广泛考虑，并为LN的最佳布局提供了新的见解。

---

## [RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models](https://arxiv.org/abs/2502.09003)

### Abstract
arXiv:2502.09003v3 Announce Type: replace-cross 
Abstract: Supervised fine-tuning is a standard method for adapting pre-trained large language models (LLMs) to downstream tasks. Quantization has been recently studied as a post-training technique for efficient LLM deployment. To obtain quantized fine-tuned LLMs, conventional pipelines would first fine-tune the pre-trained models, followed by post-training quantization. This often yields suboptimal performance as it fails to leverage the synergy between fine-tuning and quantization. To effectively realize low-bit quantization of weights, activations and KV caches in LLMs, we propose an algorithm named Rotated Straight-Through-Estimator (RoSTE), which combines quantization-aware supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that identifies an effective rotation configuration to reduce activation outliers. We provide theoretical insights on RoSTE by analyzing its prediction error when applied to an overparameterized least square quantized training problem. Our findings reveal that the prediction error is directly proportional to the quantization error of the converged weights, which can be effectively managed through an optimized rotation configuration. Experiments on Pythia, Qwen and Llama models of different sizes demonstrate the effectiveness of RoSTE. Compared to existing post-SFT quantization baselines, our method consistently achieves superior performances across various tasks and different LLM architectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE.

### 摘要
监督微调是将预训练大语言模型（LLMs）适配下游任务的标准方法。量化作为后训练技术近期被广泛研究以实现高效LLM部署。传统流程通常先对预训练模型进行微调，再实施后训练量化，这种分离操作由于未能利用微调与量化间的协同效应，往往导致次优性能。为实现LLMs中权重、激活值和KV缓存的高效低位量化，我们提出旋转直通估计器（RoSTE）算法，该算法将量化感知监督微调（QA-SFT）与自适应旋转策略相结合，通过识别最优旋转配置来减少激活值异常值。通过分析其在过参数化最小二乘量化训练问题中的预测误差，我们从理论上揭示了RoSTE的机制：预测误差与收敛权重的量化误差成正比，而后者可通过优化旋转配置有效控制。在Pythia、Qwen及不同规模Llama模型上的实验验证了RoSTE的有效性。与现有后SFT量化基线相比，我们的方法在不同任务和LLM架构中均表现出更优性能。代码已开源：https://github.com/OptimAI-Lab/RoSTE。

---

## [Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods](https://arxiv.org/abs/2502.01618)

### Abstract
arXiv:2502.01618v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code, videos, and further information available at https://probabilistic-inference-scaling.github.io.

### 摘要
大型语言模型（LLM）通过扩大模型规模和/或数据量取得了显著的性能提升。然而，近期证据表明此类方法的边际效益递减，这促使研究者转向在推理阶段进行算力扩展。现有推理阶段扩展方法通常借助奖励模型，将任务构建为搜索问题，但此类方法容易因奖励模型的近似误差导致奖励破解。本文创新性地将推理阶段扩展构建为概率推断任务，采用基于采样的技术来探索状态空间模型在近似似然下的状态分布典型集，而非直接优化其模态。我们提出一种新型推理阶段扩展方法，将基于粒子的蒙特卡洛方法适配至该任务。实证评估表明，在各类复杂数学推理任务上，我们的方法比确定性搜索方法具有4-16倍的更优扩展率。实验显示，采用本方法时Qwen2.5-Math-1.5B-Instruct仅需4次推演即可超越GPT-4o的准确率，而Qwen2.5-Math-7B-Instruct仅需32次推演即可达到o1级别准确率。本研究不仅提出了有效的推理阶段扩展方法，更将概率推断领域的丰富文献与LLM推理阶段扩展相结合，为未来开发更鲁棒的算法奠定了基础。代码、视频及更多信息详见https://probabilistic-inference-scaling.github.io。

---

## [TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking](https://arxiv.org/abs/2502.11187)

### Abstract
arXiv:2502.11187v3 Announce Type: replace-cross 
Abstract: In this paper, we present TituLLMs, the first large pretrained Bangla LLMs, available in 1b and 3b parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train TituLLMs, we collected a pretraining dataset of approximately ~37 billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference. There was a lack of benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we developed five benchmarking datasets. We benchmarked various LLMs, including TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the TituLLMs models and benchmarking datasets publicly available (https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).

### 摘要
在本论文中，我们提出了TituLLMs——首个大规模预训练的孟加拉语大语言模型，提供10亿和30亿两种参数量版本。由于训练和推理过程中的计算资源限制，我们专注于开发较小规模的模型。为训练TituLLMs，我们收集了约370亿token的预训练数据集，并通过扩展Llama-3.2的分词器来整合语言与文化特定知识，这同时提升了训练和推理效率。针对孟加拉语大语言模型缺乏基准测试数据集的问题，我们开发了五个基准测试集。我们对包括TituLLMs在内的多种大语言模型进行测试，结果表明TituLLMs优于其初始的多语言版本，但这种情况并非绝对，这揭示了语言适配的复杂性。本研究为将现有多语言开源模型适配其他低资源语言奠定了基础。为促进广泛采用和后续研究，我们已将TituLLMs模型和基准测试数据集公开（https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a）。

---

## [LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws](https://arxiv.org/abs/2502.12120)

### Abstract
arXiv:2502.12120v2 Announce Type: replace-cross 
Abstract: Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.

### 摘要
缩放定律通过提供模型规模、训练标记和计算资源最优配比的估计，指导大型语言模型（LLM）的开发。近期研究表明，预训练数据集与下游任务间损失值关联的损失-损失缩放定律，已成为理解和提升LLM性能的重要工具。本研究探究了影响损失-损失缩放的关键因素。实验表明，预训练数据与分词器决定了缩放趋势，而模型规模、优化超参数乃至显著架构差异（如Llama等基于Transformer的模型与Mamba等状态空间模型间的差异）影响有限。因此，实践者应精心筛选预训练数据以获得最优下游性能，而模型架构及其他设置可自由优化以提升训练效率。

---

## [Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2502.20332)

### Abstract
arXiv:2502.20332v2 Announce Type: replace-cross 
Abstract: Many recent studies have found evidence for emergent reasoning capabilities in large language models (LLMs), but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we study the internal mechanisms that support abstract reasoning in LLMs. We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, symbol abstraction heads convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, symbolic induction heads perform sequence induction over these abstract variables. Finally, in later layers, retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.

### 摘要
近期许多研究发现大型语言模型（LLM）存在涌现推理能力的证据，但关于这些能力的稳健性及其对结构化推理机制的依赖程度仍存争议。为阐明这些问题，我们研究了支持LLM抽象推理的内部机制。我们识别出一种涌现的符号架构，该架构通过三个连续计算步骤实现抽象推理：在早期层中，符号抽象头根据输入标记之间的关系将其转化为抽象变量；在中间层中，符号归纳头对这些抽象变量执行序列归纳；最后在深层中，检索头通过获取与预测抽象变量相关联的值来预测下一标记。这些结果表明神经网络中的涌现推理依赖于符号机制的形成，为符号主义与神经网络方法之间的长期争论提供了可能的解决路径。

---

## [MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models](https://arxiv.org/abs/2502.16671)

### Abstract
arXiv:2502.16671v2 Announce Type: replace-cross 
Abstract: As AI becomes more closely integrated with peoples' daily activities, socially intelligent AI that can understand and interact seamlessly with humans in daily lives is increasingly important. However, current works in AI social reasoning all rely on language-only or language-dominant approaches to benchmark and training models, resulting in systems that are improving in verbal communication but struggle with nonverbal social understanding. To address this limitation, we tap into a novel data source rich in nonverbal social interactions -- mime videos. Mimes refer to the art of expression through gesture and movement without spoken words, which presents unique challenges and opportunities in interpreting nonverbal social communication. We contribute a new dataset called MimeQA, obtained by sourcing 8 hours of videos clips from YouTube and developing a comprehensive video question-answering benchmark comprising 806 carefully annotated and verified question-answer pairs, designed to probe nonverbal social reasoning capabilities. Using MimeQA, we evaluate state-of-the-art video large language models (vLLMs) and find that they achieve low overall accuracy, ranging from 20-30%, while humans score 86%. Our analysis reveals that vLLMs often fail to ground imagined objects and over-rely on the text prompt while ignoring subtle nonverbal interactions. We hope to inspire future work in AI models that embody true social intelligence capable of interpreting non-verbal human interactions.

### 摘要
随着人工智能日益紧密地融入人们的日常活动，能够理解并与人类在日常生活中无缝互动的社会智能AI变得愈发重要。然而，当前AI社会推理领域的研究均依赖于纯语言或语言主导的方法进行基准测试和模型训练，导致系统在言语交流方面虽有进步，却难以理解非语言社交行为。为突破这一局限，我们挖掘了一种富含非语言社交互动的新型数据源——哑剧视频。哑剧指通过手势和动作而非言语进行表达的艺术形式，这为解读非语言社交沟通带来了独特挑战与机遇。我们提出了名为MimeQA的新数据集，通过从YouTube采集8小时视频片段，开发了包含806个精心标注和验证的问答对的综合性视频问答基准，旨在探究非语言社交推理能力。基于MimeQA的评估显示，当前最先进的视频大语言模型(vLLM)总体准确率仅为20-30%，而人类得分达到86%。分析表明，vLLM经常无法锚定想象物体，过度依赖文本提示而忽略细微的非语言互动。我们希望这项工作能启发未来开发具备真实社会智能、能解读人类非语言互动的AI模型。

---

## [Adversarial Tokenization](https://arxiv.org/abs/2503.02174)

### Abstract
arXiv:2503.02174v2 Announce Type: replace-cross 
Abstract: Current LLM pipelines account for only one possible tokenization for a given string, ignoring exponentially many alternative tokenizations during training and inference. For example, the standard Llama3 tokenization of penguin is [p,enguin], yet [peng,uin] is another perfectly valid alternative. In this paper, we show that despite LLMs being trained solely on one tokenization, they still retain semantic understanding of other tokenizations, raising questions about their implications in LLM safety. Put succinctly, we answer the following question: can we adversarially tokenize an obviously malicious string to evade safety and alignment restrictions? We show that not only is adversarial tokenization an effective yet previously neglected axis of attack, but it is also competitive against existing state-of-the-art adversarial approaches without changing the text of the harmful request. We empirically validate this exploit across three state-of-the-art LLMs and adversarial datasets, revealing a previously unknown vulnerability in subword models.

### 摘要
当前大型语言模型（LLM）流程仅考虑给定字符串的一种可能分词方式，在训练和推理过程中忽略了指数级数量的替代分词方案。例如，Llama3标准分词将"penguin"处理为[p,enguin]，而[peng,uin]同样是完全有效的替代方案。本文研究表明：尽管LLMs仅基于单一分词方案进行训练，它们仍能保持对其他分词方案的语义理解，这引发了关于其对LLM安全性影响的思考。简言之，我们回答了以下问题：能否通过对明显恶意字符串进行对抗性分词来规避安全性和对齐限制？实验证明，对抗性分词不仅是一种有效但先前被忽视的攻击维度，而且在保持有害请求文本不变的情况下，其攻击效果可与现有最先进的对抗方法相媲美。我们在三种前沿LLM和对抗数据集上实证验证了这一漏洞，揭示了子词模型中此前未知的安全缺陷。

---

## [DualSpec: Text-to-spatial-audio Generation via Dual-Spectrogram Guided Diffusion Model](https://arxiv.org/abs/2502.18952)

### Abstract
arXiv:2502.18952v2 Announce Type: replace-cross 
Abstract: Text-to-audio (TTA), which generates audio signals from textual descriptions, has received huge attention in recent years. However, recent works focused on text to monaural audio only. As we know, spatial audio provides more immersive auditory experience than monaural audio, e.g. in virtual reality. To address this issue, we propose a text-to-spatial-audio (TTSA) generation framework named DualSpec. Specifically, it first trains variational autoencoders (VAEs) for extracting the latent acoustic representations from sound event audio. Then, given text that describes sound events and event directions, the proposed method uses the encoder of a pretrained large language model to transform the text into text features. Finally, it trains a diffusion model from the latent acoustic representations and text features for the spatial audio generation. In the inference stage, only the text description is needed to generate spatial audio. Particularly, to improve the synthesis quality and azimuth accuracy of the spatial sound events simultaneously, we propose to use two kinds of acoustic features. One is the Mel spectrograms which is good for improving the synthesis quality, and the other is the short-time Fourier transform spectrograms which is good at improving the azimuth accuracy. We provide a pipeline of constructing spatial audio dataset with text prompts, for the training of the VAEs and diffusion model. We also introduce new spatial-aware evaluation metrics to quantify the azimuth errors of the generated spatial audio recordings. Experimental results demonstrate that the proposed method can generate spatial audio with high directional and event consistency.

### 摘要
文本到音频（TTA）技术通过文本描述生成音频信号，近年来受到广泛关注。然而现有研究仅聚焦于单声道音频生成。鉴于空间音频（如虚拟现实场景）能提供比单声道更沉浸的听觉体验，本文提出名为DualSpec的文本到空间音频（TTSA）生成框架。该框架首先训练变分自编码器（VAEs）从声音事件音频中提取潜在声学表征；随后利用预训练大语言模型的编码器将描述声音事件及其方位的文本转化为文本特征；最后基于潜在声学表征和文本特征训练扩散模型实现空间音频生成。在推理阶段仅需文本描述即可生成空间音频。为同步提升空间声事件的合成质量与方位精度，我们创新性地采用两种声学特征：提升合成质量的梅尔频谱图与优化方位精度的短时傅里叶变换频谱图。本文还构建了带文本提示的空间音频数据集构建流程用于VAEs和扩散模型训练，并提出了新的空间感知评估指标以量化生成音频的方位误差。实验结果表明，所提方法能生成具有高方向一致性与事件一致性的空间音频。

---

## [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613)

### Abstract
arXiv:2503.05613v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have transformed natural language processing, yet their internal mechanisms remain largely opaque. Recently, mechanistic interpretability has attracted significant attention from the research community as a means to understand the inner workings of LLMs. Among various mechanistic interpretability approaches, Sparse Autoencoders (SAEs) have emerged as a promising method due to their ability to disentangle the complex, superimposed features within LLMs into more interpretable components. This paper presents a comprehensive survey of SAEs for interpreting and understanding the internal workings of LLMs. Our major contributions include: (1) exploring the technical framework of SAEs, covering basic architecture, design improvements, and effective training strategies; (2) examining different approaches to explaining SAE features, categorized into input-based and output-based explanation methods; (3) discussing evaluation methods for assessing SAE performance, covering both structural and functional metrics; and (4) investigating real-world applications of SAEs in understanding and manipulating LLM behaviors.

### 摘要
大型语言模型（LLMs）彻底改变了自然语言处理领域，但其内部机制仍存在较大不透明性。近年来，机械可解释性作为一种理解LLMs内部运作机制的方法受到学界广泛关注。在各种机械可解释性方法中，稀疏自编码器（SAEs）因其能将LLMs中复杂叠加的特征解耦为更具可解释性的组件而成为颇具前景的技术。本文对用于解释和理解LLMs内部运作的SAEs进行了全面综述，主要贡献包括：（1）探讨SAEs的技术框架，涵盖基础架构、设计改进和有效训练策略；（2）分析解释SAE特征的不同方法，将其归类为基于输入和基于输出的解释方法；（3）讨论评估SAE性能的方法，包括结构性和功能性指标；（4）研究SAEs在理解和操控LLM行为中的实际应用。

---

## [Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations](https://arxiv.org/abs/2502.18147)

### Abstract
arXiv:2502.18147v2 Announce Type: replace-cross 
Abstract: Sparse autoencoders (SAEs) have been successfully used to discover sparse and human-interpretable representations of the latent activations of LLMs. However, we would ultimately like to understand the computations performed by LLMs and not just their representations. The extent to which SAEs can help us understand computations is unclear because they are not designed to "sparsify" computations in any sense, only latent activations. To solve this, we propose Jacobian SAEs (JSAEs), which yield not only sparsity in the input and output activations of a given model component but also sparsity in the computation (formally, the Jacobian) connecting them. With a na\"ive implementation, the Jacobians in LLMs would be computationally intractable due to their size. One key technical contribution is thus finding an efficient way of computing Jacobians in this setup. We find that JSAEs extract a relatively large degree of computational sparsity while preserving downstream LLM performance approximately as well as traditional SAEs. We also show that Jacobians are a reasonable proxy for computational sparsity because MLPs are approximately linear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a greater degree of computational sparsity on pre-trained LLMs than on the equivalent randomized LLM. This shows that the sparsity of the computational graph appears to be a property that LLMs learn through training, and suggests that JSAEs might be more suitable for understanding learned transformer computations than standard SAEs.

### 摘要
稀疏自编码器（SAEs）已成功用于发现大型语言模型（LLMs）潜在激活的稀疏且人类可解释的表示。然而，我们最终希望理解LLMs执行的计算而不仅仅是其表示。由于SAEs并非设计用于在任何意义上"稀疏化"计算（仅针对潜在激活），其对计算理解的帮助程度尚不明确。为此，我们提出雅可比稀疏自编码器（JSAEs），其不仅能在给定模型组件的输入和输出激活中实现稀疏性，还能在连接这些激活的计算（形式上即雅可比矩阵）中实现稀疏性。若采用简单实现，LLMs中的雅可比矩阵会因规模过大而难以计算，因此本研究的核心技术贡献在于找到该场景下高效计算雅可比矩阵的方法。实验表明，JSAEs在保持与传统SAEs相近的下游LLM性能的同时，能提取出较高程度的计算稀疏性。我们还证明雅可比矩阵是计算稀疏性的合理代理指标，因为多层感知器（MLPs）在JSAE基下重写时具有近似线性特性。最后，我们发现JSAEs在预训练LLMs上获得的计算稀疏性显著高于等效随机化LLMs，这表明计算图的稀疏性似乎是LLMs通过训练习得的特性，也意味着JSAEs可能比标准SAEs更适合用于理解已训练Transformer模型的计算机制。

---

## [SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal Foundation Models](https://arxiv.org/abs/2503.00211)

### Abstract
arXiv:2503.00211v2 Announce Type: replace-cross 
Abstract: Traditional autonomous driving systems often struggle to connect high-level reasoning with low-level control, leading to suboptimal and sometimes unsafe behaviors. Recent advances in multimodal large language models (MLLMs), which process both visual and textual data, offer an opportunity to unify perception and reasoning. However, effectively embedding precise safety knowledge into MLLMs for autonomous driving remains a significant challenge. To address this, we propose SafeAuto, a framework that enhances MLLM-based autonomous driving by incorporating both unstructured and structured knowledge. First, we introduce a Position-Dependent Cross-Entropy (PDCE) loss to improve low-level control signal predictions when values are represented as text. Second, to explicitly integrate safety knowledge, we develop a reasoning component that translates traffic rules into first-order logic (e.g., "red light $\implies$ stop") and embeds them into a probabilistic graphical model (e.g., Markov Logic Network) to verify predicted actions using recognized environmental attributes. Additionally, our Multimodal Retrieval-Augmented Generation (RAG) model leverages video, control signals, and environmental attributes to learn from past driving experiences. Integrating PDCE, MLN, and Multimodal RAG, SafeAuto outperforms existing baselines across multiple datasets, enabling more accurate, reliable, and safer autonomous driving. The code is available at https://github.com/AI-secure/SafeAuto.

### 摘要
传统自动驾驶系统往往难以将高层推理与底层控制有效衔接，导致行为表现欠佳甚至存在安全隐患。多模态大语言模型（MLLMs）在视觉与文本数据处理方面的最新进展，为统一感知与推理提供了新机遇。然而，如何将精确的安全知识有效嵌入MLLMs以服务于自动驾驶仍面临重大挑战。为此，我们提出SafeAuto框架，通过融合非结构化与结构化知识来增强基于MLLM的自动驾驶系统。首先，我们提出位置相关交叉熵（PDCE）损失函数，以优化文本形式表示的底层控制信号预测精度。其次，为显式整合安全知识，我们开发了推理组件：将交通规则转化为一阶逻辑（如"红灯→停车"）并嵌入概率图模型（如马尔可夫逻辑网络），利用识别的环境属性验证预测动作。此外，我们的多模态检索增强生成（RAG）模型通过融合视频、控制信号和环境属性，实现从历史驾驶经验中学习。集成PDCE、MLN和多模态RAG的SafeAuto在多个数据集上超越现有基线，实现了更精准、可靠且安全的自动驾驶。代码已开源：https://github.com/AI-secure/SafeAuto。

---

## [UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning](https://arxiv.org/abs/2503.01908)

### Abstract
arXiv:2503.01908v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents equipped with external tools have become increasingly powerful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Nevertheless, manipulating LLM agents into performing targeted malicious actions or invoking specific tools remains challenging, as these agents extensively reason or plan before executing final actions. In this work, we present UDora, a unified red teaming framework designed for LLM agents that dynamically hijacks the agent's reasoning processes to compel malicious behavior. Specifically, UDora first generates the model's reasoning trace for the given task, then automatically identifies optimal points within this trace to insert targeted perturbations. The resulting perturbed reasoning is then used as a surrogate response for optimization. By iteratively applying this process, the LLM agent will then be induced to undertake designated malicious actions or to invoke specific malicious tools. Our approach demonstrates superior effectiveness compared to existing methods across three LLM agent datasets. The code is available at https://github.com/AI-secure/UDora.

### 摘要
配备外部工具的大语言模型（LLM）代理在执行网络购物、自动邮件回复和金融交易等复杂任务时展现出日益强大的能力。然而，这些技术进步也加剧了对抗性攻击的风险，尤其是当代理能够访问敏感外部功能时。目前，操纵LLM代理执行定向恶意行为或调用特定工具仍具挑战性，因为这些代理在执行最终动作前会进行大量推理或规划。本研究提出UDora——一个专为LLM代理设计的统一红队测试框架，该框架通过动态劫持代理的推理过程来强制实施恶意行为。具体而言，UDora首先生成模型针对给定任务的推理轨迹，随后自动识别该轨迹中的最优扰动插入点，并将扰动后的推理作为替代响应进行优化。通过迭代应用此过程，LLM代理最终会被诱导执行指定的恶意动作或调用特定恶意工具。在三个LLM代理数据集上的实验表明，本方法相较于现有技术具有显著优势。代码已开源：https://github.com/AI-secure/UDora。

---

## [ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability](https://arxiv.org/abs/2503.06542)

### Abstract
arXiv:2503.06542v2 Announce Type: replace-cross 
Abstract: Unified multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate'' algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://github.com/finyorko/armor.

### 摘要
统一多模态理解与生成技术近期在视觉与语言领域备受关注。现有统一多模态系统（UniMs）虽能同时学习理解与生成能力，但需消耗大量计算资源，且常难以实现图文交错生成。本文提出ARMOR框架，该资源高效的自回归架构通过对现有多模态大语言模型（MLLMs）进行微调，即可实现理解与生成双重功能。具体而言，ARMOR从三个维度拓展现有MLLMs：（1）模型架构方面，引入具有前向切换机制的非对称编解码器结构，通过统一文本与视觉模态的嵌入空间，以最小计算开销实现自然图文交错生成；（2）训练数据方面，精心构建高质量交错数据集用于微调MLLMs；（3）训练算法方面，提出"生成内容或方式"的三阶段渐进式训练算法，在保留模型多模态理解能力的同时赋予其生成能力。实验表明，ARMOR能以有限训练资源将现有MLLMs升级为具备图像生成能力的UniMs。代码即将发布于https://github.com/finyorko/armor。

---

## [Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](https://arxiv.org/abs/2504.02821)

### Abstract
arXiv:2504.02821v2 Announce Type: replace-cross 
Abstract: Given that interpretability and steerability are crucial to AI safety, Sparse Autoencoders (SAEs) have emerged as a tool to enhance them in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity at the neuron-level in vision representations. To ensure that our evaluation aligns with human perception, we propose a benchmark derived from a large-scale user study. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons, with sparsity and wide latents being the most influential factors. Notably, we demonstrate that applying SAE interventions on CLIP's vision encoder directly steers multimodal LLM outputs (e.g., LLaVA), without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised tool for enhancing both interpretability and control of VLMs. Code is available at https://github.com/ExplainableML/sae-for-vlm.

### 摘要
鉴于可解释性和可操控性对人工智能安全至关重要，稀疏自编码器（SAEs）已成为增强大型语言模型（LLMs）中这些特性的工具。本研究将SAEs的应用扩展至视觉语言模型（VLMs，如CLIP），并提出一个评估视觉表征中神经元层面单义性的综合框架。为确保评估结果与人类感知一致，我们基于大规模用户研究构建了基准测试。实验结果表明，在VLMs上训练的SAEs能显著提升单个神经元的单义性，其中稀疏性和宽潜在层是最关键的影响因素。值得注意的是，我们证明直接在CLIP视觉编码器上应用SAE干预可操控多模态LLM输出（如LLaVA），而无需修改底层模型。这些发现凸显了SAEs作为一种无监督工具在增强VLMs可解释性和可控性方面的实用性与有效性。代码详见https://github.com/ExplainableML/sae-for-vlm。

---

## [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)

### Abstract
arXiv:2504.13818v2 Announce Type: replace-cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful paradigm for enhancing reasoning capabilities in large language models. However, it is constrained by a fundamental asymmetry in computation and memory requirements: rollout generation is embarrassingly parallel and memory-light, whereas policy updates are communication-heavy and memory-intensive. To address this, we introduce PODS (Policy Optimization with Down-Sampling). PODS produces numerous rollouts in parallel, then trains on only an informative subset, preserving learning signals while slashing update cost. We instantiate PODS with max-variance down-sampling, a principled criterion that maximises reward diversity and show it admits an $O(n\log n)$ solution. Empirically, coupling PODS with Group Relative Policy Optimization (GRPO) achieves superior performance over standard GRPO across different reasoning benchmarks and hardware environments.

### 摘要
基于可验证奖励的强化学习（RLVR）已成为增强大语言模型推理能力的有效范式。然而，该方法受限于计算与内存需求的不对称性： rollout生成过程具有高度并行性和低内存消耗特性，而策略更新则存在通信开销大、内存密集的瓶颈。为此，我们提出降采样策略优化框架（PODS）。该框架通过并行生成大量rollout后，仅选取信息量最大的子集进行训练，在保留学习信号的同时大幅降低更新成本。我们采用最大化方差降采样作为PODS的实现准则，该原则性方法能最大化奖励多样性，并证明其存在O(nlogn)复杂度的解决方案。实验表明，将PODS与组相对策略优化（GRPO）相结合时，在不同推理基准测试和硬件环境下均显著优于标准GRPO方法。

---

## [Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning](https://arxiv.org/abs/2504.05632)

### Abstract
arXiv:2504.05632v3 Announce Type: replace-cross 
Abstract: Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate stereotypical responses remains largely underexplored. In this work, we investigate the crucial relationship between a model's reasoning ability and fairness, and ask whether improved reasoning capabilities can mitigate harmful stereotypical responses, especially those arising due to shallow or flawed reasoning. We conduct a comprehensive evaluation of multiple open-source LLMs, and find that larger models with stronger reasoning abilities exhibit substantially lower stereotypical bias on existing fairness benchmarks. Building on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning, a novel approach that extracts structured reasoning traces from advanced reasoning models and infuses them into models that lack such capabilities. We use only general-purpose reasoning and do not require any fairness-specific supervision for bias mitigation. Notably, we see that models fine-tuned using ReGiFT not only improve fairness relative to their non-reasoning counterparts but also outperform advanced reasoning models on fairness benchmarks. We also analyze how variations in the correctness of the reasoning traces and their length influence model fairness and their overall performance. Our findings highlight that enhancing reasoning capabilities is an effective, fairness-agnostic strategy for mitigating stereotypical bias caused by reasoning flaws.

### 摘要
大规模生成式语言模型的最新进展表明，推理能力能显著提升模型在各类任务中的表现。然而，推理能力对模型减少刻板印象回应的影响仍缺乏深入探索。本研究揭示了模型推理能力与公平性之间的关键关联，并探讨增强的推理能力是否能缓解有害的刻板印象回应——特别是那些源于浅层或缺陷推理的回应。通过对多个开源大语言模型的系统评估，我们发现具有更强推理能力的大模型在现有公平性基准测试中表现出显著更低的刻板偏见。基于此发现，我们提出ReGiFT（推理引导微调）——一种从先进推理模型中提取结构化推理轨迹，并将其注入缺乏此类能力的模型的新方法。该方法仅使用通用推理能力，无需任何针对公平性的特定监督来实现偏见缓解。值得注意的是，经ReGiFT微调的模型不仅比非推理模型更具公平性，甚至在公平性基准测试上超越了先进推理模型。我们还分析了推理轨迹的正确性差异及其长度如何影响模型公平性与整体性能。研究结果证明，增强推理能力是一种有效且与公平性无关的策略，可缓解由推理缺陷导致的刻板偏见。

---

## [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)

### Abstract
arXiv:2504.19565v2 Announce Type: replace-cross 
Abstract: Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. This agentic framework collectively generates and refines domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.

### 摘要
针对生物医学大语言模型（LLMs）的语料蒸馏旨在解决开源标注科学语料在数量和质量上的不足这一紧迫挑战，这仍是生物医学研究中有效训练LLMs的主要瓶颈。本文提出了一种知识驱动的、主动的科学语料蒸馏框架，专门为生物医学领域的LLM训练设计，以应对生物医学知识复杂层次结构带来的挑战。我们方法的核心是一个协作的多智能体架构，其中每个由医学主题词表（MeSH）层次结构指导的专门化智能体协同工作，从海量科学文献中自主提取、合成并自我评估高质量文本数据。该主动框架共同生成并优化领域特定的问答对，确保全面覆盖并与生物医学本体保持一致，同时最大限度地减少人工参与。大量实验结果表明，基于我们多智能体蒸馏数据集训练的语言模型在生物医学问答任务中取得了显著提升，优于强大的生命科学LLM基线模型和先进的专有模型。值得注意的是，我们的AI就绪数据集使Llama3-70B超越了采用MedPrompt和Med-PaLM-2的GPT-4，尽管后者规模更大。详细的消融研究和案例分析进一步验证了框架中每个智能体的有效性和协同作用，凸显了多智能体协作在生物医学LLM训练中的潜力。

---

