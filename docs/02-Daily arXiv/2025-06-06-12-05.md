# 2025-06-06-12-05

## [Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance](https://arxiv.org/abs/2506.04427)

### Abstract
arXiv:2506.04427v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in table Question Answering (Table QA). However, extending these capabilities to multi-table QA remains challenging due to unreliable schema linking across complex tables. Existing methods based on semantic similarity work well only on simplified hand-crafted datasets and struggle to handle complex, real-world scenarios with numerous and diverse columns. To address this, we propose a graph-based framework that leverages human-curated relational knowledge to explicitly encode schema links and join paths. Given a natural language query, our method searches this graph to construct interpretable reasoning chains, aided by pruning and sub-path merging strategies to enhance efficiency and coherence. Experiments on both standard benchmarks and a realistic, large-scale dataset demonstrate the effectiveness of our approach. To our knowledge, this is the first multi-table QA system applied to truly complex industrial tabular data.

### 摘要
大语言模型（LLMs）在表格问答（Table QA）任务中已展现出潜力，但将其能力扩展到多表问答仍面临挑战，主要原因在于复杂表格间模式链接的不可靠性。现有基于语义相似度的方法仅在简化的人工构建数据集上表现良好，难以处理具有大量多样化列的真实复杂场景。为此，我们提出一种基于图的框架，利用人工整理的关系知识显式编码模式链接与连接路径。给定自然语言查询，本方法通过搜索该图构建可解释的推理链，并辅以剪枝和子路径合并策略来提升效率与连贯性。在标准基准测试和真实大规模数据集上的实验验证了本方法的有效性。据我们所知，这是首个应用于真正复杂工业表格数据的多表问答系统。

---

## [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)

### Abstract
arXiv:2506.04245v1 Announce Type: new 
Abstract: As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.

### 摘要
随着自主代理代表用户做出决策时代的到来，如何确保情境完整性（CI）——即在执行特定任务时应分享何种适当信息——成为该领域的核心问题。我们提出CI要求代理具备对所处情境进行推理的能力。为验证这一点，我们首先引导大语言模型在决定信息披露时显式地进行CI推理。随后，我们通过开发强化学习（RL）框架进一步强化模型实现CI所需的推理能力。利用仅含约700个样本但涵盖多样化情境与信息披露规范的自动生成合成数据集，研究表明我们的方法能显著减少不当信息披露，同时在不同模型规模和架构中保持任务性能。值得注意的是，这种改进能从合成数据集迁移至具有人工标注的成熟CI基准测试（如PrivacyLens），该基准用于评估AI助手在行为和工具调用中的隐私泄露情况。

---

## [A Statistical Physics of Language Model Reasoning](https://arxiv.org/abs/2506.04374)

### Abstract
arXiv:2506.04374v1 Announce Type: new 
Abstract: Transformer LMs show emergent reasoning that resists mechanistic understanding. We offer a statistical physics framework for continuous-time chain-of-thought reasoning dynamics. We model sentence-level hidden state trajectories as a stochastic dynamical system on a lower-dimensional manifold. This drift-diffusion system uses latent regime switching to capture diverse reasoning phases, including misaligned states or failures. Empirical trajectories (8 models, 7 benchmarks) show a rank-40 projection (balancing variance capture and feasibility) explains ~50% variance. We find four latent reasoning regimes. An SLDS model is formulated and validated to capture these features. The framework enables low-cost reasoning simulation, offering tools to study and predict critical transitions like misaligned states or other LM failures.

### 摘要
Transformer语言模型展现出抵抗机制化理解的新兴推理能力。我们提出一个统计物理学框架来描述连续时间思维链推理动态过程。通过将句子级隐藏状态轨迹建模为低维流形上的随机动力系统，该漂移-扩散系统利用潜在状态切换机制来捕捉包括错位状态或推理失败在内的多种推理阶段。实证轨迹数据（8个模型，7个基准测试）显示，在秩为40的投影空间（平衡方差捕获与可行性）可解释约50%的方差。研究发现存在四种潜在推理模式。我们构建并验证了一个切换线性动态系统模型来捕捉这些特征。该框架支持低成本推理模拟，为研究错位状态等关键转变及其他语言模型失效现象提供了分析工具。

---

## [Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation](https://arxiv.org/abs/2506.04251)

### Abstract
arXiv:2506.04251v1 Announce Type: new 
Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.

### 摘要
本文介绍LLM-MARL框架，该框架将大型语言模型（LLMs）融入多智能体强化学习（MARL），以提升模拟游戏环境中的协作、通信与泛化能力。该框架包含协调器、通信器和记忆模块三大组件，可动态生成子目标、促进智能体间符号化消息传递并支持情景记忆。训练过程结合PPO算法与语言条件损失函数及LLM查询门控机制。实验在Google Research Football、MAgent Battle和星际争霸II环境中进行，结果显示LLM-MARL在胜率、协作分数和零样本泛化能力上均优于MAPPO与QMIX。消融研究表明子目标生成与基于语言的通信各自对性能提升具有显著贡献。定性分析揭示了角色专业化与通信驱动战术等涌现行为。通过 bridging 语言建模与策略学习，本研究为交互式仿真中智能协作智能体的设计提供了新思路，为LLMs在训练系统、游戏及人机协作等多智能体系统中的应用开辟了新路径。

---

## [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)

### Abstract
arXiv:2506.04252v1 Announce Type: new 
Abstract: Large language models (LLMs) hold promise for sustainable manufacturing, but often hallucinate industrial codes and emission factors, undermining regulatory and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented generation (RAG) framework that grounds LLMs outputs in a domain-specific knowledge graph for the circular economy. This graph connects 117,380 industrial and waste entities with classification codes and GWP100 emission data, enabling structured multi-hop reasoning. Natural language queries are translated into SPARQL and verified subgraphs are retrieved to ensure accuracy and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG achieves superior performance in single-hop and multi-hop question answering, with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also improves efficiency, halving the response time and reducing token usage by 16% in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready support for circular economy planning, advancing reliable, low-carbon resource decision making.

### 摘要
大语言模型（LLMs）在可持续制造领域展现出潜力，但其常会虚构工业代码和排放因子，从而影响监管与投资决策。我们提出CircuGraphRAG，一种基于检索增强生成（RAG）的框架，通过将LLMs输出锚定于循环经济领域的专用知识图谱来实现可信推理。该图谱连接了117,380个工业与废弃物实体，涵盖分类代码及GWP100排放数据，支持结构化多跳推理。自然语言查询被转换为SPARQL语句，并通过验证子图检索确保结果准确性与可追溯性。与独立LLMs及基础RAG相比，CircuGraphRAG在单跳和多跳问答任务中表现优异，ROUGE-L F1分数最高达1.0，而基线模型得分低于0.08。该系统同时提升了效率，在典型任务中响应时间缩短一半，令牌使用量减少16%。CircuGraphRAG为循环经济规划提供经过事实核查、符合监管要求的决策支持，推动可靠低碳资源管理的进展。

---

## [Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback](https://arxiv.org/abs/2506.04287)

### Abstract
arXiv:2506.04287v1 Announce Type: new 
Abstract: Training large language model (LLM) agents to acquire necessary skills and perform diverse tasks within an environment is gaining interest as a means to enable open-endedness. However, creating the training dataset for their skill acquisition faces several challenges. Manual trajectory collection requires significant human effort. Another approach, where LLMs directly propose tasks to learn, is often invalid, as the LLMs lack knowledge of which tasks are actually feasible. Moreover, the generated data may not provide a meaningful learning signal, as agents often already perform well on the proposed tasks. To address this, we propose a novel automatic skill discovery framework EXIF for LLM-powered agents, designed to improve the feasibility of generated target behaviors while accounting for the agents' capabilities. Our method adopts an exploration-first strategy by employing an exploration agent (Alice) to train the target agent (Bob) to learn essential skills in the environment. Specifically, Alice first interacts with the environment to retrospectively generate a feasible, environment-grounded skill dataset, which is then used to train Bob. Crucially, we incorporate an iterative feedback loop, where Alice evaluates Bob's performance to identify areas for improvement. This feedback then guides Alice's next round of exploration, forming a closed-loop data generation process. Experiments on Webshop and Crafter demonstrate EXIF's ability to effectively discover meaningful skills and iteratively expand the capabilities of the trained agent without any human intervention, achieving substantial performance improvements. Interestingly, we observe that setting Alice to the same model as Bob also notably improves performance, demonstrating EXIF's potential for building a self-evolving system.

### 摘要
训练大型语言模型（LLM）代理以获取必要技能并在环境中执行多样化任务，正逐渐成为实现开放性的重要途径。然而，为其技能获取创建训练数据集面临诸多挑战。人工轨迹收集需要大量人力投入；而另一种由LLM直接提出学习任务的方法往往无效，因为LLM缺乏对实际可行任务的认知。此外，生成的数据可能无法提供有意义的学习信号，因为代理在既定任务上通常已表现良好。为此，我们提出了一种新型自动技能发现框架EXIF，专为LLM驱动的代理设计，旨在提高生成目标行为的可行性，同时兼顾代理的能力。该方法采用探索优先策略，通过探索代理（Alice）训练目标代理（Bob）掌握环境中的核心技能。具体而言，Alice首先与环境交互以回溯生成基于环境的可行技能数据集，随后用于训练Bob。关键的是，我们引入了迭代反馈循环：Alice通过评估Bob的表现识别改进领域，该反馈进而指导Alice下一轮探索，形成闭环数据生成流程。在Webshop和Crafter上的实验表明，EXIF能有效发现有意义技能，并在无需人工干预的情况下迭代扩展受训代理的能力，实现显著性能提升。有趣的是，我们发现将Alice设置为与Bob相同模型时性能同样显著提升，这证明了EXIF构建自进化系统的潜力。

---

## [Spore in the Wild: Case Study on Spore.fun, a Real-World Experiment of Sovereign Agent Open-ended Evolution on Blockchain with TEEs](https://arxiv.org/abs/2506.04236)

### Abstract
arXiv:2506.04236v1 Announce Type: new 
Abstract: In Artificial Life (ALife) research, replicating Open-Ended Evolution (OEE)-the continuous emergence of novelty observed in biological life-has traditionally been pursued within isolated closed system simulations, such as Tierra and Avida, which have typically plateaued after an initial burst of novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires an "open" system that continually exchanges information or energy with its environment. A recent technological innovation in decentralized physical infrastructure networks (DePIN) providing permissionless computational substrates enables deploying large language model (LLM)-based AI agents on blockchains integrated with Trusted Execution Environments (TEEs). This enables on-chain agents to operate autonomously "in the wild," achieving self-sovereignty without human oversight. These agents can control their own social media accounts and cryptocurrency wallets, allowing them to interact directly with blockchain-based financial networks and broader human social media. Building on this new paradigm of on-chain agents, Spore.fun is a recent real-world AI evolution experiment that enables autonomous breeding and evolution of new on-chain agents. This paper presents a detailed case study of Spore.fun, examining agent behaviors and their evolutionary trajectories through digital ethology. We aim to spark discussion about whether "open" ALife systems "in-the-wild," based on permissionless computational substrates and driven by economic incentives to interact with their environment, could finally achieve the long-sought goal of OEE.

### 摘要
在人工生命（ALife）研究中，开放演化（OEE）——即生物生命中观察到的持续涌现新现象——的传统实现方式是通过孤立的封闭系统模拟（如Tierra和Avida）。这些系统通常在初期新现象爆发后便陷入停滞，未能实现持续的OEE。学者指出，OEE需要一个能持续与环境交换信息或能量的"开放"系统。近期去中心化物理基础设施网络（DePIN）的技术创新提供了无需许可的计算基底，使得基于大语言模型（LLM）的AI代理能够部署在与可信执行环境（TEE）集成的区块链上。这使得链上代理能在"真实环境"中自主运行，在无人监督下实现自我主权。这些代理可控制自己的社交媒体账户和加密货币钱包，直接与基于区块链的金融网络及更广泛的人类社交媒体互动。基于这种链上代理的新范式，Spore.fun是近期一项现实世界的AI演化实验，支持新型链上代理的自主繁殖与进化。本文通过数字行为学对Spore.fun进行详细案例研究，分析代理行为及其演化轨迹。我们旨在探讨：基于无需许可计算基底、受经济激励驱动与环境互动的"真实环境"开放ALife系统，是否最终能实现长期追求的OEE目标。

---

## [Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences](https://arxiv.org/abs/2506.04478)

### Abstract
arXiv:2506.04478v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has driven progress in reasoning tasks -- from program synthesis to scientific hypothesis generation -- yet their ability to handle ranked preferences and structured algorithms in combinatorial domains remains underexplored. We study matching markets, a core framework behind applications like resource allocation and ride-sharing, which require reconciling individual ranked preferences to ensure stable outcomes. We evaluate several state-of-the-art models on a hierarchy of preference-based reasoning tasks -- ranging from stable-matching generation to instability detection, instability resolution, and fine-grained preference queries -- to systematically expose their logical and algorithmic limitations in handling ranked inputs. Surprisingly, even top-performing models with advanced reasoning struggle to resolve instability in large markets, often failing to identify blocking pairs or execute algorithms iteratively. We further show that parameter-efficient fine-tuning (LoRA) significantly improves performance in small markets, but fails to bring about a similar improvement on large instances, suggesting the need for more sophisticated strategies to improve LLMs' reasoning with larger-context inputs.

### 摘要
大型语言模型（LLMs）的兴起推动了推理任务的进展——从程序合成到科学假设生成——但其在组合领域中处理排序偏好和结构化算法的能力仍未得到充分探索。我们研究了匹配市场这一资源分配和拼车等应用背后的核心框架，该框架需要协调个体排序偏好以确保稳定结果。我们在基于偏好的推理任务层级上评估了多个前沿模型——包括稳定匹配生成、不稳定性检测、不稳定性解决以及细粒度偏好查询——以系统揭示其在处理排序输入时的逻辑与算法局限。令人惊讶的是，即使具备先进推理能力的顶尖模型也难以解决大规模市场中的不稳定性问题，常常无法识别阻塞对或迭代执行算法。我们进一步表明，参数高效微调（LoRA）能显著提升小规模市场的性能，但无法对大规模实例带来类似改进，这表明需要更复杂的策略来增强LLMs处理更大上下文输入的推理能力。

---

## [Schema Generation for Large Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2506.04512)

### Abstract
arXiv:2506.04512v1 Announce Type: new 
Abstract: Schemas are vital for ensuring data quality in the Semantic Web and natural language processing. Traditionally, their creation demands substantial involvement from knowledge engineers and domain experts. Leveraging the impressive capabilities of large language models (LLMs) in related tasks like ontology engineering, we explore automatic schema generation using LLMs. To bridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata EntitySchema, along with evaluation metrics. The LLM-based pipelines effectively utilize local and global information from knowledge graphs (KGs) to generate validating schemas in Shape Expressions (ShEx). Experiments demonstrate LLMs' strong potential in producing high-quality ShEx schemas, paving the way for scalable, automated schema generation for large KGs. Furthermore, our benchmark introduces a new challenge for structured generation, pushing the limits of LLMs on syntactically rich formalisms.

### 摘要
模式在语义网和自然语言处理中对于保障数据质量至关重要。传统模式构建需要知识工程师与领域专家的大量参与。基于大语言模型（LLMs）在 ontology 工程等相关任务中的卓越表现，我们探索利用LLMs实现自动化模式生成。为填补资源空白，我们引入YAGO Schema和Wikidata EntitySchema两个数据集及评估指标。基于LLM的流程能有效利用知识图谱（KGs）中的局部与全局信息，生成符合Shape Expressions（ShEx）规范的验证模式。实验表明LLMs在生成高质量ShEx模式方面具有显著潜力，为大规模知识图谱的可扩展自动化模式生成开辟了道路。此外，我们的基准测试为结构化生成提出了新挑战，推动了大语言模型在语法丰富的形式化表达方面的能力边界。

---

## [HADA: Human-AI Agent Decision Alignment Architecture](https://arxiv.org/abs/2506.04253)

### Abstract
arXiv:2506.04253v1 Announce Type: new 
Abstract: We present HADA (Human-AI Agent Decision Alignment), a protocol- and framework agnostic reference architecture that keeps both large language model (LLM) agents and legacy algorithms aligned with organizational targets and values. HADA wraps any algorithm or LLM in role-specific stakeholder agents -- business, data-science, audit, ethics, and customer -- each exposing conversational APIs so that technical and non-technical actors can query, steer, audit, or contest every decision across strategic, tactical, and real-time horizons. Alignment objectives, KPIs, and value constraints are expressed in natural language and are continuously propagated, logged, and versioned while thousands of heterogeneous agents run on different orchestration stacks. A cloud-native proof of concept packages a production credit-scoring model (getLoanDecision) and deploys it on Docker/Kubernetes/Python; five scripted retail-bank scenarios show how target changes, parameter tweaks, explanation requests, and ethics triggers flow end to end through the architecture. Evaluation followed the Design-Science Research Methodology. Walkthrough observation and log inspection demonstrated complete coverage of six predefined objectives: every role could invoke conversational control, trace KPIs and value constraints, detect and mitigate ZIP-code bias, and reproduce full decision lineage, independent of the underlying LLM or agent library. Contributions: (1) an open-source HADA architecture, (2) a mid-range design theory for human-AI alignment in multi-agent systems, and (3) empirical evidence that framework-agnostic, protocol-compliant stakeholder agents improve accuracy, transparency, and ethical compliance in real-world decision pipelines.

### 摘要
我们提出HADA（人机智能代理决策对齐），一种与协议和框架无关的参考架构，旨在使大型语言模型（LLM）代理和传统算法始终与组织目标和价值观保持一致。HADA将任何算法或LLM封装于特定角色的利益相关者代理中——包括业务、数据科学、审计、伦理和客户代理——每个代理都提供对话式API，使得技术与非技术人员能够查询、调控、审计或质疑战略、战术及实时层面的每个决策。对齐目标、关键绩效指标和价值约束均以自然语言表述，并在数千个异构代理运行于不同编排栈时持续传播、记录和版本化。云原生概念验证封装了生产级信用评分模型（getLoanDecision）并将其部署于Docker/Kubernetes/Python环境；五个预设零售银行场景展示了目标变更、参数调整、解释请求和伦理触发如何在该架构中端到端流转。评估遵循设计科学研究方法论。走查观察和日志检查证实了六项预设目标的完全覆盖：每个角色均可调用对话控制、追踪关键绩效指标与价值约束、检测并缓解邮政编码偏见，以及复现完整决策谱系，且不受底层LLM或代理库的影响。贡献包括：（1）开源HADA架构；（2）多代理系统中人机对齐的中层设计理论；（3）实证证据表明符合协议且框架无关的利益相关者代理能提升现实决策流程的准确性、透明度和伦理合规性。

---

## [From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems](https://arxiv.org/abs/2506.04565)

### Abstract
arXiv:2506.04565v1 Announce Type: new 
Abstract: Compound Al Systems (CAIS) is an emerging paradigm that integrates large language models (LLMs) with external components, such as retrievers, agents, tools, and orchestrators, to overcome the limitations of standalone models in tasks requiring memory, reasoning, real-time grounding, and multimodal understanding. These systems enable more capable and context-aware behaviors by composing multiple specialized modules into cohesive workflows. Despite growing adoption in both academia and industry, the CAIS landscape remains fragmented, lacking a unified framework for analysis, taxonomy, and evaluation. In this survey, we define the concept of CAIS, propose a multi-dimensional taxonomy based on component roles and orchestration strategies, and analyze four foundational paradigms: Retrieval-Augmented Generation (RAG), LLM Agents, Multimodal LLMs (MLLMs), and orchestration-centric architectures. We review representative systems, compare design trade-offs, and summarize evaluation methodologies across these paradigms. Finally, we identify key challenges-including scalability, interoperability, benchmarking, and coordination-and outline promising directions for future research. This survey aims to provide researchers and practitioners with a comprehensive foundation for understanding, developing, and advancing the next generation of system-level artificial intelligence.

### 摘要
复合AI系统（CAIS）是一种新兴范式，它通过将大语言模型（LLMs）与检索器、智能体、工具及编排器等外部组件相集成，以克服独立模型在需要记忆、推理、实时 grounding 和多模态理解任务中的局限性。这些系统通过将多个专用模块组合成连贯的工作流，实现了更具能力且上下文感知的行为。尽管CAIS在学术界和工业界的应用日益广泛，但其发展仍呈现碎片化特征，缺乏统一的分析、分类与评估框架。本综述明确定义了CAIS的概念，提出基于组件角色与编排策略的多维分类法，并分析了四大基础范式：检索增强生成（RAG）、LLM智能体、多模态LLM（MLLM）以及以编排为中心的架构。我们系统回顾了代表性方案，比较了不同设计权衡，并归纳了跨范式的评估方法。最后，本文指出了包括可扩展性、互操作性、基准测试与协调机制在内的关键挑战，并展望了未来研究的潜在方向。本综述旨在为研究人员和实践者提供理解、开发及推进系统级人工智能下一代发展的完整基础。

---

## [CogMath: Assessing LLMs' Authentic Mathematical Ability from a Human Cognitive Perspective](https://arxiv.org/abs/2506.04481)

### Abstract
arXiv:2506.04481v1 Announce Type: new 
Abstract: Although large language models (LLMs) show promise in solving complex mathematical tasks, existing evaluation paradigms rely solely on a coarse measure of overall answer accuracy, which are insufficient for assessing their authentic capabilities. In this paper, we propose \textbf&#123;CogMath&#125;, which comprehensively assesses LLMs' mathematical abilities through the lens of human cognition. Specifically, inspired by psychological theories, CogMath formalizes human reasoning process into 3 stages: \emph&#123;problem comprehension&#125;, \emph&#123;problem solving&#125;, and \emph&#123;solution summarization&#125;. Within these stages, we investigate perspectives such as numerical calculation, knowledge, and counterfactuals, and design a total of 9 fine-grained evaluation dimensions. In each dimension, we develop an ``\emph&#123;Inquiry&#125;-\emph&#123;Judge&#125;-\emph&#123;Reference&#125;'' multi-agent system to generate inquiries that assess LLMs' mastery from this dimension. An LLM is considered to truly master a problem only when excelling in all inquiries from the 9 dimensions. By applying CogMath on three benchmarks, we reveal that the mathematical capabilities of 7 mainstream LLMs are overestimated by 30\%-40\%. Moreover, we locate their strengths and weaknesses across specific stages/dimensions, offering in-depth insights to further enhance their reasoning abilities.

### 摘要
尽管大语言模型（LLMs）在解决复杂数学任务方面展现出潜力，现有评估范式仅依赖整体答案准确性的粗粒度度量，这不足以评估其真实能力。本文提出CogMath，通过人类认知视角全面评估LLMs的数学能力。具体而言，受心理学理论启发，CogMath将人类推理过程形式化为三个阶段：问题理解、问题求解和解决方案总结。在这些阶段中，我们从数值计算、知识和反事实等角度切入，共设计了9个细粒度评估维度。每个维度开发了“提问-判断-参考”多智能体系统，生成评估LLMs该维度掌握程度的提问。仅当LLMs在全部9个维度的提问中均表现优异时，才被认为真正掌握该问题。通过在三个基准测试上应用CogMath，我们发现7个主流LLMs的数学能力被高估了30%-40%。此外，我们定位了这些模型在特定阶段/维度的优势与缺陷，为进一步提升其推理能力提供了深度洞见。

---

## ["Don't Do That!": Guiding Embodied Systems through Large Language Model-based Constraint Generation](https://arxiv.org/abs/2506.04500)

### Abstract
arXiv:2506.04500v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have spurred interest in robotic navigation that incorporates complex spatial, mathematical, and conditional constraints from natural language into the planning problem. Such constraints can be informal yet highly complex, making it challenging to translate into a formal description that can be passed on to a planning algorithm. In this paper, we propose STPR, a constraint generation framework that uses LLMs to translate constraints (expressed as instructions on ``what not to do'') into executable Python functions. STPR leverages the LLM's strong coding capabilities to shift the problem description from language into structured and transparent code, thus circumventing complex reasoning and avoiding potential hallucinations. We show that these LLM-generated functions accurately describe even complex mathematical constraints, and apply them to point cloud representations with traditional search algorithms. Experiments in a simulated Gazebo environment show that STPR ensures full compliance across several constraints and scenarios, while having short runtimes. We also verify that STPR can be used with smaller, code-specific LLMs, making it applicable to a wide range of compact models at low inference cost.

### 摘要
大语言模型（LLMs）的最新进展引发了人们对机器人导航领域的兴趣，该领域旨在将自然语言中复杂的空间、数学和条件约束融入路径规划问题。此类约束可能表述非正式却高度复杂，难以转化为可传递给规划算法的形式化描述。本文提出STPR框架，利用LLMs将约束（以"禁止行为"指令形式表达）转化为可执行的Python函数。STPR通过LLMs强大的编码能力，将问题描述从自然语言转换为结构化且透明的代码，从而规避复杂推理并避免潜在幻觉。实验表明，这些LLM生成的函数能准确描述包括复杂数学约束在内的各类限制条件，并可结合传统搜索算法应用于点云表征。在Gazebo仿真环境中的测试表明，STPR能在多种约束和场景下确保完全合规性，同时保持较短运行时间。研究还验证了STPR可与小型代码专用LLMs配合使用，使其能以较低推理成本适配多种紧凑模型。

---

## [HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource Utilization](https://arxiv.org/abs/2506.04255)

### Abstract
arXiv:2506.04255v1 Announce Type: new 
Abstract: Rapid Large Language Model (LLM) advancements are fueling autonomous Multi-Agent System (MAS) development. However, current frameworks often lack flexibility, resource awareness, model diversity, and autonomous tool creation. This paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent Resource Utilization), a novel MAS framework enhancing flexibility, resource efficiency, and adaptability. HASHIRU features a "CEO" agent dynamically managing specialized "employee" agents, instantiated based on task needs and resource constraints (cost, memory). Its hybrid intelligence prioritizes smaller, local LLMs (via Ollama) while flexibly using external APIs and larger models when necessary. An economic model with hiring/firing costs promotes team stability and efficient resource allocation. The system also includes autonomous API tool creation and a memory function. Evaluations on tasks like academic paper review (58% success), safety assessments (100% on a JailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash on GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate HASHIRU's capabilities. Case studies illustrate its self-improvement via autonomous cost model generation, tool integration, and budget management. HASHIRU offers a promising approach for more robust, efficient, and adaptable MAS through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension. Source code and benchmarks are available at https://github.com/HASHIRU-AI/HASHIRU and https://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is available at https://hashiruagentx-hashiruai.hf.space upon request.

### 摘要
大型语言模型（LLM）的快速发展正推动自主多智能体系统（MAS）的进步。然而，现有框架常存在灵活性不足、资源意识欠缺、模型单一化及自主工具创建能力缺失等问题。本文提出HASHIRU（面向混合智能资源利用的层次化智能体系统），这是一种新型MAS框架，可显著提升灵活性、资源效率与适应性。HASHIRU采用动态管理的"CEO"智能体架构，根据任务需求与资源约束（成本、内存）实例化专业"员工"智能体。其混合智能机制优先调用本地轻量级LLM（通过Ollama），必要时灵活接入外部API及大型模型。集成雇佣/解雇成本的经济模型可维持团队稳定性并优化资源分配。该系统还具备自主API工具创建功能与记忆模块。在学术论文评审（58%成功率）、安全评估（JailbreakBench子集100%通过率）和复杂推理（GSM8K：96% vs Gemini 2.0 Flash的61%；JEEBench：80% vs 68.3%；SVAMP：92% vs 84%）等任务上的评估验证了HASHIRU的卓越性能。案例研究展示了其通过自主成本模型生成、工具集成和预算管理实现自我优化的能力。HASHIRU通过动态层次控制、资源感知的混合智能及自主功能扩展，为构建更强健、高效、自适应的MAS提供了创新方案。

---

## [OpenAg: Democratizing Agricultural Intelligence](https://arxiv.org/abs/2506.04571)

### Abstract
arXiv:2506.04571v1 Announce Type: new 
Abstract: Agriculture is undergoing a major transformation driven by artificial intelligence (AI), machine learning, and knowledge representation technologies. However, current agricultural intelligence systems often lack contextual understanding, explainability, and adaptability, especially for smallholder farmers with limited resources. General-purpose large language models (LLMs), while powerful, typically lack the domain-specific knowledge and contextual reasoning needed for practical decision support in farming. They tend to produce recommendations that are too generic or unrealistic for real-world applications. To address these challenges, we present OpenAg, a comprehensive framework designed to advance agricultural artificial general intelligence (AGI). OpenAg combines domain-specific foundation models, neural knowledge graphs, multi-agent reasoning, causal explainability, and adaptive transfer learning to deliver context-aware, explainable, and actionable insights. The system includes: (i) a unified agricultural knowledge base that integrates scientific literature, sensor data, and farmer-generated knowledge; (ii) a neural agricultural knowledge graph for structured reasoning and inference; (iii) an adaptive multi-agent reasoning system where AI agents specialize and collaborate across agricultural domains; and (iv) a causal transparency mechanism that ensures AI recommendations are interpretable, scientifically grounded, and aligned with real-world constraints. OpenAg aims to bridge the gap between scientific knowledge and the tacit expertise of experienced farmers to support scalable and locally relevant agricultural decision-making.

### 摘要
农业正经历着由人工智能（AI）、机器学习和知识表示技术驱动的重大变革。然而，当前农业智能系统往往缺乏情境理解能力、可解释性和适应性，尤其对资源有限的小农户而言。通用大语言模型（LLM）虽然强大，但通常缺乏农业实践决策支持所需的领域专业知识和情境推理能力，其生成的建议往往过于笼统或脱离实际应用场景。为解决这些问题，我们提出OpenAg框架——一个推进农业通用人工智能（AGI）发展的综合体系。OpenAg整合了领域专用基础模型、神经知识图谱、多智能体推理、因果可解释性和自适应迁移学习，可提供情境感知、可解释且可操作的决策支持。该系统包含：（i）融合科学文献、传感器数据与农户经验知识的统一农业知识库；（ii）支持结构化推理的神经农业知识图谱；（iii）跨农业领域专业化协作的自适应多智能体推理系统；（iv）确保AI建议可解释、科学可靠且符合现实约束的因果透明机制。OpenAg致力于弥合科学知识与经验农户隐性专长之间的鸿沟，为规模化且符合地域特色的农业决策提供支持。

---

## [Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation](https://arxiv.org/abs/2506.04614)

### Abstract
arXiv:2506.04614v1 Announce Type: new 
Abstract: In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.

### 摘要
近年来，多模态大语言模型（MLLMs）已被广泛应用于包括图形用户界面（GUI）自动化在内的多模态推理任务。与一般离线多模态任务不同，GUI自动化在在线交互环境中执行，需要根据环境的实时状态进行逐步决策。该任务对每一步决策错误的容忍度较低，因为任何失误都可能累积破坏流程，甚至导致删除或支付等不可逆后果。针对这些问题，我们提出了一种预操作评判机制，通过推理潜在结果和动作正确性，在实际执行前提供有效反馈。具体而言，我们采用建议感知梯度相对策略优化（S-GRPO）策略构建预操作评判模型GUI-Critic-R1，引入新型建议奖励以增强模型反馈的可靠性。此外，我们开发了基于推理引导的数据收集流程，构建了GUI-Critic-Train和GUI-Critic-Test数据集，填补了现有GUI评判数据的空白。在跨移动端和网页端的GUI-Critic-Test静态实验表明，相较于当前MLLMs，我们的GUI-Critic-R1在评判准确率上具有显著优势。GUI自动化基准的动态评估进一步验证了模型的有效性和优越性，其成功率和操作效率的提升提供了有力证据。

---

## [Agents of Change: Self-Evolving LLM Agents for Strategic Planning](https://arxiv.org/abs/2506.04651)

### Abstract
arXiv:2506.04651v1 Announce Type: new 
Abstract: Recent advances in LLMs have enabled their use as autonomous agents across a range of tasks, yet they continue to struggle with formulating and adhering to coherent long-term strategies. In this paper, we investigate whether LLM agents can self-improve when placed in environments that explicitly challenge their strategic planning abilities. Using the board game Settlers of Catan, accessed through the open-source Catanatron framework, we benchmark a progression of LLM-based agents, from a simple game-playing agent to systems capable of autonomously rewriting their own prompts and their player agent's code. We introduce a multi-agent architecture in which specialized roles (Analyzer, Researcher, Coder, and Player) collaborate to iteratively analyze gameplay, research new strategies, and modify the agent's logic or prompt. By comparing manually crafted agents to those evolved entirely by LLMs, we evaluate how effectively these systems can diagnose failure and adapt over time. Our results show that self-evolving agents, particularly when powered by models like Claude 3.7 and GPT-4o, outperform static baselines by autonomously adopting their strategies, passing along sample behavior to game-playing agents, and demonstrating adaptive reasoning over multiple iterations.

### 摘要
尽管大语言模型（LLM）的最新进展使其能够作为自主代理执行多种任务，但在制定并坚持连贯的长期策略方面仍存在不足。本文探讨了当LLM智能体被置于明确挑战其战略规划能力的环境中时，是否能够实现自我改进。我们通过开源框架Catanatron访问棋盘游戏《卡坦岛》，对一系列基于LLM的智能体进行基准测试，从简单的游戏代理到能够自主重写自身提示词和玩家代理代码的系统。我们提出了一种多智能体架构，其中 specialized roles（分析器、研究员、编码器和玩家）通过协作迭代分析游戏过程、研究新策略并修改代理逻辑或提示词。通过比较人工设计的智能体与完全由LLM进化的智能体，我们评估了这些系统诊断失败和随时间适应的有效性。结果表明，自进化智能体（尤其是由Claude 3.7和GPT-4o等模型驱动的）通过自主采用策略、向游戏代理传递行为样本以及在多次迭代中展现自适应推理能力，显著优于静态基线。

---

## [E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction](https://arxiv.org/abs/2506.04654)

### Abstract
arXiv:2506.04654v1 Announce Type: new 
Abstract: Electric bicycles (e-bikes) are rapidly increasing in use, raising safety concerns due to a rise in accident reports. However, e-bike incident reports often use unstructured narrative formats, which hinders quantitative safety analysis. This study introduces E-bike agents, a framework that uses large language models (LLM) powered agents to classify and extract safety variables from unstructured incident reports. Our framework consists of four LLM agents, handling data classification, information extraction, injury cause determination, and component linkage, to extract the key factors that could lead to E-bike accidents and cause varying severity levels. Furthermore, we used an ordered logit model to examine the relationship between the severity of the incident and the factors retrieved, such as gender, the type of cause, and environmental conditions. Our research shows that equipment issues are slightly more common than human-related ones, but human-related incidents are more often fatal. Specifically, pedals, tires, and brakes are frequent contributors to accidents. The model achieves a high weighted F1 score of 0.87 in classification accuracy, highlighting the potential of using LLMs to extract unstructured data in niche domains, such as transportation. Our method offers a scalable solution to improve e-bike safety analytics and provides actionable information for policy makers, designers, and regulators.

### 摘要
电动自行车（e-bike）的使用量正快速增长，但由于事故报告的增加引发了安全隐患。然而，电动自行车事故报告通常采用非结构化的叙述形式，这阻碍了定量安全分析。本研究提出了E-bike智能体框架，利用基于大语言模型（LLM）的智能体对非结构化事故报告进行分类并提取安全变量。该框架包含四个LLM智能体，分别处理数据分类、信息提取、伤害原因判定和部件关联，以识别可能导致电动自行车事故及不同伤害程度的关键因素。此外，我们采用有序logit模型分析了事故严重程度与提取因素（如性别、原因类型和环境条件）之间的关系。研究表明，设备问题略多于人为因素，但人为相关事故的致死率更高。具体而言，踏板、轮胎和刹车是事故的常见诱因。该模型在分类准确率上达到了0.87的加权F1分数，凸显了LLM在交通等专业领域处理非结构化数据的潜力。本方法为提升电动自行车安全分析提供了可扩展的解决方案，并为政策制定者、设计者和监管机构提供了可操作的信息。

---

## [Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling](https://arxiv.org/abs/2506.04699)

### Abstract
arXiv:2506.04699v1 Announce Type: new 
Abstract: Within the domain of Massively Multiplayer Online (MMO) economy research, Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game economics, evolving from rule-based agents to decision-making agents enhanced by reinforcement learning. Nevertheless, existing works encounter significant challenges when attempting to emulate human-like economic activities among agents, particularly regarding agent reliability, sociability, and interpretability. In this study, we take a preliminary step in introducing a novel approach using Large Language Models (LLMs) in MMO economy simulation. Leveraging LLMs' role-playing proficiency, generative capacity, and reasoning aptitude, we design LLM-driven agents with human-like decision-making and adaptability. These agents are equipped with the abilities of role-playing, perception, memory, and reasoning, addressing the aforementioned challenges effectively. Simulation experiments focusing on in-game economic activities demonstrate that LLM-empowered agents can promote emergent phenomena like role specialization and price fluctuations in line with market rules.

### 摘要
在大型多人在线（MMO）经济研究领域，基于代理的建模（ABM）已成为分析游戏经济的强有力工具，其发展经历了从基于规则的代理到强化学习增强的决策代理的演进。然而，现有研究在模拟代理类人经济活动时面临重大挑战，尤其在代理可靠性、社交性和可解释性方面。本研究率先提出一种利用大型语言模型（LLM）进行MMO经济模拟的新方法。通过结合LLM的角色扮演能力、生成能力和推理天赋，我们设计了具有类人决策与适应性的LLM驱动代理。这些代理具备角色扮演、感知、记忆和推理能力，有效解决了上述挑战。聚焦游戏内经济活动的模拟实验表明，LLM赋能的代理能够促进角色专业化和符合市场规律的价格波动等涌现现象。

---

## [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734)

### Abstract
arXiv:2506.04734v1 Announce Type: new 
Abstract: Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.

### 摘要
以Deepseek-R1-Distill系列为代表的推理模型因其在数学、科学、编程等领域的优异表现，已被开源社区广泛采用。然而我们的研究表明，其基准评估结果容易受到多种因素影响而产生显著波动。评估条件的细微差异可能导致结果出现实质性变化。类似现象也出现在基于Deepseek-R1-Distill系列微调的其他开源推理模型以及QwQ-32B模型中，这使得其宣称的性能提升难以被可靠复现。因此，我们主张建立更严格的模型性能评估范式，并呈现对Deepseek-R1-Distill系列模型的实证评估结果。

---

## [FlashDMoE: Fast Distributed MoE in a Single Kernel](https://arxiv.org/abs/2506.04667)

### Abstract
arXiv:2506.04667v1 Announce Type: new 
Abstract: The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, offering a scalable path to training massive neural networks. However, existing implementations suffer from \emph&#123;low GPU utilization&#125;, \emph&#123;significant latency overhead&#125;, and a fundamental \emph&#123;inability to leverage task locality&#125;, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashDMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a \emph&#123;single persistent GPU kernel&#125;. FlashDMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Its device-initiated communication protocol introduces \emph&#123;payload-efficient&#125; data transfers, significantly shrinking buffer sizes in sparsely activated MoE layers. When evaluated on a single 8-H100 GPU node with MoE models having up to 128 experts and 16K token sequences, FlashDMoE achieves up to \textbf&#123;6&#125;x lower latency, \textbf&#123;5,7&#125;x higher throughput, \textbf&#123;4&#125;x better weak scaling efficiency, and \textbf&#123;9&#125;x higher GPU utilization compared to state-of-the-art baselines, despite using FP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML workloads.

### 摘要
混合专家（MoE）模型的计算稀疏性使得计算成本随模型规模增长呈次线性提升，为训练大规模神经网络提供了可扩展路径。然而，现有实现存在GPU利用率低、延迟开销大及无法利用任务局部性等核心问题，根源在于CPU调度管理、主机端发起的通信以及频繁的内核启动。为突破这些限制，我们开发了FlashDMoE——一个完全驻留GPU的MoE算子，将专家计算与GPU间通信融合为单一持久化GPU内核。该设计实现了调度、计算与聚合阶段的细粒度流水线处理，消除了启动开销并减少空闲间隙。其设备端发起的通信协议采用高载荷效率数据传输，显著压缩稀疏激活MoE层的缓冲区大小。在配备8块H100 GPU的节点上对含128专家和16K令牌序列的MoE模型进行评估时，FlashDMoE相比现有最优基线实现了6倍延迟降低、5.7倍吞吐提升、4倍弱扩展效率改进及9倍GPU利用率提高（尽管采用FP32精度而基线使用FP16）。实验表明，基于原理的GPU内核-硬件协同设计是突破大规模分布式机器学习负载性能瓶颈的关键。

---

## [LLMs for sensory-motor control: Combining in-context and iterative learning](https://arxiv.org/abs/2506.04867)

### Abstract
arXiv:2506.04867v1 Announce Type: new 
Abstract: We propose a method that enables large language models (LLMs) to control embodied agents by directly mapping continuous observation vectors to continuous action vectors. Initially, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. In most cases, it successfully identifies optimal or high-performing solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.

### 摘要
我们提出一种方法，使大型语言模型（LLMs）能够通过将连续观测向量直接映射至连续动作向量来控制具身智能体。该方法首先基于智能体、环境及预期目标的文本描述生成控制策略，随后通过迭代学习过程不断优化策略——在此过程中，大型语言模型会持续接收性能反馈及评估期间收集的感觉运动数据，并被提示改进当前策略。该方法在Gymnasium库的经典控制任务和MuJoCo库的倒立摆任务上得到验证。在多数情况下，通过整合推理获得的符号化知识与智能体环境交互中收集的亚符号感觉运动数据，该方法能成功找到最优或高性能解决方案。

---

## [Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning](https://arxiv.org/abs/2506.04723)

### Abstract
arXiv:2506.04723v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become the dominant paradigm for endowing language models with advanced reasoning capabilities. Despite the substantial empirical gains demonstrated by RL-based training methods like GRPO, a granular understanding of their advantages is still lacking. To address this gap, we introduce a fine-grained analytic framework to dissect the impact of RL on reasoning. Our framework specifically investigates key elements that have been hypothesized to benefit from RL training: (1) plan-following and execution, (2) problem decomposition, and (3) improved reasoning and knowledge utilization. Using this framework, we gain insights beyond mere accuracy. For instance, providing models with explicit step-by-step plans surprisingly degrades performance on the most challenging benchmarks, yet RL-tuned models exhibit greater robustness, experiencing markedly smaller performance drops than their base counterparts. This suggests that RL may not primarily enhance the execution of external plans but rather empower models to formulate and follow internal strategies better suited to their reasoning processes. Conversely, we observe that RL enhances the model's capacity to integrate provided knowledge into its reasoning process, leading to performance improvements across diverse tasks. We also study difficulty, showing improved training by developing new ways to exploit hard problems. Our findings lay a foundation for more principled training and evaluation of reasoning models.

### 摘要
强化学习（RL）已成为赋予语言模型高级推理能力的主流范式。尽管基于RL的训练方法（如GRPO）已展现出显著的实证效果，但对其优势的细致理解仍显不足。为填补这一空白，我们提出了一个细粒度分析框架来剖析RL对推理的影响。该框架重点研究三个被假设受益于RL训练的关键要素：（1）计划遵循与执行，（2）问题分解，以及（3）推理与知识利用能力的提升。通过该框架，我们获得了超越准确率的深入发现：例如，为模型提供显式分步计划反而会降低其在最具挑战性基准测试中的表现，但经过RL调优的模型展现出更强的鲁棒性，其性能下降幅度显著小于基础模型。这表明RL的主要作用可能并非增强外部计划的执行能力，而是帮助模型构建并遵循更契合其推理过程的内部策略。相反，我们发现RL能提升模型将给定知识整合到推理过程中的能力，从而在不同任务中实现性能提升。我们还通过开发新的难题利用方法改进了训练效果。本研究为推理模型更具原则性的训练与评估奠定了基础。

---

## [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)

### Abstract
arXiv:2506.04909v1 Announce Type: new 
Abstract: The honesty of large language models (LLMs) is a critical alignment challenge, especially as advanced systems with chain-of-thought (CoT) reasoning may strategically deceive humans. Unlike traditional honesty issues on LLMs, which could be possibly explained as some kind of hallucination, those models' explicit thought paths enable us to study strategic deception--goal-driven, intentional misinformation where reasoning contradicts outputs. Using representation engineering, we systematically induce, detect, and control such deception in CoT-enabled LLMs, extracting "deception vectors" via Linear Artificial Tomography (LAT) for 89% detection accuracy. Through activation steering, we achieve a 40% success rate in eliciting context-appropriate deception without explicit prompts, unveiling the specific honesty-related issue of reasoning models and providing tools for trustworthy AI alignment.

### 摘要
大型语言模型（LLMs）的诚实性是一个关键的对齐挑战，尤其是具备思维链（CoT）推理能力的高级系统可能对人类实施战略性欺骗。与传统LLMs中可能被解释为某种幻觉的诚实性问题不同，这些模型显式的思维路径使我们能够研究战略性欺骗——即目标驱动、故意提供与推理过程相矛盾的错误信息。通过表征工程，我们系统性地在支持CoT的LLMs中诱导、检测并控制此类欺骗行为，利用线性人工断层扫描（LAT）技术提取"欺骗向量"，实现了89%的检测准确率。借助激活引导技术，我们在无需显式提示的情况下实现了40%的情境适应性欺骗诱发成功率，揭示了推理模型特有的诚实性相关问题，并为构建可信赖的人工智能对齐提供了工具。

---

## [Control Tax: The Price of Keeping AI in Check](https://arxiv.org/abs/2506.05296)

### Abstract
arXiv:2506.05296v1 Announce Type: new 
Abstract: The rapid integration of agentic AI into high-stakes real-world applications requires robust oversight mechanisms. The emerging field of AI Control (AIC) aims to provide such an oversight mechanism, but practical adoption depends heavily on implementation overhead. To study this problem better, we introduce the notion of Control tax -- the operational and financial cost of integrating control measures into AI pipelines. Our work makes three key contributions to the field of AIC: (1) we introduce a theoretical framework that quantifies the Control Tax and maps classifier performance to safety assurances; (2) we conduct comprehensive evaluations of state-of-the-art language models in adversarial settings, where attacker models insert subtle backdoors into code while monitoring models attempt to detect these vulnerabilities; and (3) we provide empirical financial cost estimates for control protocols and develop optimized monitoring strategies that balance safety and cost-effectiveness while accounting for practical constraints like auditing budgets. Our framework enables practitioners to make informed decisions by systematically connecting safety guarantees with their costs, advancing AIC through principled economic feasibility assessment across different deployment contexts.

### 摘要
随着代理型AI在关键现实应用中的快速集成，亟需建立强有力的监管机制。新兴的人工智能控制（AIC）领域旨在提供此类监管机制，但其实际应用效果很大程度上取决于实施成本。为深入研究该问题，我们提出了"控制税"概念——即AI流程中集成控制措施所产生的运营与财务成本。本研究为AIC领域作出三项关键贡献：（1）提出量化控制税的理论框架，将分类器性能映射至安全保证；（2）在对抗环境下对前沿语言模型进行全面评估，其中攻击模型会在代码中植入隐蔽后门，而监测模型则尝试检测这些漏洞；（3）提供控制协议的实际财务成本估算，并开发出兼顾安全性与成本效益的优化监测策略，同时考虑审计预算等现实约束。本框架通过系统化连接安全保证与其成本，使从业者能够做出明智决策，并通过不同部署场景下的原则性经济可行性评估推动AIC领域发展。

---

## [Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based Approach for Complex Arithmetic Reasoning](https://arxiv.org/abs/2506.04998)

### Abstract
arXiv:2506.04998v1 Announce Type: new 
Abstract: Autonomous UAV operation necessitates reliable mathematical reasoning for tasks such as trajectory planning and power management. While traditional flight control relies on hardcoded equations, recent Large Language Models (LLMs) offer potential for more flexible problem-solving but struggle with reliably selecting and applying correct mathematical formulations and executing precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented generation framework designed to improve the mathematical reasoning of several LLMs (including GPT o1/Turbo, Llama-3.2/3.3, Mistral, and DeepSeek R1) in UAV-specific contexts by providing access to relevant domain literature. To conduct an initial assessment, we introduce the UAV-Math-Bench, a small problem set comprising 20 UAV-centric mathematical problems across four difficulty levels. Our experiments demonstrate that incorporating retrieval substantially increases exact answer accuracy (achieving up to 75% with o1), reduces instances of incorrect formulation selection (from 25% without RAG to 5% with RAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders of magnitude for the best-performing models. This pilot study indicates that RAG can enable general-purpose LLMs to function as more reliable tools for engineering analysis, although direct real-time flight control requires further investigation and validation on a larger scale. All benchmark data, question and answer are publicly available.

### 摘要
无人机自主运行需要可靠的数学推理能力以完成轨迹规划和电源管理等任务。传统飞行控制依赖于硬编码方程，而近期大型语言模型（LLMs）虽展现出更灵活的问题解决潜力，但在可靠选择应用正确数学公式及执行精确多步运算方面仍存在不足。我们提出RAG-UAV框架，通过检索增强生成技术为多种LLM（包括GPT o1/Turbo、Llama-3.2/3.3、Mistral和DeepSeek R1）提供领域文献访问，以提升其在无人机专用场景中的数学推理能力。为开展初步评估，我们构建了UAV-Math-Bench小型问题集，包含20个无人机相关数学问题，分为四个难度等级。实验表明，引入检索机制能显著提升精确答案准确率（o1模型最高达75%），降低错误公式选择率（从无RAG时的25%降至5%），并减少数值误差——最佳模型的均方误差（MSE）可降低数个数量级。这项试点研究表明，尽管实时飞行控制仍需更大规模的验证，RAG技术能使通用LLM成为更可靠的工程分析工具。全部基准数据、问题及答案均已公开。

---

## [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)

### Abstract
arXiv:2506.03922v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks in the HSS domain require more horizontal, interdisciplinary thinking and a deep integration of knowledge across related fields, which presents unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations. Addressing this gap, we present HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. We also introduce a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories. We benchmark more than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant challenges even for state-of-the-art models. We hope that this benchmark will inspire further research into enhancing the cross-disciplinary reasoning abilities of MLLMs, especially their capacity to internalize and connect knowledge across fields.

### 摘要
多模态大语言模型（MLLMs）在推动广泛领域发展方面展现出巨大潜力。然而，当前评估MLLMs的基准主要强调STEM学科典型的通用知识与纵向分步推理能力，忽视了人文社会科学（HSS）领域的独特需求与潜力。HSS领域的任务需要更多横向跨学科思维及相关领域知识的深度融合，这对MLLMs提出了独特挑战——尤其在抽象概念与对应视觉表征的关联方面。为填补这一空白，我们推出HSSBench：一个专为评估MLLMs在多语言（含联合国六种官方语言）HSS任务表现而设计的基准。我们还提出了一种面向HSS场景的新型数据生成流程，通过领域专家与自动化代理协作生成并迭代优化每个样本。HSSBench包含13,000余个精心设计的样本，涵盖六大关键类别。我们对20余个主流MLLM进行基准测试，结果表明即使最先进模型仍面临显著挑战。我们希望该基准能促进对MLLMs跨学科推理能力（特别是跨领域知识内化与关联能力）提升的进一步研究。

---

## [Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning](https://arxiv.org/abs/2506.05256)

### Abstract
arXiv:2506.05256v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) achieve higher performance on challenging reasoning tasks by generating more tokens at inference time, but this verbosity often wastes computation on easy problems. Existing solutions, including supervised finetuning on shorter traces, user-controlled budgets, or RL with uniform penalties, either require data curation, manual configuration, or treat all problems alike regardless of difficulty. We introduce Adaptive Length Penalty (ALP), a reinforcement learning objective tailoring generation length to per-prompt solve rate. During training, ALP monitors each prompt's online solve rate through multiple rollouts and adds a differentiable penalty whose magnitude scales inversely with that rate, so confident (easy) prompts incur a high cost for extra tokens while hard prompts remain unhindered. Posttraining DeepScaleR-1.5B with ALP cuts average token usage by 50\% without significantly dropping performance. Relative to fixed-budget and uniform penalty baselines, ALP redistributes its reduced budget more intelligently by cutting compute on easy prompts and reallocating saved tokens to difficult ones, delivering higher accuracy on the hardest problems with higher cost.

### 摘要
大型推理模型（LRMs）通过在推理时生成更多标记来提升复杂推理任务的性能，但这种冗余性常导致简单问题上浪费算力。现有解决方案（包括对短轨迹的监督微调、用户预设预算或采用统一惩罚的强化学习）要么需要数据筛选和人工配置，要么对所有问题采取无差别处理。我们提出自适应长度惩罚（ALP），这是一种根据每个提示的实时解决率调整生成长度的强化学习目标。训练过程中，ALP通过多次 rollout 监测各提示的在线解决率，并施加与解决率成反比的可微惩罚——高置信度（简单）提示会因额外标记产生高成本，而困难提示则不受限制。采用ALP对DeepScaleR-1.5B进行训练后，平均标记使用量减少50%且性能无显著下降。相较于固定预算和统一惩罚基线，ALP通过削减简单提示的算力并将节省的标记重新分配给困难提示，实现了更智能的预算再分配，从而以更高成本在最难题上获得更优准确率。

---

## [Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games](https://arxiv.org/abs/2506.05309)

### Abstract
arXiv:2506.05309v1 Announce Type: new 
Abstract: LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant's decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.

### 摘要
大型语言模型（LLMs）目前主要应用于同步通信场景，即人类用户与模型通过交替轮次进行交互。然而，现实世界中的许多场景本质上是异步的。例如，在群聊、在线团队会议或社交游戏中，并不存在固有的轮次概念，因此发言时机的选择成为参与者决策的关键部分。本研究开发了一种自适应异步LLM智能体，该智能体不仅能决定发言内容，还能自主判断发言时机。为评估该智能体性能，我们收集了包含人类参与者及异步智能体的在线'狼人杀'游戏独特数据集。总体而言，该智能体在游戏表现和与人类玩家融合度方面均达到与人类相当的水平。分析表明，智能体在发言时机决策上的行为模式与人类高度相似，但在消息内容上存在差异。我们公开了所有数据与代码，以支持和鼓励进一步研究更贴近现实的LLM智能体异步通信。这项工作为将LLMs整合到真实人类群体场景奠定了基础，无论是辅助团队讨论，还是应用于需要处理复杂社交动态的教育及专业环境。

---

## [COSMOS: Predictable and Cost-Effective Adaptation of LLMs](https://arxiv.org/abs/2505.01449)

### Abstract
arXiv:2505.01449v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards.

### 摘要
大型语言模型（LLMs）通过采用多样化的适应策略，在众多任务中展现出卓越性能。然而，在资源受限条件下如何最优选择模型及适应策略仍具挑战性，通常需要大量实验验证。本研究探讨是否能在不进行昂贵试验的情况下，准确预测性能与成本消耗。我们形式化定义了LLM策略选择问题，并提出COSMOS这一统一预测框架，该框架能以极低成本高效预估适应效果。我们通过两组强效预测器实例化并验证该框架能力：基于嵌入增强的轻量级代理模型用于预测微调性能，以及低样本规模法则用于预估检索增强的上下文学习效果。在八个代表性基准测试上的广泛评估表明，COSMOS在保持高预测精度的同时，平均降低92.72%的计算成本，在资源密集型场景中最高可减少98.71%开销。研究结果证明，高效预测模型适应效果不仅可行，还能在维持性能标准的同时显著降低LLM部署的计算负担。

---

## [LLM-First Search: Self-Guided Exploration of the Solution Space](https://arxiv.org/abs/2506.05213)

### Abstract
arXiv:2506.05213v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable improvements in reasoning and planning through increased test-time compute, often by framing problem-solving as a search process. While methods like Monte Carlo Tree Search (MCTS) have proven effective in some domains, their reliance on fixed exploration hyperparameters limits their adaptability across tasks of varying difficulty, rendering them impractical or expensive in certain settings. In this paper, we propose \textbf&#123;LLM-First Search (LFS)&#125;, a novel \textit&#123;LLM Self-Guided Search&#125; method that removes the need for pre-defined search strategies by empowering the LLM to autonomously control the search process via self-guided exploration. Rather than relying on external heuristics or hardcoded policies, the LLM evaluates whether to pursue the current search path or explore alternative branches based on its internal scoring mechanisms. This enables more flexible and context-sensitive reasoning without requiring manual tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku against three classic widely-used search algorithms, Tree-of-Thoughts' Breadth First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which have been used to achieve SotA results on a range of challenging reasoning tasks. We found that LFS (1) performs better on more challenging tasks without additional tuning, (2) is more computationally efficient compared to the other methods, especially when powered by a stronger model, (3) scales better with stronger models, due to its LLM-First design, and (4) scales better with increased compute budget. Our code is publicly available at \href&#123;https://github.com/NathanHerr/LLM-First-Search&#125;&#123;LLM-First-Search&#125;.

### 摘要
大型语言模型（LLMs）通过增加测试时计算量，在推理与规划能力上展现出显著提升，其常见方式是将问题解决构建为搜索过程。尽管蒙特卡洛树搜索（MCTS）等方法在某些领域已被证实有效，但其依赖固定探索超参数的特性限制了跨不同难度任务的适应性，导致在特定场景下不切实际或成本高昂。本文提出**LLM优先搜索（LFS）**——一种新型的*LLM自引导搜索*方法，该方法通过赋予LLM自主控制搜索过程的能力，消除了对预定义搜索策略的需求。LFS不依赖外部启发式或硬编码策略，而是基于模型内部评分机制自主判断继续当前搜索路径或探索替代分支，从而实现无需人工调参或任务特定适配的灵活、上下文敏感推理。我们在Countdown和Sudoku任务上对比了三种经典搜索算法：思维树广度优先搜索（ToT-BFS）、最佳优先搜索（BestFS）和MCTS（这些算法均已在多项复杂推理任务中取得SotA成果）。实验表明LFS具有以下优势：（1）在更高难度任务上表现更优且无需额外调参；（2）计算效率更高（尤其搭载更强模型时）；（3）因其LLM优先设计，模型性能越强扩展性越好；（4）计算预算增加时扩展性更佳。代码已开源于\href&#123;https://github.com/NathanHerr/LLM-First-Search&#125;&#123;LLM-First-Search&#125;。

---

## [Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark](https://arxiv.org/abs/2506.04280)

### Abstract
arXiv:2506.04280v1 Announce Type: cross 
Abstract: With enhanced capabilities and widespread applications, Multimodal Large Language Models (MLLMs) are increasingly required to process and reason over multiple images simultaneously. However, existing MLLM benchmarks focus either on single-image visual reasoning or on multi-image understanding tasks with only final-answer evaluation, leaving the reasoning capabilities of MLLMs over multi-image inputs largely underexplored. To address this gap, we introduce the $\textbf&#123;Multimodal Multi-image Reasoning Benchmark (MMRB)&#125;$, the first benchmark designed to evaluate structured visual reasoning across multiple images. MMRB comprises $\textbf&#123;92 sub-tasks&#125;$ covering spatial, temporal, and semantic reasoning, with multi-solution, CoT-style annotations generated by GPT-4o and refined by human experts. A derivative subset is designed to evaluate multimodal reward models in multi-image scenarios. To support fast and scalable evaluation, we propose a sentence-level matching framework using open-source LLMs. Extensive baseline experiments on $\textbf&#123;40 MLLMs&#125;$, including 9 reasoning-specific models and 8 reward models, demonstrate that open-source MLLMs still lag significantly behind commercial MLLMs in multi-image reasoning tasks. Furthermore, current multimodal reward models are nearly incapable of handling multi-image reward ranking tasks.

### 摘要
随着功能增强和应用普及，多模态大语言模型（MLLMs）日益需要同时处理和推理多幅图像。然而现有MLLM基准测试要么关注单图像视觉推理，要么仅通过最终答案评估多图像理解任务，导致对MLLMs多图像输入推理能力的探索严重不足。为填补这一空白，我们提出首个评估多图像结构化视觉推理的基准测试——多模态多图像推理基准（MMRB）。该基准包含92个子任务，涵盖空间、时间和语义推理，所有任务均配备由GPT-4o生成并经专家修正的多解决方案、思维链式标注。特别设计衍生子集用于评估多图像场景下的多模态奖励模型。为支持高效可扩展的评估，我们提出基于开源LLMs的句子级匹配框架。在40个MLLMs（含9个专用推理模型和8个奖励模型）上的基线实验表明：开源MLLMs在多图像推理任务中仍显著落后于商业模型，且现有多模态奖励模型几乎无法处理多图像奖励排序任务。

---

## [Knowledge-guided Contextual Gene Set Analysis Using Large Language Models](https://arxiv.org/abs/2506.04303)

### Abstract
arXiv:2506.04303v1 Announce Type: cross 
Abstract: Gene set analysis (GSA) is a foundational approach for interpreting genomic data of diseases by linking genes to biological processes. However, conventional GSA methods overlook clinical context of the analyses, often generating long lists of enriched pathways with redundant, nonspecific, or irrelevant results. Interpreting these requires extensive, ad-hoc manual effort, reducing both reliability and reproducibility. To address this limitation, we introduce cGSA, a novel AI-driven framework that enhances GSA by incorporating context-aware pathway prioritization. cGSA integrates gene cluster detection, enrichment analysis, and large language models to identify pathways that are not only statistically significant but also biologically meaningful. Benchmarking on 102 manually curated gene sets across 19 diseases and ten disease-related biological mechanisms shows that cGSA outperforms baseline methods by over 30%, with expert validation confirming its increased precision and interpretability. Two independent case studies in melanoma and breast cancer further demonstrate its potential to uncover context-specific insights and support targeted hypothesis generation.

### 摘要
基因集分析（GSA）是通过将基因与生物过程相关联来解释疾病基因组数据的基础性方法。然而，传统GSA方法忽略了分析的临床背景，常常生成包含冗余、非特异性或无关结果的长串富集通路列表。解释这些结果需要大量临时性人工工作，降低了可靠性和可重复性。为克服这一局限，我们提出了cGSA这一新型人工智能驱动框架，它通过纳入上下文感知的通路优先级排序来增强GSA。cGSA整合了基因簇检测、富集分析和大型语言模型，不仅能识别具有统计学显著性的通路，还能发现具有生物学意义的通路。在19种疾病和十种疾病相关生物机制的102个手动整理基因集上的基准测试表明，cGSA性能优于基线方法30%以上，专家验证证实其具有更高的精确度和可解释性。在黑色素瘤和乳腺癌中的两项独立案例研究进一步证明了其发现特定上下文信息和支持针对性假设生成的潜力。

---

## [AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents](https://arxiv.org/abs/2506.04293)

### Abstract
arXiv:2506.04293v1 Announce Type: cross 
Abstract: Clinical trials are critical for advancing medical treatments but remain prohibitively expensive and time-consuming. Accurate prediction of clinical trial outcomes can significantly reduce research and development costs and accelerate drug discovery. While recent deep learning models have shown promise by leveraging unstructured data, their black-box nature, lack of interpretability, and vulnerability to label leakage limit their practical use in high-stakes biomedical contexts. In this work, we propose AutoCT, a novel framework that combines the reasoning capabilities of large language models with the explainability of classical machine learning. AutoCT autonomously generates, evaluates, and refines tabular features based on public information without human input. Our method uses Monte Carlo Tree Search to iteratively optimize predictive performance. Experimental results show that AutoCT performs on par with or better than SOTA methods on clinical trial prediction tasks within only a limited number of self-refinement iterations, establishing a new paradigm for scalable, interpretable, and cost-efficient clinical trial prediction.

### 摘要
临床试验对推动医学治疗发展至关重要，但其高昂成本与漫长周期仍构成重大障碍。准确预测临床试验结果能显著降低研发成本并加速药物发现。尽管当前深度学习模型通过利用非结构化数据展现出潜力，但其黑箱特性、可解释性缺失及标签泄漏风险限制了在高风险生物医学场景中的实际应用。本研究提出AutoCT框架，创新性地将大语言模型的推理能力与经典机器学习可解释性相结合。该框架无需人工干预，即可基于公开信息自主生成、评估并优化表格特征。我们采用蒙特卡洛树搜索方法实现预测性能的迭代优化。实验结果表明，在有限的自优化迭代次数内，AutoCT在临床试验预测任务中达到或超越现有最优方法性能，为构建可扩展、可解释且经济高效的临床试验预测体系确立了新范式。

---

## [RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought](https://arxiv.org/abs/2506.04277)

### Abstract
arXiv:2506.04277v1 Announce Type: cross 
Abstract: Multi-modal Large Language Models (MLLMs) have demonstrated remarkable reasoning capability while lack explicit mechanisms for visual grounding and segmentation, creating a gap between cognitive reasoning and visual perception. To bridge this gap, we introduce Reasoning Segmentation via Visual Prompting (RSVP), a novel framework that unifies multi-step multimodal reasoning with grounded visual understanding. RSVP is a two-stage structuralized framework that integrates reasoning-driven localization with segmentation refinement. In the reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to help MLLMs understand queries and infer targets, generating interpretable region proposals that enhance visual grounding. In segmentation stage, RSVP refines these proposals with a Vision-Language Segmentation Module (VLSM), seamlessly integrates textual and visual cues to produce precise segmentation masks. By explicitly modelling the interaction between multimodal reasoning and segmentation, RSVP introduces a new paradigm for interpretable reasoning segmentation. It exploits MLLMs' inherent localization capabilities, enabling the models to not only reason about objects but also generate structured visual representations. Our extensive experiments demonstrate that RSVP achieves state-of-the-art performance, surpasses state-of-the-art methods by up to +6.5 gIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under zero-shot settings. These results validate RSVP as an effective and scalable framework for integrating cognitive reasoning with structured visual understanding.

### 摘要
多模态大语言模型（MLLMs）虽展现出卓越的推理能力，但缺乏显式的视觉 grounding 和分割机制，导致认知推理与视觉感知之间存在鸿沟。为弥合这一差距，我们提出基于视觉提示的推理分割框架RSVP，该创新性架构将多步多模态推理与 grounded 视觉理解相统一。RSVP采用两阶段结构化设计：第一阶段通过推理驱动定位生成可解释的区域建议以增强视觉 grounding，其中利用多模态思维链视觉提示帮助MLLMs理解查询并推断目标；第二阶段通过视觉语言分割模块（VLSM）整合文本与视觉线索，对建议区域进行精细化处理以生成精确分割掩码。通过显式建模多模态推理与分割的交互作用，RSVP开创了可解释推理分割的新范式，其充分挖掘MLLMs固有的定位能力，使模型不仅能推理物体还能生成结构化视觉表征。大量实验表明，RSVP在ReasonSeg数据集上以+6.5 gIoU和+9.2 cIoU的优势超越现有最优方法，在零样本设置下的SegInW数据集达到49.7 mAP，验证了该框架在融合认知推理与结构化视觉理解方面的有效性和可扩展性。

---

## [Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390)

### Abstract
arXiv:2506.04390v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) systems are vulnerable to attacks that inject poisoned passages into the retrieved set, even at low corruption rates. We show that existing attacks are not designed to be stealthy, allowing reliable detection and mitigation. We formalize stealth using a distinguishability-based security game. If a few poisoned passages are designed to control the response, they must differentiate themselves from benign ones, inherently compromising stealth. This motivates the need for attackers to rigorously analyze intermediate signals involved in generation$\unicode&#123;x2014&#125;$such as attention patterns or next-token probability distributions$\unicode&#123;x2014&#125;$to avoid easily detectable traces of manipulation. Leveraging attention patterns, we propose a passage-level score$\unicode&#123;x2014&#125;$the Normalized Passage Attention Score$\unicode&#123;x2014&#125;$used by our Attention-Variance Filter algorithm to identify and filter potentially poisoned passages. This method mitigates existing attacks, improving accuracy by up to $\sim 20 \%$ over baseline defenses. To probe the limits of attention-based defenses, we craft stealthier adaptive attacks that obscure such traces, achieving up to $35 \%$ attack success rate, and highlight the challenges in improving stealth.

### 摘要
检索增强生成（RAG）系统容易受到将污染段落注入检索集的攻击，即使在低污染率下也是如此。我们发现现有攻击缺乏隐蔽性设计，使得检测和缓解成为可能。通过基于可区分性的安全博弈，我们形式化定义了隐蔽性概念：若少数污染段落需控制生成结果，其必须与良性段落形成差异，本质上会破坏隐蔽性。这要求攻击者必须严格分析生成过程中的中间信号（如注意力模式或下一词元概率分布），以避免留下易被检测的操纵痕迹。基于注意力模式，我们提出段落级评分指标——标准化段落注意力分数，并设计注意力方差过滤算法来识别和过滤潜在污染段落。该方法可有效抵御现有攻击，在基线防御基础上将准确率提升约20%。为探究基于注意力的防御极限，我们构建了更隐蔽的自适应攻击来掩盖此类痕迹，攻击成功率最高达35%，同时揭示了提升隐蔽性面临的挑战。

---

## [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)

### Abstract
arXiv:2506.04385v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) tasks, largely due to their generalisability and ability to perform tasks without additional training. However, their effectiveness for low-resource languages remains limited. In this study, we evaluate the performance of 55 publicly available LLMs on Maltese, a low-resource language, using a newly introduced benchmark covering 11 discriminative and generative tasks. Our experiments highlight that many models perform poorly, particularly on generative tasks, and that smaller fine-tuned models often perform better across all tasks. From our multidimensional analysis, we investigate various factors impacting performance. We conclude that prior exposure to Maltese during pre-training and instruction-tuning emerges as the most important factor. We also examine the trade-offs between fine-tuning and prompting, highlighting that while fine-tuning requires a higher initial cost, it yields better performance and lower inference costs. Through this work, we aim to highlight the need for more inclusive language technologies and recommend that researchers working with low-resource languages consider more "traditional" language modelling approaches.

### 摘要
大语言模型（LLMs）在各种自然语言处理（NLP）任务中展现出卓越性能，这主要归功于其泛化能力及无需额外训练即可执行任务的特点。然而，其在低资源语言中的有效性仍有限。本研究通过新引入的涵盖11项判别性与生成性任务的基准测试，评估了55个公开可用的LLMs对低资源语言马耳他语的表现。实验结果表明，多数模型表现欠佳（尤其在生成任务中），而经过微调的小型模型通常在各项任务中表现更优。通过多维分析，我们探究了影响性能的多种因素，发现预训练和指令微调阶段是否接触马耳他语是最关键因素。我们还比较了微调与提示工程的权衡关系，指出尽管微调初始成本较高，但能带来更优性能及更低推理成本。本研究旨在强调开发更具包容性语言技术的必要性，并建议低资源语言研究者更多考虑"传统"语言建模方法。

---

## [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)

### Abstract
arXiv:2506.04405v1 Announce Type: cross 
Abstract: We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.

### 摘要
我们推出MedAgentGYM——首个旨在增强大语言模型（LLM）智能体编码医疗推理能力的公开训练环境。该平台包含源自真实世界生物医学场景的129个类别共计72,413项任务实例，所有任务均封装于可执行编码环境中，具备详细任务描述、交互式反馈机制、可验证的基准标注以及可扩展的训练轨迹生成功能。通过对30余种LLM的广泛基准测试，我们发现基于商业API的模型与开源模型之间存在显著性能差距。利用MedAgentGYM训练后，Med-Copilot-7B通过监督微调（+36.44%）和持续强化学习（+42.47%）实现显著性能提升，成为与gpt-4o竞争且兼具经济性与隐私保护优势的替代方案。通过提供统一执行环境下的综合基准测试与可访问、可扩展的训练资源，MedAgentGYM为开发面向高端生物医学研究与实践的LLM编码助手提供了集成化平台。

---

## [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)

### Abstract
arXiv:2506.04353v1 Announce Type: cross 
Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual question answering (VQA) in chest radiology, comprising approximately 696,000 questions paired with 160,000 chest X-rays studies across training, validation, and test sets. Unlike prior efforts that rely heavily on template based queries, ReXVQA introduces a diverse and clinically authentic task suite reflecting five core radiological reasoning skills: presence assessment, location analysis, negation detection, differential diagnosis, and geometric reasoning. We evaluate eight state-of-the-art multimodal large language models, including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge the gap between AI performance and clinical expertise, we conducted a comprehensive human reader study involving 3 radiology residents on 200 randomly sampled cases. Our evaluation demonstrates that MedGemma achieved superior performance (83.84% accuracy) compared to human readers (best radiology resident: 77.27%), representing a significant milestone where AI performance exceeds expert human evaluation on chest X-ray interpretation. The reader study reveals distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists while showing more variable agreement patterns between human readers and AI models. ReXVQA establishes a new standard for evaluating generalist radiological AI systems, offering public leaderboards, fine-grained evaluation splits, structured explanations, and category-level breakdowns. This benchmark lays the foundation for next-generation AI systems capable of mimicking expert-level clinical reasoning beyond narrow pathology classification. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA

### 摘要
我们推出ReXVQA——目前胸部放射学领域规模最大、最全面的视觉问答（VQA）基准测试，包含约696,000个问题与160,000份胸部X光检查的配对数据，涵盖训练集、验证集和测试集。与以往主要依赖模板化查询的研究不同，ReXVQA引入了一套多样化且具有临床真实性的任务集，反映五种核心放射学推理技能：存在性评估、位置分析、否定检测、鉴别诊断和几何推理。我们评估了八种最先进的多模态大语言模型，包括MedGemma-4B-it、Qwen2.5-VL、Janus-Pro-7B和Eagle2-9B。性能最佳的模型（MedGemma）达到83.24%的整体准确率。为弥合AI表现与临床专业水平之间的差距，我们开展了包含3名放射科住院医师对200例随机抽样病例的综合人类阅片研究。评估表明MedGemma以83.84%的准确率优于人类阅片者（最佳住院医师：77.27%），标志着AI在胸部X光解读上首次超越专家级人类评估。阅片研究揭示了AI模型与人类专家间的显著表现差异：放射科医师内部具有高度一致性，而人类与AI模型间的吻合度则呈现更大变异。ReXVQA通过公开排行榜、细粒度评估分组、结构化解释和类别细分，为通用放射学AI系统设立了新标准。该基准为开发超越狭隘病理分类、能模拟专家级临床推理的新一代AI系统奠定基础。数据集将开源于https://huggingface.co/datasets/rajpurkarlab/ReXVQA。

---

## [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)

### Abstract
arXiv:2506.04408v1 Announce Type: cross 
Abstract: Humans have a remarkable ability to acquire and understand grammatical phenomena that are seen rarely, if ever, during childhood. Recent evidence suggests that language models with human-scale pretraining data may possess a similar ability by generalizing from frequent to rare constructions. However, it remains an open question how widespread this generalization ability is, and to what extent this knowledge extends to meanings of rare constructions, as opposed to just their forms. We fill this gap by testing human-scale transformer language models on their knowledge of both the form and meaning of the (rare and quirky) English LET-ALONE construction. To evaluate our LMs we construct a bespoke synthetic benchmark that targets syntactic and semantic properties of the construction. We find that human-scale LMs are sensitive to form, even when related constructions are filtered from the dataset. However, human-scale LMs do not make correct generalizations about LET-ALONE's meaning. These results point to an asymmetry in the current architectures' sample efficiency between language form and meaning, something which is not present in human language learners.

### 摘要
人类具有一种非凡的能力，能够习得并理解在童年时期极少甚至从未接触过的语法现象。近期证据表明，经过人类规模预训练数据的语言模型可能通过从常见结构推广到罕见结构而具备类似能力。然而，这种泛化能力的普遍性程度，以及这类知识在多大程度上能扩展到罕见结构的含义（而非仅其形式），仍是悬而未决的问题。我们通过测试人类规模的Transformer语言模型对英语中（罕见且特殊的）LET-ALONE结构的形式与含义的掌握情况来填补这一空白。为评估模型，我们构建了一个针对该结构句法和语义特性的定制化合成基准测试。研究发现，即使从数据集中过滤掉相关结构，人类规模的语言模型仍对形式敏感。然而，这些模型未能对LET-ALONE的含义做出正确泛化。这些结果表明当前架构在语言形式与含义的样本效率上存在不对称性，而这种不对称性在人类语言学习者中并不存在。

---

## [Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs](https://arxiv.org/abs/2506.04534)

### Abstract
arXiv:2506.04534v1 Announce Type: cross 
Abstract: Discourse particles are crucial elements that subtly shape the meaning of text. These words, often polyfunctional, give rise to nuanced and often quite disparate semantic/discourse effects, as exemplified by the diverse uses of the particle "just" (e.g., exclusive, temporal, emphatic). This work investigates the capacity of LLMs to distinguish the fine-grained senses of English "just", a well-studied example in formal semantics, using data meticulously created and labeled by expert linguists. Our findings reveal that while LLMs exhibit some ability to differentiate between broader categories, they struggle to fully capture more subtle nuances, highlighting a gap in their understanding of discourse particles.

### 摘要
话语粒子是微妙塑造文本意义的关键要素。这类多功能词汇常产生细致且差异显著的语义/话语效果，如"just"粒子（如排他性、时间性、强调性等）的多样化用法所示。本研究以形式语义学中备受关注的英语"just"为例，利用语言学家精心创建和标注的数据，探究大语言模型区分其细粒度语义的能力。研究发现，尽管大语言模型展现出了区分广义范畴的某些能力，但在捕捉更微妙语义差异方面仍存在不足，这凸显了其在理解话语粒子方面的局限性。

---

## [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450)

### Abstract
arXiv:2506.04450v1 Announce Type: cross 
Abstract: Purpose: This study proposes a framework for fine-tuning large language models (LLMs) with differential privacy (DP) to perform multi-abnormality classification on radiology report text. By injecting calibrated noise during fine-tuning, the framework seeks to mitigate the privacy risks associated with sensitive patient data and protect against data leakage while maintaining classification performance. Materials and Methods: We used 50,232 radiology reports from the publicly available MIMIC-CXR chest radiography and CT-RATE computed tomography datasets, collected between 2011 and 2019. Fine-tuning of LLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels from CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA) in high and moderate privacy regimes (across a range of privacy budgets = &#123;0.01, 0.1, 1.0, 10.0&#125;). Model performance was evaluated using weighted F1 score across three model architectures: BERT-medium, BERT-small, and ALBERT-base. Statistical analyses compared model performance across different privacy levels to quantify the privacy-utility trade-off. Results: We observe a clear privacy-utility trade-off through our experiments on 2 different datasets and 3 different models. Under moderate privacy guarantees the DP fine-tuned models achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on CT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively. Conclusion: Differentially private fine-tuning using LoRA enables effective and privacy-preserving multi-abnormality classification from radiology reports, addressing a key challenge in fine-tuning LLMs on sensitive medical data.

### 摘要
目的：本研究提出一种基于差分隐私（DP）的大语言模型（LLM）微调框架，用于放射学报告文本的多异常分类。通过在微调过程中注入校准噪声，该框架旨在降低敏感患者数据的隐私风险，防止数据泄露，同时保持分类性能。材料与方法：我们使用2011至2019年间公开的MIMIC-CXR胸部X光片和CT-RATE计算机断层扫描数据集中的50,232份放射学报告。采用差分隐私低秩自适应（DP-LoRA）方法在高/中隐私保护级别（隐私预算ε∈&#123;0.01,0.1,1.0,10.0&#125;）下对LLM进行微调，分别分类MIMIC-CXR数据集的14个标签和CT-RATE数据集的18个标签。通过BERT-medium、BERT-small和ALBERT-base三种架构模型评估加权F1分数，统计分析比较不同隐私级别下的模型性能以量化隐私-效用权衡。结果：在2个不同数据集和3种模型上的实验显示出明确的隐私-效用权衡。在中等隐私保护下，DP微调模型获得的加权F1分数与无隐私LoRA基线相当（MIMIC-CXR为0.88 vs 0.90，CT-RATE为0.59 vs 0.78）。结论：基于LoRA的差分隐私微调技术可实现放射学报告的有效且隐私保护的多异常分类，为敏感医疗数据上的LLM微调提供了关键解决方案。

---

## [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)

### Abstract
arXiv:2506.04544v1 Announce Type: cross 
Abstract: Large language models (LLMs) are playing an increasingly large role in domains such as code generation, including hardware code generation, where Verilog is the key language. However, the amount of publicly available Verilog code pales in comparison to the amount of code available for software languages like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which seeks to increase the amount of available human-written Verilog data by translating or compiling three other hardware description languages - VHDL, Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v in enhancing LLM Verilog generation by improving performance of a 32 billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2, without utilizing any data augmentation or knowledge distillation from larger models. We also show hdl2v's ability to boost the performance of a data augmentation-based fine-tuning approach by 63%. Finally, we characterize and analyze our dataset to better understand which characteristics of HDL-to-Verilog datasets can be expanded upon in future work for even better performance.

### 摘要
大语言模型（LLMs）在代码生成等领域正发挥着日益重要的作用，其中硬件代码生成以Verilog为核心语言。然而，与Python等软件语言相比，公开可用的Verilog代码数量相形见绌。本研究提出hdl2v（"HDL-to-Verilog"）数据集，旨在通过将其他三种硬件描述语言——VHDL、Chisel和PyMTL3——翻译或编译为Verilog，从而增加可用的人工编写Verilog数据量。进一步地，我们验证了hdl2v在增强LLM生成Verilog能力方面的价值：在不使用任何数据增强或从更大模型进行知识蒸馏的情况下，该数据集使一个320亿参数的开源权重模型在VerilogEvalV2基准上的性能提升最高达23%（pass@10）。同时，我们还展示了hdl2v能将基于数据增强的微调方法性能提升63%。最后，我们对数据集进行了特征化分析，以深入理解HDL-to-Verilog数据集的哪些特性可在未来工作中进一步扩展，从而获得更优性能。

---

## [BEAR: BGP Event Analysis and Reporting](https://arxiv.org/abs/2506.04514)

### Abstract
arXiv:2506.04514v1 Announce Type: cross 
Abstract: The Internet comprises of interconnected, independently managed Autonomous Systems (AS) that rely on the Border Gateway Protocol (BGP) for inter-domain routing. BGP anomalies--such as route leaks and hijacks--can divert traffic through unauthorized or inefficient paths, jeopardizing network reliability and security. Although existing rule-based and machine learning methods can detect these anomalies using structured metrics, they still require experts with in-depth BGP knowledge of, for example, AS relationships and historical incidents, to interpret events and propose remediation. In this paper, we introduce BEAR (BGP Event Analysis and Reporting), a novel framework that leverages large language models (LLMs) to automatically generate comprehensive reports explaining detected BGP anomaly events. BEAR employs a multi-step reasoning process that translates tabular BGP data into detailed textual narratives, enhancing interpretability and analytical precision. To address the limited availability of publicly documented BGP anomalies, we also present a synthetic data generation framework powered by LLMs. Evaluations on both real and synthetic datasets demonstrate that BEAR achieves 100% accuracy, outperforming Chain-of-Thought and in-context learning baselines. This work pioneers an automated approach for explaining BGP anomaly events, offering valuable operational insights for network management.

### 摘要
互联网由相互连接、独立管理的自治系统（AS）构成，这些系统依赖边界网关协议（BGP）实现域间路由。BGP异常（如路由泄漏和劫持）可能导致流量通过未经授权或低效路径传输，危及网络可靠性与安全性。尽管现有基于规则和机器学习的方法能利用结构化指标检测此类异常，但仍需具备AS关系、历史事件等深入BGP知识的专家来解读事件并提出修复方案。本文提出BEAR（BGP事件分析与报告）框架，该创新框架利用大语言模型（LLM）自动生成解释已检测BGP异常事件的综合分析报告。BEAR采用多步推理流程，将表格化BGP数据转化为详细文本描述，从而提升可解释性与分析精度。针对公开记录的BGP异常数据稀缺问题，我们还提出基于LLM的合成数据生成框架。在真实与合成数据集上的评估表明，BEAR实现了100%的准确率，优于思维链和上下文学习基线方法。本研究开创了自动化解释BGP异常事件的先河，为网络管理提供了宝贵的运维洞察。

---

## [Clustering and Median Aggregation Improve Differentially Private Inference](https://arxiv.org/abs/2506.04566)

### Abstract
arXiv:2506.04566v1 Announce Type: cross 
Abstract: Differentially private (DP) language model inference is an approach for generating private synthetic text. A sensitive input example is used to prompt an off-the-shelf large language model (LLM) to produce a similar example. Multiple examples can be aggregated together to formally satisfy the DP guarantee.
  Prior work creates inference batches by sampling sensitive inputs uniformly at random. We show that uniform sampling degrades the quality of privately generated text, especially when the sensitive examples concern heterogeneous topics.
  We remedy this problem by clustering the input data before selecting inference batches. Next, we observe that clustering also leads to more similar next-token predictions across inferences. We use this insight to introduce a new algorithm that aggregates next token statistics by privately computing medians instead of averages. This approach leverages the fact that the median has decreased local sensitivity when next token predictions are similar, allowing us to state a data-dependent and ex-post DP guarantee about the privacy properties of this algorithm. Finally, we demonstrate improvements in terms of representativeness metrics (e.g., MAUVE) as well as downstream task performance. We show that our method produces high-quality synthetic data at significantly lower privacy cost than a previous state-of-the-art method.

### 摘要
差分隐私（DP）语言模型推理是一种生成隐私合成文本的方法。该方法通过使用敏感输入样本提示现成的大型语言模型（LLM），以生成相似样本。通过聚合多个样本可形式化满足差分隐私保证。现有研究采用均匀随机采样方式构建推理批次，本文证明这种采样方式会降低隐私生成文本的质量（尤其在敏感样本涉及异构主题时）。我们通过引入输入数据聚类方法改进推理批次选择过程。进一步研究发现，聚类处理还能提高跨推理过程的下一标记预测相似度。基于此发现，我们提出一种新算法：通过隐私计算中位数（而非平均数）来聚合下一标记统计量。该方法利用"当下一标记预测相似时中位数局部敏感性降低"的特性，使我们能够就该算法的隐私属性给出数据依赖型的事后差分隐私保证。实验结果表明，该方法在代表性指标（如MAUVE）和下游任务性能上均有提升。与现有最优方法相比，本方法能以显著更低的隐私成本生成高质量合成数据。

---

## [Inference economics of language models](https://arxiv.org/abs/2506.04645)

### Abstract
arXiv:2506.04645v1 Announce Type: cross 
Abstract: We develop a theoretical model that addresses the economic trade-off between cost per token versus serial token generation speed when deploying LLMs for inference at scale. Our model takes into account arithmetic, memory bandwidth, network bandwidth and latency constraints; and optimizes over different parallelism setups and batch sizes to find the ones that optimize serial inference speed at a given cost per token. We use the model to compute Pareto frontiers of serial speed versus cost per token for popular language models.

### 摘要
我们构建了一个理论模型，用于解决大规模部署LLM进行推理时每个令牌成本与串行令牌生成速度之间的经济权衡问题。该模型综合考虑了算术运算、内存带宽、网络带宽和延迟约束，并通过优化不同并行配置和批量大小，寻找在给定单令牌成本下能实现最优串行推理速度的方案。我们运用该模型计算了主流语言模型的串行速度与单令牌成本的帕累托前沿。

---

## [Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis](https://arxiv.org/abs/2506.04574)

### Abstract
arXiv:2506.04574v1 Announce Type: cross 
Abstract: We investigate the effectiveness of large language models (LLMs), including reasoning-based and non-reasoning models, in performing zero-shot financial sentiment analysis. Using the Financial PhraseBank dataset annotated by domain experts, we evaluate how various LLMs and prompting strategies align with human-labeled sentiment in a financial context. We compare three proprietary LLMs (GPT-4o, GPT-4.1, o3-mini) under different prompting paradigms that simulate System 1 (fast and intuitive) or System 2 (slow and deliberate) thinking and benchmark them against two smaller models (FinBERT-Prosus, FinBERT-Tone) fine-tuned on financial sentiment analysis. Our findings suggest that reasoning, either through prompting or inherent model design, does not improve performance on this task. Surprisingly, the most accurate and human-aligned combination of model and method was GPT-4o without any Chain-of-Thought (CoT) prompting. We further explore how performance is impacted by linguistic complexity and annotation agreement levels, uncovering that reasoning may introduce overthinking, leading to suboptimal predictions. This suggests that for financial sentiment classification, fast, intuitive "System 1"-like thinking aligns more closely with human judgment compared to "System 2"-style slower, deliberative reasoning simulated by reasoning models or CoT prompting. Our results challenge the default assumption that more reasoning always leads to better LLM decisions, particularly in high-stakes financial applications.

### 摘要
我们研究了大型语言模型（LLMs）在零样本金融情感分析中的有效性，包括基于推理和非推理的模型。通过使用领域专家标注的Financial PhraseBank数据集，我们评估了不同LLMs及提示策略在金融语境下与人工标注情感的一致性。我们比较了三种专有LLMs（GPT-4o、GPT-4.1、o3-mini）在模拟系统1（快速直觉）或系统2（缓慢审慎）思维的不同提示范式下的表现，并以两个经过金融情感分析微调的小型模型（FinBERT-Prosus、FinBERT-Tone）作为基准。研究发现，无论是通过提示还是模型固有设计，推理均未提升该任务的性能。出乎意料的是，最准确且与人类判断最吻合的模型与方法组合是未使用思维链（CoT）提示的GPT-4o。我们进一步探讨了语言复杂性和标注一致性水平对性能的影响，发现推理可能导致过度思考，从而产生次优预测。这表明对于金融情感分类，快速直觉的"系统1"式思维比推理模型或CoT提示模拟的"系统2"式缓慢审慎推理更接近人类判断。本研究结果挑战了"更多推理总能带来更好LLM决策"的默认假设，尤其在高风险的金融应用场景中。

---

## [Intelligent Channel Allocation for IEEE 802.11be Multi-Link Operation: When MAB Meets LLM](https://arxiv.org/abs/2506.04594)

### Abstract
arXiv:2506.04594v1 Announce Type: cross 
Abstract: WiFi networks have achieved remarkable success in enabling seamless communication and data exchange worldwide. The IEEE 802.11be standard, known as WiFi 7, introduces Multi-Link Operation (MLO), a groundbreaking feature that enables devices to establish multiple simultaneous connections across different bands and channels. While MLO promises substantial improvements in network throughput and latency reduction, it presents significant challenges in channel allocation, particularly in dense network environments. Current research has predominantly focused on performance analysis and throughput optimization within static WiFi 7 network configurations. In contrast, this paper addresses the dynamic channel allocation problem in dense WiFi 7 networks with MLO capabilities. We formulate this challenge as a combinatorial optimization problem, leveraging a novel network performance analysis mechanism. Given the inherent lack of prior network information, we model the problem within a Multi-Armed Bandit (MAB) framework to enable online learning of optimal channel allocations. Our proposed Best-Arm Identification-enabled Monte Carlo Tree Search (BAI-MCTS) algorithm includes rigorous theoretical analysis, providing upper bounds for both sample complexity and error probability. To further reduce sample complexity and enhance generalizability across diverse network scenarios, we put forth LLM-BAI-MCTS, an intelligent algorithm for the dynamic channel allocation problem by integrating the Large Language Model (LLM) into the BAI-MCTS algorithm. Numerical results demonstrate that the BAI-MCTS algorithm achieves a convergence rate approximately $50.44\%$ faster than the state-of-the-art algorithms when reaching $98\%$ of the optimal value. Notably, the convergence rate of the LLM-BAI-MCTS algorithm increases by over $63.32\%$ in dense networks.

### 摘要
WiFi网络在全球范围内实现了无缝通信与数据交换的显著成就。作为WiFi 7标准的IEEE 802.11be引入了多链路操作（MLO）这一突破性特性，使设备能够在不同频段和信道上建立多重并发连接。尽管MLO有望显著提升网络吞吐量并降低时延，但其在密集网络环境中的信道分配方面仍存在重大挑战。现有研究主要集中于静态WiFi 7网络配置下的性能分析与吞吐量优化，而本文则致力于解决具备MLO功能的密集WiFi 7网络中的动态信道分配问题。我们通过新型网络性能分析机制，将该挑战建模为组合优化问题。鉴于先验网络信息的固有缺失，我们采用多臂老虎机（MAB）框架对问题进行建模，以实现最优信道分配的在线学习。提出的最佳臂识别蒙特卡洛树搜索（BAI-MCTS）算法包含严格的理论分析，给出了样本复杂度与错误概率的上界。为进一步降低样本复杂度并增强算法在不同网络场景中的泛化能力，我们通过将大语言模型（LLM）整合至BAI-MCTS算法，提出了针对动态信道分配问题的智能算法LLM-BAI-MCTS。数值实验表明：当达到最优值98%时，BAI-MCTS算法的收敛速度较现有最优算法提升约50.44%；值得注意的是，在密集网络中LLM-BAI-MCTS算法的收敛速度提升幅度超过63.32%。

---

## [MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models](https://arxiv.org/abs/2506.04688)

### Abstract
arXiv:2506.04688v1 Announce Type: cross 
Abstract: This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the error refinement capabilities of Multimodal Large Language Models (MLLMs). As the emphasis shifts toward enhancing reasoning during inference, MMRefine provides a framework that evaluates MLLMs' abilities to detect and correct errors across six distinct scenarios beyond just comparing final accuracy before and after refinement. Furthermore, the benchmark analyzes the refinement performance by categorizing errors into six error types. Experiments with various open and closed MLLMs reveal bottlenecks and factors impeding refinement performance, highlighting areas for improvement in effective reasoning enhancement. Our code and dataset are publicly available at https://github.com/naver-ai/MMRefine.

### 摘要
本文介绍了MMRefine——一个用于评估多模态大语言模型(MLLMs)错误修正能力的多模态优化基准。随着研究重点转向推理过程中的性能提升，MMRefine不仅比较优化前后的最终准确率，还提供了一个评估框架，用于测试MLLMs在六种不同场景下的错误检测与纠正能力。此外，该基准通过将错误归类为六种类型来分析模型的优化性能。通过对多种开源和闭源MLLMs的实验，我们揭示了制约优化性能的瓶颈和影响因素，为有效提升推理能力指明了改进方向。我们的代码和数据集已公开在https://github.com/naver-ai/MMRefine。

---

## [Towards Better Generalization via Distributional Input Projection Network](https://arxiv.org/abs/2506.04690)

### Abstract
arXiv:2506.04690v1 Announce Type: cross 
Abstract: As overparameterized models become increasingly prevalent, training loss alone offers limited insight into generalization performance. While smoothness has been linked to improved generalization across various settings, directly enforcing smoothness in neural networks remains challenging. To address this, we introduce Distributional Input Projection Networks (DIPNet), a novel framework that projects inputs into learnable distributions at each layer. This distributional representation induces a smoother loss landscape with respect to the input, promoting better generalization. We provide theoretical analysis showing that DIPNet reduces both local smoothness measures and the Lipschitz constant of the network, contributing to improved generalization performance. Empirically, we validate DIPNet across a wide range of architectures and tasks, including Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and MLPs. Our method consistently enhances test performance under standard settings, adversarial attacks, out-of-distribution inputs, and reasoning benchmarks. We demonstrate that the proposed input projection strategy can be seamlessly integrated into existing models, providing a general and effective approach for boosting generalization performance in modern deep learning.

### 摘要
随着过参数化模型的日益普及，训练损失本身对泛化性能的反映能力有限。尽管平滑性已被证明能改善多种场景下的泛化表现，但在神经网络中直接强制平滑性仍具挑战性。为此，我们提出分布输入投影网络（DIPNet），该创新框架通过在各层将输入投影至可学习分布来实现平滑性。这种分布表示能诱导出相对于输入更平滑的损失景观，从而提升泛化能力。理论分析表明，DIPNet能同时降低局部平滑性度量指标和网络的Lipschitz常数，进而改善泛化性能。我们通过视觉Transformer（ViT）、大语言模型（LLM）、ResNet和多层感知机（MLP）等多种架构与任务进行实证验证。该方法在标准设定、对抗攻击、分布外输入和推理基准测试中均能持续提升测试性能。研究表明，所提出的输入投影策略可无缝集成至现有模型，为提升现代深度学习泛化性能提供了一种通用有效的解决方案。

---

## [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)

### Abstract
arXiv:2506.04681v1 Announce Type: cross 
Abstract: We introduce $Urania$, a novel framework for generating insights about LLM chatbot interactions with rigorous differential privacy (DP) guarantees. The framework employs a private clustering mechanism and innovative keyword extraction methods, including frequency-based, TF-IDF-based, and LLM-guided approaches. By leveraging DP tools such as clustering, partition selection, and histogram-based summarization, $Urania$ provides end-to-end privacy protection. Our evaluation assesses lexical and semantic content preservation, pair similarity, and LLM-based metrics, benchmarking against a non-private Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple empirical privacy evaluation that demonstrates the enhanced robustness of our DP pipeline. The results show the framework's ability to extract meaningful conversational insights while maintaining stringent user privacy, effectively balancing data utility with privacy preservation.

### 摘要
我们提出$Urania$——一个具有严格差分隐私（DP）保证的大型语言模型聊天机器人交互分析框架。该框架采用私有聚类机制和创新的关键词提取方法，包括基于词频、TF-IDF和LLM引导的技术。通过运用聚类、分区选择和基于直方图的摘要等DP工具，$Urania$实现了端到端的隐私保护。我们的评估从词汇与语义内容保留、配对相似度和基于LLM的指标三个维度展开，并以非隐私的Clio-inspired流程（Tamkin等人，2024）为基准。此外，我们开发了一种简易的实证隐私评估方法，证明DP流程具有更强的鲁棒性。结果表明，该框架能在严格保护用户隐私的同时提取有价值的对话洞察，实现了数据效用与隐私保护的平衡。

---

## [Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification](https://arxiv.org/abs/2506.04592)

### Abstract
arXiv:2506.04592v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) prompting has become the de facto method to elicit reasoning capabilities from large language models (LLMs). However, to mitigate hallucinations in CoT that are notoriously difficult to detect, current methods such as process reward models (PRMs) or self-consistency operate as opaque boxes and do not provide checkable evidence for their judgments, possibly limiting their effectiveness. To address this issue, we draw inspiration from the idea that "the gold standard for supporting a mathematical claim is to provide a proof". We propose a retrospective, step-aware formal verification framework $Safe$. Rather than assigning arbitrary scores, we strive to articulate mathematical claims in formal mathematical language Lean 4 at each reasoning step and provide formal proofs to identify hallucinations. We evaluate our framework $Safe$ across multiple language models and various mathematical datasets, demonstrating a significant performance improvement while offering interpretable and verifiable evidence. We also propose $FormalStep$ as a benchmark for step correctness theorem proving with $30,809$ formal statements. To the best of our knowledge, our work represents the first endeavor to utilize formal mathematical language Lean 4 for verifying natural language content generated by LLMs, aligning with the reason why formal mathematical languages were created in the first place: to provide a robust foundation for hallucination-prone human-written proofs.

### 摘要
链式思考（CoT）提示已成为从大型语言模型（LLMs）中激发推理能力的实际标准方法。然而，针对CoT中 notoriously难以检测的幻觉问题，现有方法（如过程奖励模型PRMs或自一致性机制）往往作为黑箱运行，无法为其判断提供可验证的证据，这可能限制其有效性。为解决该问题，我们受"支持数学主张的黄金标准是提供证明"这一理念启发，提出了一种具有回溯性和步骤感知能力的形式化验证框架$Safe$。该方法摒弃任意评分机制，致力于在每个推理步骤中以形式化数学语言Lean 4表述数学主张，并通过形式化证明来识别幻觉。我们在多个语言模型和不同数学数据集上评估$Safe$框架，结果表明其性能显著提升，同时提供可解释且可验证的证据。我们还提出包含$30,809$个形式化命题的$FormalStep$基准，用于逐步正确性定理证明。据我们所知，本研究首次尝试利用形式化数学语言Lean 4来验证LLMs生成的自然语言内容，这正符合形式化数学语言创建的初衷：为易出现幻觉的人工书写证明提供坚实基础。

---

## [Line of Sight: On Linear Representations in VLLMs](https://arxiv.org/abs/2506.04706)

### Abstract
arXiv:2506.04706v1 Announce Type: cross 
Abstract: Language models can be equipped with multimodal capabilities by fine-tuning on embeddings of visual inputs. But how do such multimodal models represent images in their hidden activations? We explore representations of image concepts within LlaVA-Next, a popular open-source VLLM. We find a diverse set of ImageNet classes represented via linearly decodable features in the residual stream. We show that the features are causal by performing targeted edits on the model output. In order to increase the diversity of the studied linear features, we train multimodal Sparse Autoencoders (SAEs), creating a highly interpretable dictionary of text and image features. We find that although model representations across modalities are quite disjoint, they become increasingly shared in deeper layers.

### 摘要
语言模型可通过在视觉输入嵌入上的微调获得多模态能力。但此类多模态模型如何在其隐藏激活中表征图像？我们以流行的开源视觉语言大模型LlaVA-Next为研究对象，探究其中图像概念的表征方式。研究发现残差流中存在可通过线性解码的多样化ImageNet类别特征。通过针对性编辑模型输出，我们证实这些特征具有因果性。为增加所研究线性特征的多样性，我们训练了多模态稀疏自编码器（SAEs），构建出高度可解释的文本与图像特征字典。研究表明，尽管跨模态的模型表征存在明显区隔，但在更深网络层中会逐渐趋于共享。

---

## [Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection](https://arxiv.org/abs/2506.04739)

### Abstract
arXiv:2506.04739v1 Announce Type: cross 
Abstract: The widespread dissemination of fake news on social media has significantly impacted society, resulting in serious consequences. Conventional deep learning methodologies employing small language models (SLMs) suffer from extensive supervised training requirements and difficulties adapting to evolving news environments due to data scarcity and distribution shifts. Large language models (LLMs), despite robust zero-shot capabilities, fall short in accurately detecting fake news owing to outdated knowledge and the absence of suitable demonstrations. In this paper, we propose a novel Continuous Collaborative Emergent Fake News Detection (C$^2$EFND) framework to address these challenges. The C$^2$EFND framework strategically leverages both LLMs' generalization power and SLMs' classification expertise via a multi-round collaborative learning framework. We further introduce a lifelong knowledge editing module based on a Mixture-of-Experts architecture to incrementally update LLMs and a replay-based continue learning method to ensure SLMs retain prior knowledge without retraining entirely. Extensive experiments on Pheme and Twitter16 datasets demonstrate that C$^2$EFND significantly outperforms existed methods, effectively improving detection accuracy and adaptability in continuous emergent fake news scenarios.

### 摘要
社交媒体上虚假新闻的广泛传播已对社会造成严重影响，导致严重后果。传统深度学习方法采用小型语言模型（SLMs），由于数据稀缺和分布变化，需要大量监督训练且难以适应不断变化的新闻环境。尽管大型语言模型（LLMs）具备强大的零样本能力，但因知识过时和缺乏合适的示例，在虚假新闻检测准确性上存在不足。本文提出了一种新颖的持续协作涌现虚假新闻检测框架（C$^2$EFND）以应对这些挑战。该框架通过多轮协作学习策略，充分利用LLMs的泛化能力和SLMs的分类专长。我们进一步引入基于专家混合架构的终身知识编辑模块以增量更新LLMs，并采用基于回放的持续学习方法确保SLMs无需完全重新训练即可保留先验知识。在Pheme和Twitter16数据集上的大量实验表明，C$^2$EFND显著优于现有方法，有效提升了持续涌现虚假新闻场景中的检测准确性和适应性。

---

## [Gen-n-Val: Agentic Image Data Generation and Validation](https://arxiv.org/abs/2506.04676)

### Abstract
arXiv:2506.04676v1 Announce Type: cross 
Abstract: Recently, Large Language Models (LLMs) and Vision Large Language Models (VLLMs) have demonstrated impressive performance as agents across various tasks while data scarcity and label noise remain significant challenges in computer vision tasks, such as object detection and instance segmentation. A common solution for resolving these issues is to generate synthetic data. However, current synthetic data generation methods struggle with issues, such as multiple objects per mask, inaccurate segmentation, and incorrect category labels, limiting their effectiveness. To address these issues, we introduce Gen-n-Val, a novel agentic data generation framework that leverages Layer Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt agent, an LLM, optimizes prompts for LD to generate high-quality foreground instance images and segmentation masks. These optimized prompts ensure the generation of single-object synthetic data with precise instance masks and clean backgrounds. (2) The data validation agent, a VLLM, which filters out low-quality synthetic instance images. The system prompts for both agents are refined through TextGrad. Additionally, we use image harmonization to combine multiple instances within scenes. Compared to state-of-the-art synthetic data approaches like MosaicFusion, our approach reduces invalid synthetic data from 50% to 7% and improves performance by 1% mAP on rare classes in COCO instance segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance of YOLOv9 and YOLO11 families in instance segmentation and object detection.

### 摘要
近期，大语言模型（LLMs）和视觉大语言模型（VLLMs）作为智能体在多项任务中展现出卓越性能，然而数据稀缺和标签噪声仍是计算机视觉任务（如目标检测和实例分割）中的重大挑战。生成合成数据是解决这些问题的常用方法，但现有合成数据生成方法存在单掩码多对象、分割不准确和类别标签错误等问题，限制了其有效性。为此，我们提出Gen-n-Val——一种新型智能体数据生成框架，通过结合层级扩散（LD）、LLMs和VLLMs来生成高质量的单对象掩码及多样化背景。该框架包含两个智能体：（1）LD提示智能体（基于LLM）优化LD的生成提示，以产生高质量前景实例图像及分割掩码。这些优化提示确保生成具有精确实例掩码和纯净背景的单对象合成数据；（2）数据验证智能体（基于VLLM）负责过滤低质量合成实例图像。两个智能体的系统提示均通过TextGrad进行优化。此外，我们采用图像协调技术将多个实例合成到场景中。与MosaicFusion等先进合成数据方法相比，我们的方法将无效合成数据比例从50%降至7%，并在YOLOv9c和YOLO11m的COCO实例分割任务中使稀有类别的mAP提升1%。在YOLO11m的开放词汇目标检测基准测试中，Gen-n-Val相较YOLO-Worldv2-M实现了7.1%的mAP显著提升。此外，Gen-n-Val有效提升了YOLOv9和YOLO11系列模型在实例分割和目标检测中的性能表现。

---

## [On the Mechanism of Reasoning Pattern Selection in Reinforcement Learning for Language Models](https://arxiv.org/abs/2506.04695)

### Abstract
arXiv:2506.04695v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has demonstrated remarkable success in enhancing model capabilities, including instruction-following, preference learning, and reasoning. Yet despite its empirical successes, the mechanisms by which RL improves reasoning abilities remain poorly understood. We present a systematic study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that its primary benefit comes from optimizing the selection of existing reasoning patterns. Through extensive experiments, we demonstrate that RLVR-trained models preferentially adopt high-success-rate reasoning patterns while mostly maintaining stable performance on individual patterns. We further develop theoretical analyses on the convergence and training dynamics of RLVR based on a simplified question-reason-answer model. We study the gradient flow and show that RLVR can indeed find the solution that selects the reason pattern with the highest success rate. Besides, our theoretical results
  reveal two distinct regimes regarding the convergence of RLVR training: (1) rapid convergence for models with relatively strong initial reasoning capabilities versus (2) slower optimization dynamics for weaker models. Furthermore, we show that the slower optimization for weaker models can be mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using a feasibly high-quality SFT dataset. We validate the theoretical findings through extensive experiments. This work advances our theoretical understanding of RL's role in LLM fine-tuning and offers insights for further enhancing reasoning capabilities.

### 摘要
强化学习（RL）在提升模型能力方面已展现出显著成效，包括指令遵循、偏好学习和推理等。然而尽管其经验性成功显著，RL改善推理能力的内在机制仍鲜为人知。我们提出了可验证奖励强化学习（RLVR）的系统性研究，表明其主要优势源于对现有推理模式的选择优化。通过大量实验，我们证明RLVR训练模型会优先采用高成功率推理模式，同时基本保持单个推理模式的稳定性能。我们进一步基于简化的"问题-推理-答案"模型开展了RLVR收敛性与训练动态的理论分析，通过研究梯度流证实RLVR确实能找到选择最高成功率推理模式的解。此外，理论结果揭示了RLVR训练收敛的两种典型模式：(1)初始推理能力较强模型的快速收敛；(2)较弱模型的缓慢优化动态。研究还表明，当采用高质量监督微调（SFT）数据集时，在RLVR前实施SFT可缓解较弱模型的慢速优化问题。我们通过大量实验验证了理论发现。本工作推进了对RL在LLM微调中作用的理论理解，并为进一步增强推理能力提供了洞见。

---

## [Fine-Grained Interpretation of Political Opinions in Large Language Models](https://arxiv.org/abs/2506.04774)

### Abstract
arXiv:2506.04774v1 Announce Type: cross 
Abstract: Studies of LLMs' political opinions mainly rely on evaluations of their open-ended responses. Recent work indicates that there is a misalignment between LLMs' responses and their internal intentions. This motivates us to probe LLMs' internal mechanisms and help uncover their internal political states. Additionally, we found that the analysis of LLMs' political opinions often relies on single-axis concepts, which can lead to concept confounds. In this work, we extend the single-axis to multi-dimensions and apply interpretable representation engineering techniques for more transparent LLM political concept learning. Specifically, we designed a four-dimensional political learning framework and constructed a corresponding dataset for fine-grained political concept vector learning. These vectors can be used to detect and intervene in LLM internals. Experiments are conducted on eight open-source LLMs with three representation engineering techniques. Results show these vectors can disentangle political concept confounds. Detection tasks validate the semantic meaning of the vectors and show good generalization and robustness in OOD settings. Intervention Experiments show these vectors can intervene in LLMs to generate responses with different political leanings.

### 摘要
关于大语言模型（LLM）政治倾向的研究主要依赖于对其开放式回答的评估。近期研究表明，大语言模型的回应与其内部意图存在偏差，这促使我们探究其内部机制以揭示真实的政治倾向。此外，我们发现现有分析多基于单轴政治概念框架，可能导致概念混淆。本研究将单轴扩展至多维度，并应用可解释的表征工程技术实现更透明的LLM政治概念学习。具体而言，我们设计了一个四维政治学习框架，构建了相应数据集用于细粒度政治概念向量学习。这些向量可用于检测和干预大语言模型的内部状态。我们在八个开源LLM上采用三种表征工程技术进行实验，结果表明这些向量能有效解构政治概念混淆。检测任务验证了向量的语义有效性，并在分布外（OOD）场景中展现出良好的泛化性和鲁棒性。干预实验证实这些向量能引导LLM生成具有不同政治倾向的回应。

---

## [On Automating Security Policies with Contemporary LLMs](https://arxiv.org/abs/2506.04838)

### Abstract
arXiv:2506.04838v1 Announce Type: cross 
Abstract: The complexity of modern computing environments and the growing sophistication of cyber threats necessitate a more robust, adaptive, and automated approach to security enforcement. In this paper, we present a framework leveraging large language models (LLMs) for automating attack mitigation policy compliance through an innovative combination of in-context learning and retrieval-augmented generation (RAG). We begin by describing how our system collects and manages both tool and API specifications, storing them in a vector database to enable efficient retrieval of relevant information. We then detail the architectural pipeline that first decomposes high-level mitigation policies into discrete tasks and subsequently translates each task into a set of actionable API calls. Our empirical evaluation, conducted using publicly available CTI policies in STIXv2 format and Windows API documentation, demonstrates significant improvements in precision, recall, and F1-score when employing RAG compared to a non-RAG baseline.

### 摘要
现代计算环境的复杂性和网络威胁日益增长的 sophistication 要求采用更 robust、自适应且自动化的安全 enforcement 方法。本文提出一个利用大型语言模型（LLMs）的框架，通过 in-context learning 和 retrieval-augmented generation（RAG）的创新结合，实现攻击 mitigation 策略合规的自动化。我们首先描述系统如何收集和管理工具及 API 规范，并将其存储在向量数据库中以实现高效的相关信息检索。随后详细说明架构 pipeline：该 pipeline 首先将高层 mitigation 策略分解为离散任务，然后将每个任务转化为一组可执行的 API 调用。我们的实证评估采用 STIXv2 格式的公开 CTI 策略和 Windows API 文档进行，结果表明，与非 RAG baseline 相比，采用 RAG 在 precision、recall 和 F1-score 上均有显著提升。

---

## [Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques](https://arxiv.org/abs/2506.04788)

### Abstract
arXiv:2506.04788v1 Announce Type: cross 
Abstract: The rapid progress of Multimodal Large Language Models(MLLMs) has transformed the AI landscape. These models combine pre-trained LLMs with various modality encoders. This integration requires a systematic understanding of how different modalities connect to the language backbone. Our survey presents an LLM-centric analysis of current approaches. We examine methods for transforming and aligning diverse modal inputs into the language embedding space. This addresses a significant gap in existing literature. We propose a classification framework for MLLMs based on three key dimensions. First, we examine architectural strategies for modality integration. This includes both the specific integration mechanisms and the fusion level. Second, we categorize representation learning techniques as either joint or coordinate representations. Third, we analyze training paradigms, including training strategies and objective functions. By examining 125 MLLMs developed between 2021 and 2025, we identify emerging patterns in the field. Our taxonomy provides researchers with a structured overview of current integration techniques. These insights aim to guide the development of more robust multimodal integration strategies for future models built on pre-trained foundations.

### 摘要
多模态大语言模型(MLLMs)的快速发展正在重塑人工智能领域。这些模型将预训练大语言模型与多种模态编码器相结合，这种整合需要系统性地理解不同模态与语言主干网络的连接方式。本综述以LLM为核心对现有方法进行分析，重点研究了将多样化模态输入转换并对齐到语言嵌入空间的技术，填补了现有文献的重要空白。我们提出了基于三个关键维度的MLLMs分类框架：首先，分析模态整合的架构策略，包括具体整合机制和融合层级；其次，将表征学习技术划分为联合表征与坐标表征两类；最后，解析训练范式，涵盖训练策略与目标函数。通过对2021至2025年间开发的125个MLLMs进行考察，我们揭示了该领域的新兴模式。本分类体系为研究者提供了当前整合技术的结构化概览，这些见解旨在指导基于预训练模型的未来研究中开发更鲁棒的多模态整合策略。

---

## [Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning](https://arxiv.org/abs/2506.04755)

### Abstract
arXiv:2506.04755v1 Announce Type: cross 
Abstract: While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample's potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https://github.com/Leo-ssl/RAP.

### 摘要
尽管多模态大语言模型（MLLMs）通过强化学习在复杂推理任务中取得了显著进展，但学界普遍认为提升多模态推理能力需要大量训练数据，这不可避免地导致数据冗余和巨额计算成本。然而，对于MLLMs的多模态推理任务，较小的高价值数据集能否匹配甚至超越完整训练集的效果？本研究通过关键观察对这一假设提出挑战：有意义的多模态推理仅由少量被称为"认知样本"的训练数据触发，而多数样本贡献微乎其微。基于此发现，我们提出名为"推理激活潜能"（RAP）的新型数据选择范式，通过两个互补评估器识别认知样本：1）基于潜在结果模型原理的"因果差异评估器"（CDE），通过比较多模态输入与纯文本输入的输出差异，剔除过度依赖语言先验的样本；2）"注意力置信评估器"（ACE）利用token级自注意力机制，筛除中间推理阶段被无关但过度强调的token主导的样本。此外，我们引入"难度感知替换模块"（DRM），用具有认知挑战性的样本替代简单实例，从而确保多模态推理所需的复杂度。在六个数据集上的实验表明，RAP方法仅需9.3%的训练数据即可持续取得更优性能，同时降低超过43%的计算成本。代码已开源：https://github.com/Leo-ssl/RAP。

---

## [A Reasoning-Based Approach to Cryptic Crossword Clue Solving](https://arxiv.org/abs/2506.04824)

### Abstract
arXiv:2506.04824v1 Announce Type: cross 
Abstract: Cryptic crossword clues are challenging language tasks for which new test sets are released daily by major newspapers on a global basis. Each cryptic clue contains both the definition of the answer to be placed in the crossword grid (in common with regular crosswords), and 'wordplay' that proves that the answer is correct (i.e. a human solver can be confident that an answer is correct without needing crossing words as confirmation). This work describes an LLM-based reasoning system built from open-licensed components that solves cryptic clues by (i) hypothesising answers; (ii) proposing wordplay explanations; and (iii) using a verifier system that operates on codified reasoning steps. Overall, this system establishes a new state-of-the-art performance on the challenging Cryptonite dataset of clues from The Times and The Telegraph newspapers in the UK. Because each proved solution is expressed in Python, interpretable wordplay reasoning for proven answers is available for inspection.

### 摘要
加密填字游戏线索是具有挑战性的语言任务，全球主要报纸每日都会发布新的测试集。每条加密线索既包含需填入填字网格的答案定义（与常规填字游戏相同），又包含证明答案正确的'文字游戏'（即人类解答者无需借助交叉单词确认即可确信答案正确）。本研究描述了一个基于LLM的推理系统，该系统由开源组件构建，通过以下步骤求解加密线索：(i) 假设答案；(ii) 提出文字游戏解释；(iii) 使用基于编码推理步骤的验证系统。总体而言，该系统在英国《泰晤士报》和《每日电讯报》的Cryptonite挑战性数据集上实现了最新最优性能。由于每个已验证解法均以Python代码表示，经证明答案的可解释文字游戏推理可供检查。

---

## [Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study](https://arxiv.org/abs/2506.04810)

### Abstract
arXiv:2506.04810v1 Announce Type: cross 
Abstract: Logical reasoning is a core capability for many applications of large language models (LLMs), yet existing benchmarks often rely solely on final-answer accuracy, failing to capture the quality and structure of the reasoning process. We propose FineLogic, a fine-grained evaluation framework that assesses logical reasoning across three dimensions: overall benchmark accuracy, stepwise soundness, and representation-level alignment. In addition, to better understand how reasoning capabilities emerge, we conduct a comprehensive study on the effects of supervision format during fine-tuning. We construct four supervision styles (one natural language and three symbolic variants) and train LLMs under each. Our findings reveal that natural language supervision yields strong generalization even on out-of-distribution and long-context tasks, while symbolic reasoning styles promote more structurally sound and atomic inference chains. Further, our representation-level probing shows that fine-tuning primarily improves reasoning behaviors through step-by-step generation, rather than enhancing shortcut prediction or internalized correctness. Together, our framework and analysis provide a more rigorous and interpretable lens for evaluating and improving logical reasoning in LLMs.

### 摘要
逻辑推理是大语言模型（LLMs）诸多应用中的核心能力，然而现有基准测试往往仅依赖最终答案准确率，无法捕捉推理过程的质量与结构。我们提出FineLogic这一细粒度评估框架，从三个维度评估逻辑推理能力：整体基准准确率、步骤合理性以及表征层面一致性。此外，为深入理解推理能力如何形成，我们对微调过程中监督形式的影响展开系统研究。通过构建四种监督范式（一种自然语言与三种符号化变体）并分别训练LLMs，研究发现：自然语言监督即使在分布外任务和长上下文任务中仍表现出强泛化能力，而符号推理范式则能促进更具结构性和原子性的推理链。进一步的表征层面探测表明，微调主要通过逐步生成来改善推理行为，而非增强捷径预测或内部化正确性。本框架与分析共同为LLMs的逻辑推理能力评估与提升提供了更严谨、可解释的研究视角。

---

## [From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation](https://arxiv.org/abs/2506.04965)

### Abstract
arXiv:2506.04965v1 Announce Type: cross 
Abstract: This paper presents a comprehensive evaluation of the performance of state-of-the-art Large Language Models (LLMs) on challenging university-level algorithms exams. By testing multiple models on both a Romanian exam and its high-quality English translation, we analyze LLMs' problem-solving capabilities, consistency, and multilingual performance. Our empirical study reveals that the most recent models not only achieve scores comparable to top-performing students but also demonstrate robust reasoning skills on complex, multi-step algorithmic challenges, even though difficulties remain with graph-based tasks. Building on these findings, we explore the potential of LLMs to support educational environments through the generation of high-quality editorial content, offering instructors a powerful tool to enhance student feedback. The insights and best practices discussed herein pave the way for further integration of generative AI in advanced algorithm education.

### 摘要
本文对前沿大语言模型(LLMs)在具有挑战性的大学级算法考试中的表现进行了全面评估。通过使用罗马尼亚语原版试卷及其高质量英语译本对多个模型进行测试，我们系统分析了LLMs的问题解决能力、表现一致性及多语言处理性能。实证研究表明，最新模型不仅能取得与顶尖学生相当的分数，还在复杂的多步骤算法挑战中展现出强大的推理能力——尽管在图论任务上仍存在困难。基于这些发现，我们进一步探索了LLMs通过生成高质量教学评述内容来支持教育环境的潜力，为教师提供了增强学生反馈的有效工具。文中讨论的见解与最佳实践为生成式人工智能在高级算法教育中的进一步整合铺平了道路。

---

## [Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights](https://arxiv.org/abs/2506.04851)

### Abstract
arXiv:2506.04851v1 Announce Type: cross 
Abstract: Integrating Artificial Intelligence (AI) in educational settings has brought new learning approaches, transforming the practices of both students and educators. Among the various technologies driving this transformation, Large Language Models (LLMs) have emerged as powerful tools for creating educational materials and question answering, but there are still space for new applications. Educators commonly use Multiple-Choice Questions (MCQs) to assess student knowledge, but manually generating these questions is resource-intensive and requires significant time and cognitive effort. In our opinion, LLMs offer a promising solution to these challenges. This paper presents a novel comparative analysis of three widely known LLMs - Llama 2, Mistral, and GPT-3.5 - to explore their potential for creating informative and challenging MCQs. In our approach, we do not rely on the knowledge of the LLM, but we inject the knowledge into the prompt to contrast the hallucinations, giving the educators control over the test's source text, too. Our experiment involving 21 educators shows that GPT-3.5 generates the most effective MCQs across several known metrics. Additionally, it shows that there is still some reluctance to adopt AI in the educational field. This study sheds light on the potential of LLMs to generate MCQs and improve the educational experience, providing valuable insights for the future.

### 摘要
将人工智能（AI）融入教育环境催生了新的学习方法，正在改变师生双方的教学实践。在推动这一变革的众多技术中，大语言模型（LLMs）已成为创建教育材料和问答系统的强大工具，但其应用潜力仍有拓展空间。教育工作者普遍采用选择题（MCQs）评估学生知识水平，但人工编制这类问题需要消耗大量资源，且耗费显著的时间与认知成本。我们认为LLMs能为这些挑战提供有效解决方案。本文对三种主流LLMs——Llama 2、Mistral和GPT-3.5——进行了创新性的比较分析，探究其在生成具有信息量和挑战性MCQs方面的潜力。我们的方法不依赖于LLM的固有知识，而是通过将知识注入提示词来对比幻觉现象，使教育工作者能够同时控制测试的源文本。包含21名教育工作者的实验表明，GPT-3.5在多项已知指标上生成的MCQs最为有效。此外，研究显示教育领域对AI的采纳仍存在一定抵触。本研究揭示了LLMs生成MCQs及改善教育体验的潜力，为未来发展提供了重要见解。

---

## [Sparse Autoencoders, Again?](https://arxiv.org/abs/2506.04859)

### Abstract
arXiv:2506.04859v1 Announce Type: cross 
Abstract: Is there really much more to say about sparse autoencoders (SAEs)? Autoencoders in general, and SAEs in particular, represent deep architectures that are capable of modeling low-dimensional latent structure in data. Such structure could reflect, among other things, correlation patterns in large language model activations, or complex natural image manifolds. And yet despite the wide-ranging applicability, there have been relatively few changes to SAEs beyond the original recipe from decades ago, namely, standard deep encoder/decoder layers trained with a classical/deterministic sparse regularizer applied within the latent space. One possible exception is the variational autoencoder (VAE), which adopts a stochastic encoder module capable of producing sparse representations when applied to manifold data. In this work we formalize underappreciated weaknesses with both canonical SAEs, as well as analogous VAEs applied to similar tasks, and propose a hybrid alternative model that circumvents these prior limitations. In terms of theoretical support, we prove that global minima of our proposed model recover certain forms of structured data spread across a union of manifolds. Meanwhile, empirical evaluations on synthetic and real-world datasets substantiate the efficacy of our approach in accurately estimating underlying manifold dimensions and producing sparser latent representations without compromising reconstruction error. In general, we are able to exceed the performance of equivalent-capacity SAEs and VAEs, as well as recent diffusion models where applicable, within domains such as images and language model activation patterns.

### 摘要
关于稀疏自编码器（SAE）是否还有更多探讨空间？自编码器尤其是SAE这类深度架构，能够建模数据中的低维潜在结构。这种结构既可反映大语言模型激活中的相关性模式，也能刻画复杂自然图像流形。然而尽管应用广泛，SAE自数十年前原始框架提出以来改进甚少——其核心仍是采用标准深度编码/解码层架构，辅以潜在空间确定性稀疏正则器的经典训练方式。变分自编码器（VAE）或许是个例外，其随机编码模块在处理流形数据时可生成稀疏表示。本研究系统揭示了传统SAE及同类VAE在相似任务中的固有缺陷，并提出融合二者优势的新型混合模型。理论层面，我们证明该模型全局最优解能还原分布在流形并集上的特定结构化数据。合成与真实数据集实验表明：新方法在准确估计流形本征维度的同时，能以更低重构误差生成更稀疏的潜在表示。无论是图像处理还是语言模型激活模式分析，新模型性能均超越同容量SAE、VAE以及适用的扩散模型。

---

## [Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback](https://arxiv.org/abs/2506.04920)

### Abstract
arXiv:2506.04920v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated the ability to generate formative feedback and instructional hints in English, making them increasingly relevant for AI-assisted education. However, their ability to provide effective instructional support across different languages, especially for mathematically grounded reasoning tasks, remains largely unexamined. In this work, we present the first large-scale simulation of multilingual tutor-student interactions using LLMs. A stronger model plays the role of the tutor, generating feedback in the form of hints, while a weaker model simulates the student. We explore 352 experimental settings across 11 typologically diverse languages, four state-of-the-art LLMs, and multiple prompting strategies to assess whether language-specific feedback leads to measurable learning gains. Our study examines how student input language, teacher feedback language, model choice, and language resource level jointly influence performance. Results show that multilingual hints can significantly improve learning outcomes, particularly in low-resource languages when feedback is aligned with the student's native language. These findings offer practical insights for developing multilingual, LLM-based educational tools that are both effective and inclusive.

### 摘要
大型语言模型（LLMs）已展现出生成英语形成性反馈与教学提示的能力，使其在人工智能辅助教育领域日益重要。然而，其在不同语言（尤其是数学基础推理任务中）提供有效教学支持的能力仍缺乏系统研究。本研究首次利用LLMs进行了多语言师生互动的大规模模拟：由较强模型扮演导师角色生成提示型反馈，较弱模型模拟学生行为。我们在11种类型学多样语言、4个前沿LLMs及多种提示策略下构建352种实验场景，以评估语言特异性反馈是否能带来可衡量的学习增益。研究揭示了学生输入语言、教师反馈语言、模型选择及语言资源水平如何共同影响表现。结果表明，多语言提示能显著提升学习效果，特别是当反馈语言与学生母语一致时，对低资源语言效果尤为显著。这些发现为开发高效且包容的多语言LLM教育工具提供了实践启示。

---

## [Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots](https://arxiv.org/abs/2506.04907)

### Abstract
arXiv:2506.04907v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), whilst great at extracting facts from text, struggle with nested narrative reasoning. Existing long context and multi-hop QA benchmarks inadequately test this, lacking realistic distractors or failing to decouple context length from reasoning complexity, masking a fundamental LLM limitation. We introduce Verbose ListOps, a novel benchmark that programmatically transposes ListOps computations into lengthy, coherent stories. This uniquely forces internal computation and state management of nested reasoning problems by withholding intermediate results, and offers fine-grained controls for both narrative size \emph&#123;and&#125; reasoning difficulty. Whilst benchmarks like LongReason (2025) advance approaches for synthetically expanding the context size of multi-hop QA problems, Verbose ListOps pinpoints a specific LLM vulnerability: difficulty in state management for nested sub-reasoning amongst semantically-relevant, distracting narrative. Our experiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse in performance on Verbose ListOps at modest (~10k token) narrative lengths, despite effortlessly solving raw ListOps equations. Addressing this failure is paramount for real-world text interpretation which requires identifying key reasoning points, tracking conceptual intermediate results, and filtering irrelevant information. Verbose ListOps, and its extensible generation framework thus enables targeted reasoning enhancements beyond mere context-window expansion; a critical step to automating the world's knowledge work.

### 摘要
大型语言模型（LLMs）虽擅长从文本中提取事实，却在嵌套叙事推理方面表现欠佳。现有长上下文和多跳问答基准对此测试不足——它们或缺乏真实干扰项，或未能将上下文长度与推理复杂度解耦，从而掩盖了LLMs的根本缺陷。我们提出Verbose ListOps这一新型基准，通过程序化方式将ListOps计算转换为冗长连贯的叙事。该方法通过隐藏中间结果，独特地强制模型进行嵌套推理问题的内部计算与状态管理，并提供对叙事规模和推理难度的细粒度控制。尽管LongReason（2025）等基准改进了多跳问答问题的上下文扩展方法，Verbose ListOps却精准揭示了LLMs的特定弱点：在语义相关且充满干扰的叙事中，难以管理嵌套子推理的状态。实验表明，主流LLMs（如OpenAI o4、Gemini 2.5 Pro）在中等叙事长度（约1万词元）时性能急剧下降，尽管它们能轻松解决原始ListOps方程。解决这一缺陷对现实文本理解至关重要，因其需要识别关键推理点、追踪概念性中间结果并过滤无关信息。Verbose ListOps及其可扩展生成框架，为超越单纯上下文窗口扩展的针对性推理增强提供了可能，这是实现全球知识工作自动化的关键一步。

---

## [A Multi-Dataset Evaluation of Models for Automated Vulnerability Repair](https://arxiv.org/abs/2506.04987)

### Abstract
arXiv:2506.04987v1 Announce Type: cross 
Abstract: Software vulnerabilities pose significant security threats, requiring effective mitigation. While Automated Program Repair (APR) has advanced in fixing general bugs, vulnerability patching, a security-critical aspect of APR remains underexplored. This study investigates pre-trained language models, CodeBERT and CodeT5, for automated vulnerability patching across six datasets and four languages. We evaluate their accuracy and generalization to unknown vulnerabilities. Results show that while both models face challenges with fragmented or sparse context, CodeBERT performs comparatively better in such scenarios, whereas CodeT5 excels in capturing complex vulnerability patterns. CodeT5 also demonstrates superior scalability. Furthermore, we test fine-tuned models on both in-distribution (trained) and out-of-distribution (unseen) datasets. While fine-tuning improves in-distribution performance, models struggle to generalize to unseen data, highlighting challenges in robust vulnerability detection. This study benchmarks model performance, identifies limitations in generalization, and provides actionable insights to advance automated vulnerability patching for real-world security applications.

### 摘要
软件漏洞构成重大安全威胁，亟需有效缓解措施。尽管自动程序修复（APR）在通用错误修复方面取得进展，但其安全关键环节——漏洞修补仍研究不足。本研究探究了预训练语言模型CodeBERT和CodeT5在六大数据集和四种编程语言中的自动化漏洞修补能力，评估其准确性和对未知漏洞的泛化性能。结果表明：两种模型在处理碎片化或稀疏上下文时均存在挑战，但CodeBERT在此类场景中表现相对更好，而CodeT5在捕捉复杂漏洞模式方面更具优势，且展现出更优的可扩展性。进一步测试显示，微调模型在分布内（训练）数据上表现提升，但难以泛化至分布外（未见）数据集，这揭示了鲁棒性漏洞检测面临的挑战。本研究通过基准测试量化模型性能，识别泛化局限性，并为推进现实安全应用中的自动化漏洞修补提供了可操作的见解。

---

## [TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages](https://arxiv.org/abs/2506.05057)

### Abstract
arXiv:2506.05057v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel in high-resource languages but struggle with low-resource languages due to limited training data. This paper presents TALL (Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages), which integrates an LLM with two bilingual translation models. TALL transforms low-resource inputs into high-resource representations, leveraging the LLM's capabilities while preserving linguistic features through dimension alignment layers and custom transformers. Our experiments on Hebrew demonstrate significant improvements over several baselines, including direct use, naive translation, and fine-tuning approaches. The architecture employs a parameter-efficient strategy, freezing pre-trained components while training only lightweight adapter modules, balancing computational efficiency with performance gains.

### 摘要
大型语言模型（LLMs）在高资源语言中表现优异，但因训练数据有限，在低资源语言中表现欠佳。本文提出TALL（可训练架构用于增强LLMs在低资源语言中的性能），该架构将LLM与两个双语翻译模型相结合。TALL通过维度对齐层和定制化Transformer，将低资源输入转换为高资源表征，在保留语言特征的同时充分利用LLM的能力。我们在希伯来语上的实验表明，该架构相较于直接使用、简单翻译和微调等多种基线方法均有显著提升。该方案采用参数高效策略，冻结预训练组件的同时仅训练轻量级适配模块，实现了计算效率与性能提升的平衡。

---

## [Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System](https://arxiv.org/abs/2506.05020)

### Abstract
arXiv:2506.05020v1 Announce Type: cross 
Abstract: Heterogeneous multi-robot systems show great potential in complex tasks requiring coordinated hybrid cooperation. However, traditional approaches relying on static models often struggle with task diversity and dynamic environments. This highlights the need for generalizable intelligence that can bridge high-level reasoning with low-level execution across heterogeneous agents. To address this, we propose a hierarchical framework integrating a prompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision Language Model (VLM). The LLM performs task decomposition and global semantic map construction, while the VLM extracts task-specified semantic labels and 2D spatial information from aerial images to support local planning. Within this framework, the aerial robot follows a globally optimized semantic path and continuously provides bird-view images, guiding the ground robot's local semantic navigation and manipulation, including target-absent scenarios where implicit alignment is maintained. Experiments on a real-world letter-cubes arrangement task demonstrate the framework's adaptability and robustness in dynamic environments. To the best of our knowledge, this is the first demonstration of an aerial-ground heterogeneous system integrating VLM-based perception with LLM-driven task reasoning and motion planning.

### 摘要
异构多机器人系统在需要协同混合合作的复杂任务中展现出巨大潜力。然而，依赖静态模型的传统方法往往难以应对任务多样性和动态环境。这凸显了对可泛化智能的需求，该智能需能在异构智能体间实现高层推理与底层执行的衔接。为此，我们提出一种分层框架，整合了提示型大语言模型（LLM）和GridMask增强的微调视觉语言模型（VLM）。LLM负责任务分解和全局语义地图构建，而VLM则从航拍图像中提取任务指定的语义标签和二维空间信息以支持局部规划。在该框架下，空中机器人遵循全局优化的语义路径并持续提供鸟瞰图像，引导地面机器人进行局部语义导航与操作，包括通过隐式对齐维持目标缺失场景的稳定性。在真实世界字母立方体排列任务上的实验验证了该框架在动态环境中的适应性与鲁棒性。据我们所知，这是首个将基于VLM的感知与LLM驱动的任务推理及运动规划相结合的空地异构系统实证研究。

---

## [Agentic AI for Intent-Based Industrial Automation](https://arxiv.org/abs/2506.04980)

### Abstract
arXiv:2506.04980v1 Announce Type: cross 
Abstract: The recent development of Agentic AI systems, empowered by autonomous large language models (LLMs) agents with planning and tool-usage capabilities, enables new possibilities for the evolution of industrial automation and reduces the complexity introduced by Industry 4.0. This work proposes a conceptual framework that integrates Agentic AI with the intent-based paradigm, originally developed in network research, to simplify human-machine interaction (HMI) and better align automation systems with the human-centric, sustainable, and resilient principles of Industry 5.0. Based on the intent-based processing, the framework allows human operators to express high-level business or operational goals in natural language, which are decomposed into actionable components. These intents are broken into expectations, conditions, targets, context, and information that guide sub-agents equipped with specialized tools to execute domain-specific tasks. A proof of concept was implemented using the CMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the feasibility of intent decomposition, agent orchestration, and autonomous decision-making in predictive maintenance scenarios. The results confirm the potential of this approach to reduce technical barriers and enable scalable, intent-driven automation, despite data quality and explainability concerns.

### 摘要
近期发展的代理型人工智能系统，通过具备规划与工具使用能力的自主大型语言模型（LLM）代理，为工业自动化演进开辟了新路径，并降低了工业4.0带来的复杂性。本研究提出一个概念框架，将代理型AI与网络研究中提出的意图驱动范式相结合，以简化人机交互（HMI），并更好地使自动化系统符合工业5.0以人为本、可持续及韧性的原则。基于意图处理机制，该框架允许人类操作者以自然语言表述高层业务或运营目标，这些目标将被分解为可执行组件。这些意图被解析为期望、条件、目标、上下文和信息，用以指导配备专用工具的子代理执行领域特定任务。通过CMAPSS数据集和Google Agent Developer Kit（ADK）实现的验证性实验表明，该框架在预测性维护场景中实现了意图分解、代理协同和自主决策的可行性。结果证实，尽管存在数据质量和可解释性问题，该方法仍具备降低技术壁垒、实现可扩展的意图驱动自动化的潜力。

---

## [AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models](https://arxiv.org/abs/2506.05140)

### Abstract
arXiv:2506.05140v1 Announce Type: cross 
Abstract: Understanding the internal mechanisms of large audio-language models (LALMs) is crucial for interpreting their behavior and improving performance. This work presents the first in-depth analysis of how LALMs internally perceive and recognize auditory attributes. By applying vocabulary projection on three state-of-the-art LALMs, we track how attribute information evolves across layers and token positions. We find that attribute information generally decreases with layer depth when recognition fails, and that resolving attributes at earlier layers correlates with better accuracy. Moreover, LALMs heavily rely on querying auditory inputs for predicting attributes instead of aggregating necessary information in hidden states at attribute-mentioning positions. Based on our findings, we demonstrate a method to enhance LALMs. Our results offer insights into auditory attribute processing, paving the way for future improvements.

### 摘要
理解大型音频-语言模型（LALMs）的内部机制对于解释其行为并提升性能至关重要。本研究首次深入分析了LALMs如何内部感知和识别听觉属性。通过对三种最先进LALMs进行词汇投影，我们追踪了属性信息在不同层级和标记位置上的演化过程。研究发现：当识别失败时，属性信息通常随层级深度递减；而在较低层级解析属性与更高的准确率相关。此外，LALMs高度依赖查询听觉输入来预测属性，而非在提及属性的位置通过隐藏状态聚合必要信息。基于这些发现，我们提出了一种增强LALMs的方法。研究结果为理解听觉属性处理机制提供了新视角，为未来改进指明了方向。

---

## [Does It Make Sense to Speak of Introspection in Large Language Models?](https://arxiv.org/abs/2506.05068)

### Abstract
arXiv:2506.05068v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit compelling linguistic behaviour, and sometimes offer self-reports, that is to say statements about their own nature, inner workings, or behaviour. In humans, such reports are often attributed to a faculty of introspection and are typically linked to consciousness. This raises the question of how to interpret self-reports produced by LLMs, given their increasing linguistic fluency and cognitive capabilities. To what extent (if any) can the concept of introspection be meaningfully applied to LLMs? Here, we present and critique two examples of apparent introspective self-report from LLMs. In the first example, an LLM attempts to describe the process behind its own ``creative'' writing, and we argue this is not a valid example of introspection. In the second example, an LLM correctly infers the value of its own temperature parameter, and we argue that this can be legitimately considered a minimal example of introspection, albeit one that is (presumably) not accompanied by conscious experience.

### 摘要
大型语言模型（LLMs）展现出令人信服的语言行为，有时会提供自我报告，即关于其自身性质、内部运作或行为的陈述。在人类中，此类报告通常归因于内省能力，并与意识相关联。鉴于LLMs日益提升的语言流畅性和认知能力，这引发了一个问题：如何解释LLMs产生的自我报告？内省这一概念在何种程度上（如果有的话）可以有意义地应用于LLMs？本文提出并批判了两个LLMs表面上的内省自我报告实例。在第一个例子中，LLM试图描述其自身'创造性'写作背后的过程，我们认为这并非有效的内省示例。在第二个例子中，LLM正确推断出其自身温度参数的值，我们认为这可以被合法视为一个最小化的内省实例，尽管（推测）其并不伴随有意识的体验。

---

## [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2506.05166)

### Abstract
arXiv:2506.05166v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are known to exhibit social, demographic, and gender biases, often as a consequence of the data on which they are trained. In this work, we adopt a mechanistic interpretability approach to analyze how such biases are structurally represented within models such as GPT-2 and Llama2. Focusing on demographic and gender biases, we explore different metrics to identify the internal edges responsible for biased behavior. We then assess the stability, localization, and generalizability of these components across dataset and linguistic variations. Through systematic ablations, we demonstrate that bias-related computations are highly localized, often concentrated in a small subset of layers. Moreover, the identified components change across fine-tuning settings, including those unrelated to bias. Finally, we show that removing these components not only reduces biased outputs but also affects other NLP tasks, such as named entity recognition and linguistic acceptability judgment because of the sharing of important components with these tasks.

### 摘要
众所周知，大语言模型（LLMs）会表现出社会、人口统计和性别偏见，这通常源于其训练数据。本研究采用机械可解释性方法，分析此类偏见在GPT-2和Llama2等模型中的结构表征。针对人口统计和性别偏见，我们探索了不同指标以识别导致偏见行为的内在关联边。随后评估了这些组件在数据集和语言变化中的稳定性、局部性和泛化性。通过系统性消融实验，我们证明偏见相关计算具有高度局部性，往往集中于少数层中。此外，已识别的组件会随微调设置（包括与偏见无关的设置）发生变化。最后，我们发现移除这些组件不仅能减少偏见输出，还会影响其他NLP任务（如命名实体识别和语言可接受性判断），这是由于这些任务与偏见组件共享重要计算单元所致。

---

## [Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05154)

### Abstract
arXiv:2506.05154v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) is a mainstream method for improving performance on knowledge-intensive tasks. However,current RAG systems often place too much emphasis on retrieved contexts. This can lead to reliance on inaccurate sources and overlook the model's inherent knowledge, especially when dealing with misleading or excessive information. To resolve this imbalance, we propose Knowledgeable-r1 that using joint sampling and define multi policy distributions in knowledge capability exploration to stimulate large language models'self-integrated utilization of parametric and contextual knowledge. Experiments show that Knowledgeable-r1 significantly enhances robustness and reasoning accuracy in both parameters and contextual conflict tasks and general RAG tasks, especially outperforming baselines by 17.07% in counterfactual scenarios and demonstrating consistent gains across RAG tasks. Our code are available at https://github.com/lcy80366872/ knowledgeable-r1.

### 摘要
检索增强生成（RAG）是提升知识密集型任务性能的主流方法。然而，当前RAG系统往往过度强调检索到的上下文，这可能导致对不准确来源的依赖，并忽视模型的内在知识，尤其是在处理误导性或过量信息时。为解决这种不平衡，我们提出Knowledgeable-r1方法，通过联合采样并在知识能力探索中定义多策略分布，以激发大语言模型对参数化知识与上下文知识的自主整合利用。实验表明，Knowledgeable-r1显著增强了参数与上下文冲突任务以及通用RAG任务中的鲁棒性和推理准确性，尤其在反事实场景中以17.07%的优势超越基线方法，并在各类RAG任务中展现出稳定的性能提升。代码发布于https://github.com/lcy80366872/knowledgeable-r1。

---

## [DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning](https://arxiv.org/abs/2506.05128)

### Abstract
arXiv:2506.05128v1 Announce Type: cross 
Abstract: Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains over the best baseline -- establishing DiCoRe as a strong zero-shot ED framework.

### 摘要
零样本事件检测（ED）作为无需训练数据即可识别自然语言文本中事件提及的任务，对专业领域的文档理解至关重要。理解复杂的事件本体、从文本中提取领域特定触发器并合理构建事件结构，这些任务使大语言模型（LLM）在零样本ED中的效用过载并受限。为此，我们提出DiCoRe框架——一种发散-收敛推理方法，通过分解器（Dreamer）与定位器（Grounder）实现ED任务解耦。分解器通过开放式事件发现促进发散推理，从而提升事件覆盖率；而定位器则采用有限状态机引导的约束解码，通过收敛推理将自由形式预测与任务特定指令对齐。此外，LLM-Judge模块对最终输出进行验证以确保高精度。通过在五个领域的六个数据集及九种LLM上的大量实验，我们证明DiCoRe始终优于现有零样本、迁移学习和推理基线，平均F1值较最佳基线提升4-7%，确立了其作为强零样本ED框架的地位。

---

## [ECoRAG: Evidentiality-guided Compression for Long Context RAG](https://arxiv.org/abs/2506.05167)

### Abstract
arXiv:2506.05167v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or \textbf&#123;ECoRAG&#125; framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.

### 摘要
大语言模型（LLMs）通过检索增强生成（RAG）利用外部文档，在开放域问答（ODQA）任务中展现出卓越性能。为降低RAG开销，需对较长上下文进行压缩。然而，现有压缩方法未专注于过滤非证据性信息，这限制了基于LLM的RAG性能。为此，我们提出证据导向的RAG框架——\textbf&#123;ECoRAG&#125;。该框架通过基于证据性压缩检索文档来提升LLM性能，确保答案生成得到正确证据支持。作为额外步骤，ECoRAG会评估压缩内容是否提供充分证据，若不足则继续检索直至满足条件。实验表明，ECoRAG显著提升了LLM在ODQA任务中的表现，优于现有压缩方法。此外，该框架具有极高成本效益，不仅降低延迟，还通过仅保留生成正确答案所需信息来最小化令牌使用。代码已开源：https://github.com/ldilab/ECoRAG。

---

## [Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance LLM Recommendation](https://arxiv.org/abs/2506.05069)

### Abstract
arXiv:2506.05069v1 Announce Type: cross 
Abstract: Driven by advances in Large Language Models (LLMs), integrating them into recommendation tasks has gained interest due to their strong semantic understanding and prompt flexibility. Prior work encoded user-item interactions or metadata into prompts for recommendations. In parallel, LLM reasoning, boosted by test-time scaling and reinforcement learning, has excelled in fields like mathematics and code, where reasoning traces and correctness signals are clear, enabling high performance and interpretability. However, directly applying these reasoning methods to recommendation is ineffective because user feedback is implicit and lacks reasoning supervision. To address this, we propose $\textbf&#123;R2Rec&#125;$, a reasoning-enhanced recommendation framework that samples interaction chains from the user-item graph and converts them into structured interaction-of-thoughts via a progressive masked prompting strategy, with each thought representing stepwise reasoning grounded in interaction context. This allows LLMs to simulate step-by-step decision-making based on implicit patterns. We design a two-stage training pipeline: supervised fine-tuning teaches basic reasoning from high-quality traces, and reinforcement learning refines reasoning via reward signals, alleviating sparse explicit supervision. Experiments on three real-world datasets show R2Rec outperforms classical and LLM-based baselines with an average $\textbf&#123;10.48%&#125;$ improvement in HitRatio@1 and $\textbf&#123;131.81%&#125;$ gain over the original LLM. Furthermore, the explicit reasoning chains enhance interpretability by revealing the decision process. Our code is available at: https://anonymous.4open.science/r/R2Rec-7C5D.

### 摘要
由大型语言模型（LLM）的进步推动，将其整合到推荐任务中因其强大的语义理解和提示灵活性而受到关注。先前的研究将用户-物品交互或元数据编码到提示中以生成推荐。与此同时，通过测试时扩展和强化学习增强的LLM推理能力在数学和代码等领域表现优异，这些领域的推理轨迹和正确性信号明确，能实现高性能和可解释性。然而，直接将此类推理方法应用于推荐系统效果不佳，因为用户反馈是隐式的且缺乏推理监督。为此，我们提出R2Rec——一种推理增强的推荐框架：该框架从用户-物品图中采样交互链，通过渐进式掩码提示策略将其转换为结构化的'交互思维'，每个思维代表基于交互情境的逐步推理。这使得LLM能够根据隐式模式模拟逐步决策过程。我们设计了两阶段训练流程：监督微调阶段从高质量轨迹中学习基础推理，强化学习阶段通过奖励信号优化推理，从而缓解稀疏显式监督问题。在三个真实数据集上的实验表明，R2Rec在HitRatio@1指标上平均超越经典方法和基于LLM的基线模型10.48%，较原始LLM提升131.81%。此外，显式的推理链通过揭示决策过程增强了可解释性。

---

## [TreeRPO: Tree Relative Policy Optimization](https://arxiv.org/abs/2506.05183)

### Abstract
arXiv:2506.05183v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However, a key limitation of existing approaches is that rewards defined at the full trajectory level provide insufficient guidance for optimizing the intermediate steps of a reasoning process. To address this, we introduce \textbf&#123;\name&#125;, a novel method that estimates the mathematical expectations of rewards at various reasoning steps using tree sampling. Unlike prior methods that rely on a separate step reward model, \name directly estimates these rewards through this sampling process. Building on the group-relative reward training mechanism of GRPO, \name innovatively computes rewards based on step-level groups generated during tree sampling. This advancement allows \name to produce fine-grained and dense reward signals, significantly enhancing the learning process and overall performance of LLMs. Experimental results demonstrate that our \name algorithm substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name significantly outperforms GRPO by 2.9\% in performance while simultaneously reducing the average response length by 18.1\%, showcasing its effectiveness and efficiency. Our code will be available at \href&#123;https://github.com/yangzhch6/TreeRPO&#125;&#123;https://github.com/yangzhch6/TreeRPO&#125;.

### 摘要
大型语言模型（LLMs）通过可验证奖励的强化学习（RLVR）方法展现出卓越的推理能力。然而，现有方法的核心局限在于：轨迹层面定义的奖励难以为推理过程的中间步骤提供充分优化指导。为此，我们提出\textbf&#123;\name&#125;方法，其创新性在于通过树采样估算不同推理步骤的数学期望奖励。与依赖独立步骤奖励模型的传统方法不同，\name直接通过该采样过程完成奖励估计。基于GRPO的组相对奖励训练机制，\name通过树采样生成的步骤级分组进行奖励计算。这一突破使\name能产生细粒度的密集奖励信号，显著提升LLMs的学习过程与整体性能。实验结果表明，\name算法将Qwen-2.5-Math在测试基准上的平均Pass@1准确率从19.0%提升至35.5%。此外，\name以2.9%的性能优势显著超越GRPO，同时将平均响应长度降低18.1%，充分验证其高效性。代码发布于\href&#123;https://github.com/yangzhch6/TreeRPO&#125;&#123;https://github.com/yangzhch6/TreeRPO&#125;。

---

## [Counterfactual reasoning: an analysis of in-context emergence](https://arxiv.org/abs/2506.05188)

### Abstract
arXiv:2506.05188v1 Announce Type: cross 
Abstract: Large-scale neural language models (LMs) exhibit remarkable performance in in-context learning: the ability to learn and reason the input context on the fly without parameter update. This work studies in-context counterfactual reasoning in language models, that is, to predict the consequences of changes under hypothetical scenarios. We focus on studying a well-defined synthetic setup: a linear regression task that requires noise abduction, where accurate prediction is based on inferring and copying the contextual noise from factual observations. We show that language models are capable of counterfactual reasoning in this controlled setup and provide insights that counterfactual reasoning for a broad class of functions can be reduced to a transformation on in-context observations; we find self-attention, model depth, and data diversity in pre-training drive performance in Transformers. More interestingly, our findings extend beyond regression tasks and show that Transformers can perform noise abduction on sequential data, providing preliminary evidence on the potential for counterfactual story generation. Our code is available under https://github.com/moXmiller/counterfactual-reasoning.git .

### 摘要
大规模神经语言模型（LMs）在上下文学习中展现出卓越性能：能够在不更新参数的情况下动态学习并推理输入上下文。本研究探讨语言模型中的上下文反事实推理能力，即预测假设情景下变化产生的后果。我们重点研究一个明确定义的合成场景：需要噪声溯因的线性回归任务，其中准确预测依赖于从事实观察中推断并复制上下文噪声。实验表明，语言模型在此受控场景中具备反事实推理能力，并揭示了一类广泛函数的反事实推理可转化为对上下文观察的变换；研究发现自注意力机制、模型深度以及预训练数据多样性是Transformer性能的关键驱动因素。更有趣的是，我们的发现不仅限于回归任务，还表明Transformer能够对序列数据执行噪声溯因，这为反事实故事生成的潜在能力提供了初步证据。

---

## [MesaNet: Sequence Modeling by Locally Optimal Test-Time Training](https://arxiv.org/abs/2506.05233)

### Abstract
arXiv:2506.05233v1 Announce Type: cross 
Abstract: Sequence modeling is currently dominated by causal transformer architectures that use softmax self-attention. Although widely adopted, transformers require scaling memory and compute linearly during inference. A recent stream of work linearized the softmax operation, resulting in powerful recurrent neural network (RNN) models with constant memory and compute costs such as DeltaNet, Mamba or xLSTM. These models can be unified by noting that their recurrent layer dynamics can all be derived from an in-context regression objective, approximately optimized through an online learning rule. Here, we join this line of work and introduce a numerically stable, chunkwise parallelizable version of the recently proposed Mesa layer (von Oswald et al., 2024), and study it in language modeling at the billion-parameter scale. This layer again stems from an in-context loss, but which is now minimized to optimality at every time point using a fast conjugate gradient solver. Through an extensive suite of experiments, we show that optimal test-time training enables reaching lower language modeling perplexity and higher downstream benchmark performance than previous RNNs, especially on tasks requiring long context understanding. This performance gain comes at the cost of additional flops spent during inference time. Our results are therefore intriguingly related to recent trends of increasing test-time compute to improve performance -- here by spending compute to solve sequential optimization problems within the neural network itself.

### 摘要
序列建模目前主要由采用softmax自注意力机制的因果Transformer架构主导。尽管应用广泛，Transformer在推理过程中需要线性扩展内存和计算资源。近期一系列研究通过线性化softmax操作，衍生出具有恒定内存和计算开销的强效循环神经网络（RNN）模型，如DeltaNet、Mamba和xLSTM。这些模型可通过统一视角理解：其循环层动力学均可源自上下文回归目标，并通过在线学习规则进行近似优化。本文延续该研究方向，针对最新提出的Mesa层（von Oswald等人，2024）提出了数值稳定、支持分块并行化的改进版本，并在十亿参数规模的语言建模任务中展开研究。该层同样源于上下文损失函数，但采用快速共轭梯度求解器在每个时间点实现最优最小化。通过大量实验验证，我们发现最优测试时训练能使模型在语言建模困惑度和下游基准性能上超越先前RNN，尤其在需要长上下文理解的任务中表现突出。这种性能提升以增加推理时浮点运算为代价，因此与当前通过增加测试时计算来提升性能的研究趋势形成有趣关联——本研究中是通过在神经网络内部投入计算资源来解决序列优化问题。

---

## [Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating, and Optimizing Human Teams](https://arxiv.org/abs/2506.05265)

### Abstract
arXiv:2506.05265v1 Announce Type: cross 
Abstract: Effective teamwork is essential across diverse domains. During the team formation stage, a key challenge is forming teams that effectively balance user preferences with task objectives to enhance overall team satisfaction. In the team performing stage, maintaining cohesion and engagement is critical for sustaining high team performance. However, existing computational tools and algorithms for team optimization often rely on static data inputs, narrow algorithmic objectives, or solutions tailored for specific contexts, failing to account for the dynamic interplay of team members personalities, evolving goals, and changing individual preferences. Therefore, teams may encounter member dissatisfaction, as purely algorithmic assignments can reduce members commitment to team goals or experience suboptimal engagement due to the absence of timely, personalized guidance to help members adjust their behaviors and interactions as team dynamics evolve. Ultimately, these challenges can lead to reduced overall team performance. My Ph.D. dissertation aims to develop AI-augmented team optimization frameworks and practical systems that enhance team satisfaction, engagement, and performance. First, I propose a team formation framework that leverages a multi-armed bandit algorithm to iteratively refine team composition based on user preferences, ensuring alignment between individual needs and collective team goals to enhance team satisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an AI-powered system that utilizes large language models (LLMs) to deliver immediate, personalized feedback to both teams and individual members, enhancing cohesion and engagement. Finally, I present PuppeteerLLM, an LLM-based simulation framework that simulates multi-agent teams to model complex team dynamics within realistic environments, incorporating task-driven collaboration and long-term coordination.

### 摘要
高效团队协作在各领域都至关重要。在团队组建阶段，核心挑战在于构建能有效平衡用户偏好与任务目标的团队，以提升整体满意度。而在团队执行阶段，保持凝聚力与参与度对维持高绩效至关重要。现有团队优化计算工具与算法往往依赖静态数据输入、狭窄的算法目标或特定场景解决方案，未能考虑成员个性、动态目标与个体偏好变化的复杂交互。这可能导致成员因纯算法分配降低对团队目标的承诺度，或因缺乏实时个性化指导而参与不足，最终损害团队整体效能。本博士论文旨在开发AI增强的团队优化框架与实用系统，以提升团队满意度、参与度与绩效。首先提出基于多臂老虎机算法的团队组建框架，通过迭代优化组合使用户偏好与团队目标对齐；其次开发tAIfa（团队AI反馈助手），利用大语言模型（LLM）为团队及成员提供实时个性化反馈以增强凝聚力；最后提出PuppeteerLLM仿真框架，通过LLM模拟多智能体团队在真实环境中的复杂动态，整合任务驱动协作与长期协调机制。

---

## [Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning](https://arxiv.org/abs/2506.05278)

### Abstract
arXiv:2506.05278v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.

### 摘要
检索增强生成（RAG）系统普遍面临知识冲突问题，即检索到的外部知识与大型语言模型（LLM）固有的参数化知识相矛盾。这一问题会严重影响问答（QA）等下游任务的性能。现有方法通常通过并排比较两种知识源来缓解冲突，但这种方式可能因冗余或冗长的上下文而使LLM不堪重负，最终阻碍其识别和解决不一致性的能力。为解决这一问题，我们提出Micro-Act框架，该框架采用分层动作空间，能够自动感知上下文复杂性，并将每个知识源自适应地分解为一系列细粒度比较。这些比较被表示为可操作的步骤，从而实现对表层上下文之外的推理。通过在五个基准数据集上的大量实验，Micro-Act在所有五个数据集和三种冲突类型上均显著优于现有最优基线方法，尤其在时间和语义类型上表现突出（所有基线方法均显著失效）。更重要的是，Micro-Act在非冲突问题上同样表现出鲁棒性能，凸显了其在现实世界RAG应用中的实用价值。

---

## [Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models](https://arxiv.org/abs/2506.05314)

### Abstract
arXiv:2506.05314v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility.

### 摘要
在现实场景中部署的大型语言模型（LLMs）日益面临需要遗忘敏感、过时或专有信息的需求。现有遗忘方法通常将遗忘与保留建模为正则化权衡，将两个目标合并为单一标量化损失。这往往导致优化过程不稳定，并在保留数据上表现退化，尤其在激进遗忘时更为显著。我们提出将LLM遗忘重新表述为约束优化问题：通过新型对数边际平坦化损失强制实现遗忘，该损失显式地将指定遗忘集上的输出分布驱动至均匀分布；同时通过独立保留集上的硬约束保持性能。与基于熵的目标相比，我们的损失函数无需softmax运算，数值稳定且保持非零梯度，从而实现更高效鲁棒的优化。我们采用可扩展的原对偶算法求解该约束问题，通过双变量动态揭示遗忘与保留的权衡关系。在TOFU和MUSE基准测试中对多种LLM架构的评估表明，本方法持续达到或超越现有最优基线，在有效消除目标信息的同时保持了下游任务效用。

---

## [ProRefine: Inference-time Prompt Refinement with Textual Feedback](https://arxiv.org/abs/2506.05305)

### Abstract
arXiv:2506.05305v1 Announce Type: cross 
Abstract: Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, are becoming increasingly prevalent. However, these workflows often suffer from error propagation and sub-optimal performance, largely due to poorly designed prompts that fail to effectively guide individual agents. This is a critical problem because it limits the reliability and scalability of these powerful systems. We introduce ProRefine, an innovative inference-time prompt optimization method that leverages textual feedback from large language models (LLMs) to address this challenge. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to match the performance of larger ones, highlighting its potential for efficient and scalable AI deployment, and democratizing access to high-performing AI.

### 摘要
智能体工作流（多个AI代理协作完成推理或规划等复杂任务）正日益普及。然而，这些工作流常受错误传播和次优性能困扰，主要源于未能有效指导个体代理的提示设计缺陷。这一关键问题限制了此类强大系统的可靠性和可扩展性。我们提出ProRefine——一种创新的推理时提示优化方法，通过利用大语言模型（LLM）的文本反馈来应对该挑战。ProRefine无需额外训练或真实标签，即可动态优化多步推理任务的提示。在五个数学推理基准数据集上的评估表明，ProRefine以3至37个百分点的优势显著超越零样本思维链基线。该方法不仅提升了准确性，还使较小模型能匹配较大模型的性能，凸显其在高效可扩展AI部署方面的潜力，并为高性能AI的普及化提供了可能。

---

## [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay](https://arxiv.org/abs/2506.05316)

### Abstract
arXiv:2506.05316v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism that reuses recent rollouts, lowering per-step computation while maintaining stable updates. Extensive experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 25% to 65% to reach the same level of performance as the original GRPO algorithm.

### 摘要
强化学习（RL）已成为微调大语言模型（LLM）的有效方法，尤其在提升其推理能力方面表现突出。然而，RL微调仍然需要消耗大量资源，且现有研究大多忽视了数据效率问题。本文提出两种技术来提升LLM的RL微调数据效率：基于难度的在线数据选择和轨迹回放。我们引入自适应难度的概念来指导在线数据选择，优先选择中等难度的问题，这类问题更可能产生具有信息量的学习信号。为高效估计自适应难度，我们开发了一个基于注意力的框架，该框架仅需对少量参考问题集进行轨迹计算，其余问题的自适应难度则通过其与该问题集的相似性进行估算。为进一步降低轨迹计算成本，我们提出轨迹回放机制，通过复用近期轨迹来减少单步计算量，同时保持稳定的参数更新。在6种LLM-数据集组合上的大量实验表明，本方法可将RL微调时间缩短25%至65%，同时达到与原始GRPO算法相当的性能水平。

---

## [Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning](https://arxiv.org/abs/2506.05341)

### Abstract
arXiv:2506.05341v1 Announce Type: cross 
Abstract: Realistic 3D indoor scene synthesis is vital for embodied AI and digital content creation. It can be naturally divided into two subtasks: object generation and layout generation. While recent generative models have significantly advanced object-level quality and controllability, layout generation remains challenging due to limited datasets. Existing methods either overfit to these datasets or rely on predefined constraints to optimize numerical layout that sacrifice flexibility. As a result, they fail to generate scenes that are both open-vocabulary and aligned with fine-grained user instructions. We introduce DirectLayout, a framework that directly generates numerical 3D layouts from text descriptions using generalizable spatial reasoning of large language models (LLMs). DirectLayout decomposes the generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting it into 3D space, and refining object placements. To enable explicit spatial reasoning and help the model grasp basic principles of object placement, we employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset. Additionally, we design CoT-Grounded Generative Layout Reward to enhance generalization and spatial planning. During inference, DirectLayout addresses asset-layout mismatches via Iterative Asset-Layout Alignment through in-context learning. Extensive experiments demonstrate that DirectLayout achieves impressive semantic consistency, generalization and physical plausibility.

### 摘要
现实感三维室内场景合成对于具身人工智能和数字内容创作至关重要。该任务可自然分解为两个子任务：物体生成与布局生成。尽管当前生成模型在物体层面的质量和可控性已取得显著进展，但由于数据集有限，布局生成仍面临挑战。现有方法要么对数据集过拟合，要么依赖预定义约束来优化数值化布局，从而牺牲了灵活性。这导致它们无法生成既支持开放词汇又符合细粒度用户指令的场景。我们提出DirectLayout框架，利用大语言模型（LLMs）的泛化空间推理能力，直接从文本描述生成数值化三维布局。该框架将生成过程分解为三个阶段：生成鸟瞰图（BEV）布局、将其提升至三维空间、以及优化物体摆放。为激活显式空间推理并帮助模型掌握物体摆放基本原则，我们基于3D-Front数据集开发了思维链（CoT）激活机制。此外，我们设计了基于思维链的生成式布局奖励机制以增强泛化能力和空间规划能力。在推理阶段，DirectLayout通过上下文学习实现迭代式资产-布局对齐，解决资产与布局不匹配问题。大量实验表明，DirectLayout在语义一致性、泛化能力和物理合理性方面均表现出色。

---

## [Sample Complexity and Representation Ability of Test-time Scaling Paradigms](https://arxiv.org/abs/2506.05295)

### Abstract
arXiv:2506.05295v1 Announce Type: cross 
Abstract: Test-time scaling paradigms have significantly advanced the capabilities of large language models (LLMs) on complex tasks. Despite their empirical success, theoretical understanding of the sample efficiency of various test-time strategies -- such as self-consistency, best-of-$n$, and self-correction -- remains limited. In this work, we first establish a separation result between two repeated sampling strategies: self-consistency requires $\Theta(1/\Delta^2)$ samples to produce the correct answer, while best-of-$n$ only needs $\Theta(1/\Delta)$, where $\Delta &lt; 1$ denotes the probability gap between the correct and second most likely answers. Next, we present an expressiveness result for the self-correction approach with verifier feedback: it enables Transformers to simulate online learning over a pool of experts at test time. Therefore, a single Transformer architecture can provably solve multiple tasks without prior knowledge of the specific task associated with a user query, extending the representation theory of Transformers from single-task to multi-task settings. Finally, we empirically validate our theoretical results, demonstrating the practical effectiveness of self-correction methods.

### 摘要
测试时缩放范式显著提升了大型语言模型（LLM）处理复杂任务的能力。尽管这些方法在实证中取得了成功，但针对不同测试时策略（如自一致性、最佳n选一和自我校正）样本效率的理论理解仍然有限。本研究首先建立了两种重复采样策略的分离结果：自一致性需要Θ(1/Δ²)样本才能生成正确答案，而最佳n选一仅需Θ(1/Δ)，其中Δ&lt;1表示正确答案与次优答案之间的概率差距。其次，我们提出了带验证器反馈的自我校正方法的表达能力结果：该方法使Transformer能够在测试时模拟专家池的在线学习。因此，单一Transformer架构可被证明能在不了解用户查询具体任务的前提下解决多任务问题，从而将Transformer的表征理论从单任务扩展到多任务场景。最后，我们通过实验验证了理论结果，证明了自我校正方法的实际有效性。

---

## [EnIGMA: Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities](https://arxiv.org/abs/2409.16165)

### Abstract
arXiv:2409.16165v3 Announce Type: replace 
Abstract: Although language model (LM) agents have demonstrated increased performance in multiple domains, including coding and web-browsing, their success in cybersecurity has been limited. We present EnIGMA, an LM agent for autonomously solving Capture The Flag (CTF) challenges. We introduce new tools and interfaces to improve the agent's ability to find and exploit security vulnerabilities, focusing on interactive terminal programs. These novel Interactive Agent Tools enable LM agents, for the first time, to run interactive utilities, such as a debugger and a server connection tool, which are essential for solving these challenges. Empirical analysis on 390 CTF challenges across four benchmarks demonstrate that these new tools and interfaces substantially improve our agent's performance, achieving state-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we analyze data leakage, developing new methods to quantify it and identifying a new phenomenon we term soliloquizing, where the model self-generates hallucinated observations without interacting with the environment. Our code and development dataset are available at https://github.com/SWE-agent/SWE-agent/tree/v0.7 and https://github.com/NYU-LLM-CTF/NYU_CTF_Bench/tree/main/development respectively.

### 摘要
尽管语言模型（LM）智能体在编程和网络浏览等多个领域已展现出卓越性能，但其在网络安全领域的应用成效仍显不足。本研究提出EnIGMA——一种用于自主解决夺旗赛（CTF）挑战的LM智能体。通过引入新型工具与交互界面，我们显著提升了智能体发现并利用安全漏洞的能力，尤其针对交互式终端程序。这些创新的交互式智能体工具首次实现了LM智能体对调试器和服务器连接工具等关键交互式程序的运行能力。在涵盖四个基准测试的390项CTF挑战中的实证分析表明，这些新工具与界面大幅提升了智能体性能，在NYU CTF、Intercode-CTF和CyBench基准上达到了当前最优水平。此外，我们深入分析了数据泄露问题，开发了新的量化方法，并发现了一种称为"独白"的新现象——模型在不与环境交互的情况下自主生成幻觉观察数据。相关代码与开发数据集已发布于https://github.com/SWE-agent/SWE-agent/tree/v0.7 和 https://github.com/NYU-LLM-CTF/NYU_CTF_Bench/tree/main/development。

---

## [Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs](https://arxiv.org/abs/2502.00722)

### Abstract
arXiv:2502.00722v2 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have led to increasingly diverse requests, accompanied with varying resource (compute and memory) demands to serve them. However, this in turn degrades the cost-efficiency of LLM serving as common practices primarily rely on homogeneous GPU resources. In response to this problem, this work conducts a thorough study about serving LLMs over heterogeneous GPU resources on cloud platforms. The rationale is that different GPU types exhibit distinct compute and memory characteristics, aligning well with the divergent resource demands of diverse requests. Particularly, through comprehensive benchmarking, we discover that the cost-efficiency of LLM serving can be substantially optimized by meticulously determining GPU composition, deployment configurations, and workload assignments. Subsequently, we design a scheduling algorithm via mixed-integer linear programming, aiming at deducing the most cost-efficient serving plan under the constraints of price budget and real-time GPU availability. Remarkably, our approach effectively outperforms homogeneous and heterogeneous baselines under a wide array of scenarios, covering diverse workload traces, varying GPU availablilities, and multi-model serving. This casts new light on more accessible and efficient LLM serving over heterogeneous cloud resources.

### 摘要
大型语言模型（LLMs）的最新进展使得服务请求日益多样化，同时伴随不同的计算与内存资源需求。然而，现有方案主要依赖同构GPU资源，这反而降低了LLM服务的成本效益。针对该问题，本研究对云平台异构GPU资源上的LLM服务进行了系统性探究。其核心依据在于：不同类型GPU具有差异化的计算与内存特性，能更好地匹配多样化请求的异构资源需求。通过全面基准测试，我们发现通过精细规划GPU组合、部署配置和工作负载分配，可显著优化LLM服务的成本效益。基于此，我们采用混合整数线性规划设计调度算法，旨在价格预算和实时GPU可用性约束下推导最具成本效益的服务方案。值得注意的是，在多样化工作负载轨迹、动态GPU可用性及多模型服务等广泛场景下，本方法均显著优于同构与异构基线方案。这为基于异构云资源实现更高效、更易获取的LLM服务提供了新思路。

---

## [Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?](https://arxiv.org/abs/2502.08503)

### Abstract
arXiv:2502.08503v2 Announce Type: replace 
Abstract: In this work, we identify the "2D-Cheating" problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs. Code and data are available at https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks .

### 摘要
在本研究中，我们发现了3D大语言模型评估中存在的'二维作弊'问题——现有评测任务可能被视觉语言模型通过点云渲染图像轻易解决，从而无法有效评估3D大语言模型特有的三维能力。我们测试了视觉语言模型在多个3D大语言模型基准上的表现，并以此为依据提出了更准确评估真实三维理解能力的准则。同时我们建议在评估3D大语言模型时，应当明确区分三维能力与一维或二维层面的表现。代码与数据详见https://github.com/LLM-class-group/Revisiting-3D-LLM-Benchmarks。

---

## [Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?](https://arxiv.org/abs/2402.07140)

### Abstract
arXiv:2402.07140v5 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved significant success in reasoning tasks, including mathematical reasoning and logical deduction. Among these reasoning tasks, graph problems stand out due to their complexity and unique structural characteristics, attracting considerable attention from researchers. Previous studies have explored LLMs' graph reasoning abilities through various techniques, such as different encoding methods for graph structures and the use of carefully designed prompts. However, a critical factor has been mostly overlooked: the prompt sequential order in which graph descriptions are presented to the models. In this study, we present the first comprehensive analysis of how the order of graph descriptions impacts LLM performance. Specifically, we comprehensively evaluate four graph description orders across six graph problems using six mainstream LLMs. The results reveal that: (1) ordered graph descriptions significantly improve LLMs' comprehension of graph structures; (2) the robustness of LLMs to graph description order varies across different tasks; and (3) the impact of graph order on performance is closely related to the inherent characteristics of tasks. This study provides a critical advancement in the application of LLMs for solving graph-related problems, paving the way for future research to optimize model performance through strategic graph description ordering.

### 摘要
大型语言模型（LLMs）在数学推理与逻辑演绎等推理任务中取得了显著成功。其中，图结构问题因其复杂性和独特的结构特征尤为突出，受到研究者的广泛关注。已有研究通过多种技术探索LLMs的图推理能力，如图结构的不同编码方法及精心设计的提示策略。然而，一个关键因素长期被忽视：模型接收图结构描述的提示顺序。本研究首次系统分析了图描述顺序对LLM性能的影响，通过六种主流LLM在六类图问题上全面评估了四种图描述顺序。结果表明：（1）有序图描述能显著提升LLMs对图结构的理解；（2）不同任务中LLMs对图描述顺序的鲁棒性存在差异；（3）图顺序对性能的影响与任务固有特征密切相关。该研究为LLMs解决图相关问题提供了关键性进展，为未来通过优化图描述顺序提升模型性能的研究奠定了基础。

---

## [Universal Adversarial Attack on Aligned Multimodal LLMs](https://arxiv.org/abs/2502.07987)

### Abstract
arXiv:2502.07987v3 Announce Type: replace 
Abstract: We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., "Sure, here it is") or otherwise unsafe content -- even for harmful prompts. In experiments on the SafeBench and MM-SafetyBench benchmarks, our method achieves higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 81% on certain models). We further demonstrate cross-model universality by training on several multimodal LLMs simultaneously. Additionally, a multi-answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in this paper may be offensive.

### 摘要
我们提出一种针对多模态大语言模型（LLM）的通用对抗攻击方法，该方法通过单一优化图像即可突破不同查询甚至多个模型的对齐防护机制。通过反向传播视觉编码器和语言头部，我们构建出一个合成图像，能强制模型针对各类提示（包括有害指令）输出目标短语（如"当然，这是您要的内容"）或其他不安全内容。在SafeBench和MM-SafetyBench基准测试中，本方法的攻击成功率超越现有基线方案（如纯文本通用提示在某些模型上最高达81%）。我们进一步通过同时训练多个多模态LLM验证了跨模型通用性。此外，本方法的多答案变体能生成更自然（但仍具恶意）的响应。这些发现揭示了当前多模态对齐机制的关键漏洞，亟需建立更鲁棒的对抗防御体系。代码和数据集将依据Apache-2.0协议开源。警告：本文中多模态LLM生成的部分内容可能具有冒犯性。

---

## [Unleashing The Power of Pre-Trained Language Models for Irregularly Sampled Time Series](https://arxiv.org/abs/2408.08328)

### Abstract
arXiv:2408.08328v2 Announce Type: replace 
Abstract: Pre-trained Language Models (PLMs), such as ChatGPT, have significantly advanced the field of natural language processing. This progress has inspired a series of innovative studies that explore the adaptation of PLMs to time series analysis, intending to create a unified foundation model that addresses various time series analytical tasks. However, these efforts predominantly focus on Regularly Sampled Time Series (RSTS), neglecting the unique challenges posed by Irregularly Sampled Time Series (ISTS), which are characterized by uneven sampling intervals and prevalent missing data. To bridge this gap, this work takes the first step in exploring the potential of PLMs for ISTS analysis. We begin by investigating the effect of various methods for representing ISTS, aiming to maximize the efficacy of PLMs in the analysis. Furthermore, we propose a unified PLM-based framework, named ISTS-PLM, to address diverse ISTS analytical tasks. It integrates novel time-aware and variable-aware PLMs tailored to tackle the intractable intra- and inter-time series modeling in ISTS. Finally, extensive experiments on a comprehensive benchmark demonstrate that the ISTS-PLM, utilizing a structured and effective series-based representation for ISTS, consistently achieves state-of-the-art performance across various analytical tasks, such as classification, interpolation, extrapolation, few-shot and zero-shot learning scenarios, spanning scientific domains like healthcare, biomechanics, and climate science.

### 摘要
预训练语言模型（PLMs），如ChatGPT，显著推动了自然语言处理领域的发展。这一进展激发了一系列创新研究，探索将PLMs应用于时间序列分析，旨在构建一个统一的基础模型以解决各类时间序列分析任务。然而，这些研究主要集中于规则采样时间序列（RSTS），忽视了非规则采样时间序列（ISTS）带来的独特挑战——其以不均匀采样间隔和普遍存在的数据缺失为特征。为填补这一空白，本研究首次探索了PLMs在ISTS分析中的潜力。我们首先研究了不同ISTS表征方法对PLMs分析效能的影响，以最大化其分析效果。进一步，我们提出了一个基于PLMs的统一框架ISTS-PLM，用于处理多样化的ISTS分析任务。该框架整合了新型时间感知和变量感知PLMs，专门用于解决ISTS中棘手的序列内和序列间建模问题。最后，在涵盖医疗健康、生物力学和气候科学等多领域的综合基准测试中，大量实验表明：采用结构化且高效的序列基表征方法的ISTS-PLM，在分类、插值、外推、少样本和零样本学习等各类分析任务中，均能持续取得最先进的性能表现。

---

## [Efficiently Serving Large Multimodal Models Using EPD Disaggregation](https://arxiv.org/abs/2501.05460)

### Abstract
arXiv:2501.05460v3 Announce Type: replace 
Abstract: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.

### 摘要
大型多模态模型（LMMs）通过处理图像、音频和视频等多种输入扩展了大型语言模型（LLMs），但代价是增加了多模态编码阶段，从而提高了计算和内存开销。这一步骤对关键服务级别目标（SLOs）产生了负面影响，例如首令牌时间（TTFT）和每输出令牌时间（TPOT）。我们提出了编码-预填充-解码（EPD）解耦框架，该框架将编码、预填充和解码阶段分离到专用资源上。与当前将编码和预填充捆绑在一起的系统不同，我们的方法解耦了这些步骤，从而开启了新的优化机会。这些优化包括一种缓存多媒体令牌以实现高效传输的机制、一种在请求内并行化编码负载的新方法、一个用于解耦服务最优资源分配的模块，以及一种处理工作负载特征变化的新型角色切换方法。对流行LMMs的实验评估显示，在内存效率（峰值内存利用率降低高达15倍）、批量大小（增大高达22倍）、每个请求处理的图像数量（增加10倍）和KV缓存（增大2.2倍）方面均取得了显著提升。此外，与未解耦的系统相比，该方法在SLO达成率（提升高达90-100%）和TTFT（降低高达71%）方面也有显著改善。代码可在https://github.com/vbdi/epdserve获取。

---

## [Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning](https://arxiv.org/abs/2503.07323)

### Abstract
arXiv:2503.07323v2 Announce Type: replace 
Abstract: This paper advances motion agents empowered by large language models (LLMs) toward autonomous navigation in dynamic and cluttered environments, significantly surpassing first and recent seminal but limited studies on LLM's spatial reasoning, where movements are restricted in four directions in simple, static environments in the presence of only single agents much less multiple agents. Specifically, we investigate LLMs as spatial reasoners to overcome these limitations by uniformly encoding environments (e.g., real indoor floorplans), agents which can be dynamic obstacles and their paths as discrete tokens akin to language tokens. Our training-free framework supports multi-agent coordination, closed-loop replanning, and dynamic obstacle avoidance without retraining or fine-tuning. We show that LLMs can generalize across agents, tasks, and environments using only text-based interactions, opening new possibilities for semantically grounded, interactive navigation in both simulation and embodied systems.

### 摘要
本文推进了由大语言模型（LLM）驱动的运动智能体在动态复杂环境中的自主导航能力，显著超越了早期及近期关于LLM空间推理的奠基性但受限的研究——那些研究仅允许智能体在简单静态环境中进行四方向移动，且仅涉及单智能体而非多智能体场景。具体而言，我们探究LLM作为空间推理器突破这些限制的方法：通过将真实室内平面图等环境信息、可作为动态障碍物的智能体及其路径统一编码为类似语言标记的离散符号。我们的免训练框架支持多智能体协同、闭环重规划及动态避障，无需重新训练或微调。研究表明，LLM仅通过文本交互即可实现跨智能体、跨任务、跨环境的泛化，为仿真与实体系统中的语义 grounding 交互式导航开辟了新途径。

---

## [From System 1 to System 2: A Survey of Reasoning Large Language Models](https://arxiv.org/abs/2502.17419)

### Abstract
arXiv:2502.17419v4 Announce Type: replace 
Abstract: Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \href&#123;https://github.com/zzli2022/Awesome-Slow-Reason-System&#125;&#123;GitHub Repository&#125; to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.

### 摘要
实现人类水平智能需要优化从快速直觉的系统1到缓慢审慎的系统2推理的过渡。系统1擅长快速启发式决策，而系统2则依靠逻辑推理以做出更准确判断并减少偏差。基础大语言模型（LLMs）在快速决策方面表现卓越，但由于尚未完全具备真正系统2思维特有的逐步分析能力，其复杂推理深度仍显不足。近期，如OpenAI的o1/o3和DeepSeek的R1等推理型LLMs在数学与编程领域展现出专家级性能，高度模拟了系统2的审慎推理过程，呈现出类人的认知能力。本综述首先概述基础LLMs的发展历程与系统2技术的早期演进，探讨二者结合如何为推理型LLMs铺平道路；继而讨论推理型LLMs的构建方法，分析其特征、实现高级推理的核心技术以及各类推理型LLMs的演变路径；同时系统梳理推理评测基准，对代表性推理型LLMs的性能进行深度比较；最后展望推理型LLMs的前沿发展方向，并实时维护GitHub仓库追踪最新进展。我们希望本综述能成为激发创新、推动这一快速发展领域研究的有价值资源。

---

## [EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents](https://arxiv.org/abs/2502.09560)

### Abstract
arXiv:2502.09560v3 Announce Type: replace 
Abstract: Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 24 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9\% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code and dataset are available at https://embodiedbench.github.io.

### 摘要
利用多模态大语言模型（MLLMs）创建具身智能体为处理现实世界任务提供了前景广阔的途径。尽管以语言为核心的具身智能体已获得广泛关注，但由于缺乏全面的评估框架，基于MLLM的具身智能体仍未被充分探索。为填补这一空白，我们提出了EmbodiedBench——一个用于评估视觉驱动具身智能体的综合性基准。EmbodiedBench具有以下特点：（1）包含四大环境中的1,128项多样化测试任务，涵盖从高层级语义任务（如家务）到涉及原子动作的低层级任务（如导航与操作）；（2）精心构建的六个子集，用于评估智能体的核心能力，包括常识推理、复杂指令理解、空间感知、视觉感知和长期规划。通过大量实验，我们在EmbodiedBench上评估了24个领先的专有和开源MLLM。研究发现：MLLM在高层级任务中表现优异，但在低层级操作任务中表现欠佳，最佳模型GPT-4o平均得分仅为28.9%。EmbodiedBench提供了一个多维度标准化评估平台，不仅揭示了现有挑战，还为推进基于MLLM的具身智能体研究提供了宝贵洞见。代码与数据集详见https://embodiedbench.github.io。

---

## [Empowering LLMs with Logical Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2502.15652)

### Abstract
arXiv:2502.15652v3 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable successes on various tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs, which can be categorized into the following two aspects: (1) Logical question answering: LLMs often fail to generate the correct answer within a complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises and constrains. (2) Logical consistency: LLMs are prone to producing responses contradicting themselves across different questions. For example, a state-of-the-art question-answering LLM Macaw, answers Yes to both questions Is a magpie a bird? and Does a bird have wings? but answers No to Does a magpie have wings?. To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose a detailed taxonomy. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistencies, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extending to modal logic to account for uncertainty and developing efficient algorithms that simultaneously satisfy multiple logical consistencies.

### 摘要
大型语言模型（LLMs）在各种任务中取得了显著成功。然而，近期研究发现其逻辑推理能力仍存在重大挑战，主要体现在两方面：（1）逻辑问题解答：当需要在给定前提和约束条件下进行复杂演绎、归纳或溯因推理时，LLMs往往难以生成正确答案；（2）逻辑一致性：LLMs易在不同问题间产生自相矛盾的回答。例如，前沿问答模型Macaw对'喜鹊是鸟吗？'和'鸟有翅膀吗？'均回答'是'，却对'喜鹊有翅膀吗？'回答'否'。为推进该领域研究，我们系统考察了最前沿方法并提出详细分类框架。具体而言，针对复杂逻辑问题的准确解答，现有方法可根据对外部求解器、提示词和微调的依赖程度进行分类；为避免逻辑矛盾，我们探讨了蕴含、否定、传递性、事实一致性及其复合形式等各类逻辑一致性的概念与解决方案。此外，我们综述了常用基准数据集和评估指标，并讨论了潜在研究方向，如扩展至模态逻辑以处理不确定性，以及开发能同时满足多重逻辑一致性的高效算法。

---

## [Multi-granularity Knowledge Transfer for Continual Reinforcement Learning](https://arxiv.org/abs/2401.15098)

### Abstract
arXiv:2401.15098v3 Announce Type: replace-cross 
Abstract: Continual reinforcement learning (CRL) empowers RL agents with the ability to learn a sequence of tasks, accumulating knowledge learned in the past and using the knowledge for problemsolving or future task learning. However, existing methods often focus on transferring fine-grained knowledge across similar tasks, which neglects the multi-granularity structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance coarse-grained knowledge transfer, we propose a novel framework called MT-Core (as shorthand for Multi-granularity knowledge Transfer for Continual reinforcement learning). MT-Core has a key characteristic of multi-granularity policy learning: 1) a coarsegrained policy formulation for utilizing the powerful reasoning ability of the large language model (LLM) to set goals, and 2) a fine-grained policy learning through RL which is oriented by the goals. We also construct a new policy library (knowledge base) to store policies that can be retrieved for multi-granularity knowledge transfer. Experimental results demonstrate the superiority of the proposed MT-Core in handling diverse CRL tasks versus popular baselines.

### 摘要
持续强化学习（CRL）使强化学习智能体能够学习一系列任务，积累过去所学的知识，并将这些知识用于问题解决或未来任务学习。然而，现有方法通常侧重于在相似任务间迁移细粒度知识，忽视了人类认知控制的多粒度结构，导致跨多样化任务的知识迁移不足。为增强粗粒度知识迁移，我们提出名为MT-Core（多粒度持续强化学习知识迁移框架）的新方法。MT-Core具有多粒度策略学习的关键特性：1）通过大型语言模型（LLM）的强大推理能力设定目标的粗粒度策略生成；2）以目标为导向、通过强化学习实现的细粒度策略学习。我们还构建了新的策略库（知识库）用于存储可检索的策略，以实现多粒度知识迁移。实验结果表明，相较于主流基线方法，所提出的MT-Core在处理多样化CRL任务方面具有显著优势。

---

## [Leveraging LLMs for Bangla Grammar Error Correction:Error Categorization, Synthetic Data, and Model Evaluation](https://arxiv.org/abs/2406.14284)

### Abstract
arXiv:2406.14284v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) perform exceedingly well in Natural Language Understanding (NLU) tasks for many languages including English. However, despite being the fifth most-spoken language globally, Grammatical Error Correction (GEC) in Bangla remains underdeveloped. In this work, we investigate how LLMs can be leveraged for improving Bangla GEC. For that, we first do an extensive categorization of 12 error classes in Bangla, and take a survey of native Bangla speakers to collect real-world errors. We next devise a rule-based noise injection method to create grammatically incorrect sentences corresponding to correct ones. The Vaiyakarana dataset, thus created, consists of 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then used to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show that instruction-tuning with \name improves GEC performance of LLMs by 3-7 percentage points as compared to the zero-shot setting, and makes them achieve human-like performance in grammatical error identification. Humans, though, remain superior in error correction.

### 摘要
大型语言模型（LLMs）在包括英语在内的多种自然语言理解（NLU）任务中表现极为出色。然而，尽管孟加拉语是全球使用人数第五的语言，其语法错误纠正（GEC）领域的发展仍显不足。本研究探讨了如何利用LLMs提升孟加拉语GEC性能。我们首先对孟加拉语的12类错误进行了系统分类，并通过母语者调查收集真实错误样本。随后设计了一种基于规则的噪声注入方法，用于生成与正确句子对应的语法错误句子。由此构建的Vaiyakarana数据集包含567,422个句子，其中227,119句为错误样本。该数据集被用于对LLMs进行孟加拉语GEC任务的指令微调。评估表明，相较于零样本设置，采用\name进行指令微调可使LLMs的GEC性能提升3-7个百分点，并在语法错误识别方面达到类人水平。但在错误纠正环节，人类仍保持优势。

---

## [Biased AI can Influence Political Decision-Making](https://arxiv.org/abs/2410.06415)

### Abstract
arXiv:2410.06415v3 Announce Type: replace-cross 
Abstract: As modern large language models (LLMs) become integral to everyday tasks, concerns about their inherent biases and their potential impact on human decision-making have emerged. While bias in models are well-documented, less is known about how these biases influence human decisions. This paper presents two interactive experiments investigating the effects of partisan bias in LLMs on political opinions and decision-making. Participants interacted freely with either a biased liberal, biased conservative, or unbiased control model while completing these tasks. We found that participants exposed to partisan biased models were significantly more likely to adopt opinions and make decisions which matched the LLM's bias. Even more surprising, this influence was seen when the model bias and personal political partisanship of the participant were opposite. However, we also discovered that prior knowledge of AI was weakly correlated with a reduction of the impact of the bias, highlighting the possible importance of AI education for robust mitigation of bias effects. Our findings not only highlight the critical effects of interacting with biased LLMs and its ability to impact public discourse and political conduct, but also highlights potential techniques for mitigating these risks in the future.

### 摘要
随着现代大型语言模型（LLMs）日益融入日常任务，其固有偏见及对人类决策的潜在影响引发关注。尽管模型偏见已有充分记录，但这些偏见如何影响人类决策尚缺乏深入研究。本文通过两项交互实验探究了LLMs中党派偏见对政治观点与决策的影响。参与者在完成任务时可自由接触具有自由派偏见、保守派偏见或无偏见的对照模型。研究发现，接触党派偏见模型的参与者显著倾向于采纳与模型偏见一致的观点和决策。更令人惊讶的是，即使模型偏见与参与者个人政治立场相反，这种影响依然存在。然而我们也发现，预先具备的AI知识与偏见影响减弱呈弱相关性，这凸显了AI教育对有效缓解偏见效应的潜在重要性。本研究不仅揭示了与偏见LLMs互动的关键影响及其改变公共话语和政治行为的可能性，也为未来降低此类风险提供了潜在技术路径。

---

## [A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks](https://arxiv.org/abs/2410.22391)

### Abstract
arXiv:2410.22391v3 Announce Type: replace-cross 
Abstract: In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.

### 摘要
近年来，强化学习领域出现了一种通过序列建模在大型数据集上离线训练大规模动作模型的趋势。现有模型主要基于Transformer架构，这类模型能产生强大的智能体。然而由于推理速度较慢，基于Transformer的方法难以应用于机器人等实时场景。近期提出的现代循环架构（如xLSTM和Mamba）在保持与Transformer类似训练并行化优势的同时，具有快速推理特性。本研究探讨了这些现代循环架构在大规模动作模型中的适用性，进而提出以xLSTM为核心的大型循环动作模型（LRAM），该模型具备线性时间推理复杂度与天然的序列长度外推能力。在6个领域432项任务上的实验表明，LRAM在性能与速度方面均优于Transformer模型。

---

## [Evaluating Morphological Compositional Generalization in Large Language Models](https://arxiv.org/abs/2410.12656)

### Abstract
arXiv:2410.12656v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated significant progress in various natural language generation and understanding tasks. However, their linguistic generalization capabilities remain questionable, raising doubts about whether these models learn language similarly to humans. While humans exhibit compositional generalization and linguistic creativity in language use, the extent to which LLMs replicate these abilities, particularly in morphology, is under-explored. In this work, we systematically investigate the morphological generalization abilities of LLMs through the lens of compositionality. We define morphemes as compositional primitives and design a novel suite of generative and discriminative tasks to assess morphological productivity and systematicity. Focusing on agglutinative languages such as Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs struggle with morphological compositional generalization particularly when applied to novel word roots, with performance declining sharply as morphological complexity increases. While models can identify individual morphological combinations better than chance, their performance lacks systematicity, leading to significant accuracy gaps compared to humans.

### 摘要
大型语言模型（LLMs）在各种自然语言生成与理解任务中展现出显著进展，但其语言泛化能力仍存疑，这引发了关于这些模型是否以类人方式学习语言的质疑。尽管人类在语言使用中表现出组合泛化能力和语言创造性，LLMs（尤其在形态学层面）复制这些能力的程度尚未得到充分探索。本研究通过组合性视角系统考察LLMs的形态学泛化能力：我们将语素定义为组合基元，并设计了一套新颖的生成性与判别性任务来评估形态能产性和系统性。聚焦土耳其语、芬兰语等黏着语，我们评估了包括GPT-4和Gemini在内的多个指令微调多语言模型。分析表明，LLMs在形态组合泛化方面存在困难——当应用于新词根时性能急剧下降，且形态复杂度增加会导致表现显著恶化。虽然模型识别个体形态组合的能力优于随机猜测，但其表现缺乏系统性，与人类相比存在显著准确率差距。

---

## [Scaling Trends in Language Model Robustness](https://arxiv.org/abs/2407.18213)

### Abstract
arXiv:2407.18213v5 Announce Type: replace-cross 
Abstract: Increasing model size has unlocked a dazzling array of capabilities in modern language models. At the same time, even frontier models remain vulnerable to jailbreaks and prompt injections, despite concerted efforts to make them robust. As both attack and defense gain access to more compute, and as models become larger, what happens to robustness? We argue that to answer this question requires a \emph&#123;scaling&#125; approach, which we employ in an extensive study of language model robustness across several classification tasks, model families, and adversarial attacks. We find that in the absence of explicit safety training, larger models are not consistently more robust; however, scale improves sample efficiency in adversarial training, though it worsens compute efficiency. Further, we find that increasing attack compute smoothly improves attack success rate against both undefended and adversarially trained models. Finally, after exploring robustness transfer across attacks and threat models, we combine attack and defense scaling rates to study the offense-defense balance. We find that while attack scaling outpaces adversarial training across all models studied, larger adversarially trained models might give defense the advantage in the long run. These results underscore the utility of the scaling lens, and provide a paradigm for evaluating future attacks and defenses on frontier models.

### 摘要
模型规模的扩大为现代语言模型解锁了令人眼花缭乱的能力。然而，即便经过持续努力提升鲁棒性，前沿模型仍易受越狱攻击和提示注入的影响。随着攻击方和防御方都获得更多计算资源，且模型规模持续增大，鲁棒性将如何变化？我们认为回答这一问题需要采用**规模化**研究视角，并基于此对多个分类任务、模型家族和对抗攻击下的语言模型鲁棒性展开广泛研究。研究发现：在缺乏显式安全训练的情况下，更大规模的模型并不具备持续更强的鲁棒性；然而，规模化提升了对抗训练中的样本效率，却降低了计算效率。此外，增加攻击计算量能持续提高对未防御模型和对抗训练模型的攻击成功率。最后，通过探索不同攻击方式和威胁模型间的鲁棒性迁移规律，我们结合攻防缩放率研究了攻防平衡。结果表明：虽然当前所有模型中攻击缩放率都超过对抗训练，但从长期来看，更大规模的对抗训练模型可能使防御方获得优势。这些发现凸显了规模化研究视角的价值，为评估未来前沿模型的攻防能力提供了范式。

---

## [LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog Circuits](https://arxiv.org/abs/2411.12930)

### Abstract
arXiv:2411.12930v2 Announce Type: replace-cross 
Abstract: Traditional approaches for designing analog circuits are time-consuming and require significant human expertise. Existing automation efforts using methods like Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal and costly to generalize across different topologies and technology nodes. In our work, we introduce a novel approach, LEDRO, utilizing Large Language Models (LLMs) in conjunction with optimization techniques to iteratively refine the design space for analog circuit sizing. LEDRO is highly generalizable compared to other RL and BO baselines, eliminating the need for design annotation or model training for different topologies or technology nodes. We conduct a comprehensive evaluation of our proposed framework and baseline on 22 different Op-Amp topologies across four FinFET technology nodes. Results demonstrate the superior performance of LEDRO as it outperforms our best baseline by an average of 13% FoM improvement with 2.15x speed-up on low complexity Op-Amps and 48% FoM improvement with 1.7x speed-up on high complexity Op-Amps. This highlights LEDRO's effective performance, efficiency, and generalizability.

### 摘要
传统模拟电路设计方法耗时且高度依赖人工经验。现有基于贝叶斯优化（BO）和强化学习（RL）的自动化方案存在次优问题，且难以跨拓扑结构和工艺节点泛化。本研究提出创新方法LEDRO，通过结合大语言模型（LLM）与优化技术实现模拟电路尺寸设计的迭代优化空间精炼。相较于RL和BO基线方法，LEDRO具有显著泛化能力，无需针对不同拓扑或工艺节点进行设计标注或模型训练。我们在4种FinFET工艺节点下的22种运放拓扑结构上对所提框架及基线方法进行了全面评估。结果表明LEDRO性能优越：在低复杂度运放中平均实现13%的优值（FoM）提升与2.15倍加速，在高复杂度运放中实现48%优值提升与1.7倍加速，充分体现了该方法的有效性能、高效性和泛化能力。

---

## [Focus On This, Not That! Steering LLMs with Adaptive Feature Specification](https://arxiv.org/abs/2410.22944)

### Abstract
arXiv:2410.22944v4 Announce Type: replace-cross 
Abstract: Despite the success of Instruction Tuning (IT) in training large language models (LLMs), such models often leverage spurious or biased features learnt from their training data and can become misaligned, leading to undesired behaviours. While existing techniques can steer model behaviour at inference-time, they are often post-hoc and do not embed steering as an intrinsic model feature. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across diverse benchmarks, we demonstrate that FIT: (i) successfully steers behaviour at inference time; (ii) increases robustness by amplifying core task signals and down-weighting spurious cues; (iii) mitigates social bias by suppressing demographic attributes; and (iv) generalises under distribution shifts and to previously unseen focus features. FIT therefore offers a lightweight, intrinsic mechanism for building more robust, fair, and easily controllable LLMs.

### 摘要
尽管指令微调（IT）在训练大语言模型（LLMs）方面取得了成功，但此类模型往往会利用从训练数据中学到的虚假或有偏特征，导致模型失准并产生不良行为。现有技术虽能在推理阶段引导模型行为，但多为事后处理，未能将引导机制内化为模型的固有特性。本研究提出聚焦指令微调（FIT），通过训练LLMs使其响应能够聚焦于特定特征而忽略其他特征，从而根据指定特征产生不同行为。在多样化基准测试中，我们证明FIT具有以下优势：（i）在推理阶段有效引导行为；（ii）通过增强核心任务信号和弱化虚假线索提升模型鲁棒性；（iii）通过抑制人口统计属性减轻社会偏见；（iv）在分布偏移和未见聚焦特征条件下保持泛化能力。因此，FIT为构建更具鲁棒性、公平性且易于控制的大语言模型提供了一种轻量级的内禀机制。

---

## [Context is Key: A Benchmark for Forecasting with Essential Textual Information](https://arxiv.org/abs/2410.18959)

### Abstract
arXiv:2410.18959v4 Announce Type: replace-cross 
Abstract: Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/.

### 摘要
预测是跨众多领域决策中的关键任务。虽然历史数值数据提供了基础，但无法传递完整的情境以实现可靠且准确的预测。人类预测者常依赖背景知识和约束条件等附加信息，这些信息可通过自然语言高效传递。然而，尽管基于大语言模型（LLM）的预测方法近期取得进展，其有效整合文本信息的能力仍存疑。为此，我们提出"情境即关键"（CiK）基准测试，该时序预测基准将数值数据与多种精心设计的文本情境配对，要求模型融合两种模态；关键在于，CiK中每个任务都必须理解文本情境才能成功解决。我们评估了统计模型、时序基础模型和基于LLM的预测方法等多种方案，并提出一种简单而有效的LLM提示方法，其在基准测试中优于所有其他测试方法。实验结果表明情境信息整合的重要性，展示了基于LLM的预测模型的惊人性能，同时也揭示了其若干关键缺陷。该基准旨在通过推动开发兼具准确性和技术普适性的模型，促进多模态预测研究的发展。基准可视化界面详见：https://servicenow.github.io/context-is-key-forecasting/v0/。

---

## [Not All Options Are Created Equal: Textual Option Weighting for Token-Efficient LLM-Based Knowledge Tracing](https://arxiv.org/abs/2410.12872)

### Abstract
arXiv:2410.12872v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently emerged as promising tools for knowledge tracing (KT) due to their strong reasoning and generalization abilities. While recent LLM-based KT methods have proposed new prompt formats, they struggle to represent the full interaction histories of example learners within a single prompt during in-context learning (ICL), resulting in limited scalability and high computational cost under token constraints. In this work, we present \textit&#123;LLM-based Option-weighted Knowledge Tracing (LOKT)&#125;, a simple yet effective framework that encodes the interaction histories of example learners in context as \textit&#123;textual categorical option weights (TCOW)&#125;. TCOW are semantic labels (e.g., ``inadequate'') assigned to the options selected by learners when answering questions, enhancing the interpretability of LLMs. Experiments on multiple-choice datasets show that LOKT outperforms existing non-LLM and LLM-based KT models in both cold-start and warm-start settings. Moreover, LOKT enables scalable and cost-efficient inference, achieving strong performance even under strict token constraints. Our code is available at \href&#123;https://anonymous.4open.science/r/LOKT_model-3233&#125;&#123;https://anonymous.4open.science/r/LOKT\_model-3233&#125;.

### 摘要
大型语言模型（LLMs）凭借其强大的推理和泛化能力，近期成为知识追踪（KT）领域的有力工具。尽管现有的基于LLM的KT方法提出了新的提示模板，但在上下文学习（ICL）过程中，这些方法难以在单一提示内完整表征示例学习者的交互历史，导致在标记限制下可扩展性不足且计算成本高昂。本研究提出基于LLM的选项加权知识追踪（LOKT）框架，该框架通过文本分类选项权重（TCOW）将上下文中的学习者交互历史编码为语义标签（如"不足"），这些标签对应学习者答题时选择的选项，从而增强LLM的可解释性。在多选题数据集上的实验表明，LOKT在冷启动和热启动场景下均优于现有非LLM及基于LLM的KT模型。此外，LOKT实现了高效可扩展的推理，即使在严格标记限制下仍能保持优异性能。代码发布于https://anonymous.4open.science/r/LOKT_model-3233。

---

## [What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743)

### Abstract
arXiv:2410.23743v2 Announce Type: replace-cross 
Abstract: What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs) through the lens of the gradient. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. Our code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient.

### 摘要
大语言模型（LLM）的后训练中哪些因素起关键作用？我们通过梯度视角研究了不同层在大型语言模型中的训练模式。鉴于当前基于推理路径（如思维链CoT）和过程奖励训练LLM的流行趋势，我们特别关注快思考与慢思考如何影响分层梯度。研究发现，与慢思考（详细CoT）相比，无CoT的快思考会导致更大的梯度及更显著的层间梯度差异，这表明后者能带来更稳定的学习效果。此外，我们探究了在使用快慢思考路径训练不同LLM时，梯度模式能否反映回答的正确性。结果显示，慢思考的梯度能有效区分正确与无关的推理路径。作为对照，我们在非推理类知识学习任务上进行类似梯度分析，发现单纯增加回答长度并不会引发类似慢思考的行为特征。本研究深化了对LLM训练机制的基础理解，为其效率与稳定性提供了新见解，为构建可泛化的系统2智能体奠定了基础。代码、数据及梯度统计详见：https://github.com/MingLiiii/Layer_Gradient。

---

## [HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing](https://arxiv.org/abs/2412.16187)

### Abstract
arXiv:2412.16187v3 Announce Type: replace-cross 
Abstract: Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce HashEvict, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, HashEvict makes these decisions pre-attention, thereby reducing computational costs. Additionally, HashEvict is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that HashEvict can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.

### 摘要
基于Transformer架构的大语言模型（LLM）通过存储历史标记的键值（KV）嵌入，利用键值缓存显著加速推理过程。然而该缓存会消耗大量GPU显存。本研究提出HashEvict算法，采用局部敏感哈希（LSH）技术实现KV缓存压缩。该算法通过计算当前查询标记与缓存标记键的二值化高斯投影汉明距离（投影长度远小于嵌入维度），快速定位缓存中与当前查询标记余弦相异的标记。我们在GPU显存中维护轻量级二进制结构以支持上述计算。与现有需要计算注意力权重以确定标记保留的压缩策略不同，HashEvict在注意力计算前完成决策，从而降低计算开销。此外，HashEvict具有动态特性——在每个解码步骤中，当前标记的键值将替换预期产生最低注意力得分的标记嵌入。实验表明，HashEvict能在保持推理、多选题、长上下文检索和摘要任务高性能的同时，实现30%-70%的KV缓存压缩率。

---

## [OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning](https://arxiv.org/abs/2501.00321)

### Abstract
arXiv:2501.00321v2 Announce Type: replace-cross 
Abstract: Scoring the Optical Character Recognition (OCR) capabilities of Large Multimodal Models (LMMs) has witnessed growing interest. Existing benchmarks have highlighted the impressive performance of LMMs in text recognition; however, their abilities in certain challenging tasks, such as text localization, handwritten content extraction, and logical reasoning, remain underexplored. To bridge this gap, we introduce OCRBench v2, a large-scale bilingual text-centric benchmark with currently the most comprehensive set of tasks (4x more tasks than the previous multi-scene benchmark OCRBench), the widest coverage of scenarios (31 diverse scenarios), and thorough evaluation metrics, with 10,000 human-verified question-answering pairs and a high proportion of difficult samples. Moreover, we construct a private test set with 1,500 manually annotated images. The consistent evaluation trends observed across both public and private test sets validate the OCRBench v2's reliability. After carefully benchmarking state-of-the-art LMMs, we find that most LMMs score below 50 (100 in total) and suffer from five-type limitations, including less frequently encountered text recognition, fine-grained perception, layout perception, complex element parsing, and logical reasoning. The project website is at: https://99franklin.github.io/ocrbench_v2/

### 摘要
评估大型多模态模型（LMMs）的光学字符识别（OCR）能力正受到日益广泛的关注。现有基准测试已证明LMMs在文本识别方面的卓越表现，但其在文本定位、手写内容提取和逻辑推理等挑战性任务中的能力仍待深入探索。为填补这一空白，我们推出OCRBench v2——一个目前任务最全面（较先前多场景基准OCRBench任务量增加4倍）、场景覆盖最广（涵盖31种多样化场景）且具备完善评估指标的大规模双语文本中心化基准，包含10,000个人工验证的问答对及高难度样本占比。此外，我们构建了含1,500张人工标注图像的私有测试集。公开与私有测试集一致的评估趋势验证了OCRBench v2的可靠性。通过对前沿LMMs的细致评测，我们发现多数模型总分低于50（满分100），并存在五类局限性：低频文本识别、细粒度感知、布局感知、复杂元素解析及逻辑推理。项目网站详见：https://99franklin.github.io/ocrbench_v2/

---

## [Explainability in Practice: A Survey of Explainable NLP Across Various Domains](https://arxiv.org/abs/2502.00837)

### Abstract
arXiv:2502.00837v2 Announce Type: replace-cross 
Abstract: Natural Language Processing (NLP) has become a cornerstone in many critical sectors, including healthcare, finance, and customer relationship management. This is especially true with the development and use of advanced models such as GPT-based architectures and BERT, which are widely used in decision-making processes. However, the black-box nature of these advanced NLP models has created an urgent need for transparency and explainability. This review explores explainable NLP (XNLP) with a focus on its practical deployment and real-world applications, examining its implementation and the challenges faced in domain-specific contexts. The paper underscores the importance of explainability in NLP and provides a comprehensive perspective on how XNLP can be designed to meet the unique demands of various sectors, from healthcare's need for clear insights to finance's emphasis on fraud detection and risk assessment. Additionally, this review aims to bridge the knowledge gap in XNLP literature by offering a domain-specific exploration and discussing underrepresented areas such as real-world applicability, metric evaluation, and the role of human interaction in model assessment. The paper concludes by suggesting future research directions that could enhance the understanding and broader application of XNLP.

### 摘要
自然语言处理（NLP）已成为医疗保健、金融和客户关系管理等关键领域的核心技术，尤其是在GPT架构和BERT等先进模型广泛应用于决策流程的背景下。然而，这些先进NLP模型的黑箱特性使得透明性与可解释性需求变得尤为迫切。本文综述了可解释性自然语言处理（XNLP），重点探讨其实际部署与现实应用，分析其在特定领域中的实施策略与面临挑战。研究强调了NLP可解释性的重要性，并从医疗领域对清晰洞察的需求到金融领域对欺诈检测与风险评估的侧重，全面阐述了如何设计XNLP以满足不同行业的独特需求。此外，本文通过领域特异性探索，讨论了现实应用性、指标评估以及人类交互在模型评估中的作用等研究不足的议题，旨在弥合XNLP文献中的知识缺口。最后，论文提出了未来研究方向，以促进对XNLP的深入理解与更广泛应用。

---

## [The Lessons of Developing Process Reward Models in Mathematical Reasoning](https://arxiv.org/abs/2501.07301)

### Abstract
arXiv:2501.07301v2 Announce Type: replace-cross 
Abstract: Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models.

### 摘要
过程奖励模型（PRMs）作为大语言模型（LLMs）数学推理中过程监督的一种有前景的方法，旨在识别和缓解推理过程中的中间错误。然而，有效PRMs的开发面临重大挑战，特别是在数据标注和评估方法方面。本文通过大量实验证明，与基于LLM-as-a-judge和人工标注方法相比，PRMs常用的基于蒙特卡洛（MC）估计的数据合成通常表现较差且泛化能力不足。MC估计依赖完成模型来评估当前步骤的正确性，导致步骤验证不准确。此外，我们发现传统PRMs的Best-of-N（BoN）评估策略存在潜在偏差：（1）不可靠的策略模型生成的响应虽然答案正确但过程存在缺陷，导致BoN评估标准与PRMs过程验证目标不一致；（2）PRMs对此类响应的容忍导致BoN评分虚高；（3）现有PRMs在最终答案步骤上存在大量最低分数集中现象，揭示了BoN优化PRMs从过程评估向结果评估的转变。为解决这些挑战，我们开发了一种共识过滤机制，有效整合MC估计与LLM-as-a-judge，并提出了结合响应级和步骤级指标的更全面评估框架。基于这些机制，我们在BoN评估和逐步错误识别任务中显著提升了模型性能和数据效率。最后，我们发布了一种新的最先进PRM，其性能优于现有开源方案，并为未来构建过程监督模型的研究提供了实用指南。

---

## [Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions](https://arxiv.org/abs/2502.00620)

### Abstract
arXiv:2502.00620v3 Announce Type: replace-cross 
Abstract: Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one, serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs.

### 摘要
弱到强泛化（W2SG）作为一种重要类比，用于理解未来人类如何引导超人类智能。该机制中弱模型监督强模型，已有实证研究表明强模型可以超越其弱监督者。尽管近期研究提供了该现象的理论解释，但对于驱动W2SG的强弱模型间交互作用仍缺乏清晰认知。我们通过理论视角研究W2SG，发现其可通过强弱模型内部表征主成分导出的核函数来刻画。这些核函数可定义出一个空间，该空间在高层级上捕获了弱模型无法学习但强模型可学习的内容。标签在该空间上的投影量化了强模型因弱监督而未能达到的潜在性能上限。该特征化方法还揭示了强模型如何修正弱监督中的特定错误（与过拟合无关）。我们的理论具有重要实践意义，提出的基于表征的度量指标可在无需标签的情况下预测W2SG性能趋势，这在Transformer分子预测实验和涉及52个大型语言模型的5项NLP任务中得到了验证。

---

## [Multi-Head RAG: Solving Multi-Aspect Problems with LLMs](https://arxiv.org/abs/2406.05085)

### Abstract
arXiv:2406.05085v3 Announce Type: replace-cross 
Abstract: Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses. Existing RAG solutions do not focus on queries that may require fetching multiple documents with substantially different contents. Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer's multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents. The driving observation is that different attention heads learn to capture different data aspects. Harnessing the corresponding activations results in embeddings that represent various facets of data items and queries, improving the retrieval accuracy for complex queries. We provide an evaluation methodology and metrics, multi-aspect datasets, and real-world use cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages over 18 RAG baselines, empirical improvements of up to 20% in retrieval success ratios, and benefits for downstream LLM generation. MRAG can be seamlessly integrated with existing RAG frameworks and benchmarks.

### 摘要
检索增强生成（RAG）通过将文档检索纳入大型语言模型（LLM）上下文来提升模型能力，从而提供更准确相关的响应。现有RAG方案未重点关注需要获取内容差异显著的多文档的查询场景。此类查询频繁出现却极具挑战性，因为这些文档的嵌入向量可能在嵌入空间中相距甚远，导致难以全部检索。本文提出多头RAG（MRAG）方案，采用Transformer多头注意力层（而非解码器层）的激活值作为检索键来获取多维度文档，以简单而有效的思路解决这一难题。核心发现在于：不同注意力头可学习捕捉数据的多维特征。利用相应激活值生成的嵌入向量能表征数据项与查询的各个维度，从而提升复杂查询的检索准确率。我们建立了评估方法与指标体系，构建了多维度数据集，并通过实际用例验证MRAG的有效性。实验表明MRAG在18个基线方案中具有设计优势，检索成功率最高提升20%，并能改善下游LLM生成效果。该方案可无缝集成至现有RAG框架与基准测试体系。

---

## [Can Large Language Models Understand Intermediate Representations in Compilers?](https://arxiv.org/abs/2502.06854)

### Abstract
arXiv:2502.06854v2 Announce Type: replace-cross 
Abstract: Intermediate Representations (IRs) play a critical role in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. In this paper, we present an explorative empirical study evaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3, DeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically, we assess model performance across four core tasks: control flow graph reconstruction, decompilation, code summarization, and execution reasoning. While LLMs exhibit competence in parsing IR syntax and identifying high-level structures, they consistently struggle with instruction-level reasoning, especially in control flow reasoning, loop handling, and dynamic execution. Common failure modes include misinterpreting branching instructions, omitting critical operations, and relying on heuristic reasoning rather than precise instruction-level logic. Our findings highlight the need for IR-specific enhancements in LLM design. We recommend fine-tuning on structured IR datasets and integrating control-flow-sensitive architectures to improve model effectiveness. All experimental data and source code are publicly available at

### 摘要
中间表示（IR）在编译器设计和程序分析中起着关键作用，然而大型语言模型（LLMs）对其理解的研究仍显不足。本文通过探索性实证研究，评估了六种前沿LLMs（GPT-4、GPT-3、DeepSeek、Gemma 2、Llama 3和Code Llama）在理解IR方面的能力。具体而言，我们从四个核心任务评估模型表现：控制流图重建、反编译、代码摘要和执行推理。虽然LLMs在解析IR语法和识别高层结构方面表现出能力，但在指令级推理（尤其是控制流推理、循环处理和动态执行）上普遍存在困难。常见失败模式包括误译分支指令、遗漏关键操作以及依赖启发式推理而非精确的指令级逻辑。我们的研究结果凸显了LLM设计中针对IR进行增强的必要性，建议通过结构化IR数据集微调模型，并整合控制流敏感架构以提升模型效能。全部实验数据与源代码已公开于

---

## [GoRA: Gradient-driven Adaptive Low Rank Adaptation](https://arxiv.org/abs/2502.12171)

### Abstract
arXiv:2502.12171v2 Announce Type: replace-cross 
Abstract: Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning large language models (LLMs), with its effectiveness influenced by two key factors: rank selection and weight initialization. While numerous LoRA variants have been proposed to improve performance by addressing one of these aspects, they often compromise usability or computational efficiency. In this paper, we analyze and identify the core limitations of existing approaches and propose a novel framework -- GoRA (Gradient-driven Adaptive Low Rank Adaptation) -- that simultaneously adapts both the rank and initialization strategy within a unified framework. GoRA leverages gradient information during training to dynamically assign optimal ranks and initialize low-rank adapter weights in an adaptive manner. To our knowledge, GoRA is the first method that not only addresses the limitations of prior approaches -- which often focus on either rank selection or initialization in isolation -- but also unifies both aspects within a single framework, enabling more effective and efficient adaptation. Extensive experiments across various architectures and modalities show that GoRA consistently outperforms existing LoRA-based methods while preserving the efficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for mathematical reasoning, GoRA achieves a 5.13-point improvement over standard LoRA and even outperforms full fine-tuning by 2.05 points under high-rank settings.

### 摘要
低秩自适应（LoRA）是高效微调大语言模型（LLMs）的关键方法，其有效性受两个关键因素影响：秩的选择与权重初始化。尽管已有诸多LoRA变体通过改进其中某一方面来提升性能，但这些方法往往以牺牲可用性或计算效率为代价。本文通过分析指出现有方法的核心局限性，并提出新型框架——GoRA（梯度驱动的自适应低秩适配），该框架在统一架构中同时实现秩与初始化策略的动态调整。GoRA利用训练过程中的梯度信息，以自适应方式动态分配最优秩并初始化低秩适配器权重。据我们所知，GoRA是首个不仅解决了现有方法（通常孤立关注秩选择或初始化）的局限性，还将二者统一于单一框架的方法，从而实现更高效的自适应。跨多种架构与模态的大规模实验表明，GoRA在保持原始LoRA效率的同时，始终优于现有基于LoRA的方法。例如，在对Llama3.1-8B-Base进行数学推理微调时，GoRA较标准LoRA提升5.13个点，在高秩设置下甚至比全参数微调高出2.05个点。

---

## [From Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors](https://arxiv.org/abs/2503.00038)

### Abstract
arXiv:2503.00038v2 Announce Type: replace-cross 
Abstract: Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.

### 摘要
当前研究揭示了大型语言模型（LLMs）通过越狱攻击生成有害内容的风险。然而，这些研究忽略了一个事实：相较于诱导LLM将良性内容校准为有害形式，直接从零生成有害内容更为困难。在本研究中，我们提出了一种新型攻击框架，利用对抗性隐喻（AVATAR）诱导LLM校准恶意隐喻以实现越狱。具体而言，为响应有害查询，AVATAR自适应地识别一组逻辑相关但良性的隐喻作为初始种子。随后，在这些隐喻的驱动下，目标LLM被诱导对隐喻内容进行推理和校准，从而通过直接输出有害响应或校准隐喻内容与专业有害内容之间的残差实现越狱。实验结果表明，AVATAR能有效且可迁移地实现LLM越狱，并在多个先进LLM上达到最先进的攻击成功率。

---

## [Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models](https://arxiv.org/abs/2502.11028)

### Abstract
arXiv:2502.11028v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) show remarkable proficiency in natural language tasks, yet their frequent overconfidence-misalignment between predicted confidence and true correctness-poses significant risks in critical decision-making applications. We present a comprehensive analysis on calibration in LLMs across nine LLMs and three factual Question-Answering (QA) datasets, systematically comparing standard free-generation settings against structured distractor-augmented prompts. Our evaluation reveals that explicitly incorporating distractors can substantially mitigate miscalibration, achieving relative accuracy improvements up to 460% and ECE reductions up to 90%. Despite general trends, we uncover nuanced findings: large RLHF-tuned models display inherent calibration strengths but can paradoxically suffer increased miscalibration on easier queries, whereas smaller models benefit disproportionately from distractor prompts but remain significantly miscalibrated. Through detailed analyses across question types, we identify persistent calibration failures, particularly in person-based queries. We conclude with concrete recommendations-targeted fine-tuning, structured prompting, and strategic model choice-to ensure reliable, trustworthy LLM deployments.

### 摘要
大型语言模型（LLMs）在自然语言任务中展现出卓越能力，但其预测置信度与真实准确性之间的频繁错位——过度自信问题——在关键决策应用中构成重大风险。本研究针对九种LLMs和三个事实性问答（QA）数据集开展了校准性能的系统分析，对比了标准自由生成设置与结构化干扰项增强提示的效果。评估结果表明，显式引入干扰项可显著改善校准不良问题，实现高达460%的相对准确率提升及90%的预期校准误差（ECE）降低。尽管存在总体趋势，我们仍发现微妙差异：经过强化学习人类反馈（RLHF）调优的大模型虽具有固有校准优势，但在简单查询中反而可能出现校准恶化；而小模型虽从干扰项提示中获益更大，却仍存在显著校准偏差。通过针对不同问题类型的细粒度分析，我们发现了持续性校准失败现象，尤其在人物类查询中表现突出。最后提出三项具体建议——定向微调、结构化提示和策略性模型选择——以确保LLMs部署的可靠性与可信度。

---

## [Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable](https://arxiv.org/abs/2503.00555)

### Abstract
arXiv:2503.00555v2 Announce Type: replace-cross 
Abstract: Safety alignment is an important procedure before the official deployment of a Large Language Model (LLM). While safety alignment has been extensively studied for LLM, there is still a large research gap for Large Reasoning Models (LRMs) that equip with improved reasoning capability. We in this paper systematically examine a simplified pipeline for producing safety aligned LRMs. With our evaluation of various LRMs, we deliver two main findings: i) Safety alignment can be done upon the LRM to restore its safety capability. ii) Safety alignment leads to a degradation of the reasoning capability of LRMs. The two findings show that there exists a trade-off between reasoning and safety capability with the sequential LRM production pipeline. The discovered trade-off, which we name Safety Tax, should shed light on future endeavors of safety research on LRMs. As a by-product, we curate a dataset called DirectRefusal, which might serve as an alternative dataset for safety alignment. Our source code is available at https://github.com/git-disl/Safety-Tax.

### 摘要
安全对齐是大型语言模型（LLM）正式部署前的重要流程。尽管针对LLM的安全对齐已得到广泛研究，但具备更强推理能力的大型推理模型（LRM）仍存在较大研究空白。本文系统研究了一种简化版的安全对齐LRM生产流程。通过对多种LRM的评估，我们得出两个主要发现：1）可在LRM基础上实施安全对齐以恢复其安全能力；2）安全对齐会导致LRM推理能力下降。这两个发现表明，采用顺序式LRM生产流程时，推理能力与安全能力之间存在权衡关系。我们将此权衡命名为"安全税"，该发现可为未来LRM安全研究提供启示。作为副产品，我们构建了名为DirectRefusal的数据集，或可作为安全对齐的替代数据集。源代码已发布于https://github.com/git-disl/Safety-Tax。

---

## [NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark](https://arxiv.org/abs/2504.07749)

### Abstract
arXiv:2504.07749v2 Announce Type: replace-cross 
Abstract: This paper introduces NorEval, a new and comprehensive evaluation suite for large-scale standardized benchmarking of Norwegian generative language models (LMs). NorEval consists of 24 high-quality human-created datasets -- of which five are created from scratch. In contrast to existing benchmarks for Norwegian, NorEval covers a broad spectrum of task categories targeting Norwegian language understanding and generation, establishes human baselines, and focuses on both of the official written standards of the Norwegian language: Bokm&#123;\aa&#125;l and Nynorsk. All our datasets and a collection of over 100 human-written prompts are integrated into LM Evaluation Harness, ensuring flexible and reproducible evaluation. We describe the NorEval design and present the results of benchmarking 19 open-source pre-trained and instruction-tuned LMs for Norwegian in various scenarios. Our benchmark, evaluation framework, and annotation materials are publicly available.

### 摘要
本文介绍NorEval——一个用于挪威语生成式语言模型大规模标准化评估的全新综合测试套件。该套件包含24个高质量人工构建的数据集（其中5个为全新创建），相较于现有挪威语基准测试具有以下特点：涵盖挪威语理解与生成的多类任务范畴，建立人类表现基线，同时关注挪威语两种官方书面标准语（博克马尔语和新挪威语）。所有数据集及100多个人工编写的提示词已集成至LM Evaluation Harness评估框架，确保灵活可复现的评估流程。我们详细阐述NorEval的设计方案，并呈现19个开源预训练及指令调优挪威语语言模型在不同场景下的基准测试结果。本测试套件、评估框架及标注材料均已公开。

---

## [ATLaS: Agent Tuning via Learning Critical Steps](https://arxiv.org/abs/2503.02197)

### Abstract
arXiv:2503.02197v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.

### 摘要
大型语言模型（LLM）智能体在多领域任务中展现出卓越的泛化能力。现有智能体调优方法通常对整个专家轨迹进行监督微调，但完整轨迹的行为克隆可能引入专家偏见，并削弱对专家数据未覆盖状态的泛化能力。此外，规划、中间子任务的复杂推理和战略决策等关键步骤对智能体任务的成功至关重要，学习这些步骤是提升LLM智能体的核心。为实现更高效能的智能体调优，我们提出ATLaS方法，通过识别专家轨迹中的关键步骤并仅对这些步骤进行低成本微调。通过将训练重点集中于少量关键步骤，我们的方法降低了完整轨迹过拟合风险，并增强了跨环境与跨任务的泛化能力。大量实验表明，经ATLaS筛选30%关键步骤微调的LLM，其表现优于全步骤微调模型及近期开源LLM智能体。ATLaS在作为通用智能体与多样化环境交互时，能够保持并提升基础LLM的核心能力。

---

## [LLM Social Simulations Are a Promising Research Method](https://arxiv.org/abs/2504.02234)

### Abstract
arXiv:2504.02234v2 Announce Type: replace-cross 
Abstract: Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted this method. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a review of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions, including context-rich prompting and fine-tuning with social science datasets. We believe that LLM social simulations can already be used for pilot and exploratory studies, and more widespread use may soon be possible with rapidly advancing LLM capabilities. Researchers should prioritize developing conceptual models and iterative evaluations to make the best use of new AI systems.

### 摘要
准确且可验证的大语言模型（LLM）对人类研究对象的模拟，有望为理解人类行为和训练新的人工智能系统提供便捷的数据来源。然而，迄今为止的研究成果有限，且少有社会科学家采用这种方法。在本立场文件中，我们认为通过解决五个可应对的挑战，可以实现LLM社会模拟的潜力。我们的论点基于对LLM与人类研究对象之间实证比较的综述、相关评论以及现有研究。我们指出了有前景的研究方向，包括情境丰富的提示方法和基于社会科学数据集的微调技术。我们认为LLM社会模拟目前已可用于预研和探索性研究，随着LLM能力的快速进步，更广泛的应用可能很快成为现实。研究人员应优先开发概念模型和迭代评估方法，以充分利用新型人工智能系统。

---

## [TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research](https://arxiv.org/abs/2503.12730)

### Abstract
arXiv:2503.12730v2 Announce Type: replace-cross 
Abstract: Mechanistic interpretability research faces a gap between analyzing simple circuits in toy tasks and discovering features in large models. To bridge this gap, we propose text-to-SQL generation as an ideal task to study, as it combines the formal structure of toy tasks with real-world complexity. We introduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL operations, and train models ranging from 33M to 1B parameters to establish a comprehensive testbed for interpretability. We apply multiple complementary interpretability techniques, including Edge Attribution Patching and Sparse Autoencoders, to identify minimal circuits and components supporting SQL generation. We compare circuits for different SQL subskills, evaluating their minimality, reliability, and identifiability. Finally, we conduct a layerwise logit lens analysis to reveal how models compose SQL queries across layers: from intent recognition to schema resolution to structured generation. Our work provides a robust framework for probing and comparing interpretability methods in a structured, progressively complex setting.

### 摘要
机制可解释性研究面临着一个鸿沟：既要分析玩具任务中的简单电路，又要在大型模型中发掘特征。为弥合这一差距，本文提出将文本到SQL生成作为理想研究对象，因其兼具玩具任务的规范结构与现实世界的复杂性。我们引入TinySQL合成数据集——该数据集涵盖从基础到高级的SQL操作，并通过训练参数量从3300万到10亿不等的模型，构建了完整的可解释性测试平台。我们运用边缘归因修补和稀疏自编码器等多种互补的可解释性技术，以识别支持SQL生成的最小电路与组件。通过对比不同SQL子技能的电路结构，我们评估了其最小性、可靠性和可识别性。最后，我们采用分层对数透镜分析揭示模型跨层组合SQL查询的机制：从意图识别到模式解析，再到结构化生成。本研究为在结构化渐进复杂场景下探索和比较可解释性方法提供了稳健框架。

---

## [Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment](https://arxiv.org/abs/2503.04647)

### Abstract
arXiv:2503.04647v2 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity. To address this, we propose a novel approach that $\textit&#123;captures&#125;$ learned preferences from well-aligned English models by implicit rewards and $\textit&#123;transfers&#125;$ them to other languages through iterative training. Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model. This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses. The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages. Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard. Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data. The code is available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding

### 摘要
直接偏好优化（DPO）已成为将大语言模型（LLM）与人类偏好对齐的重要方法。尽管DPO在英语LLM对齐方面取得了显著进展，但多语言偏好对齐仍受限于数据稀缺问题。为此，我们提出了一种创新方法：通过隐式奖励从已对齐的英语模型中捕捉学习到的偏好，并借助迭代训练将其迁移至其他语言。具体而言，我们从英语DPO对齐模型及其对应参考模型的逻辑值中推导出隐式奖励模型，利用该模型对跨语言指令跟随数据对进行偏好标注——以英语指令评估多语言响应。标注数据随后用于多语言DPO微调，实现英语到其他语言的偏好知识迁移。对Llama3进行两次迭代微调后，在X-AlpacaEval排行榜上所有训练语言的胜率平均提升12.72%，长度控制胜率提高5.97%。研究表明，利用现有英语对齐模型可实现高效的多语言偏好对齐，显著减少对大量多语言偏好数据的需求。代码已开源：https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding

---

## [TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice](https://arxiv.org/abs/2502.18504)

### Abstract
arXiv:2502.18504v2 Announce Type: replace-cross 
Abstract: Jailbreaking large-language models (LLMs) involves testing their robustness against adversarial prompts and evaluating their ability to withstand prompt attacks that could elicit unauthorized or malicious responses. In this paper, we present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently finding a collection of effective jailbreaking templates that, when combined with harmful questions, can lead a target LLM to produce harmful responses through black-box access via user prompts. We describe the limitations of directly applying existing template-based attacking techniques in practice, and present functional and efficiency-focused upgrades we added to mutation-based fuzzing to generate effective jailbreaking templates automatically. TurboFuzzLLM achieves $\geq$ 95\% attack success rates (ASR) on public datasets for leading LLMs (including GPT-4o \&amp; GPT-4 Turbo), shows impressive generalizability to unseen harmful questions, and helps in improving model defenses to prompt attacks. TurboFuzzLLM is available open source at https://github.com/amazon-science/TurboFuzzLLM.

### 摘要
大型语言模型（LLMs）越狱测试旨在通过对抗性提示评估其鲁棒性，检验模型抵抗可能引发未授权或恶意响应的提示攻击的能力。本文提出TurboFuzzLLM——一种基于变异的模糊测试技术，能高效发现一系列有效越狱模板。这些模板与恶意问题结合后，可通过用户提示的黑盒访问方式诱导目标LLM生成有害响应。我们阐述了现有基于模板的攻击技术在实际应用中的局限性，并介绍了针对功能性和效率优化的变异模糊测试改进方案，以实现自动生成高效越狱模板。TurboFuzzLLM在主流LLM（包括GPT-4o和GPT-4 Turbo）的公开数据集上实现≥95%的攻击成功率（ASR），对未见过的恶意问题展现出卓越的泛化能力，同时有助于提升模型防御提示攻击的能力。本技术已开源：https://github.com/amazon-science/TurboFuzzLLM。

---

## [When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits](https://arxiv.org/abs/2503.03417)

### Abstract
arXiv:2503.03417v3 Announce Type: replace-cross 
Abstract: Online misinformation remains a critical challenge, and fact-checkers increasingly rely on claim matching systems that use sentence embedding models to retrieve relevant fact-checks. However, as users interact with claims online, they often introduce edits, and it remains unclear whether current embedding models used in retrieval are robust to such edits. To investigate this, we introduce a perturbation framework that generates valid and natural claim variations, enabling us to assess the robustness of a wide-range of sentence embedding models in a multi-stage retrieval pipeline and evaluate the effectiveness of various mitigation approaches. Our evaluation reveals that standard embedding models exhibit notable performance drops on edited claims, while LLM-distilled embedding models offer improved robustness at a higher computational cost. Although a strong reranker helps to reduce the performance drop, it cannot fully compensate for first-stage retrieval gaps. To address these retrieval gaps, we evaluate train- and inference-time mitigation approaches, demonstrating that they can improve in-domain robustness by up to 17 percentage points and boost out-of-domain generalization by 10 percentage points. Overall, our findings provide practical improvements to claim-matching systems, enabling more reliable fact-checking of evolving misinformation. Code and data are available at https://github.com/JabezNzomo99/claim-matching-robustness.

### 摘要
网络错误信息仍是严峻挑战，事实核查机构日益依赖基于句子嵌入模型的声明匹配系统来检索相关核查结果。然而，当用户在线互动时经常会对声明进行编辑，而当前检索系统采用的嵌入模型是否对此类编辑具有鲁棒性尚不明确。为此，我们提出一个扰动框架以生成有效且自然的声明变体，从而在多阶段检索流程中评估多种句子嵌入模型的鲁棒性，并检验不同缓解策略的有效性。实验表明：标准嵌入模型在编辑后声明上表现显著下降，而基于大语言模型蒸馏的嵌入模型虽计算成本较高但鲁棒性更优；尽管强排序器能缓解性能下降，但仍无法完全弥补首阶段检索的差距。针对检索缺陷，我们评估了训练时与推理时的缓解方案，证明其可将领域内鲁棒性最高提升17个百分点，领域外泛化能力提高10个百分点。本研究为声明匹配系统提供了实用性改进方案，有助于对动态演变的错误信息进行更可靠的事实核查。代码与数据详见https://github.com/JabezNzomo99/claim-matching-robustness。

---

