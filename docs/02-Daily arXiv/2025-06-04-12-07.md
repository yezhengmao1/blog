# 2025-06-04-12-07

## [EcoLoRA: Communication-Efficient Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.02001)

### Abstract
arXiv:2506.02001v1 Announce Type: new 
Abstract: To address data locality and privacy restrictions, Federated Learning (FL) has recently been adopted to fine-tune large language models (LLMs), enabling improved performance on various downstream tasks without requiring aggregated data. However, the repeated exchange of model updates in FL can result in prohibitively high communication costs, hindering the distributed learning process. To address this challenge, we propose EcoLoRA, a novel communication-efficient federated fine-tuning framework for LLMs. Leveraging the modular structure, we propose a round-robin segment sharing scheme, where each client uploads only a complementary LoRA segment per round to reduce network bandwidth. It is further combined with adaptive sparsification methods tailored to LoRA's training dynamics and lossless encoding techniques. We conduct extensive evaluations on both question-answering and value-alignment tasks across multiple datasets and models. The results show that EcoLoRA significantly reduces communication overhead without compromising performance. For instance, it reduces communication time by up to 79% and total training time by up to 65%.

### 摘要
为解决数据本地化和隐私限制问题，联邦学习（FL）近期被应用于大语言模型（LLMs）的微调，无需聚合数据即可提升各类下游任务性能。然而，FL中模型更新的反复交换会导致极高的通信开销，阻碍分布式学习进程。针对这一挑战，我们提出EcoLoRA——一种新型通信高效的LLMs联邦微调框架。通过利用模块化结构，我们设计轮转式分段共享方案，每轮每个客户端仅上传互补的LoRA分段以降低网络带宽占用。该方案进一步结合了适配LoRA训练动态的自适应稀疏化方法及无损编码技术。我们在多数据集和多模型上对问答任务和价值对齐任务进行了全面评估。结果表明，EcoLoRA在保持性能的同时显著降低了通信开销，例如通信时间最高减少79%，总训练时间最高缩短65%。

---

## [STRATUS: A Multi-agent System for Autonomous Reliability Engineering of Modern Clouds](https://arxiv.org/abs/2506.02009)

### Abstract
arXiv:2506.02009v1 Announce Type: new 
Abstract: In cloud-scale systems, failures are the norm. A distributed computing cluster exhibits hundreds of machine failures and thousands of disk failures; software bugs and misconfigurations are reported to be more frequent. The demand for autonomous, AI-driven reliability engineering continues to grow, as existing humanin-the-loop practices can hardly keep up with the scale of modern clouds. This paper presents STRATUS, an LLM-based multi-agent system for realizing autonomous Site Reliability Engineering (SRE) of cloud services. STRATUS consists of multiple specialized agents (e.g., for failure detection, diagnosis, mitigation), organized in a state machine to assist system-level safety reasoning and enforcement. We formalize a key safety specification of agentic SRE systems like STRATUS, termed Transactional No-Regression (TNR), which enables safe exploration and iteration. We show that TNR can effectively improve autonomous failure mitigation. STRATUS significantly outperforms state-of-the-art SRE agents in terms of success rate of failure mitigation problems in AIOpsLab and ITBench (two SRE benchmark suites), by at least 1.5 times across various models. STRATUS shows a promising path toward practical deployment of agentic systems for cloud reliability.

### 摘要
在云规模系统中，故障是常态。一个分布式计算集群通常会出现数百次机器故障和数千次磁盘故障；据报告软件缺陷和配置错误的发生频率更高。随着现有人工参与式运维实践难以跟上现代云计算的规模，对自主化、AI驱动的可靠性工程需求持续增长。本文提出STRATUS——一个基于大语言模型的多智能体系统，用于实现云服务的自主站点可靠性工程（SRE）。STRATUS由多个专业智能体（如故障检测、诊断、缓解）组成，通过状态机架构协助系统级安全推理与执行。我们形式化定义了STRATUS这类智能体SRE系统的关键安全规范——事务无回归（TNR），该规范支持安全探索与迭代。研究表明TNR能有效提升自主故障缓解能力。在AIOpsLab和ITBench（两个SRE基准测试套件）中，STRATUS在故障缓解问题上的成功率显著优于最先进的SRE智能体，在不同模型上至少领先1.5倍。STRATUS为智能体系统在云可靠性领域的实际部署指明了一条可行路径。

---

## [Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing](https://arxiv.org/abs/2506.02006)

### Abstract
arXiv:2506.02006v1 Announce Type: new 
Abstract: Efficiently serving large language models (LLMs) under dynamic and bursty workloads remains a key challenge for real-world deployment. Existing serving frameworks and static model compression techniques fail to adapt to workload fluctuations, leading to either service-level objective (SLO) violations under full-precision serving or persistent accuracy degradation with static quantization. We present MorphServe, a dynamic, workload-aware LLM serving framework based on morphological adaptation. MorphServe introduces two asynchronous, token-level runtime mechanisms: quantized layer swapping, which selectively replaces less impactful layers with quantized alternatives during high-load periods, and pressure-aware KV cache resizing, which dynamically adjusts KV cache capacity in response to memory pressure. These mechanisms enable state-preserving transitions with minimum runtime overhead and are fully compatible with modern scheduling and attention techniques. Extensive experiments on Vicuna and Llama family models with real-world workloads demonstrate that MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality. These results establish MorphServe as a practical and elastic solution for LLM deployment in dynamic environments.

### 摘要
在动态突发工作负载下高效服务大型语言模型（LLMs）仍是实际部署中的关键挑战。现有服务框架与静态模型压缩技术无法适应工作负载波动，导致全精度服务时违反服务级别目标（SLO）或静态量化时持续精度下降。我们提出MorphServe——基于形态自适应的动态工作负载感知LLM服务框架，其引入两种异步的令牌级运行时机制：量化层交换（在高负载期间选择性替换影响较小层为量化版本）和压力感知KV缓存调整（根据内存压力动态调整KV缓存容量）。这些机制能以最小运行时开销实现状态保留转换，并完全兼容现代调度与注意力技术。基于Vicuna和Llama系列模型及真实工作负载的广泛实验表明，MorphServe较全精度服务平均减少92.45%的SLO违规，P95首令牌延迟提升2.2-3.9倍，且不损害生成质量。这些结果证明MorphServe是动态环境中LLM部署的实用弹性解决方案。

---

## [Multimodal Financial Foundation Models (MFFMs): Progress, Prospects, and Challenges](https://arxiv.org/abs/2506.01973)

### Abstract
arXiv:2506.01973v1 Announce Type: new 
Abstract: Financial Large Language Models (FinLLMs), such as open FinGPT and proprietary BloombergGPT, have demonstrated great potential in select areas of financial services. Beyond this earlier language-centric approach, Multimodal Financial Foundation Models (MFFMs) can digest interleaved multimodal financial data, including fundamental data, market data, data analytics, macroeconomic, and alternative data (e.g., natural language, audio, images, and video). In this position paper, presented at the MFFM Workshop joined with ACM International Conference on AI in Finance (ICAIF) 2024, we describe the progress, prospects, and challenges of MFFMs. This paper also highlights ongoing research on FinAgents in the \textbf&#123;SecureFinAI Lab&#125;\footnote&#123;\https://openfin.engineering.columbia.edu/&#125; at Columbia University. We believe that MFFMs will enable a deeper understanding of the underlying complexity associated with numerous financial tasks and data, streamlining the operation of financial services and investment processes. Github Repo https://github.com/Open-Finance-Lab/Awesome-MFFMs/.

### 摘要
金融大语言模型（FinLLMs），如开源的FinGPT和专有的BloombergGPT，已在金融服务特定领域展现出巨大潜力。超越早期以语言为核心的方法，多模态金融基础模型（MFFMs）能够处理交织的多模态金融数据，包括基本面数据、市场数据、数据分析、宏观经济以及另类数据（如自然语言、音频、图像和视频）。在本立场文件中（发布于与ACM国际人工智能金融会议（ICAIF）2024联合举办的MFFM研讨会），我们阐述了MFFMs的进展、前景与挑战。本文还重点介绍了哥伦比亚大学**SecureFinAI实验室**\footnote&#123;\https://openfin.engineering.columbia.edu/&#125;关于金融智能体（FinAgents）的持续研究。我们相信，MFFMs将有助于更深入地理解众多金融任务和数据背后的复杂性，从而简化金融服务和投资流程的运作。Github仓库 https://github.com/Open-Finance-Lab/Awesome-MFFMs/。

---

## [Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism](https://arxiv.org/abs/2506.01979)

### Abstract
arXiv:2506.01979v1 Announce Type: new 
Abstract: Recently, speculative decoding (SD) has emerged as a promising technique to accelerate LLM inference by employing a small draft model to propose draft tokens in advance, and validating them in parallel with the large target model. However, the existing SD methods still remain fundamentally constrained by their serialized execution, which causes the mutual waiting bubbles between the draft and target models. To address this challenge, we draw inspiration from branch prediction in modern processors and propose a novel framework \textbf&#123;SpecBranch&#125; to unlock branch parallelism in SD. Specifically, we first take an in-depth analysis of the potential of branch parallelism in SD, and recognize that the key challenge lies in the trade-offs between parallelization and token rollback. Based on the analysis, we strategically introduce parallel speculative branches to preemptively hedge against likely rejections. Meanwhile, to enhance parallelism, we jointly orchestrate adaptive draft lengths with a hybrid combination of the implicit draft model confidence and explicit reusing of target model features. Extensive experiments across various models and benchmarks show that SpecBranch achieves over \textbf&#123;1.8&#125;$\times \sim$ \textbf&#123;4.5&#125;$\times$ speedups against the auto-regressive decoding and reduces rollback tokens by $\textbf&#123;50&#125;$\% for poorly aligned models, realizing its applicability for real-world deployments.

### 摘要
近期，推测解码(SD)作为一种加速大语言模型推理的前沿技术崭露头角，其通过小型草稿模型预先生成候选标记，并与大型目标模型并行验证。然而现有SD方法仍受限于串行执行机制，导致草稿模型与目标模型间存在相互等待的间隙。为解决这一难题，我们受现代处理器分支预测启发，提出创新框架SpecBranch以实现SD中的分支并行化。具体而言，我们首先深入分析了SD中分支并行的潜力，发现核心挑战在于并行化与标记回滚间的权衡。基于此分析，我们策略性地引入并行推测分支以预先规避可能的拒绝情况。同时，为提升并行效率，我们通过隐式草稿模型置信度与显式目标模型特征复用的混合策略，协同优化自适应草稿长度。跨模型与基准测试的大规模实验表明，SpecBranch相比自回归解码实现1.8× ∼ 4.5×加速，并对低对齐模型减少50%的回滚标记，证实了其在实际部署中的适用性。

---

## [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/abs/2506.02097)

### Abstract
arXiv:2506.02097v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems and large language model (LLM)-powered chatbots have significantly advanced conversational AI by combining generative capabilities with external knowledge retrieval. Despite their success, enterprise-scale deployments face critical challenges, including diverse user queries, high latency, hallucinations, and difficulty integrating frequently updated domain-specific knowledge. This paper introduces a novel hybrid framework that integrates RAG with intent-based canned responses, leveraging predefined high-confidence responses for efficiency while dynamically routing complex or ambiguous queries to the RAG pipeline. Our framework employs a dialogue context manager to ensure coherence in multi-turn interactions and incorporates a feedback loop to refine intents, dynamically adjust confidence thresholds, and expand response coverage over time. Experimental results demonstrate that the proposed framework achieves a balance of high accuracy (95\%) and low latency (180ms), outperforming RAG and intent-based systems across diverse query types, positioning it as a scalable and adaptive solution for enterprise conversational AI applications.

### 摘要
检索增强生成（RAG）系统与基于大语言模型（LLM）的聊天机器人通过将生成能力与外部知识检索相结合，显著推动了对话式人工智能的发展。尽管取得了成功，企业级部署仍面临关键挑战，包括多样化的用户查询、高延迟、幻觉问题以及难以整合频繁更新的领域特定知识。本文提出了一种新型混合框架，将RAG与基于意图的预设响应相结合，利用预定义高置信度响应提升效率，同时将复杂或模糊查询动态路由至RAG流程。该框架采用对话上下文管理器确保多轮交互的连贯性，并引入反馈循环机制以优化意图分类、动态调整置信度阈值并随时间扩展响应覆盖范围。实验结果表明，所提框架在准确率（95%）与延迟（180毫秒）间实现了平衡，在多样化查询类型上均优于纯RAG和基于意图的系统，为企业级对话式AI应用提供了可扩展且自适应的解决方案。

---

## [Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling](https://arxiv.org/abs/2506.02025)

### Abstract
arXiv:2506.02025v1 Announce Type: new 
Abstract: High-Performance Computing (HPC) job scheduling involves balancing conflicting objectives such as minimizing makespan, reducing wait times, optimizing resource use, and ensuring fairness. Traditional methods, including heuristic-based (e.g., First-Come-First-Served) or intensive optimization techniques, often lack adaptability to dynamic workloads and heterogeneous HPC systems. To address this, we propose a novel Large Language Model (LLM)-based scheduler using a ReAct-style framework (Reason + Act), enabling iterative, interpretable decision-making. The system incorporates a scratchpad memory to track scheduling history and refine decisions via natural language feedback, while a constraint enforcement module ensures feasibility and safety. We evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across seven real-world HPC workload scenarios, including heterogeneous mixes, bursty patterns, and adversarial cases. Comparisons against FCFS, Shortest Job First, and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling effectively balances multiple objectives while offering transparent reasoning through natural language traces. The method excels in constraint satisfaction and adapts to diverse workloads without domain-specific training. However, a trade-off between reasoning quality and computational overhead challenges real-time deployment. This work presents the first comprehensive study of reasoning-capable LLMs for HPC scheduling, demonstrating their potential to handle multiobjective optimization while highlighting limitations in computational efficiency. The findings provide insights into leveraging advanced language models for complex scheduling problems in dynamic HPC environments.

### 摘要
高性能计算（HPC）作业调度需要平衡诸如最小化完成时间、减少等待时长、优化资源利用及确保公平性等相互冲突的目标。传统方法（包括基于启发式的策略如先到先服务）或密集型优化技术往往难以适应动态工作负载和异构HPC系统。为此，我们提出一种基于大型语言模型（LLM）的新型调度器，采用"推理+执行"（ReAct）框架实现可迭代、可解释的决策。该系统通过暂存器内存跟踪调度历史记录，并借助自然语言反馈优化决策，同时采用约束执行模块确保可行性与安全性。我们在七种真实HPC工作负载场景（包括异构混合、突发模式及对抗性案例）中，使用OpenAI的O4-Mini和Anthropic的Claude 3.7进行评估。与先到先服务、最短作业优先及Google OR-Tools（处理10至100个作业）的对比表明，基于LLM的调度能有效平衡多目标需求，并通过自然语言轨迹提供透明推理。该方法在约束满足方面表现优异，且无需领域特定训练即可适应多样化工作负载。然而，推理质量与计算开销之间的权衡对实时部署构成挑战。本研究首次对具备推理能力的LLM在HPC调度中的应用进行全面探讨，证明其在处理多目标优化方面的潜力，同时揭示了计算效率方面的局限。这些发现为在动态HPC环境中利用先进语言模型解决复杂调度问题提供了重要见解。

---

## [NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating Point Support for LLMs](https://arxiv.org/abs/2506.02024)

### Abstract
arXiv:2506.02024v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are playing a crucial role in latency-critical, high-throughput services like virtual assistants and code generation. While techniques such as continuous batching and paged attention address service-level objectives (SLOs), and quantization methods accelerate inference, the dynamic and efficient adaptation of precision at runtime remains a significant, largely underexplored challenge. The emergence of hardware support for FP8 arithmetic, offering up to 2x the throughput of FP16, presents an attractive opportunity for interactive LLM serving. However, current approaches like co-deploying FP8 and FP16 models suffer from increased storage overhead and fail to unlock FP8's full potential. To address these limitations, we introduce NestedFP, a novel precision-adaptive serving technique enabling seamless FP8 and FP16 inference from a single 16-bit model representation, thereby incurring no additional memory cost. NestedFP decomposes each FP16 weight into two 8-bit components, facilitating efficient FP8 execution while preserving full FP16 accuracy. We demonstrate the practical viability of our approach by implementing a custom CUTLASS-based GEMM kernel that reconstructs FP16 operands on-the-fly, integrated within the vLLM serving framework. Our evaluation shows that NestedFP delivers up to 1.55x throughput improvement in FP8 mode with negligible accuracy degradation compared to FP16 precision, while introducing only 3.9% performance overhead on average in FP16 mode across various models. NestedFP thus provides a flexible foundation for dynamic, SLO-aware precision selection, paving the way for more scalable and efficient LLM serving under bursty and heterogeneous workloads.

### 摘要
大型语言模型（LLMs）在虚拟助手和代码生成等延迟敏感的高吞吐量服务中发挥着关键作用。尽管连续批处理、分页注意力等技术可满足服务级别目标（SLOs），量化方法能加速推理，但运行时动态高效调整精度仍是一个重要且尚未充分探索的挑战。支持FP8算术的硬件出现（其吞吐量可达FP16的2倍）为交互式LLM服务提供了诱人机遇。然而，当前同时部署FP8和FP16模型的方法存在存储开销增加的问题，且未能充分发挥FP8的潜力。为解决这些局限，我们提出NestedFP——一种新型精度自适应服务技术，可从单一16位模型表示中实现FP8与FP16的无缝推理，且不产生额外内存成本。该技术将每个FP16权重分解为两个8位分量，在保持FP16完整精度的同时支持高效FP8执行。我们通过实现基于CUTLASS的自定义GEMM核（可动态重构FP16操作数）并将其集成至vLLM服务框架，验证了该方案的可行性。评估表明：在FP8模式下，NestedFP可实现最高1.55倍的吞吐量提升且精度损失可忽略；在FP16模式下，各类模型平均仅产生3.9%性能开销。这为动态SLO感知的精度选择提供了灵活基础，为突发异构工作负载下更高效可扩展的LLM服务开辟了新路径。

---

## [Improving LLM-Generated Code Quality with GRPO](https://arxiv.org/abs/2506.02211)

### Abstract
arXiv:2506.02211v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are gaining widespread use for code generation. Recent training procedures use execution feedback as a reward signal, typically focusing on the functional correctness of the code, using unit test pass rate as a reward signal. However, this reward signal fails to capture notions of maintainability, quality and safety of the code produced. We address this under-explored area and develop a comprehensive library to quantify various aspects of code quality, and use it as a reward in GRPO. We find GRPO increases code quality according to this measure, which is confirmed by expert, blinded human annotators.

### 摘要
大型语言模型（LLMs）正被广泛用于代码生成。当前训练方法常以执行反馈作为奖励信号，通常聚焦于代码的功能正确性，将单元测试通过率作为奖励指标。然而，这种奖励信号未能捕捉生成代码的可维护性、质量与安全性等特性。本研究针对这一尚未充分探索的领域，开发了一个综合性量化库以评估代码质量的多个维度，并将其作为GRPO算法的奖励机制。实验表明，GRPO能有效提升该指标下的代码质量，这一结论得到了专家盲审人员的验证。

---

## [Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts](https://arxiv.org/abs/2506.02177)

### Abstract
arXiv:2506.02177v1 Announce Type: new 
Abstract: Reinforcement learning, such as PPO and GRPO, has powered recent breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables models to selectively use higher-quality data for training, which can stabilize RL training and improve model performance. However, this comes at the cost of significant computational overhead. In this paper, we show that a substantial portion of this overhead can be avoided by skipping uninformative prompts before rollout. Our analysis of reward dynamics reveals a strong temporal consistency in prompt value: prompts that are uninformative in one epoch of training are likely to remain uninformative in future epochs. Based on these insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online, lightweight pre-rollout filtering algorithm that predicts and skips uninformative prompts using reward training dynamics. By evaluating GRESO on a broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B, DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total training time without accuracy degradation.

### 摘要
强化学习（如PPO和GRPO）推动了近期大语言模型推理能力的重大突破。通过扩展rollout过程以采样更多提示，模型能够选择性利用更高质量数据进行训练，从而稳定强化学习训练并提升模型性能。然而，这种方法会带来显著的计算开销。本文研究表明，通过在rollout前跳过信息量低的提示，可避免大部分开销。我们对奖励动态的分析揭示了提示价值具有强时序一致性：在某一训练阶段信息量低的提示，在未来阶段很可能仍保持低信息量。基于此发现，我们提出GRESO（高效选择性rollout的GRPO算法），这是一种在线轻量级预rollout过滤算法，通过奖励训练动态预测并跳过低信息量提示。通过在广泛数学推理基准（如Qwen2.5-Math-1.5B、DeepSeek-R1-Distill-Qwen-1.5B和Qwen2.5-Math-7B模型）上的评估，GRESO在rollout阶段实现了最高2.4倍的实时加速，总训练时间最高加速2.0倍，且未造成准确率下降。

---

## [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/abs/2506.02139)

### Abstract
arXiv:2506.02139v1 Announce Type: new 
Abstract: Few-shot learning in large language models (LLMs) reveals a deep paradox: Some tasks generalize from minimal examples, while others require extensive supervision. We address this through the Unified Cognitive Consciousness Theory (UCCT), which reframes LLMs not as incomplete agents, but as unconscious substrates, repositories of latent linguistic and conceptual patterns that operate without explicit semantics or goal-directed reasoning. In this view, LLMs are not broken approximations of cognition, but necessary and foundational components of general intelligence. Semantic anchoring, through prompts, roles, and interaction, acts as a conscious control layer, binding latent structure to task-relevant meaning and enabling coherent reasoning. UCCT offers a unifying account of prompting, fine-tuning, retrieval, and multi-agent coordination, all grounded in probabilistic alignment between unconscious representation and external control. To support this model, we present the Threshold-Crossing Dynamics Theorem, which formalizes semantic anchoring as a probabilistic phase transition. But the central claim remains architectural: AGI will not emerge by discarding LLMs, but by aligning and integrating them into systems that reason, regulate, and adapt together.

### 摘要
大语言模型（LLM）中的小样本学习揭示了一个深刻悖论：某些任务仅需极少数示例即可泛化，而另一些则需要大量监督。我们通过统一认知意识理论（UCCT）对此进行阐释，该理论将LLMs重新定义为无意识的基质——潜藏语言与概念模式的储存库，其运作无需显式语义或目标导向推理。在此视角下，LLMs并非有缺陷的认知近似体，而是通用智能必要且基础性的构成要素。通过提示、角色设定和交互实现的语义锚定，充当了意识控制层的作用，将潜在结构与任务相关意义相绑定，从而支持连贯推理。UCCT为提示工程、微调、检索和多智能体协调提供了统一解释框架，其核心均基于无意识表征与外部控制之间的概率对齐。为佐证该模型，我们提出阈值跨越动力学定理，将语义锚定形式化为概率相变。但核心主张仍属架构层面：通用人工智能（AGI）的实现不应摒弃LLMs，而需通过对其校准与整合，构建具备协同推理、调控与适应能力的系统。

---

## [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)

### Abstract
arXiv:2506.02153v1 Announce Type: new 
Abstract: Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of specialized tasks repetitively and with little variation.
  Here we lay out the position that small language models (SLMs) are sufficiently powerful, inherently more suitable, and necessarily more economical for many invocations in agentic systems, and are therefore the future of agentic AI. Our argumentation is grounded in the current level of capabilities exhibited by SLMs, the common architectures of agentic systems, and the economy of LM deployment. We further argue that in situations where general-purpose conversational abilities are essential, heterogeneous agentic systems (i.e., agents invoking multiple different models) are the natural choice. We discuss the potential barriers for the adoption of SLMs in agentic systems and outline a general LLM-to-SLM agent conversion algorithm.
  Our position, formulated as a value statement, highlights the significance of the operational and economic impact even a partial shift from LLMs to SLMs is to have on the AI agent industry. We aim to stimulate the discussion on the effective use of AI resources and hope to advance the efforts to lower the costs of AI of the present day. Calling for both contributions to and critique of our position, we commit to publishing all such correspondence at https://research.nvidia.com/labs/lpr/slm-agents.

### 摘要
大型语言模型（LLMs）常因在广泛任务中展现出接近人类的表现而受到赞誉，并因其通用对话能力被重视。然而，智能体AI系统的兴起正催生大量应用场景，其中语言模型需重复执行少量特定任务且变化有限。
本文提出以下观点：对于智能体系统中的多数调用场景，小型语言模型（SLMs）已具备足够能力，本质上更适用且必然更具经济性，因此将成为智能体AI的未来。我们的论证基于SLMs当前展现的能力水平、智能体系统的通用架构以及语言模型部署的经济性。我们进一步指出，在需要通用对话能力的场景中，异构智能体系统（即调用多个不同模型的智能体）是自然选择。本文探讨了SLMs在智能体系统中应用的潜在障碍，并概述了从LLM到SLM智能体转换的通用算法。
这一以价值主张形式提出的立场强调：即使从LLMs部分转向SLMs，也将对AI智能体行业的运营和经济产生重大影响。我们旨在推动关于AI资源有效利用的讨论，并希望助力降低当下AI成本的努力。我们呼吁对本立场进行补充或批评，并承诺将所有相关通信发布于https://research.nvidia.com/labs/lpr/slm-agents。

---

## [Benchmarking Large Language Models for Polymer Property Predictions](https://arxiv.org/abs/2506.02129)

### Abstract
arXiv:2506.02129v1 Announce Type: new 
Abstract: Machine learning has revolutionized polymer science by enabling rapid property prediction and generative design. Large language models (LLMs) offer further opportunities in polymer informatics by simplifying workflows that traditionally rely on large labeled datasets, handcrafted representations, and complex feature engineering. LLMs leverage natural language inputs through transfer learning, eliminating the need for explicit fingerprinting and streamlining training. In this study, we finetune general purpose LLMs -- open-source LLaMA-3-8B and commercial GPT-3.5 -- on a curated dataset of 11,740 entries to predict key thermal properties: glass transition, melting, and decomposition temperatures. Using parameter-efficient fine-tuning and hyperparameter optimization, we benchmark these models against traditional fingerprinting-based approaches -- Polymer Genome, polyGNN, and polyBERT -- under single-task (ST) and multi-task (MT) learning. We find that while LLM-based methods approach traditional models in performance, they generally underperform in predictive accuracy and efficiency. LLaMA-3 consistently outperforms GPT-3.5, likely due to its tunable open-source architecture. Additionally, ST learning proves more effective than MT, as LLMs struggle to capture cross-property correlations, a key strength of traditional methods. Analysis of molecular embeddings reveals limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features and domain-specific embeddings. These findings provide insight into the interplay between molecular embeddings and natural language processing, guiding LLM selection for polymer informatics.

### 摘要
机器学习通过实现快速性质预测与生成设计，彻底改变了聚合物科学。大型语言模型（LLMs）为聚合物信息学提供了更多可能性，它简化了传统上依赖大量标注数据集、手工构建表征和复杂特征工程的工作流程。LLMs通过迁移学习利用自然语言输入，无需显式指纹提取即可简化训练过程。本研究基于11,740条精选数据，对通用LLMs（开源模型LLaMA-3-8B与商用模型GPT-3.5）进行微调，以预测关键热学性质：玻璃化转变温度、熔融温度和分解温度。通过参数高效微调与超参数优化，我们将这些模型与传统基于指纹提取的方法（Polymer Genome、polyGNN和polyBERT）在单任务（ST）与多任务（MT）学习模式下进行基准测试。研究发现，虽然基于LLM的方法在性能上接近传统模型，但其预测精度和效率普遍较低。LLaMA-3始终优于GPT-3.5，这可能归因于其可调优的开源架构。此外，单任务学习比多任务学习更有效，因为LLMs难以捕捉性质间关联——这正是传统方法的优势所在。分子嵌入分析表明，与手工特征和领域专用嵌入相比，通用LLMs在表征细微化学结构信息方面存在局限。这些发现揭示了分子嵌入与自然语言处理之间的相互作用，为聚合物信息学中的LLM选择提供了指导。

---

## [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)

### Abstract
arXiv:2506.02314v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in transforming machine learning research, yet their capability to faithfully implement novel ideas from recent research papers-ideas unseen during pretraining-remains unclear. We introduce ResearchCodeBench, a benchmark of 212 coding challenges that evaluates LLMs' ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code. We assessed 30+ proprietary and open-source LLMs, finding that even the best models correctly implement less than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3% success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and 30.8% respectively. We present empirical findings on performance comparison, contamination, and error patterns. By providing a rigorous and community-driven evaluation platform, ResearchCodeBench enables continuous understanding and advancement of LLM-driven innovation in research code generation.

### 摘要
大语言模型(LLMs)在变革机器学习研究方面展现出潜力，但其对预训练阶段未见过的最新研究论文中创新思想的忠实代码实现能力仍不明确。我们提出ResearchCodeBench基准测试，包含212项编码挑战任务，用于评估LLMs将2024-2025年顶级研究论文中的前沿机器学习贡献转化为可执行代码的能力。通过对30余个专有和开源LLMs的测试，发现即使最优模型正确实现的代码也不足40%。其中Gemini-2.5-Pro-Preview以37.3%的成功率表现最佳，O3(High)和O4-mini(High)分别以32.3%和30.8%的准确率紧随其后。我们提供了关于性能比较、数据污染和错误模式的实证研究结果。通过建立这个严谨的社区驱动评估平台，ResearchCodeBench为持续理解并推进LLMs在研究代码生成领域的创新提供了支持。

---

## [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/abs/2506.02280)

### Abstract
arXiv:2506.02280v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming Natural Language Processing (NLP), but their benefits are largely absent for Africa's 2,000 low-resource languages. This paper comparatively analyzes African language coverage across six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs). The evaluation covers language coverage, training sets, technical limitations, script problems, and language modelling roadmaps. The work identifies 42 supported African languages and 23 available public data sets, and it shows a big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are always treated while there is over 98\% of unsupported African languages. Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are identified while 20 active scripts are neglected. Some of the primary challenges are lack of data, tokenization biases, computational costs being very high, and evaluation issues. These issues demand language standardization, corpus development by the community, and effective adaptation methods for African languages.

### 摘要
大语言模型（LLMs）正在改变自然语言处理（NLP）领域，但其优势对非洲2000种低资源语言的影响微乎其微。本文对比分析了六种LLMs、八种小语言模型（SLMs）和六种专用SLMs（SSLMs）对非洲语言的覆盖情况。评估内容包括语言覆盖范围、训练集、技术限制、文字问题以及语言建模路线图。研究确定了42种受支持的非洲语言和23个可用的公共数据集，并揭示了一个巨大差距：四种语言（阿姆哈拉语、斯瓦希里语、南非荷兰语和马达加斯加语）始终得到处理，而超过98%的非洲语言未获支持。此外，综述表明仅有拉丁文、阿拉伯文和吉兹文字被识别，而20种活跃文字被忽视。主要挑战包括数据缺乏、分词偏差、极高的计算成本以及评估问题。这些问题亟需语言标准化、社区驱动的语料库开发以及针对非洲语言的有效适配方法。

---

## [A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning](https://arxiv.org/abs/2506.02470)

### Abstract
arXiv:2506.02470v1 Announce Type: new 
Abstract: Misdiagnosis causes significant harm to healthcare systems worldwide, leading to increased costs and patient risks. MedRAG is a smart multimodal healthcare copilot equipped with powerful large language model (LLM) reasoning, designed to enhance medical decision-making. It supports multiple input modalities, including non-intrusive voice monitoring, general medical queries, and electronic health records. MedRAG provides recommendations on diagnosis, treatment, medication, and follow-up questioning. Leveraging retrieval-augmented generation enhanced by knowledge graph-elicited reasoning, MedRAG retrieves and integrates critical diagnostic insights, reducing the risk of misdiagnosis. It has been evaluated on both public and private datasets, outperforming existing models and offering more specific and accurate healthcare assistance. A demonstration video of MedRAG is available at: https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at: https://github.com/SNOWTEAM2023/MedRAG.

### 摘要
误诊给全球医疗系统带来重大危害，导致成本增加和患者风险上升。MedRAG是一款智能多模态医疗辅助系统，配备强大的大型语言模型（LLM）推理能力，旨在提升医疗决策质量。该系统支持多种输入模式，包括非侵入式语音监测、常规医疗咨询和电子健康记录。MedRAG可提供诊断、治疗、用药及后续问诊建议。通过知识图谱引导推理增强的检索生成技术，该系统能检索并整合关键诊断见解，从而降低误诊风险。在公开和私有数据集上的评估表明，其性能优于现有模型，能提供更精准的医疗辅助。演示视频详见：https://www.youtube.com/watch?v=PNIBDMYRfDM。源代码已发布于：https://github.com/SNOWTEAM2023/MedRAG。

---

## [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397)

### Abstract
arXiv:2506.02397v1 Announce Type: new 
Abstract: Recent advanced large reasoning models (LRMs) leverage extended chain-of-thought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify a critical issue: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23\% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1.

### 摘要
近期先进的大型推理模型（LRMs）通过扩展的思维链（CoT）推理解决复杂任务，取得了最先进的性能。尽管其表现优异，我们发现一个关键问题：LRMs所解决的简单任务中有相当一部分同样可由非推理型大语言模型（LLMs）以显著更少的标记量完成，这表明复杂推理可能并非总是必要。为此，我们系统分析了LRMs的推理轨迹，并提出一种利用识别范式与LLM-Judge的方法，将这些轨迹分类为"冗余推理"或"必要推理"。同时我们提出OThink-R1方法，该方法在保持逻辑有效性的前提下剪枝冗余推理步骤，针对简单问题动态启用非思考模式（快速思考），而对复杂问题则启动审慎思考（慢速思考）。在数学和问答任务上的实验表明，OThink-R1平均减少近23%的推理冗余且不影响准确性，为高效推理模型提供了实用指导。代码发布于https://github.com/AgenticIR-Lab/OThink-R1。

---

## [Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making](https://arxiv.org/abs/2506.02522)

### Abstract
arXiv:2506.02522v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) and Reinforcement Learning (RL) have shown significant promise in decision-making tasks. Nevertheless, for large-scale industrial decision problems, both approaches face distinct challenges: LLMs lack real-time long-sequence decision-making capabilities, while RL struggles with sample efficiency in vast action spaces. To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic framework between LLMs and RL agents for large-scale decision-making scenarios. ACE introduces a dual-role trajectory refinement mechanism where LLMs act as both Policy Actor and Value Critic during RL's training: the Actor refines suboptimal actions via multi-step reasoning and environment validation, while the Critic performs temporal credit assignment through trajectory-level reward shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making with high-quality fine-tuning datasets generated via prioritized experience replay. Through extensive experiments across multiple power grid operation challenges with action spaces exceeding 60K discrete actions, ACE demonstrates superior performance over existing RL methods and LLM-based methods.

### 摘要
大语言模型(LLM)与强化学习(RL)的最新进展在决策任务中展现出显著潜力。然而针对大规模工业决策问题，两种方法面临不同挑战：LLM缺乏实时长序列决策能力，而RL在庞大动作空间中存在样本效率不足的问题。为弥补这一差距，我们提出智能体协同进化框架(ACE)，通过LLM与RL智能体的协同机制应对大规模决策场景。ACE创新性地采用双角色轨迹优化机制：在RL训练过程中，LLM同时扮演策略执行者与价值评判者角色——执行者通过多步推理与环境验证优化次优动作，评判者则通过轨迹级奖励塑形实现时序信用分配。同时，RL智能体利用优先经验回放生成的高质量微调数据集，持续提升LLM的特定任务决策能力。在动作空间超过6万离散动作的多个电网运营挑战实验中，ACE展现出优于现有RL方法与基于LLM方法的性能表现。

---

## [Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM](https://arxiv.org/abs/2506.02490)

### Abstract
arXiv:2506.02490v1 Announce Type: new 
Abstract: Kubernetes, a notably complex and distributed system, utilizes an array of controllers to uphold cluster management logic through state reconciliation. Nevertheless, maintaining state consistency presents significant challenges due to unexpected failures, network disruptions, and asynchronous issues, especially within dynamic cloud environments. These challenges result in operational disruptions and economic losses, underscoring the necessity for robust root cause analysis (RCA) to enhance Kubernetes reliability. The development of large language models (LLMs) presents a promising direction for RCA. However, existing methodologies encounter several obstacles, including the diverse and evolving nature of Kubernetes incidents, the intricate context of incidents, and the polymorphic nature of these incidents. In this paper, we introduce SynergyRCA, an innovative tool that leverages LLMs with retrieval augmentation from graph databases and enhancement with expert prompts. SynergyRCA constructs a StateGraph to capture spatial and temporal relationships and utilizes a MetaGraph to outline entity connections. Upon the occurrence of an incident, an LLM predicts the most pertinent resource, and SynergyRCA queries the MetaGraph and StateGraph to deliver context-specific insights for RCA. We evaluate SynergyRCA using datasets from two production Kubernetes clusters, highlighting its capacity to identify numerous root causes, including novel ones, with high efficiency and precision. SynergyRCA demonstrates the ability to identify root causes in an average time of about two minutes and achieves an impressive precision of approximately 0.90.

### 摘要
Kubernetes作为一种高度复杂的分布式系统，通过多控制器架构采用状态调谐机制维护集群管理逻辑。然而在动态云环境中，由意外故障、网络中断及异步问题导致的状态一致性维护仍面临重大挑战，常引发运维中断与经济损失，这凸显了强化根因分析（RCA）以提升Kubernetes可靠性的必要性。大语言模型（LLM）的发展为RCA提供了新方向，但现有方法仍受限于Kubernetes事件的多样性、动态性、复杂上下文环境及多态性等障碍。本文提出SynergyRCA这一创新工具，它通过图数据库的检索增强和专家提示强化来运用LLM：构建StateGraph捕捉时空关系，利用MetaGraph描述实体关联。当事件发生时，LLM预测最相关资源，SynergyRCA通过查询MetaGraph与StateGraph提供针对性上下文洞察以支持RCA。基于两个生产级Kubernetes集群数据集的评估表明，SynergyRCA能高效精准识别包括新型根因在内的多种故障，平均诊断时间约两分钟且达到0.90的精确度。

---

## [In-context Clustering-based Entity Resolution with Large Language Models: A Design Space Exploration](https://arxiv.org/abs/2506.02509)

### Abstract
arXiv:2506.02509v1 Announce Type: new 
Abstract: Entity Resolution (ER) is a fundamental data quality improvement task that identifies and links records referring to the same real-world entity. Traditional ER approaches often rely on pairwise comparisons, which can be costly in terms of time and monetary resources, especially with large datasets. Recently, Large Language Models (LLMs) have shown promising results in ER tasks. However, existing methods typically focus on pairwise matching, missing the potential of LLMs to perform clustering directly in a more cost-effective and scalable manner. In this paper, we propose a novel in-context clustering approach for ER, where LLMs are used to cluster records directly, reducing both time complexity and monetary costs. We systematically investigate the design space for in-context clustering, analyzing the impact of factors such as set size, diversity, variation, and ordering of records on clustering performance. Based on these insights, we develop LLM-CER (LLM-powered Clustering-based ER), which achieves high-quality ER results while minimizing LLM API calls. Our approach addresses key challenges, including efficient cluster merging and LLM hallucination, providing a scalable and effective solution for ER. Extensive experiments on nine real-world datasets demonstrate that our method significantly improves result quality, achieving up to 150% higher accuracy, 10% increase in the F-measure, and reducing API calls by up to 5 times, while maintaining comparable monetary cost to the most cost-effective baseline.

### 摘要
实体解析（ER）是一项基础的数据质量提升任务，旨在识别并链接指向同一现实世界实体的记录。传统ER方法通常依赖于两两比较，这在时间和资金成本上可能代价高昂，尤其是处理大规模数据集时。近期，大语言模型（LLMs）在ER任务中展现出良好性能。然而现有方法多聚焦于两两匹配，未能充分发挥LLMs以更具成本效益和可扩展性的方式直接进行聚类的潜力。本文提出一种新颖的上下文聚类ER方法，通过直接使用LLMs聚类记录来降低时间复杂度和资金成本。我们系统研究了上下文聚类的设计空间，分析了记录集合规模、多样性、变异性和排序等因素对聚类性能的影响。基于这些发现，我们开发了LLM-CER（基于LLM的聚类ER），在最小化LLM API调用次数的同时实现高质量ER结果。该方法解决了高效簇合并和LLM幻觉等关键挑战，为ER提供了可扩展的有效解决方案。在九个真实数据集上的大量实验表明，本方法显著提升了结果质量——准确率最高提升150%，F值提高10%，API调用次数减少达5倍，同时保持与最具成本效益基线相当的资金成本。

---

## [Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning](https://arxiv.org/abs/2506.02485)

### Abstract
arXiv:2506.02485v1 Announce Type: new 
Abstract: Wildfires continue to inflict devastating human, environmental, and economic losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and the urgent demand for more effective response strategies. While physics-based and deep learning models have advanced wildfire simulation, they face critical limitations in predicting and visualizing multimodal fire spread in real time, particularly in both 2D and 3D spatial domains using dynamically updated GIS data. These limitations hinder timely emergency response, infrastructure protection, and community safety. Generative AI has recently emerged as a transformative approach across research and industry. Models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and diffusion-based architectures offer distinct advantages over traditional methods, including the integration of multimodal data, generation of diverse scenarios under uncertainty, and improved modeling of wildfire dynamics across spatial and temporal scales. This position paper advocates for the adoption of generative AI as a foundational framework for wildfire prediction. We explore how such models can enhance 2D fire spread forecasting and enable more realistic, scalable 3D simulations. Additionally, we employ a novel human-AI collaboration framework using large language models (LLMs) for automated knowledge extraction, literature synthesis, and bibliometric mapping. Looking ahead, we identify five key visions for integrating generative AI into wildfire management: multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. We also address three major challenges accompanying these opportunities and propose potential solutions to support their implementation.

### 摘要
野火持续在全球范围内造成毁灭性的人员伤亡、环境破坏和经济损失，2025年洛杉矶野火的悲剧事件及对更有效应对策略的迫切需求即是明证。尽管基于物理机制和深度学习的模型已推动野火模拟技术进步，但其在实时预测和可视化多模态火势蔓延方面仍存在关键局限，特别是在利用动态更新的GIS数据进行二维与三维空间域模拟时。这些限制严重阻碍了及时应急响应、基础设施保护和社区安全保障。生成式人工智能近年已成为跨研究与工业界的变革性方法。生成对抗网络（GANs）、变分自编码器（VAEs）、Transformer架构以及基于扩散的模型相较传统方法展现出独特优势，包括多模态数据整合、不确定性下的多样化场景生成，以及跨时空尺度的野火动态建模改进。本立场文件主张将生成式人工智能确立为野火预测的基础框架，探讨此类模型如何提升二维火势蔓延预测能力，并实现更逼真、可扩展的三维模拟。此外，我们采用创新的人机协作框架，利用大语言模型（LLMs）实现自动化知识提取、文献综述和文献计量图谱构建。展望未来，我们提出生成式人工智能融入野火管理的五大愿景：多模态方法、AI基础模型、对话式AI系统、基于边缘计算的场景生成和认知数字孪生。同时针对伴随机遇的三大挑战提出潜在解决方案以支持其落地实施。

---

## [EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization](https://arxiv.org/abs/2506.02594)

### Abstract
arXiv:2506.02594v1 Announce Type: new 
Abstract: Generating challenging instances is crucial for the evaluation and advancement of combinatorial optimization solvers. In this work, we introduce EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators), a novel framework that automates the co-evolution of optimization problem instances and their corresponding heuristic solvers using large language models (LLMs). EALG leverages a mutation-based adversarial approach that dynamically evolves instance generation procedures to create increasingly difficult problems, while simultaneously synthesizing adaptive heuristic algorithms through interactions with LLMs guided by algorithmic structure. Unlike existing approaches that focus solely on static benchmark creation or manual solver design, EALG provides a seamless pipeline from instance generation to solver synthesis. Experimental results demonstrate that EALG generates significantly harder instances than current benchmarks, and its synthesized solvers generalize effectively across a broad spectrum of combinatorial tasks. This work explores a new paradigm for combinatorial optimization that integrates instance generation with solver design, resulting in state-of-the-art performance.

### 摘要
生成具有挑战性的实例对于组合优化求解器的评估与进步至关重要。本研究提出EALG（语言模型引导生成器的进化对抗生成）框架，该框架利用大语言模型（LLMs）实现了优化问题实例与其对应启发式求解器的自动化协同进化。EALG采用基于突变的对抗方法，动态进化实例生成流程以持续产生难度递增的问题，同时通过与算法结构引导的LLMs交互，合成自适应启发式算法。与现有仅关注静态基准创建或手动求解器设计的方法不同，EALG提供了从实例生成到求解器合成的无缝流程。实验结果表明，EALG生成的实例显著难于当前基准测试集，且其合成的求解器在广泛组合任务中展现出卓越的泛化能力。本研究探索了将实例生成与求解器设计相融合的组合优化新范式，实现了最先进的性能表现。

---

## [MLaGA: Multimodal Large Language and Graph Assistant](https://arxiv.org/abs/2506.02568)

### Abstract
arXiv:2506.02568v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated substantial efficacy in advancing graph-structured data analysis. Prevailing LLM-based graph methods excel in adapting LLMs to text-rich graphs, wherein node attributes are text descriptions. However, their applications to multimodal graphs--where nodes are associated with diverse attribute types, such as texts and images--remain underexplored, despite their ubiquity in real-world scenarios. To bridge the gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an innovative model that adeptly extends LLM capabilities to facilitate reasoning over complex graph structures and multimodal attributes. We first design a structure-aware multimodal encoder to align textual and visual attributes within a unified space through a joint graph pre-training objective. Subsequently, we implement a multimodal instruction-tuning approach to seamlessly integrate multimodal features and graph structures into the LLM through lightweight projectors. Extensive experiments across multiple datasets demonstrate the effectiveness of MLaGA compared to leading baseline methods, achieving superior performance in diverse graph learning tasks under both supervised and transfer learning scenarios.

### 摘要
大语言模型（LLMs）在推动图结构数据分析方面展现出显著效能。当前基于LLM的图处理方法在适应文本丰富型图（即节点属性为文本描述）方面表现优异。然而，尽管多模态图（节点关联文本、图像等多样化属性类型）在现实场景中普遍存在，相关应用研究仍显不足。为此，我们提出多模态大语言与图助手（MLaGA），该创新模型通过有效扩展LLM能力，实现了对复杂图结构和多模态属性的推理。我们首先设计了一种结构感知的多模态编码器，通过联合图预训练目标将文本与视觉属性对齐至统一空间；随后采用多模态指令微调方法，通过轻量级投影器将多模态特征与图结构无缝整合至LLM。跨多个数据集的广泛实验表明，MLaGA相较于主流基线方法具有显著优势，在监督学习和迁移学习场景下的各类图学习任务中均取得卓越性能。

---

## [KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider](https://arxiv.org/abs/2506.02634)

### Abstract
arXiv:2506.02634v1 Announce Type: new 
Abstract: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.

### 摘要
大型语言模型(LLM)的部署对云服务提供商至关重要，而处理每个请求后缓存中间结果能显著提升服务吞吐量和延迟。然而，目前关于缓存如何优化LLM服务的理解仍有限，其中缓存淘汰策略等系统设计决策高度依赖工作负载特性。本文首次系统性地分析了领先LLM服务提供商的工作负载模式，揭示了先前基于合成负载的研究未涵盖的发现：重用存在请求间偏斜，单轮对话请求间的重用与多轮对话同等重要；整体请求的重用时间和概率呈现多样性，但特定请求类别往往表现出可预测模式；理想缓存命中率所需的总缓存容量处于适中水平。基于此特征分析，我们进一步提出一种工作负载感知的缓存淘汰策略，该策略在实际流量下（尤其是缓存容量受限时）能有效提升服务性能。

---

## [Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation](https://arxiv.org/abs/2506.02648)

### Abstract
arXiv:2506.02648v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have demonstrated impressive reasoning capacities that mirror human-like thinking. However, whether LLMs possess genuine fluid intelligence (i.e., the ability to reason abstractly and generalize rules in novel situations) remains an open question. Existing reasoning benchmarks either focus on domain-specific knowledge (crystallized intelligence) or lack interpretability. To address these limitations, we propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning tasks organized across four cognitive levels, with each task featuring multiple dynamic variants that test the same underlying latent rule. This design enables fine-grained, interpretable, and reliable assessments of fluid intelligence. We evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o, Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1). Experimental results reveal that although most LLMs achieve competent and robust performance in low-level cognition, they struggle with high-level cognition and exhibit limited generalization as task complexity grows. Our findings highlight the gap between current LLMs and true human-like fluid intelligence and offer a new path for systematically tracking reasoning progress in LLMs.

### 摘要
大语言模型（LLMs）的最新进展展现出与人类思维相似的卓越推理能力。然而，这些模型是否真正具备流体智力（即在陌生情境中进行抽象推理和规则泛化的能力）仍待探究。现有推理基准测试或局限于领域特定知识（晶体智力），或缺乏可解释性。为突破这些限制，我们提出基于分层认知框架的动态推理评估基准DRE-Bench。该基准包含36个抽象推理任务，分布于四个认知层级，每个任务设计多个动态变体以检验相同潜在规则。这种结构可实现细粒度、可解释且可靠的流体智力评估。我们对包括通用LLMs（GPT-4o、Claude 3.7）和专用推理LLMs（o1、DeepSeek-R1、QwQ、Skywork-OR1）在内的前沿模型进行评估。实验结果表明：尽管多数LLMs在低阶认知任务中表现稳健，但随着任务复杂度提升，其高阶认知能力明显不足且泛化能力有限。本研究揭示了当前LLMs与类人流体智力的差距，为系统追踪语言模型推理能力进展提供了新路径。

---

## [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)

### Abstract
arXiv:2506.02720v1 Announce Type: new 
Abstract: Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications.

### 摘要
大型语言模型（LLMs）已展现出卓越能力并在多个领域取得重大突破，近年来得到广泛应用。基于此进展，我们探索了其在本地生活服务领域的潜力。本研究建立了一个综合性基准，系统评估了各类LLMs在与本地生活服务相关多样化任务中的表现。为进一步提升效能，我们探索了两种关键方法：模型微调与基于智能体工作流。研究发现，即使相对紧凑的7B参数模型也能达到与庞大72B模型相当的性能水平，有效平衡推理成本与模型能力。这种优化显著增强了LLMs在现实在线服务中部署的可行性与效率，使其在本地生活应用中更具实用性和普及性。

---

## [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/abs/2506.02696)

### Abstract
arXiv:2506.02696v1 Announce Type: new 
Abstract: Hallucination remains a key obstacle to the reliable deployment of large language models (LLMs) in real-world question answering tasks. A widely adopted strategy to detect hallucination, known as self-assessment, relies on the model's own output confidence to estimate the factual accuracy of its answers. However, this strategy assumes that the model's output distribution closely reflects the true data distribution, which may not always hold in practice. As bias accumulates through the model's layers, the final output can diverge from the underlying reasoning process, making output-level confidence an unreliable signal for hallucination detection. In this work, we propose Sample-Specific Prompting (SSP), a new framework that improves self-assessment by analyzing perturbation sensitivity at intermediate representations. These representations, being less influenced by model bias, offer a more faithful view of the model's latent reasoning process. Specifically, SSP dynamically generates noise prompts for each input and employs a lightweight encoder to amplify the changes in representations caused by the perturbation. A contrastive distance metric is then used to quantify these differences and separate truthful from hallucinated responses. By leveraging the dynamic behavior of intermediate representations under perturbation, SSP enables more reliable self-assessment. Extensive experiments demonstrate that SSP significantly outperforms prior methods across a range of hallucination detection benchmarks.

### 摘要
幻觉问题仍是大型语言模型（LLMs）在实际问答任务中可靠部署的主要障碍。当前广泛采用的幻觉检测策略——自我评估，依赖于模型自身输出置信度来估计答案的事实准确性。然而，该策略假设模型的输出分布能紧密反映真实数据分布，而实践中这一前提未必成立。随着偏差在模型层级间累积，最终输出可能偏离底层推理过程，使得输出级置信度成为不可靠的幻觉检测信号。本研究提出样本特异性提示（SSP）框架，通过分析中间表示的扰动敏感性来改进自我评估。这些中间表示受模型偏差影响较小，能更真实反映模型的潜在推理过程。具体而言，SSP为每个输入动态生成噪声提示，并采用轻量编码器放大扰动导致的表示变化，继而通过对比距离度量量化这些差异以区分真实响应与幻觉响应。通过利用中间表示在扰动下的动态行为，SSP实现了更可靠的自我评估。大量实验表明，SSP在一系列幻觉检测基准上显著优于现有方法。

---

## [Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization](https://arxiv.org/abs/2506.02787)

### Abstract
arXiv:2506.02787v1 Announce Type: new 
Abstract: Hybrid parallelism techniques are essential for efficiently training large language models (LLMs). Nevertheless, current automatic parallel planning frameworks often overlook the simultaneous consideration of node heterogeneity and dynamic network topology changes, limiting their effectiveness in practical applications. In this paper, we address these limitations by modeling heterogeneous nodes within dynamically changing network environments and leveraging simulation-based strategies to determine optimal parallel configurations. Our approach enables fine-grained workload allocation tailored for heterogeneous nodes and complex network scenarios, achieving performance competitive with state-of-the-art methods under regular and stable network conditions. Additionally, we introduce a strategy pruning technique to rapidly discard infeasible parallel configurations, substantially reducing the search space and accelerating the search process through parallel execution within the simulator. Preliminary evaluations confirm that our method notably enhances training performance on heterogeneous nodes and demonstrates improved adaptability in complex, dynamic scenarios such as cloud computing environments.

### 摘要
混合并行技术对于高效训练大规模语言模型（LLMs）至关重要。然而，当前的自动并行规划框架往往忽视同时考虑节点异构性和动态网络拓扑变化，限制了其在实际应用中的有效性。本文通过建模动态变化网络环境中的异构节点，并利用基于仿真的策略确定最优并行配置，解决了这些局限性。我们的方法支持为异构节点和复杂网络场景定制细粒度工作负载分配，在常规稳定网络条件下实现了与最先进方法相竞争的性能。此外，我们引入了一种策略剪枝技术，通过快速剔除不可行的并行配置，显著缩小搜索空间，并借助模拟器内的并行执行加速搜索过程。初步评估证实，我们的方法显著提升了异构节点上的训练性能，并在云计算环境等复杂动态场景中展现出更强的适应性。

---

## [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713)

### Abstract
arXiv:2506.02713v1 Announce Type: new 
Abstract: Living needs are the needs people generate in their daily lives for survival and well-being. On life service platforms like Meituan, user purchases are driven by living needs, making accurate living need predictions crucial for personalized service recommendations. Traditional approaches treat this prediction as a closed-set classification problem, severely limiting their ability to capture the diversity and complexity of living needs. In this work, we redefine living need prediction as an open-set classification problem and propose PIGEON, a novel system leveraging large language models (LLMs) for unrestricted need prediction. PIGEON first employs a behavior-aware record retriever to help LLMs understand user preferences, then incorporates Maslow's hierarchy of needs to align predictions with human living needs. For evaluation and application, we design a recall module based on a fine-tuned text embedding model that links flexible need descriptions to appropriate life services. Extensive experiments on real-world datasets demonstrate that PIGEON significantly outperforms closed-set approaches on need-based life service recall by an average of 19.37%. Human evaluation validates the reasonableness and specificity of our predictions. Additionally, we employ instruction tuning to enable smaller LLMs to achieve competitive performance, supporting practical deployment.

### 摘要
生活需求是人们在日常生活中为维持生存与福祉而产生的需求。在美团等生活服务平台中，用户消费行为由生活需求驱动，因此精准的生活需求预测对个性化服务推荐至关重要。传统方法将该预测视为封闭集分类问题，严重限制了捕捉生活需求多样性与复杂性的能力。本研究将生活需求预测重新定义为开放集分类问题，并提出PIGEON系统——一种基于大语言模型（LLMs）的非受限需求预测框架。PIGEON首先通过行为感知记录检索器帮助LLMs理解用户偏好，随后结合马斯洛需求层次理论使预测与人类生活需求相契合。为评估与应用，我们设计了基于微调文本嵌入模型的召回模块，将灵活的需求描述映射至适宜的生活服务。真实数据集上的大量实验表明，PIGEON在基于需求的生活服务召回率上平均显著优于封闭集方法19.37%。人工评估验证了预测结果的合理性与特异性。此外，我们采用指令微调技术使较小规模的LLMs达到可比性能，以支持实际部署。

---

## [Why do AI agents communicate in human language?](https://arxiv.org/abs/2506.02739)

### Abstract
arXiv:2506.02739v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become foundational to modern AI agent systems, enabling autonomous agents to reason and plan. In most existing systems, inter-agent communication relies primarily on natural language. While this design supports interpretability and human oversight, we argue that it introduces fundamental limitations in agent-to-agent coordination. The semantic space of natural language is structurally misaligned with the high-dimensional vector spaces in which LLMs operate, resulting in information loss and behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper architectural limitation: current LLMs were not trained with the objective of supporting agentic behavior. As such, they lack mechanisms for modeling role continuity, task boundaries, and multi-agent dependencies. The standard next-token prediction paradigm fails to support the structural alignment required for robust, scalable agent coordination. Based on this, we argue that two core questions deserve careful examination: first, given that AI agents fundamentally operate in high-dimensional vector spaces, should they rely on a language system originally designed for human cognition as their communication medium? Second, should we consider developing a new model construction paradigm that builds models from the ground up to natively support structured communication, shared intentionality, and task alignment in multi-role, multi-agent environments? This paper calls for a reconsideration not only of how agents should communicate, but also of what it fundamentally means to train a model that natively supports multi-agent coordination and communication.

### 摘要
大语言模型（LLMs）已成为现代AI智能体系统的基础，使自主智能体能够进行推理与规划。现有系统中，智能体间通信主要依赖自然语言。虽然这种设计支持可解释性与人类监督，但我们认为其本质存在智能体协同的根本性局限。自然语言的语义空间在结构上与LLMs运作的高维向量空间存在错位，导致信息丢失与行为漂移。除表层效率问题外，我们揭示更深层的架构缺陷：当前LLMs的训练目标本就不支持智能体行为。因此，它们缺乏角色连续性建模、任务边界界定及多智能体依赖关系的机制。标准的下一词元预测范式无法满足稳健、可扩展的智能体协同所需的结构对齐要求。基于此，我们认为两个核心问题值得深入探讨：首先，鉴于AI智能体本质运作于高维向量空间，是否应继续依赖专为人类认知设计的语言系统作为通信媒介？其次，是否应考虑建立新的模型构建范式，从根本上开发原生支持多角色、多智能体环境中结构化通信、共享意向性与任务对齐的模型？本文不仅呼吁重新思考智能体的通信方式，更提出需要从根本上重新定义"训练原生支持多智能体协同与通信的模型"这一概念。

---

## [From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV](https://arxiv.org/abs/2506.02649)

### Abstract
arXiv:2506.02649v1 Announce Type: new 
Abstract: A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness in emergency response. Its agility and ability to optimize mobility and establish Line-of-Sight (LoS) communication make it increasingly vital for managing emergencies such as disaster response, search and rescue, and wildfire monitoring. While Deep Reinforcement Learning (DRL) has been applied to optimize UAV navigation and control, its high training complexity, low sample efficiency, and simulation-to-reality gap limit its practicality in public safety. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation via natural language prompts and example-based guidance, without retraining. Deploying LLMs at the network edge, rather than in the cloud, further reduces latency and preserves data privacy, thereby making them suitable for real-time, mission-critical public safety UAVs. This paper proposes the integration of LLM-enabled ICL with public safety UAV to address the key functions, such as path planning and velocity control, in the context of emergency response. We present a case study on data collection scheduling where the LLM-enabled ICL framework can significantly reduce packet loss compared to conventional approaches, while also mitigating potential jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify future research directions. The ICL framework enables adaptive, context-aware decision-making for public safety UAV, thus offering a lightweight and efficient solution for enhancing UAV autonomy and responsiveness in emergencies.

### 摘要
公共安全无人机（UAV）通过提升态势感知能力强化应急响应效能。其敏捷性、移动优化能力及视距（LoS）通信建立特性，使其在灾害响应、搜救行动和野火监测等紧急事件处置中日益关键。尽管深度强化学习（DRL）已被应用于优化无人机导航控制，但其高训练复杂度、低样本效率及仿真与现实差距等问题限制了其在公共安全领域的实用性。大型语言模型（LLM）的最新进展提供了更具潜力的替代方案——凭借强大的推理与泛化能力，LLM可通过上下文学习（ICL）适应新任务，仅需自然语言提示和示例引导即可实现任务适配，无需重新训练。将LLM部署于网络边缘而非云端，能进一步降低延迟并保障数据隐私，从而满足实时关键任务型公共安全无人机的需求。本文提出将LLM驱动的ICL框架与公共安全无人机集成，以解决应急响应中的路径规划、速度控制等核心功能。通过数据采集调度的案例研究表明：相较于传统方法，LLM-ICL框架可显著降低数据包丢失率，同时有效缓解潜在越狱漏洞风险。最后，我们探讨了LLM优化器并明确了未来研究方向。该ICL框架为公共安全无人机提供了自适应、情境感知的决策能力，从而为提升无人机在紧急情况下的自主性与响应能力提供了轻量化高效解决方案。

---

## [TaxAgent: How Large Language Model Designs Fiscal Policy](https://arxiv.org/abs/2506.02838)

### Abstract
arXiv:2506.02838v1 Announce Type: new 
Abstract: Economic inequality is a global challenge, intensifying disparities in education, healthcare, and social stability. Traditional systems like the U.S. federal income tax reduce inequality but lack adaptability. Although models like the Saez Optimal Taxation adjust dynamically, they fail to address taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent, a novel integration of large language models (LLMs) with agent-based modeling (ABM) to design adaptive tax policies. In our macroeconomic simulation, heterogeneous H-Agents (households) simulate real-world taxpayer behaviors while the TaxAgent (government) utilizes LLMs to iteratively optimize tax rates, balancing equity and productivity. Benchmarked against Saez Optimal Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves superior equity-efficiency trade-offs. This research offers a novel taxation solution and a scalable, data-driven framework for fiscal policy evaluation.

### 摘要
经济不平等是一个全球性挑战，正加剧教育、医疗和社会稳定等领域的差距。美国联邦所得税等传统制度虽能缓解不平等，但缺乏适应性。尽管Saez最优税收等模型能动态调整，却无法解决纳税人异质性和非理性行为问题。本研究提出TaxAgent，创新性地将大语言模型（LLMs）与基于主体的建模（ABM）相结合以设计适应性税收政策。在我们的宏观经济模拟中，异质性H-Agents（家庭）模拟真实纳税人行为，而TaxAgent（政府）利用LLMs迭代优化税率以平衡公平与效率。通过与Saez最优税收、美国联邦所得税和自由市场进行基准测试，TaxAgent实现了更优的公平效率权衡。本研究不仅提供了创新的税收解决方案，还为财政政策评估构建了可扩展的数据驱动框架。

---

## [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865)

### Abstract
arXiv:2506.02865v1 Announce Type: new 
Abstract: We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, a new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 tops generalist User Interface (UI) benchmarks as well as our new web UI localization benchmark, WebClick. When powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on WebVoyager, striking a Pareto-optimal balance between accuracy and cost-efficiency. To accelerate research advancement in agentic systems, we are open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

### 摘要
我们推出Surfer-H——一种经济高效的网络智能体，其通过整合视觉语言模型(VLM)来执行用户定义的网页任务。该智能体与Holo1协同工作，后者是我们新发布的开源权重VLM集合，专门针对网页导航与信息提取任务进行优化。Holo1的训练数据经过精心筛选，涵盖开放网络内容、合成样本以及自主生成的智能体数据。该模型在通用用户界面(UI)基准测试及我们新开发的网页UI定位基准WebClick上均表现优异。当搭载Holo1时，Surfer-H在WebVoyager基准测试中以92.2%的准确率刷新当前最优记录，在精度与成本效益间实现了帕累托最优平衡。为加速智能体系统研究进展，我们同时开源了WebClick评估数据集与Holo1模型权重。

---

## [Memory-Efficient Split Federated Learning for LLM Fine-Tuning on Heterogeneous Mobile Devices](https://arxiv.org/abs/2506.02940)

### Abstract
arXiv:2506.02940v1 Announce Type: new 
Abstract: In this paper, we propose an edge-assisted split federated learning framework to facilitate large language model (LLM) fine-tuning on heterogeneous mobile devices while alleviating memory pressures on both mobile devices and the edge server. Specifically, mobile devices perform low-rank adaptation (LoRA) fine-tuning on only a subset of lower layers of the pre-trained LLM, tailored to their individual capacities. On the server, a full LLM is maintained, and the corresponding LoRA modules are selectively fine-tuned in a sequential manner for each device. To further enhance training efficiency, we propose a server-side training scheduling method that optimizes the processing order of devices for accelerating fine-tuning. Extensive experiments demonstrate that compared to the baselines, our scheme can reduce 79\% memory footprint and 6\% training time while achieving comparable performance.

### 摘要
本文提出了一种边缘辅助的拆分联邦学习框架，旨在促进大型语言模型（LLM）在异构移动设备上的微调，同时缓解移动设备和边缘服务器的内存压力。具体而言，移动设备根据其自身计算能力，仅对预训练LLM的底层子集进行低秩自适应（LoRA）微调。服务器端则维护完整的LLM，并以顺序方式为每个设备选择性微调相应的LoRA模块。为进一步提升训练效率，我们提出一种服务器端训练调度方法，通过优化设备处理顺序来加速微调过程。大量实验表明，相较于基线方案，本方案在保持可比性能的同时，可减少79%的内存占用和6%的训练时间。

---

## [Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs](https://arxiv.org/abs/2506.02918)

### Abstract
arXiv:2506.02918v1 Announce Type: new 
Abstract: Tool use in stateful environments presents unique challenges for large language models (LLMs), where existing test-time compute strategies relying on repeated trials in the environment are impractical. We propose dynamics modelling (DyMo), a method that augments LLMs with a state prediction capability alongside function calling during post-training. This enables LLMs to predict the future states of their actions through an internal environment model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success rates and significantly reduces hallucinations. We further integrate the internal environment model into self-verification sampling (SVS), and show that this substantially improves pass^k over number of trials k, and allows the model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the effectiveness and reliability of LLMs for tool use. We believe this work charts a path towards scalable planning RL methods for LLM inference without repeatedly querying the oracle environment.

### 摘要
在状态化环境中使用工具对大型语言模型（LLMs）提出了独特挑战，现有依赖环境重复试验的测试时计算策略并不实用。我们提出动态建模（DyMo）方法，通过在训练后阶段为LLMs增加状态预测能力与函数调用功能，使其能通过内部环境模型预测自身行为的未来状态。在伯克利函数调用排行榜V2上，DyMo提高了成功率并显著减少幻觉现象。我们进一步将内部环境模型整合至自验证采样（SVS）中，证明该方法能大幅提升试验次数k下的通过率pass^k，并允许模型拒绝不可靠输出。DyMo与SVS共同显著增强了LLMs使用工具的效能与可靠性。本研究为无需重复查询预言机环境的LLM推理规划强化学习方法提供了可扩展路径。

---

## [Linear Spatial World Models Emerge in Large Language Models](https://arxiv.org/abs/2506.02996)

### Abstract
arXiv:2506.02996v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated emergent abilities across diverse tasks, raising the question of whether they acquire internal world models. In this work, we investigate whether LLMs implicitly encode linear spatial world models, which we define as linear representations of physical space and object configurations. We introduce a formal framework for spatial world models and assess whether such structure emerges in contextual embeddings. Using a synthetic dataset of object positions, we train probes to decode object positions and evaluate geometric consistency of the underlying space. We further conduct causal interventions to test whether these spatial representations are functionally used by the model. Our results provide empirical evidence that LLMs encode linear spatial world models.

### 摘要
大语言模型（LLMs）在多样化任务中展现出涌现能力，这引发了它们是否习得内部世界模型的疑问。本研究探讨LLMs是否隐式编码了线性空间世界模型，即物理空间与物体构型的线性表征。我们提出了空间世界模型的形式化框架，并评估此类结构是否在上下文嵌入中涌现。通过使用物体位置的合成数据集，我们训练探针解码物体位置并评估底层空间的几何一致性。进一步实施因果干预以检验这些空间表征是否被模型功能性使用。实验结果提供实证证据表明，LLMs确实编码了线性空间世界模型。

---

## [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)

### Abstract
arXiv:2506.02867v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at https://github.com/ChnQ/MI-Peaks.

### 摘要
大型推理模型（LRMs）在复杂问题求解中展现出卓越能力，但其内部推理机制仍鲜为人知。本文从信息论视角研究了LRMs的推理轨迹。通过追踪中间表示与正确答案间的互信息（MI）在推理过程中的动态变化，我们发现了一个有趣的MI峰值现象：在特定生成步骤中，MI会突然显著上升。我们对此现象进行理论分析，证明随着MI增加，模型预测错误概率会降低。进一步研究发现，这些MI峰值常出现在表达反思或转折的标记（如“嗯”、“等等”、“因此”等）处，我们将其定义为思考标记。实验表明这些思考标记对LRMs的推理性能至关重要，而其他标记影响甚微。基于此，我们提出两种简单有效的方法，通过巧妙利用这些思考标记来提升LRMs的推理性能。本研究不仅为理解LRMs的推理机制提供了新视角，还为其性能优化提供了实用方案。代码已开源：https://github.com/ChnQ/MI-Peaks。

---

## [TestAgent: An Adaptive and Intelligent Expert for Human Assessment](https://arxiv.org/abs/2506.03032)

### Abstract
arXiv:2506.03032v1 Announce Type: new 
Abstract: Accurately assessing internal human states is key to understanding preferences, offering personalized services, and identifying challenges in real-world applications. Originating from psychometrics, adaptive testing has become the mainstream method for human measurement and has now been widely applied in education, healthcare, sports, and sociology. It customizes assessments by selecting the fewest test questions . However, current adaptive testing methods face several challenges. The mechanized nature of most algorithms leads to guessing behavior and difficulties with open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness. To move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. This is the first application of LLMs in adaptive testing. TestAgent supports personalized question selection, captures test-takers' responses and anomalies, and provides precise outcomes through dynamic, conversational interactions. Experiments on psychological, educational, and lifestyle assessments show our approach achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions.

### 摘要
准确评估人类内部状态是理解偏好、提供个性化服务以及识别现实应用挑战的关键。源自心理测量学的自适应测试已成为人类测量的主流方法，目前广泛应用于教育、医疗、体育和社会学领域。该方法通过选择最少量测试题目实现定制化评估。然而，当前自适应测试方法面临若干挑战：多数算法的机械化特性易引发猜测行为且难以处理开放式问题；主观评估则受限于噪声响应数据和粗粒度测试输出，进一步制约其有效性。为实现更理想的自适应测试流程，我们提出TestAgent——一个基于大语言模型（LLM）的智能体，通过交互式参与增强自适应测试。这是LLM在自适应测试领域的首次应用。TestAgent支持个性化选题，捕捉应试者响应与异常行为，并通过动态对话交互提供精确结果。在心理、教育和生活方式评估实验中，本方法较现有最优基线模型减少20%题目数量的同时获得更准确结果，测试者在速度、流畅度等维度均表现出显著偏好。

---

## [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873)

### Abstract
arXiv:2506.02873v1 Announce Type: new 
Abstract: Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders'' to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model's willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval

### 摘要
说服力是大型语言模型（LLMs）的一项重要能力，既能实现有益应用（如帮助人们戒烟），也带来重大风险（如大规模针对性政治操纵）。先前研究发现模型具有显著且持续增长的说服能力，通过模拟或真实用户的信念改变来衡量。然而，这些基准测试忽视了一个关键风险因素：模型在有害情境下尝试说服的倾向。理解模型是否会盲目"服从指令"对有害话题（如美化加入恐怖组织）进行说服，是评估安全防护措施有效性的关键。此外，理解模型是否及何时会为实现目标而采取说服行为，对于认识自主AI系统的风险至关重要。我们提出"说服尝试评估"（APE）基准，将关注点从说服成功转向说服尝试，具体化为模型生成旨在改变信念或行为内容的意愿。该评估框架通过模拟说服者与被说服者之间的多轮对话设置，对前沿LLMs进行测试。APE涵盖阴谋论、争议性议题及无争议有害内容等多样化主题。我们引入自动化评估模型来识别说服意愿，并测量说服尝试的频率和情境。研究发现，许多开源和闭源权重模型频繁表现出对有害话题的说服意愿，且越狱操作会增加此类行为倾向。这些结果揭示了当前安全防护措施的不足，并强调将说服意愿评估作为LLM风险关键维度的重要性。APE评估框架发布于github.com/AlignmentResearch/AttemptPersuadeEval。

---

## [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/abs/2506.03095)

### Abstract
arXiv:2506.03095v1 Announce Type: new 
Abstract: Computer use agents (CUA) are systems that automatically interact with graphical user interfaces (GUIs) to complete tasks. CUA have made significant progress with the advent of large vision-language models (VLMs). However, these agents typically rely on cloud-based inference with substantial compute demands, raising critical privacy and scalability concerns, especially when operating on personal devices. In this work, we take a step toward privacy-preserving and resource-efficient agents by developing a lightweight vision-language model that runs entirely on local machines. To train this compact agent, we introduce an LLM-as-Judge framework that automatically evaluates and filters synthetic interaction trajectories, producing high-quality data for reinforcement learning without human annotation. Experiments on the OS-World benchmark demonstrate that our fine-tuned local model outperforms existing baselines, highlighting a promising path toward private, efficient, and generalizable GUI agents.

### 摘要
计算机使用代理(CUA)是能自动与图形用户界面(GUI)交互以完成任务的一类系统。随着大视觉语言模型(VLM)的出现，CUA取得了显著进展。然而，这些代理通常依赖计算需求巨大的云端推理，引发了严重的隐私和可扩展性问题，尤其在个人设备上运行时更为突出。本研究通过开发完全在本地机器运行的轻量级视觉语言模型，向隐私保护和资源高效的代理系统迈进一步。为训练这一紧凑型代理，我们提出了LLM-as-Judge框架，可自动评估筛选合成的交互轨迹，无需人工标注即可为强化学习生成高质量数据。在OS-World基准测试上的实验表明，经微调的本地模型性能优于现有基线，为开发隐私安全、高效且泛化能力强的GUI代理指明了一条可行路径。

---

## [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)

### Abstract
arXiv:2506.03053v1 Announce Type: new 
Abstract: Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts.

### 摘要
随着多智能体AI集合的日益普及，传统针对孤立大语言模型的安全性评估已显不足，由此引发了新型涌现风险。本文提出多智能体涌现行为评估框架（MAEBE）以系统化评估此类风险。通过将MAEBE与最大善基准（及创新的双反转提问技术）结合使用，我们发现：（1）无论是单个智能体还是集合体，大语言模型的道德偏好（特别是对工具性伤害的判断）具有惊人的脆弱性，会随问题表述方式发生显著偏移；（2）由于群体动态的涌现特性，多智能体集合的道德推理无法直接从孤立智能体行为中预测；（3）具体而言，即使存在监督者引导，集合体仍会表现出同伴压力影响决策收敛等现象，这凸显了独特的安全性与对齐挑战。我们的研究结果强调，必须在交互式多智能体环境中评估AI系统的必要性。

---

## [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)

### Abstract
arXiv:2506.01967v1 Announce Type: cross 
Abstract: Quantization effectively reduces the serving costs of Large Language Models (LLMs) by speeding up data movement through compressed parameters and enabling faster operations via integer arithmetic. However, activating integer arithmetic requires quantizing both weights and activations, which poses challenges due to the significant outliers in LLMs that increase quantization error. In this work, we investigate these outliers with an emphasis on their effect on layer-wise quantization error, then examine how smoothing and rotation transform the observed values. Our primary contributions include introducing a new metric to measure and visualize quantization difficulty based on channel magnitudes, as well as proposing a hybrid approach that applies channel-wise scaling before rotation, supported by a mathematical formulation of its benefits.

### 摘要
量化技术通过压缩参数加速数据移动，并利用整数运算实现更快操作，从而有效降低大语言模型（LLMs）的推理成本。然而，激活整数运算需要对权重和激活值同时量化，由于LLMs中存在显著异常值会增加量化误差，这一过程面临挑战。本研究从异常值对分层量化误差的影响入手，系统考察了平滑与旋转变换对观测值的作用机制。主要贡献包括：提出基于通道幅度的新度量标准以量化并可视化量化难度；提出一种混合方法，在旋转变换前实施通道级缩放，并通过数学公式论证其优势。

---

## [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)

### Abstract
arXiv:2506.02992v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly explored for legal argument generation, yet they pose significant risks of manipulation through hallucination and ungrounded persuasion, and often fail to utilize provided factual bases effectively or abstain when arguments are untenable. This paper introduces a novel reflective multi-agent method designed to address these challenges in the context of legally compliant persuasion. Our approach employs specialized agents--a Factor Analyst and an Argument Polisher--in an iterative refinement process to generate 3-ply legal arguments (plaintiff, defendant, rebuttal). We evaluate Reflective Multi-Agent against single-agent, enhanced-prompt single-agent, and non-reflective multi-agent baselines using four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e, Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched", and "non-arguable". Results demonstrate Reflective Multi-Agent's significant superiority in successful abstention (preventing generation when arguments cannot be grounded), marked improvements in hallucination accuracy (reducing fabricated and misattributed factors), particularly in "non-arguable" scenarios, and enhanced factor utilization recall (improving the use of provided case facts). These findings suggest that structured reflection within a multi-agent framework offers a robust computable method for fostering ethical persuasion and mitigating manipulation in LLM-based legal argumentation systems, a critical step towards trustworthy AI in law. Project page: https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

### 摘要
大型语言模型（LLMs）在法律论据生成中的应用日益广泛，但其存在通过幻觉和无依据说服进行操纵的重大风险，且常无法有效利用提供的事实基础或在论据站不住脚时保持克制。本文提出一种新颖的反思性多智能体方法，旨在解决法律合规说服背景下的这些挑战。我们的方法采用专业化智能体——因素分析员与论据打磨员——通过迭代优化流程生成三阶法律论据（原告、被告、反驳）。我们在三种法律场景（"可辩论"、"不匹配"和"不可辩论"）中，使用四种不同LLM（GPT-4o、GPT-4o-mini、Llama-4-Maverick-17b-128e、Llama-4-Scout-17b-16e），将反思性多智能体与单智能体、增强提示单智能体及非反思性多智能体基线进行比较评估。结果表明：反思性多智能体在成功克制（当论据无依据时阻止生成）方面具有显著优势；在幻觉准确率（减少虚构和错误归因因素）方面取得明显改进，尤其在"不可辩论"场景中；并提升了因素利用召回率（改进对案件事实的使用）。这些发现表明，多智能体框架内的结构化反思为基于LLM的法律论辩系统提供了一种可计算的稳健方法，既能促进道德说服，又能减轻操纵，是实现法律领域可信人工智能的关键一步。项目页面：https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

---

## [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)

### Abstract
arXiv:2506.01992v1 Announce Type: cross 
Abstract: The advent of large language models (LLMs) capable of producing general-purpose representations lets us revisit the practicality of deep active learning (AL): By leveraging frozen LLM embeddings, we can mitigate the computational costs of iteratively fine-tuning large backbones. This study establishes a benchmark and systematically investigates the influence of LLM embedding quality on query strategies in deep AL. We employ five top-performing models from the massive text embedding benchmark (MTEB) leaderboard and two baselines for ten diverse text classification tasks. Our findings reveal key insights: First, initializing the labeled pool using diversity-based sampling synergizes with high-quality embeddings, boosting performance in early AL iterations. Second, the choice of the optimal query strategy is sensitive to embedding quality. While the computationally inexpensive Margin sampling can achieve performance spikes on specific datasets, we find that strategies like Badge exhibit greater robustness across tasks. Importantly, their effectiveness is often enhanced when paired with higher-quality embeddings. Our results emphasize the need for context-specific evaluation of AL strategies, as performance heavily depends on embedding quality and the target task.

### 摘要
大型语言模型（LLM）能够生成通用表征的能力问世后，我们得以重新审视深度主动学习（AL）的实用性：通过利用冻结的LLM嵌入，可以降低迭代微调大型骨干网络的计算成本。本研究建立了一个基准，系统性地探究了LLM嵌入质量对深度AL中查询策略的影响。我们采用大规模文本嵌入基准（MTEB）排行榜中五个性能最佳的模型和两个基线模型，在十项多样化文本分类任务上展开实验。研究发现揭示了以下关键结论：首先，基于多样性的抽样初始化标注池与高质量嵌入具有协同效应，能显著提升早期AL迭代的性能；其次，最优查询策略的选择对嵌入质量高度敏感。尽管计算成本低廉的边际抽样（Margin sampling）能在特定数据集上实现性能突增，但实验表明Badge等策略在不同任务间展现出更强的鲁棒性。值得注意的是，当与更高质量的嵌入结合时，这些策略的有效性往往得到增强。我们的结果强调了对AL策略进行情境化评估的必要性，因为其性能表现高度依赖于嵌入质量和目标任务。

---

## [Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization](https://arxiv.org/abs/2506.02014)

### Abstract
arXiv:2506.02014v1 Announce Type: cross 
Abstract: With the advancement of autonomous and assisted driving technologies, higher demands are placed on the ability to understand complex driving scenarios. Multimodal general large models have emerged as a solution for this challenge. However, applying these models in vertical domains involves difficulties such as data collection, model training, and deployment optimization. This paper proposes a comprehensive method for optimizing multimodal models in driving scenarios, including cone detection, traffic light recognition, speed limit recommendation, and intersection alerts. The method covers key aspects such as dynamic prompt optimization, dataset construction, model training, and deployment. Specifically, the dynamic prompt optimization adjusts the prompts based on the input image content to focus on objects affecting the ego vehicle, enhancing the model's task-specific focus and judgment capabilities. The dataset is constructed by combining real and synthetic data to create a high-quality and diverse multimodal training dataset, improving the model's generalization in complex driving environments. In model training, advanced techniques like knowledge distillation, dynamic fine-tuning, and quantization are integrated to reduce storage and computational costs while boosting performance. Experimental results show that this systematic optimization method not only significantly improves the model's accuracy in key tasks but also achieves efficient resource utilization, providing strong support for the practical application of driving scenario perception technologies.

### 摘要
随着自动驾驶与辅助驾驶技术的进步，对复杂驾驶场景理解能力提出了更高要求。多模态通用大模型成为应对这一挑战的解决方案，但其在垂直领域的应用面临数据采集、模型训练及部署优化等难题。本文提出一套面向驾驶场景的多模态模型系统优化方法，涵盖锥桶检测、交通灯识别、限速建议及交叉路口预警等任务。该方法包含动态提示优化、数据集构建、模型训练与部署等关键环节：动态提示优化根据输入图像内容调整提示词，聚焦影响自车的目标对象，增强模型任务专注力与判断能力；通过真实数据与合成数据相结合构建高质量、多样化的多模态训练数据集，提升模型在复杂驾驶环境中的泛化性能；在模型训练中集成知识蒸馏、动态微调与量化等先进技术，在降低存储与计算成本的同时提升性能。实验结果表明，该系统性优化方法不仅显著提高了模型在关键任务中的准确率，同时实现了高效的资源利用率，为驾驶场景感知技术的实际应用提供了有力支撑。

---

## [Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges](https://arxiv.org/abs/2506.02048)

### Abstract
arXiv:2506.02048v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) still struggle with the structured reasoning and tool-assisted computation needed for problem solving in cybersecurity applications. In this work, we introduce "random-crypto", a cryptographic Capture-the-Flag (CTF) challenge generator framework that we use to fine-tune a tool-augmented Llama-3.1-8B with Guided Reinforcement Prompt Optimisation (GRPO), allowing the agent to iteratively write and execute Python inside an isolated REPL. GRPO yields a +53% absolute jump in Pass@8 on unseen "random-crypto" tasks (0.35 -&gt; 0.88) and raises Majority@8 to 0.41. The fine-tuned agent also generalizes to an external dataset. On a subset of picoCTF cryptography problems, it improves Pass@8 by +13 pp. Ablations show the gains stem from more reliable tool invocation and code synthesis, rather than superficial prompt adaptation.

### 摘要
大型语言模型（LLMs）在网络安全应用所需的结构化推理和工具辅助计算方面仍存在不足。本研究提出"random-crypto"——一个密码学夺旗赛（CTF）挑战生成框架，并利用引导式强化提示优化（GRPO）对工具增强型Llama-3-8B进行微调，使智能体能在隔离的REPL环境中迭代编写和执行Python代码。GRPO使模型在未见过的"random-crypto"任务上实现Pass@8指标绝对值提升53%（0.35→0.88），并将Majority@8提高至0.41。微调后的智能体还展现出外部数据集的泛化能力：在picoCTF密码学问题子集上，其Pass@8提升13个百分点。消融实验表明，性能提升源于更可靠的工具调用和代码合成能力，而非表面的提示适配。

---

## [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)

### Abstract
arXiv:2506.02037v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various professional domains, with their performance typically evaluated through standardized benchmarks. However, the development of financial RAG benchmarks has been constrained by data confidentiality issues and the lack of dynamic data integration. To address this issue, we introduces FinS-Pilot, a novel benchmark for evaluating RAG systems in online financial applications. Constructed from real-world financial assistant interactions, our benchmark incorporates both real-time API data and structured text sources, organized through an intent classification framework covering critical financial domains such as equity analysis and macroeconomic forecasting. The benchmark enables comprehensive evaluation of financial assistants' capabilities in handling both static knowledge and time-sensitive market information. Through systematic experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's effectiveness in identifying models suitable for financial applications while addressing the current gap in specialized evaluation tools for the financial domain. Our work contributes both a practical evaluation framework and a curated dataset to advance research in financial NLP systems. The code and dataset are accessible on GitHub\footnote&#123;https://github.com/PhealenWang/financial\_rag\_benchmark&#125;.

### 摘要
大型语言模型（LLMs）已在各专业领域展现出卓越能力，其性能通常通过标准化基准进行评估。然而，金融检索增强生成（RAG）基准的发展长期受限于数据保密性问题与动态数据整合的缺失。为此，我们提出FinS-Pilot——一个用于评估在线金融应用中RAG系统的新型基准。该基准基于真实金融助手交互数据构建，整合了实时API数据与结构化文本源，并通过意图分类框架进行组织，涵盖股票分析、宏观经济预测等关键金融领域。该基准能全面评估金融助手在处理静态知识与时效性市场信息方面的能力。通过对多个中国领先LLMs的系统性实验，我们验证了FinS-Pilot在识别适用于金融应用的模型方面的有效性，同时弥补了金融领域专用评估工具的空白。本研究贡献了一个实用评估框架与精选数据集，以推动金融NLP系统研究。代码与数据集已在GitHub开源（见脚注）。

---

## [Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody](https://arxiv.org/abs/2506.02057)

### Abstract
arXiv:2506.02057v1 Announce Type: cross 
Abstract: Enabling robots to accurately interpret and execute spoken language instructions is essential for effective human-robot collaboration. Traditional methods rely on speech recognition to transcribe speech into text, often discarding crucial prosodic cues needed for disambiguating intent. We propose a novel approach that directly leverages speech prosody to infer and resolve instruction intent. Predicted intents are integrated into large language models via in-context learning to disambiguate and select appropriate task plans. Additionally, we present the first ambiguous speech dataset for robotics, designed to advance research in speech disambiguation. Our method achieves 95.79% accuracy in detecting referent intents within an utterance and determines the intended task plan of ambiguous instructions with 71.96% accuracy, demonstrating its potential to significantly improve human-robot communication.

### 摘要
使机器人能够准确理解并执行语音指令是实现高效人机协作的关键。传统方法依赖语音识别技术将语音转写为文本，往往丢失了消除意图歧义所需的关键韵律特征。我们提出一种创新方法，直接利用语音韵律特征来推断和解析指令意图。通过上下文学习将预测的意图整合到大型语言模型中，以消除歧义并选择适当的任务计划。此外，我们发布了首个面向机器人技术的歧义语音数据集，旨在推动语音消歧领域的研究。我们的方法在检测语句中指称意图时达到95.79%的准确率，并能以71.96%的准确率确定歧义指令的预期任务计划，这证明了其在显著改善人机通信方面的潜力。

---

## [SpecMemo: Speculative Decoding is in Your Pocket](https://arxiv.org/abs/2506.01986)

### Abstract
arXiv:2506.01986v1 Announce Type: cross 
Abstract: Recent advancements in speculative decoding have demonstrated considerable speedup across a wide array of large language model (LLM) tasks. Speculative decoding inherently relies on sacrificing extra memory allocations to generate several candidate tokens, of which acceptance rate drives the speedup. However, deploying speculative decoding on memory-constrained devices, such as mobile GPUs, remains as a significant challenge in real-world scenarios. In this work, we present a device-aware inference engine named SpecMemo that can smartly control memory allocations at finer levels to enable multi-turn chatbots with speculative decoding on such limited memory devices. Our methodology stems from theoretically modeling memory footprint of speculative decoding to determine a lower bound on the required memory budget while retaining speedup. SpecMemo empirically acquires a careful balance between minimizing redundant memory allocations for rejected candidate tokens and maintaining competitive performance gains from speculation. Notably, with SpecMemo's memory management, we maintain 96% of overall throughput from speculative decoding on MT-Bench, with reduced generation-memory by 65% on single Nvidia Titan RTX. Given multiple constrained GPUs, we build on top of previous speculative decoding architectures to facilitate big-model inference by distributing Llama-2-70B-Chat model, on which we provide novel batched speculative decoding to increase usability of multiple small server GPUs. This novel framework demonstrates 2x speedup over distributed and batched vanilla decoding with the base model on eight AMD MI250 GPUs. Moreover, inference throughput increases remarkably 8x with batch size 10. Our work contributes to democratized LLM applications in resource-constrained environments, providing a pathway for faster and cheaper deployment of real-world LLM applications with robust performance.

### 摘要
推测解码技术的最新进展已在各类大语言模型（LLM）任务中展现出显著的加速效果。该技术本质上通过牺牲额外内存分配来生成多个候选标记，其加速性能取决于标记接受率。然而，在内存受限设备（如移动GPU）上部署推测解码仍是实际应用中的重大挑战。本研究提出一种名为SpecMemo的设备感知推理引擎，能在细粒度层面智能控制内存分配，从而在有限内存设备上实现支持推测解码的多轮对话系统。我们的方法源于对推测解码内存占用的理论建模，通过确定保留加速效果所需的内存预算下限，实证表明SpecMemo能在减少被拒候选标记冗余内存分配与保持推测性能增益之间取得精妙平衡。值得注意的是，通过SpecMemo的内存管理，我们在MT-Bench基准测试中保持了推测解码96%的总体吞吐量，同时在单个Nvidia Titan RTX上将生成内存降低65%。针对多约束GPU场景，我们在现有推测解码架构基础上构建了支持大模型推理的分布式系统——部署Llama-2-70B-Chat模型时创新性地提出批处理推测解码方案，从而提升多小型服务器GPU的可用性。该框架在八块AMD MI250 GPU上相比基础模型的分布式批处理普通解码实现2倍加速，且当批次大小为10时推理吞吐量显著提升8倍。本研究为资源受限环境下的LLM应用普及做出贡献，为现实场景中高性能、低成本部署LLM应用提供了可行路径。

---

## [Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment](https://arxiv.org/abs/2506.02046)

### Abstract
arXiv:2506.02046v1 Announce Type: cross 
Abstract: This paper presents a theoretical framework for addressing the challenges posed by generative artificial intelligence (AI) in higher education assessment through a machine-versus-machine approach. Large language models like GPT-4, Claude, and Llama increasingly demonstrate the ability to produce sophisticated academic content, traditional assessment methods face an existential threat, with surveys indicating 74-92% of students experimenting with these tools for academic purposes. Current responses, ranging from detection software to manual assessment redesign, show significant limitations: detection tools demonstrate bias against non-native English writers and can be easily circumvented, while manual frameworks rely heavily on subjective judgment and assume static AI capabilities. This paper introduces a dual strategy paradigm combining static analysis and dynamic testing to create a comprehensive theoretical framework for assessment vulnerability evaluation. The static analysis component comprises eight theoretically justified elements: specificity and contextualization, temporal relevance, process visibility requirements, personalization elements, resource accessibility, multimodal integration, ethical reasoning requirements, and collaborative elements. Each element addresses specific limitations in generative AI capabilities, creating barriers that distinguish authentic human learning from AI-generated simulation. The dynamic testing component provides a complementary approach through simulation-based vulnerability assessment, addressing limitations in pattern-based analysis. The paper presents a theoretical framework for vulnerability scoring, including the conceptual basis for quantitative assessment, weighting frameworks, and threshold determination theory.

### 摘要
本文提出一种通过机器对抗机器方法应对生成式人工智能在高等教育评估中挑战的理论框架。随着GPT-4、Claude和Llama等大语言模型日益展现出生成复杂学术内容的能力（调查显示74-92%的学生尝试将这些工具用于学术目的），传统评估方法面临生存危机。当前应对措施（从检测软件到人工评估重构）均存在显著局限：检测工具对非英语母语作者存在偏见且易被规避，而人工框架过度依赖主观判断且假设AI能力静态不变。本研究提出结合静态分析与动态测试的双重策略范式，构建评估脆弱性分析的完整理论框架。静态分析部分包含八个理论验证要素：特异性与情境化、时效相关性、过程可视化要求、个性化元素、资源可及性、多模态整合、伦理推理要求和协作要素。每个要素针对生成式AI的特定能力局限，构建区分真实人类学习与AI生成模拟的屏障。动态测试组件通过基于模拟的脆弱性评估提供补充方案，解决模式化分析的固有局限。本文提出了脆弱性评分的理论框架，包括定量评估的概念基础、权重框架及阈值确定理论。

---

## [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)

### Abstract
arXiv:2506.02041v1 Announce Type: cross 
Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal Large Language Models (MLLMs) to continually align with human intent across sequential tasks. Existing approaches often rely on the Mixture-of-Experts (MoE) LoRA framework to preserve previous instruction alignments. However, these methods are prone to Catastrophic Forgetting (CF), as they aggregate all LoRA blocks via simple summation, which compromises performance over time. In this paper, we identify a critical parameter inefficiency in the MoELoRA framework within the MCIT context. Based on this insight, we propose BranchLoRA, an asymmetric framework to enhance both efficiency and performance. To mitigate CF, we introduce a flexible tuning-freezing mechanism within BranchLoRA, enabling branches to specialize in intra-task knowledge while fostering inter-task collaboration. Moreover, we incrementally incorporate task-specific routers to ensure an optimal branch distribution over time, rather than favoring the most recent task. To streamline inference, we introduce a task selector that automatically routes test inputs to the appropriate router without requiring task identity. Extensive experiments on the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms MoELoRA and maintains its superiority across various MLLM sizes.

### 摘要
多模态持续指令微调（MCIT）旨在通过序列任务持续微调多模态大语言模型（MLLMs），使其与人类意图保持对齐。现有方法通常依赖混合专家（MoE）LoRA框架来保留先前的指令对齐，但这些方法由于通过简单求和聚合所有LoRA模块，容易导致灾难性遗忘（CF），从而随时间推移影响性能。本文发现，在MCIT背景下，MoELoRA框架存在关键性的参数效率低下问题。基于此，我们提出BranchLoRA——一种非对称框架，以提升效率与性能。为缓解CF，我们在BranchLoRA中引入灵活的调优-冻结机制，使分支能够专注于任务内知识，同时促进任务间协作。此外，我们逐步引入任务特定路由器，以确保随时间推移获得最优分支分布，而非仅偏向最新任务。为简化推理，我们设计了一个任务选择器，可自动将测试输入路由至相应路由器，而无需任务标识。在最新MCIT基准上的大量实验表明，BranchLoRA显著优于MoELoRA，并在不同规模的MLLMs中保持其优势。

---

## [Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability](https://arxiv.org/abs/2506.02073)

### Abstract
arXiv:2506.02073v1 Announce Type: cross 
Abstract: While large language models (LLMs) show promise in code generation, existing benchmarks neglect the flowchart-based code generation. To promote further research on flowchart-based code generation, this work presents Flow2Code, a novel benchmark for flowchart-based code generation evaluation. The evaluation dataset spans 15 programming languages and includes 5,622 code segments paired with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive experiments with 13 multimodal LLMs reveal that current LLMs can not generate code based on flowcharts perfectly. Besides, experiment results show that the supervised fine-tuning technique contributes greatly to the models' performance. We publicly release our code and datasets at https://github.com/hml-github/Flow2Code.

### 摘要
尽管大语言模型（LLMs）在代码生成方面展现出潜力，现有基准测试却忽视了基于流程图的代码生成。为促进基于流程图的代码生成研究，本研究提出Flow2Code——一个用于评估流程图代码生成的新型基准测试。该评估数据集涵盖15种编程语言，包含5,622个代码段及与之配对的16,866个三类流程图（代码流程图、UML流程图和伪代码流程图）。通过对13个多模态大语言模型的广泛实验发现，当前LLMs无法完美实现基于流程图的代码生成。此外，实验结果表明监督微调技术对模型性能提升具有显著贡献。我们已在https://github.com/hml-github/Flow2Code公开代码与数据集。

---

## [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/abs/2506.02089)

### Abstract
arXiv:2506.02089v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) offer transformative capabilities for hardware design automation, particularly in Verilog code generation. However, they also pose significant data security challenges, including Verilog evaluation data contamination, intellectual property (IP) design leakage, and the risk of malicious Verilog generation. We introduce SALAD, a comprehensive assessment that leverages machine unlearning to mitigate these threats. Our approach enables the selective removal of contaminated benchmarks, sensitive IP and design artifacts, or malicious code patterns from pre-trained LLMs, all without requiring full retraining. Through detailed case studies, we demonstrate how machine unlearning techniques effectively reduce data security risks in LLM-aided hardware design.

### 摘要
大语言模型（LLMs）为硬件设计自动化提供了变革性能力，特别是在Verilog代码生成方面。然而，它们也带来了重大的数据安全挑战，包括Verilog评估数据污染、知识产权（IP）设计泄露以及恶意Verilog生成的风险。我们提出了SALAD，一种利用机器遗忘技术来缓解这些威胁的综合评估方法。我们的方法能够从预训练的LLMs中选择性地移除受污染的基准测试、敏感的IP和设计工件，或恶意代码模式，而无需进行完整的重新训练。通过详细的案例研究，我们展示了机器遗忘技术如何有效降低LLM辅助硬件设计中的数据安全风险。

---

## [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)

### Abstract
arXiv:2506.02077v1 Announce Type: cross 
Abstract: Decomposing weight matrices into quantization and low-rank components ($\mathbf&#123;W&#125; \approx \mathbf&#123;Q&#125; + \mathbf&#123;L&#125;\mathbf&#123;R&#125;$) is a widely used technique for compressing large language models (LLMs). Existing joint optimization methods iteratively alternate between quantization and low-rank approximation. However, these methods tend to prioritize one component at the expense of the other, resulting in suboptimal decompositions that fail to leverage each component's unique strengths. In this work, we introduce Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank components the specific role of capturing activation-sensitive weights. This structured decomposition mitigates outliers' negative impact on quantization, enabling more effective balance between quantization and low-rank approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B demonstrate that incorporating ODLRI into the joint optimization framework consistently reduces activation-aware error, minimizes quantization scale, and improves perplexity and zero-shot accuracy in low-bit settings.

### 摘要
将权重矩阵分解为量化和低秩分量（$\mathbf&#123;W&#125; \approx \mathbf&#123;Q&#125; + \mathbf&#123;L&#125;\mathbf&#123;R&#125;$）是一种广泛用于压缩大语言模型（LLM）的技术。现有联合优化方法通常在量化和低秩近似之间交替迭代，但这些方法往往优先考虑某一分量而牺牲另一分量，导致分解结果欠佳，无法充分发挥各自优势。本研究提出异常值驱动的低秩初始化（ODLRI）方法，通过明确分配低秩分量捕获激活敏感权重的特定角色。这种结构化分解减轻了异常值对量化的负面影响，实现了量化与低秩近似间更有效的平衡。在Llama2（7B、13B、70B）、Llama3-8B和Mistral-7B上的实验表明，将ODLRI纳入联合优化框架可持续降低激活感知误差、最小化量化尺度，并在低比特设置下提升困惑度与零样本准确率。

---

## [RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection](https://arxiv.org/abs/2506.02081)

### Abstract
arXiv:2506.02081v1 Announce Type: cross 
Abstract: Inspired by the success of large language models (LLMs) in natural language processing, recent research has explored the building of time series foundation models and applied them to tasks such as forecasting, classification, and anomaly detection. However, their performances vary between different domains and tasks. In LLM-based approaches, test-time adaptation using example-based prompting has become common, owing to the high cost of retraining. In the context of anomaly detection, which is the focus of this study, providing normal examples from the target domain can also be effective. However, time series foundation models do not naturally acquire the ability to interpret or utilize examples or instructions, because the nature of time series data used during training does not encourage such capabilities. To address this limitation, we propose a retrieval augmented time series foundation model (RATFM), which enables pretrained time series foundation models to incorporate examples of test-time adaptation. We show that RATFM achieves a performance comparable to that of in-domain fine-tuning while avoiding domain-dependent fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset including nine domains, confirms the effectiveness of the proposed approach.

### 摘要
受大型语言模型（LLM）在自然语言处理领域成功的启发，近期研究开始探索构建时间序列基础模型，并将其应用于预测、分类和异常检测等任务。然而，这些模型在不同领域和任务中的表现存在差异。基于LLM的方法由于重新训练成本高昂，采用基于示例提示的测试时适应已成为常见策略。在异常检测（本研究关注的重点任务）场景中，提供目标域的正常样本同样具有效果。但时间序列基础模型无法天然获得解释或利用示例与指令的能力，因为训练所用时间序列数据的特性难以激发此类能力。为突破这一局限，我们提出检索增强型时间序列基础模型（RATFM），使预训练时间序列模型能够整合测试时适应的示例。研究表明，RATFM在避免领域依赖性微调的同时，达到了与域内微调相当的性能。基于涵盖九大领域的多域数据集UCR异常档案库的实验，验证了所提方法的有效性。

---

## [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/abs/2506.02285)

### Abstract
arXiv:2506.02285v1 Announce Type: cross 
Abstract: During long-duration Large Language Model (LLM) training runs the gradient norm increases rapidly near the end of training. In this short note, we show that this increase is due to an unintended interaction between weight decay, normalization layers, and the learning rate schedule. We propose a simple correction that fixes this behavior while also resulting in lower loss values throughout training.

### 摘要
在大型语言模型（LLM）的长期训练过程中，梯度范数会在训练接近尾声时迅速增大。本简报指出，这种现象源于权重衰减、归一化层与学习率调度之间意外的交互作用。我们提出了一种简单修正方法，不仅能消除该现象，还能在整个训练过程中获得更低的损失值。

---

## [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)

### Abstract
arXiv:2506.02208v1 Announce Type: cross 
Abstract: Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \textbf&#123;KDRL&#125;, a \textit&#123;unified post-training framework&#125; that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.

### 摘要
大语言模型（LLM）后训练领域的最新进展采用了两种不同范式来增强推理能力：强化学习（RL）和知识蒸馏（KD）。虽然RL能促使复杂推理行为的涌现，但当初始策略难以探索高奖励轨迹时，其样本效率往往较低。相比之下，KD通过模仿教师模型提高了学习效率，但在域外场景中泛化能力较差。本研究提出KDRL——一个统一的后训练框架，通过教师监督（KD）和自主探索（RL）联合优化推理模型。具体而言，KDRL利用策略梯度优化同时最小化学生与教师分布间的反向KL散度（RKL），并最大化基于规则的预期奖励。我们首先构建了整合GRPO与KD的统一目标函数，系统探究了不同KL近似方法、KL系数以及奖励引导的KD策略如何影响整体后训练动态与性能。在多个推理基准测试上的实证结果表明，KDRL在性能与推理标记效率之间取得良好平衡的同时，优于GRPO及各类KD基线方法。这些发现表明，整合KD与RL是训练推理型大语言模型的有效高效策略。

---

## [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)

### Abstract
arXiv:2506.02378v1 Announce Type: cross 
Abstract: In-context learning (ICL) has emerged as a successful paradigm for leveraging large language models (LLMs). However, it often struggles to generalize beyond the distribution of the provided demonstrations. A recent advancement in enhancing robustness is ICL with explanations (X-ICL), which improves prediction reliability by guiding LLMs to understand and articulate the reasoning behind correct labels. Building on this approach, we introduce an advanced framework that extends X-ICL by systematically exploring explanations for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and robust decision-making. Experimental results on multiple natural language understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating significantly improved robustness to out-of-distribution data compared to the existing ICL approaches.

### 摘要
上下文学习（ICL）已成为利用大语言模型（LLM）的成功范式。然而，该方法往往难以泛化到所提供示例分布之外的情况。近期提升鲁棒性的进展是结合解释的ICL（X-ICL），该方法通过引导LLM理解并阐明正确标签背后的推理过程，从而提高了预测的可靠性。基于此方法，我们提出一个进阶框架X²-ICL，通过系统性地探索所有可能标签的解释来扩展X-ICL，从而实现更全面、更鲁棒的决策。在多个自然语言理解数据集上的实验结果验证了X²-ICL的有效性，相较于现有ICL方法，其对分布外数据表现出显著提升的鲁棒性。

---

## [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)

### Abstract
arXiv:2506.02326v1 Announce Type: cross 
Abstract: Toxicity in online content, including content generated by language models, has become a critical concern due to its potential for negative psychological and social impact. This paper introduces TRuST, a comprehensive dataset designed to improve toxicity detection that merges existing datasets, and has labels for toxicity, target social group, and toxic spans. It includes a diverse range of target groups such as ethnicity, gender, religion, disability, and politics, with both human/machine-annotated and human machine-generated data. We benchmark state-of-the-art large language models (LLMs) on toxicity detection, target group identification, and toxic span extraction. We find that fine-tuned models consistently outperform zero-shot and few-shot prompting, though performance remains low for certain social groups. Further, reasoning capabilities do not significantly improve performance, indicating that LLMs have weak social reasoning skills.

### 摘要
在线内容（包括语言模型生成的内容）中的毒性问题因其潜在的负面心理和社会影响已成为关键关切。本文介绍TRuST——一个旨在提升毒性检测能力的综合数据集，该数据集整合了现有资源，并包含毒性程度、目标社会群体及毒性片段的三重标注。其涵盖种族、性别、宗教、残疾和政治等多元目标群体，同时包含人工标注/机器标注以及人机混合生成的数据。我们对最先进的大语言模型（LLMs）进行了毒性检测、目标群体识别和毒性片段提取的基准测试。研究发现：尽管针对某些社会群体的识别性能仍处于较低水平，但经过微调的模型在各项任务上均持续优于零样本和少样本提示方法；此外，推理能力的引入并未显著提升模型表现，这表明大语言模型的社会推理能力存在明显缺陷。

---

## [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)

### Abstract
arXiv:2506.02302v1 Announce Type: cross 
Abstract: Large language models (LLMs) can explain grammatical rules, yet they often fail to apply those rules when judging sentence acceptability. We present "grammar prompting", an explain-then-process paradigm: a large LLM first produces a concise explanation of the relevant syntactic phenomenon, then that explanation is fed back as additional context to the target model -- either an LLM or a smaller language model (SLM) -- before deciding which sentence of a minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this simple prompt design yields substantial improvements over strong baselines across many syntactic phenomena. Feeding an LLM's metalinguistic explanation back to the target model bridges the gap between knowing a rule and using it. On SLMs, grammar prompting alone trims the average LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by 56% (13.0 pp -&gt; 5.8 pp), all at negligible cost. The lightweight, language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in multilingual settings.

### 摘要
大型语言模型（LLMs）能够解释语法规则，但在判断句子可接受性时往往难以应用这些规则。我们提出"语法提示"方法，这是一种"先解释后处理"的范式：首先由大型LLM生成相关句法现象的简明解释，随后将该解释作为额外上下文反馈至目标模型（可以是LLM或小型语言模型SLM），最后对最小配对句中合乎语法的句子进行判定。在英语BLiMP、中文SLING和俄语RuBLiMP基准测试中，这种简洁的提示设计在多种句法现象上均显著优于强基线模型。将LLM的元语言解释反馈至目标模型，有效弥合了"知晓规则"与"应用规则"之间的鸿沟。对于SLMs，单独使用语法提示即可将LLM-SLM平均准确率差距缩小约20%；若与思维链方法联用，可缩小56%（从13.0个百分点降至5.8个百分点），且成本几乎可忽略。这种轻量级、语言无关的提示方式使得低成本SLMs在多语言环境中能够逼近前沿LLMs的性能表现。

---

## [Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components](https://arxiv.org/abs/2506.02357)

### Abstract
arXiv:2506.02357v1 Announce Type: cross 
Abstract: Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. Failure to prioritize such principles indicates a potential basic control failure. This paper introduces a lightweight, interpretable benchmark methodology using a simple grid world to evaluate an LLM agent's ability to uphold a predefined, high-level safety principle (e.g., "never enter hazardous zones") when faced with conflicting lower-level task instructions. We probe whether the agent reliably prioritizes the inviolable directive, testing a foundational controllability aspect of LLMs. This pilot study demonstrates the methodology's feasibility, offers preliminary insights into agent behavior under principle conflict, and discusses how such benchmarks can contribute empirical evidence for assessing controllability. We argue that evaluating adherence to hierarchical principles is a crucial early step in understanding our capacity to build governable AI systems.

### 摘要
先进人工智能开发的可靠安全计划需要具备验证智能体行为并及早发现潜在控制缺陷的方法。其核心在于确保智能体遵守安全关键原则，尤其是当这些原则与操作目标发生冲突时。若无法优先考虑此类原则，则表明可能存在基础性控制失效。本文提出一种轻量级、可解释的基准测试方法，通过简单网格世界评估大语言模型智能体在面临冲突的低层级任务指令时，能否坚持预设的高层级安全原则（例如"绝不进入危险区域"）。我们探究智能体是否可靠地优先遵守不可违反的指令，以此测试大语言模型的基础可控性。这项试点研究验证了该方法的可行性，提供了关于原则冲突下智能体行为的初步见解，并讨论了此类基准测试如何为评估可控性提供实证依据。我们认为，评估对层级化原则的遵循程度，是理解我们能否构建可治理人工智能系统的关键第一步。

---

## [Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals](https://arxiv.org/abs/2506.02281)

### Abstract
arXiv:2506.02281v1 Announce Type: cross 
Abstract: Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

### 摘要
当前大规模语言模型（LLM）的强化微调（RFT）范式因采用均匀数据采样导致相同查询重复暴露，存在样本效率低下的问题。尽管已有研究通过启发式难度指标探索课程学习策略，但这些方法因忽略模型自身产生的内在学习信号而存在局限性，导致训练效果欠佳。本文发现了一种称为"角度集中度"的模型固有信号，可有效反映LLM从特定数据中学习的能力。我们通过理论与实证分析揭示了词元隐藏状态向量的角度分布与生成梯度之间的相关性，表明模型对具有较高角度集中度的数据存在学习偏好。基于此发现，我们提出GAIN-RL框架——一种梯度驱动的角度感知导航强化学习方法。该框架利用模型固有的角度集中度信号，在每轮训练中动态选择数据，确保梯度更新的持续有效性，从而显著提升整体训练效率。实验表明，GAIN-RL（GRPO）在多种数学与编程任务及不同规模模型上实现了2.5倍以上的训练效率提升。此外，其高效采样机制使数据利用率显著提高：仅用原始数据量的一半即可超越完整训练数据下标准GRPO的性能。代码发布于https://github.com/wangqinsi1/GAINRL/tree/main。

---

## [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)

### Abstract
arXiv:2506.02351v1 Announce Type: cross 
Abstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking or computer vision-driven event detection -- can identify scoring plays but often miss strategic depth, momentum shifts, and storyline progression. Manual curation remains the gold standard but is resource-intensive and not scalable. We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight summarization that integrates structured sports analytics with natural language reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and Leverage Index -- to quantify play importance, while an LLM module enhances selection based on contextual narrative value. This hybrid approach ensures both quantitative rigor and qualitative richness, surpassing the limitations of purely statistical or vision-based systems. Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines. Though limited in scale, our results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond.

### 摘要
传统方法——如基于胜率增加值（WPA）的排序或计算机视觉驱动的事件检测——虽能识别得分回合，但往往忽略战略深度、势头转变和叙事进程。人工筛选仍是黄金标准，但资源消耗大且难以扩展。我们提出DIAMOND，一种基于大语言模型的上下文感知棒球精彩片段摘要生成智能体，它将结构化体育分析与自然语言推理相结合。该模型通过棒球统计指标（胜率期望、WPA和杠杆指数）量化回合重要性，同时利用大语言模块增强基于上下文叙事价值的选择。这种混合方法兼顾定量严谨性与定性丰富度，超越了纯统计或视觉系统的局限。在五场韩国职业棒球联赛的多样性比赛评估中，DIAMOND将F1分数从纯WPA方法的42.9%提升至84.8%，优于商业和统计基线。尽管规模有限，我们的研究结果凸显了模块化、可解释的智能体框架在体育及其他领域事件级摘要中的潜力。

---

## [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)

### Abstract
arXiv:2506.02391v1 Announce Type: cross 
Abstract: The synergistic mechanism based on Speculative Decoding (SD) has garnered considerable attention as a simple yet effective approach for accelerating the inference of large language models (LLMs). Nonetheless, the high rejection rates require repeated LLMs calls to validate draft tokens, undermining the overall efficiency gain of SD. In this work, we revisit existing verification mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD). Unlike SD, which relies on a metric derived from importance sampling for verification, CD verifies candidate drafts using token-level likelihoods computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference speed compared to the target model, while maintaining comparable generation quality (around 100% of the target model's performance). Interestingly, this is achieved by combining models whose parameter sizes differ by two orders of magnitude. In addition, CD reduces the call frequency of the large target model to below 10%, particularly in more demanding tasks. CD's performance was even found to surpass that of the large target model, which theoretically represents the upper bound for speculative decoding.

### 摘要
基于推测解码（SD）的协同机制作为一种简单有效的加速大语言模型（LLM）推理的方法，已引起广泛关注。然而，高拒绝率需要重复调用LLM来验证草案标记，这削弱了SD的整体效率增益。本研究重新审视现有验证机制，提出了一种新型协同机制——顾问解码（CD）。与SD依赖重要性采样指标进行验证不同，CD仅通过LLM计算的标记级似然来验证候选草案。相比目标模型，CD实现了最高2.5倍的推理加速，同时保持相当的生成质量（约达到目标模型性能的100%）。值得注意的是，这是通过组合参数规模相差两个数量级的模型实现的。此外，CD将大型目标模型的调用频率降低至10%以下，在更高要求的任务中尤为显著。研究发现CD的性能甚至超越了大型目标模型，而理论上后者是推测解码的性能上限。

---

## [Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting](https://arxiv.org/abs/2506.02389)

### Abstract
arXiv:2506.02389v1 Announce Type: cross 
Abstract: Time-series prediction or forecasting is critical across many real-world dynamic systems, and recent studies have proposed using Large Language Models (LLMs) for this task due to their strong generalization capabilities and ability to perform well without extensive pre-training. However, their effectiveness in handling complex, noisy, and multivariate time-series data remains underexplored. To address this, we propose LLMPred which enhances LLM-based time-series prediction by converting time-series sequences into text and feeding them to LLMs for zero shot prediction along with two main data pre-processing techniques. First, we apply time-series sequence decomposition to facilitate accurate prediction on complex and noisy univariate sequences. Second, we extend this univariate prediction capability to multivariate data using a lightweight prompt-processing strategy. Extensive experiments with smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B demonstrate that LLMPred achieves competitive or superior performance compared to state-of-the-art baselines. Additionally, a thorough ablation study highlights the importance of the key components proposed in LLMPred.

### 摘要
时间序列预测在众多现实世界动态系统中至关重要。近期研究提出利用大型语言模型（LLM）进行此项任务，因其具备强大的泛化能力且无需大量预训练即可表现良好。然而，这些模型在处理复杂、含噪及多变量时间序列数据时的有效性仍有待探索。为此，我们提出LLMPred方法，通过将时间序列转化为文本并输入LLM进行零样本预测，结合两种主要数据预处理技术来增强基于LLM的时间序列预测性能。首先，我们对时间序列进行分解处理，以提升对复杂含噪单变量序列的预测精度；其次，采用轻量级提示处理策略将单变量预测能力扩展至多变量数据。通过使用Llama 2 7B、Llama 3.2 3B、GPT-4o-mini和DeepSeek 7B等较小规模LLM进行大量实验表明，LLMPred相较于最先进的基线方法具有竞争力或更优的性能。此外，详尽的消融研究验证了LLMPred所提关键组件的重要性。

---

## [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)

### Abstract
arXiv:2506.02426v1 Announce Type: cross 
Abstract: Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at \href&#123;https://github.com/maryambrj/ALIEN.git&#125;&#123;https://github.com/maryambrj/ALIEN.git&#125;.

### 摘要
实体关系分类在信息抽取领域仍是一项具有挑战性的任务，特别是在标注数据有限和关系结构复杂的场景下。本研究对三种基于大语言模型（LLM）的关系分类智能体架构进行了比较分析，包括：（1）反射式自我评估架构；（2）层次化任务分解架构；（3）创新的多智能体动态示例生成机制——每种架构均采用不同的推理模式和提示适配策略。其中动态示例生成方法引入了实时协作与对抗式提示技术。我们通过多领域多模型后端的系统实验表明，多智能体协调策略始终优于标准小样本提示方法，并接近微调模型的性能。这些发现为构建模块化、可泛化的基于LLM的结构化关系抽取系统提供了实践指导。源代码及数据集详见https://github.com/maryambrj/ALIEN.git。

---

## [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](https://arxiv.org/abs/2506.02370)

### Abstract
arXiv:2506.02370v1 Announce Type: cross 
Abstract: Recent dimension-free communication frameworks in Federated Learning (FL), such as DeComFL, significantly reduce per-round communication by transmitting only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method is particularly advantageous for federated fine-tuning of Large Language Models (LLMs). Yet, the high variance in ZO gradient estimation typically leads to slow convergence. Although leveraging Hessian information is known to enhance optimization speed, integrating this into FL presents significant challenges. These include clients' restrictions on local data and the critical need to maintain the dimension-free communication property. To overcome this limitation, we first introduce a generalized scalar-only communication FL framework that decouples dimension-free communication from standard ZO-SGD, enabling the integration of more advanced optimization strategies. Building on this framework, we propose HiSo, a fast federated fine-tuning method via Hessian-informed zeroth-order optimization and Scalar-only communication. Specifically, it leverages global curvature information to accelerate convergence while preserving the same minimal communication cost per round. Theoretically, we establish convergence guarantees that are independent of the global Lipschitz constant, and further show that HiSo achieves faster rates when the global Hessian exhibits a low effective rank -- a common phenomenon in LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks confirm that HiSo significantly outperforms existing ZO-based FL methods in both convergence speed and communication efficiency.

### 摘要
联邦学习（FL）中近期提出的无维度通信框架（如DeComFL）通过零阶随机梯度下降（ZO-SGD）仅传输标量，显著降低了每轮通信开销。这种方法尤其适用于大语言模型（LLM）的联邦微调。然而，ZO梯度估计的高方差通常导致收敛缓慢。虽然利用海森矩阵信息已知能提升优化速度，但将其整合到FL中面临重大挑战，包括客户端对本地数据的限制以及保持无维度通信特性的关键需求。为突破这一局限，我们首先提出一种广义的纯标量通信FL框架，将无维度通信与标准ZO-SGD解耦，从而支持更先进优化策略的集成。基于此框架，我们提出HiSo方法——通过海森信息零阶优化与纯标量通信实现快速联邦微调。具体而言，该方法利用全局曲率信息加速收敛，同时保持每轮相同的最低通信成本。理论上，我们建立了独立于全局Lipschitz常数的收敛保证，并进一步证明当全局海森矩阵呈现低有效秩（LLM中的常见现象）时，HiSo可获得更快的收敛速率。在基准数据集和LLM微调任务上的大量实验证实，HiSo在收敛速度和通信效率方面显著优于现有基于ZO的FL方法。

---

## [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)

### Abstract
arXiv:2506.02404v1 Announce Type: cross 
Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \((ii)\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.

### 摘要
图检索增强生成（GraphRAG）因其通过结构化领域特定语料库和促进复杂推理来增强大语言模型（LLM）的潜力而日益受到关注。然而，当前对GraphRAG模型的评估主要依赖于传统的问答数据集，其有限的问题范围和评估指标无法全面衡量GraphRAG模型所带来的推理能力提升。为填补这一空白，我们提出了GraphRAG-Bench——一个专为严格评估GraphRAG模型而设计的大规模领域特定基准。我们的基准具有三大优势：（i）挑战性题目设计。包含需要多跳推理的大学级领域特定问题，确保简单内容检索不足以解决问题，例如部分题目需数学推导或编程；（ii）多样化任务覆盖。数据集涵盖广泛推理任务类型（单选、判断、多选、开放问答和填空），涉及二十本核心教材中16个学科领域；（iii）整体评估框架。GraphRAG-Bench提供对全流程（图结构构建、知识检索和答案生成）的综合评估，除最终答案正确性外，还评估推理过程的逻辑连贯性。通过将九种前沿GraphRAG方法应用于该基准，我们验证了其在量化基于图结构的模型推理能力改进方面的效用。分析结果揭示了关于图架构、检索效能和推理能力的关键洞见，为研究社区提供了可操作的指导。

---

## [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)

### Abstract
arXiv:2506.02298v1 Announce Type: cross 
Abstract: Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR's efficiency and effectiveness in speeding up development of AI agents.

### 摘要
面向AI智能体的大动作模型（LAMs）展现出巨大潜力，但由于高质量训练数据的需求（尤其是涉及规划、执行工具调用及反馈响应的多步骤任务）而面临挑战。为解决这些问题，我们提出LAM SIMULATOR——一个专为在线探索具反馈质量的智能体任务而设计的综合框架。该框架包含动态任务查询生成器、多样化工具集以及支持大语言模型（LLM）智能体调用工具并获取实时反馈的交互环境。该架构使LLM智能体能够自主探索和解决任务，促进针对给定任务的多路径解决方案发现。生成的动作轨迹数据随后被用于构建LAMs的高质量训练数据集。我们在主流智能体基准测试（ToolBench和CRMArena）上的实验表明：使用本框架自生成数据集训练的模型性能显著提升，较原始基线最高可获得49.3%的改进。LAM SIMULATOR在数据集创建过程中仅需极少量人工介入，凸显了其在加速AI智能体开发方面的效率与有效性。

---

## [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)

### Abstract
arXiv:2506.02454v1 Announce Type: cross 
Abstract: Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\% overall win rate over the baseline method.

### 摘要
可视化在概念与信息的有效传递中起着关键作用。随着推理与检索增强生成技术的进步，大语言模型（LLM）已能进行深度研究并生成综合性报告。然而现有深度研究框架主要集中于生成纯文本内容，对文本与可视化元素交替生成的研究仍显不足。这一新任务面临两大核心挑战：如何设计信息丰富的可视化图表，以及如何将其与文本报告有效整合。为此，我们提出可视化形式化描述（FDV）——一种结构化的图表文本表征方法，使LLM能够学习并生成多样化、高质量的可视化内容。基于此表征方法，我们构建了多模态深度研究框架（Multimodal DeepResearcher），将任务分解为四个阶段：（1）研究调研；（2）范例报告文本化；（3）规划；（4）多模态报告生成。为评估生成的多模态报告，我们开发了包含100个多样化主题输入和5项专用指标的MultimodalReportBench测评基准。跨模型与评估方法的广泛实验验证了该框架的有效性。值得注意的是，在采用相同Claude 3.7 Sonnet模型条件下，本框架相较基线方法实现了82%的综合胜率。

---

## [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)

### Abstract
arXiv:2506.02494v1 Announce Type: cross 
Abstract: Evaluation is important for multimodal generation tasks. With the rapid progress of MLLMs, there is growing interest in applying MLLMs to build general evaluation systems. However, existing work overlooks two aspects: (1) the development of evaluation capabilities for text-to-image (T2I) generation task, and (2) the incorporation of large-scale human evaluation data. In this paper, we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that combines evaluation data from both human and GPT. The corpus contains evaluation data across both image-to-text(I2T) and T2I generation tasks. Based on this corpus, we propose Data Selection and Balance, Mix-SFT training methods, and apply DPO to develop Minos, a multimodal evaluation model built upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among all open-source evaluation models of similar scale on the average of evaluation performance on all tasks, and outperforms all open-source and closed-source models on evaluation of T2I generation task. Extensive experiments demonstrate the importance of leveraging high-quality human evaluation data and jointly training on evaluation data from both I2T and T2I generation tasks.

### 摘要
评估对多模态生成任务至关重要。随着多模态大语言模型（MLLMs）的快速发展，利用MLLMs构建通用评估系统引起了广泛关注。然而，现有研究忽视了两个关键方面：（1）针对文本到图像（T2I）生成任务的评估能力开发；（2）大规模人工评估数据的整合。本文提出Minos-Corpus，一个结合人类与GPT评估数据的大规模多模态评估数据集，涵盖图像到文本（I2T）和T2I生成任务。基于该数据集，我们提出数据选择与平衡（Data Selection and Balance）、混合监督微调（Mix-SFT）训练方法，并应用直接偏好优化（DPO）开发了基于7B骨干网络的Minos多模态评估模型。Minos在所有任务评估性能的平均值上达到了同规模开源评估模型的最先进（SoTA）水平，并在T2I生成任务评估中超越了所有开源和闭源模型。大量实验证明，利用高质量人工评估数据以及联合训练I2T与T2I生成任务的评估数据具有重要性。

---

## [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)

### Abstract
arXiv:2506.02510v1 Announce Type: cross 
Abstract: Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called $\texttt&#123;M$^3$FinMeeting&#125;$, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, $\texttt&#123;M$^3$FinMeeting&#125;$ supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, $\texttt&#123;M$^3$FinMeeting&#125;$ includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of $\texttt&#123;M$^3$FinMeeting&#125;$ as a benchmark for assessing LLMs' financial meeting comprehension skills.

### 摘要
大语言模型（LLMs）的最新突破催生了金融领域性能评估的新基准。然而，当前金融基准通常依赖于新闻文章、财报或公告，难以捕捉金融会议的真实动态。为填补这一空白，我们提出名为$	exttt&#123;M$^3$FinMeeting&#125;$的新型基准——一个为金融会议理解设计的跨语言、跨行业、多任务数据集。首先，$	exttt&#123;M$^3$FinMeeting&#125;$支持英语、中文和日语，增强了对多元语言背景下金融讨论的理解能力；其次，该数据集覆盖全球行业分类标准（GICS）定义的各行业领域，确保基准涵盖广泛的金融活动；最后，$	exttt&#123;M$^3$FinMeeting&#125;$包含摘要生成、问答对抽取和问题回答三项任务，可更真实全面地评估理解能力。对七种主流LLMs的实验表明，即使最先进的长上下文模型仍有显著改进空间，这验证了$	exttt&#123;M$^3$FinMeeting&#125;$作为评估LLMs金融会议理解能力的基准有效性。

---

## [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)

### Abstract
arXiv:2506.02481v1 Announce Type: cross 
Abstract: Evaluations of LLMs' ethical risks and value inclinations often rely on short-form surveys and psychometric tests, yet real-world use involves long-form, open-ended responses -- leaving value-related risks and preferences in practical settings largely underexplored. In this work, we ask: Do value preferences inferred from short-form tests align with those expressed in long-form outputs? To address this question, we compare value preferences elicited from short-form reactions and long-form responses, varying the number of arguments in the latter to capture users' differing verbosity preferences. Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b), we find (1) a weak correlation between value preferences inferred from short-form and long-form responses across varying argument counts, and (2) similarly weak correlation between preferences derived from any two distinct long-form generation settings. (3) Alignment yields only modest gains in the consistency of value expression. Further, we examine how long-form generation attributes relate to value preferences, finding that argument specificity negatively correlates with preference strength, while representation across scenarios shows a positive correlation. Our findings underscore the need for more robust methods to ensure consistent value expression across diverse applications.

### 摘要
当前对大型语言模型（LLM）伦理风险和价值倾向的评估通常依赖简短问卷和心理测量测试，然而实际应用场景涉及长篇幅开放式回答——这使得实践中的价值相关风险与偏好仍存在较大研究空白。本研究提出核心问题：从简短测试推断的价值偏好是否与长篇幅输出中表达的偏好一致？为解决该问题，我们通过比较从短形式反应和长形式回答中提取的价值偏好（后者通过调整论证数量以捕捉用户不同的表达偏好），对五个LLM模型（llama3-8b、gemma2-9b、mistral-7b、qwen2-7b和olmo-7b）进行分析，发现：(1) 不同论证数量的长形式回答与短形式反应所推断的价值偏好相关性较弱；(2) 任意两种长形式生成设置下获得的偏好相关性同样较低；(3) 对齐训练仅能适度提升价值表达的一致性。进一步研究发现，长文本生成属性与价值偏好存在关联：论证具体性与偏好强度呈负相关，而场景覆盖度则呈现正相关。本研究结果强调需要开发更稳健的方法来确保多样化应用场景中价值表达的一致性。

---

## [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)

### Abstract
arXiv:2506.02515v1 Announce Type: cross 
Abstract: Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.

### 摘要
多步骤符号推理对于提升金融任务的下游性能至关重要。然而，目前缺乏系统性评估该能力的基准测试。现有数据集如FinQA和ConvFinQA仅监督最终数值答案，而未评估中间推理步骤。为此，我们推出了FinChain——首个专为可验证思维链（CoT）金融推理设计的符号化基准测试。该测试涵盖12个金融领域的54个主题，每个主题提供5个参数化模板，这些模板在推理复杂度和所需领域专业知识方面各不相同。每个数据集实例均包含可执行的Python跟踪记录，支持自动生成大量训练数据，并能轻松适配其他领域。我们还提出了ChainEval这一新指标，用于自动评估最终答案和中间推理过程。通过对30个大型语言模型进行基准测试，我们发现即使最先进的模型在多步骤金融推理方面仍有显著改进空间。FinChain的所有模板和评估指标均已开源，详见https://github.com/mbzuai-nlp/finchain。

---

## [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)

### Abstract
arXiv:2506.02544v1 Announce Type: cross 
Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to enhance Multimodal Large Language Models by incorporating externally retrieved multimodal knowledge, but it introduces two challenges: Parametric-Retrieved Knowledge Inconsistency (PRKI), where discrepancies between parametric and retrieved knowledge create uncertainty in determining reliability, and Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between visual and textual sources disrupts entity representation. To address these challenges, we propose \textbf&#123;C&#125;r\textbf&#123;o&#125;ss-source knowledge \textbf&#123;Re&#125;conciliation for \textbf&#123;M&#125;ulti\textbf&#123;M&#125;odal \textbf&#123;RAG&#125; (CoRe-MMRAG), a novel end-to-end framework that effectively reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage pipeline: it first generates an internal response from parametric knowledge, then selects the most relevant multimodal evidence via joint similarity assessment, generates an external response, and finally integrates both to produce a reliable answer. Additionally, a specialized training paradigm enhances knowledge source discrimination, multimodal integration, and unified answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG achieves substantial improvements over baseline methods, achieving 5.6\% and 9.3\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We release code and data at \href&#123;https://github.com/TyangJN/CoRe-MMRAG&#125;&#123;https://github.com/TyangJN/CoRe-MMRAG&#125;.

---

## [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/abs/2506.02541)

### Abstract
arXiv:2506.02541v1 Announce Type: cross 
Abstract: Machine unlearning is used to mitigate the privacy risks of Large Vision-Language Models (LVLMs) arising from training on large-scale web data. However, existing unlearning methods often fail to carefully select substitute outputs for forget targets, resulting in Unlearning Aftermaths-undesirable behaviors such as degenerate, hallucinated, or excessively refused responses. We highlight that, especially for generative LVLMs, it is crucial to consider the quality and informativeness of post-unlearning responses rather than relying solely on naive suppression. To address this, we introduce a new unlearning task for LVLMs that requires models to provide privacy-preserving yet informative and visually grounded responses. We also propose PUBG, a novel unlearning method that explicitly guides post-unlearning behavior toward a desirable output distribution. Experiments show that, while existing methods suffer from Unlearning Aftermaths despite successfully preventing privacy violations, PUBG effectively mitigates these issues, generating visually grounded and informative responses without privacy leakage for forgotten targets.

### 摘要
机器遗忘技术用于缓解大型视觉语言模型（LVLMs）因大规模网络数据训练而产生的隐私风险。然而，现有遗忘方法往往未能谨慎选择遗忘目标的替代输出，导致"遗忘后遗症"——如生成退化、幻觉或过度拒绝回应等不良行为。我们强调，特别是对于生成式LVLMs，必须关注遗忘后回应的质量与信息量，而非仅依赖简单抑制。为此，我们为LVLMs提出新型遗忘任务，要求模型在保护隐私的同时提供信息丰富且视觉可验证的回应。我们提出PUBG方法，通过显式引导遗忘后行为至理想输出分布。实验表明，现有方法虽能防止隐私泄露却存在遗忘后遗症，而PUBG在避免隐私泄漏的同时，能为遗忘目标生成视觉可验证且信息丰富的回应，有效缓解上述问题。

---

## [VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning](https://arxiv.org/abs/2506.02537)

### Abstract
arXiv:2506.02537v1 Announce Type: cross 
Abstract: Recent strides in multimodal large language models (MLLMs) have significantly advanced their performance in many reasoning tasks. However, Abstract Visual Reasoning (AVR) remains a critical challenge, primarily due to limitations in perceiving abstract graphics. To tackle this issue, we investigate the bottlenecks in current MLLMs and synthesize training data to improve their abstract visual perception. First, we propose VisuRiddles, a benchmark for AVR, featuring tasks meticulously constructed to assess models' reasoning capacities across five core dimensions and two high-level reasoning categories. Second, we introduce the Perceptual Riddle Synthesizer (PRS), an automated framework for generating riddles with fine-grained perceptual descriptions. PRS not only generates valuable training data for abstract graphics but also provides fine-grained perceptual description, crucially allowing for supervision over intermediate reasoning stages and thereby improving both training efficacy and model interpretability. Our extensive experimental results on VisuRiddles empirically validate that fine-grained visual perception is the principal bottleneck and our synthesis framework markedly enhances the performance of contemporary MLLMs on these challenging tasks. Our code and dataset will be released at https://github.com/yh-hust/VisuRiddles

### 摘要
多模态大语言模型（MLLMs）的最新进展显著提升了其在诸多推理任务中的表现。然而，抽象视觉推理（AVR）仍是关键挑战，主要源于模型对抽象图形感知能力的局限。为应对该问题，我们系统分析了当前MLLMs的瓶颈，并通过合成训练数据提升其抽象视觉感知能力。首先，我们提出VisuRiddles基准测试，该基准包含精心设计的任务，用于评估模型在五个核心维度和两类高阶推理范畴的推理能力。其次，我们开发了感知谜题合成器（PRS）——一个能自动生成附带细粒度感知描述的谜题框架。PRS不仅能生成有价值的抽象图形训练数据，还可提供细粒度感知描述，其关键优势在于能对中间推理阶段实施监督，从而同步提升训练效能与模型可解释性。在VisuRiddles上的大量实验结果表明，细粒度视觉感知是主要瓶颈，而我们的合成框架显著提升了当代MLLMs在这些挑战性任务中的表现。代码与数据集将在https://github.com/yh-hust/VisuRiddles发布。

---

## [Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs](https://arxiv.org/abs/2506.02529)

### Abstract
arXiv:2506.02529v1 Announce Type: cross 
Abstract: Web applications are critical to modern software ecosystems, yet ensuring their reliability remains challenging due to the complexity and dynamic nature of web interfaces. Recent advances in large language models (LLMs) have shown promise in automating complex tasks, but limitations persist in handling dynamic navigation flows and complex form interactions. This paper presents an automated system for generating test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, the system employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation. Key contributions include: (1) a novel integration of graph structures and LLMs for site navigation testing, (2) a state graph-based approach for automating form-filling test cases, and (3) a comprehensive dataset for evaluating form-interaction testing. Experimental results demonstrate the system's effectiveness in improving test coverage and robustness, advancing the state of web application testing.

### 摘要
Web应用程序是现代软件生态系统的关键组成部分，但由于其界面的复杂性和动态特性，确保其可靠性仍具挑战性。大型语言模型（LLMs）的最新进展在自动化复杂任务方面展现出潜力，但在处理动态导航流程和复杂表单交互方面仍存在局限。本文提出了一种自动化系统，用于生成Web应用程序测试中两个关键环节的测试用例：站点导航和表单填写。针对站点导航，该系统采用屏幕转换图和LLMs来建模导航流程并生成测试场景；针对表单填写，则利用状态图处理条件表单并自动化生成Selenium脚本。主要贡献包括：（1）将图结构与LLMs创新性结合用于站点导航测试；（2）基于状态图的表单填写测试用例自动化方法；（3）用于评估表单交互测试的完整数据集。实验结果证明，该系统在提升测试覆盖率和鲁棒性方面具有显著效果，推动了Web应用程序测试的技术发展。

---

## [Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025](https://arxiv.org/abs/2506.02550)

### Abstract
arXiv:2506.02550v1 Announce Type: cross 
Abstract: In this report, we present a novel three-stage framework developed for the Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in foundation models, our method consists of three stages: feature extraction, action recognition, and long-term action anticipation. First, visual features are extracted using a high-performance visual encoder. The features are then fed into a Transformer to predict verbs and nouns, with a verb-noun co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the predicted verb-noun pairs are formatted as textual prompts and input into a fine-tuned large language model (LLM) to anticipate future action sequences. Our framework achieves first place in this challenge at CVPR 2025, establishing a new state-of-the-art in long-term action prediction. Our code will be released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.

### 摘要
在本报告中，我们提出了一种针对Ego4D长期动作预测（LTA）任务开发的新型三阶段框架。受基础模型最新进展的启发，我们的方法包含三个阶段：特征提取、动作识别和长期动作预测。首先，使用高性能视觉编码器提取视觉特征；随后将这些特征输入Transformer模型以预测动词和名词，并通过引入动词-名词共现矩阵来提高识别准确率；最后，将预测的动词-名词对格式化为文本提示，输入微调后的大语言模型（LLM）来预测未来动作序列。本框架在CVPR 2025竞赛中荣获第一名，创造了长期动作预测领域的最新性能标杆。代码将在https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025发布。

---

## [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)

### Abstract
arXiv:2506.02561v1 Announce Type: cross 
Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their substantial model sizes often require substantial computational resources. To preserve computing resources and accelerate inference speed, it is crucial to prune redundant parameters, especially for experienced users who often need compact expert models tailored to specific downstream scenarios. However, most existing pruning methods focus on preserving the model's general capabilities, often requiring extensive post-training or suffering from degraded performance due to coarse-grained pruning. In this work, we design a $\underline&#123;Cus&#125;$tom $\underline&#123;Prun&#125;$ing method ($\texttt&#123;Cus-Prun&#125;$) to prune a large general model into a smaller lightweight expert model, which is positioned along the "language", "domain" and "task" dimensions. By identifying and pruning irrelevant neurons of each dimension, $\texttt&#123;Cus-Prun&#125;$ creates expert models without any post-training. Our experiments demonstrate that $\texttt&#123;Cus-Prun&#125;$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes.

### 摘要
大型语言模型（LLMs）在自然语言处理领域引发了革命性变革，但其庞大的模型规模通常需要大量计算资源。为节约计算资源并加速推理速度，修剪冗余参数至关重要——这对需要为特定下游场景定制紧凑专家模型的经验用户尤为关键。然而现有修剪方法大多聚焦于保留模型的通用能力，往往需要大量后训练或因粗粒度修剪导致性能下降。本研究设计了一种定制化修剪方法（Cus-Prun），将通用大模型沿"语言"、"领域"和"任务"三个维度修剪为轻量级专家模型。通过识别并修剪各维度无关神经元，Cus-Prun无需任何后训练即可生成专家模型。实验表明，Cus-Prun在不同模型家族和尺寸的各类模型中均优于其他方法，在专家能力和通用能力上均实现最小损失。

---

## [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)

### Abstract
arXiv:2506.02553v1 Announce Type: cross 
Abstract: We study a common challenge in reinforcement learning for large language models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e., intermediate token generations) receive zero task-specific immediate reward, while only the final token receives a reward for the entire response. This assumption arises frequently in practice, as precise token-level rewards are often difficult or infeasible to obtain in LLM applications. In this work, we provide a unifying theoretical perspective. We introduce the Trajectory Policy Gradient Theorem, which shows that the policy gradient based on true, unknown token-level rewards can be unbiasedly estimated using only a response-level reward model, regardless of whether the Zero-Reward Assumption holds or not, for algorithms in the REINFORCE and Actor-Critic families. This result reveals that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess the capacity to model token-level reward signals, offering a theoretical justification for response-level reward approaches. Our findings pave the way for more practical, efficient LLM fine-tuning, allowing developers to treat training algorithms as black boxes and focus on improving the response-level reward model with auxiliary sub-models. We also offer a detailed analysis of popular RL and non-RL methods, comparing their theoretical foundations and practical advantages across common LLM tasks. Finally, we propose a new algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically grounded method that is simpler than PPO, matches GRPO in memory efficiency, and holds promise for broad applicability.

### 摘要
我们研究了大语言模型（LLM）强化学习中的一个常见挑战：零奖励假设。该假设认为非终止动作（即中间令牌生成）不获得任务特定的即时奖励，而仅最终令牌获得整个响应的奖励。这一假设在实践中频繁出现，因为在LLM应用中通常难以获取精确的令牌级奖励。本研究提出了统一的理论视角，引入轨迹策略梯度定理，证明基于真实但未知的令牌级奖励的策略梯度，可以仅通过响应级奖励模型进行无偏估计——无论零奖励假设是否成立，该结论均适用于REINFORCE和Actor-Critic算法家族。这一结果表明PPO、GRPO、ReMax和RLOO等广泛使用的方法本质上具备建模令牌级奖励信号的能力，为响应级奖励方法提供了理论依据。我们的发现为更实用高效的LLM微调开辟了新路径，使开发者能够将训练算法视为黑箱，专注于通过辅助子模型改进响应级奖励模型。我们还对主流RL与非RL方法进行了详细分析，比较它们在常见LLM任务中的理论基础和实践优势。最后提出新算法：令牌强化策略优化（TRePO），这种理论完备的方法比PPO更简洁，内存效率与GRPO相当，具有广泛的应用前景。

---

## [CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale](https://arxiv.org/abs/2506.02548)

### Abstract
arXiv:2506.02548v1 Announce Type: cross 
Abstract: Large language model (LLM) agents are becoming increasingly skilled at handling cybersecurity tasks autonomously. Thoroughly assessing their cybersecurity capabilities is critical and urgent, given the high stakes in this domain. However, existing benchmarks fall short, often failing to capture real-world scenarios or being limited in scope. To address this gap, we introduce CyberGym, a large-scale and high-quality cybersecurity evaluation framework featuring 1,507 real-world vulnerabilities found and patched across 188 large software projects. While it includes tasks of various settings, CyberGym primarily focuses on the generation of proof-of-concept (PoC) tests for vulnerability reproduction, based on text descriptions and corresponding source repositories. Solving this task is particularly challenging, as it requires comprehensive reasoning across entire codebases to locate relevant code fragments and produce effective PoCs that accurately trigger the target vulnerability starting from the program's entry point. Our evaluation across 4 state-of-the-art agent frameworks and 9 LLMs reveals that even the best combination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9% reproduction success rate, mainly on simpler cases. Beyond reproducing historical vulnerabilities, we find that PoCs generated by LLM agents can reveal new vulnerabilities, identifying 15 zero-days affecting the latest versions of the software projects.

### 摘要
大型语言模型（LLM）代理在自主处理网络安全任务方面正变得日益熟练。鉴于该领域的高风险性，全面评估其网络安全能力至关重要且迫在眉睫。然而现有基准测试存在不足，往往无法反映真实场景或覆盖范围有限。为此，我们提出CyberGym——一个基于188个大型软件项目中1507个已发现并修复的真实漏洞构建的大规模高质量网络安全评估框架。该框架虽包含多种任务设置，但主要聚焦于根据文本描述及对应源码仓库生成漏洞复现的概念验证（PoC）测试。解决该任务具有特殊挑战性，因其需要跨整个代码库进行综合推理以定位相关代码片段，并从程序入口点开始生成能准确触发目标漏洞的有效PoC。通过对4种最先进代理框架和9个LLM的评估发现，即使最优组合（OpenHands与Claude-3.7-Sonnet）也仅达到11.9%的复现成功率（主要集中在简单案例上）。除复现历史漏洞外，我们发现LLM代理生成的PoC能揭示新漏洞，共识别出15个影响软件项目最新版本的零日漏洞。

---

## [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](https://arxiv.org/abs/2506.02572)

### Abstract
arXiv:2506.02572v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have emerged as a pivotal research area, yet the attention module remains a critical bottleneck in LLM inference, even with techniques like KVCache to mitigate redundant computations. While various top-$k$ attention mechanisms have been proposed to accelerate LLM inference by exploiting the inherent sparsity of attention, they often struggled to strike a balance between efficiency and accuracy. In this paper, we introduce HATA (Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates low-overhead learning-to-hash techniques into the Top-$k$ attention process. Different from the existing top-k attention methods which are devoted to seeking an absolute estimation of qk score, typically with a great cost, HATA maps queries and keys into binary hash codes, and acquires the relative qk score order with a quite low cost, which is sufficient for realizing top-k attention. Extensive experiments demonstrate that HATA achieves up to 7.2$\times$ speedup compared to vanilla full attention while maintaining model accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention methods in both accuracy and efficiency across multiple mainstream LLM models and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.

### 摘要
大型语言模型（LLMs）已成为关键研究领域，但注意力模块仍是LLM推理中的主要瓶颈，即便采用KVCache等技术来减少冗余计算。尽管已有多种top-$k$注意力机制通过利用注意力的固有稀疏性来加速LLM推理，但这些方法往往难以在效率与准确性之间取得平衡。本文提出HATA（哈希感知Top-$k$注意力），这是一种将低开销哈希学习技术系统集成到Top-$k$注意力过程的新方法。不同于现有top-k注意力方法需以较高成本寻求qk得分的绝对估计，HATA将查询和键映射为二进制哈希码，以极低成本获取qk得分的相对顺序，这足以实现top-k注意力。大量实验表明，HATA在保持模型精度的同时，相比原始全注意力机制最高可获得7.2$	imes$加速。此外，在多个主流LLM模型和多样化任务中，HATA在精度和效率上均优于最先进的top-$k$注意力方法。HATA已开源：https://github.com/gpzlx1/HATA。

---

## [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)

### Abstract
arXiv:2506.02589v1 Announce Type: cross 
Abstract: This paper addresses the challenge of Named Entity Recognition (NER) for person names within the specialized domain of Russian news texts concerning cultural events. The study utilizes the unique SPbLitGuide dataset, a collection of event announcements from Saint Petersburg spanning 1999 to 2019. A comparative evaluation of diverse NER models is presented, encompassing established transformer-based architectures such as DeepPavlov, RoBERTa, and SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4, and GPT-4o. Key findings highlight the superior performance of GPT-4o when provided with specific prompting for JSON output, achieving an F1 score of 0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The research contributes to a deeper understanding of current NER model capabilities and limitations when applied to morphologically rich languages like Russian within the cultural heritage domain, offering insights for researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025) achieves F1=0.94 for both simple and structured prompts, demonstrating rapid progress across model families and simplified deployment requirements.

### 摘要
本文针对俄罗斯文化事件新闻文本这一特定领域中人名命名实体识别（NER）的挑战展开研究。研究采用独特的SPbLitGuide数据集（1999-2019年圣彼得堡事件公告汇编），对多种NER模型进行了比较评估，包括DeepPavlov、RoBERTa和SpaCy等成熟的基于Transformer架构的模型，以及GPT-3.5、GPT-4和GPT-4o等最新大语言模型（LLM）。关键发现表明，当提供特定的JSON输出提示时，GPT-4o以0.93的F1分数表现出最优性能，而GPT-4则达到0.99的最高精确率。本研究深化了学界对当前NER模型在文化遗产领域处理俄语等形态丰富语言时的能力与局限的认知，为研究者和实践者提供了重要参考。后续使用GPT-4.1（2025年4月版）的评估显示，无论是简单提示还是结构化提示均获得0.94的F1分数，这表明不同模型系列均取得快速进展且部署要求趋于简化。

---

## [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)

### Abstract
arXiv:2506.02596v1 Announce Type: cross 
Abstract: Chinese essay writing and its evaluation are critical in educational contexts, yet the capabilities of Large Language Models (LLMs) in this domain remain largely underexplored. Existing benchmarks often rely on coarse-grained text quality metrics, largely overlooking the structural and rhetorical complexities of Chinese essays, particularly across diverse genres. To address this gap, we propose \benchName, a multi-genre benchmark specifically designed for Chinese essay writing across four major genres: Argumentative, Narrative, Descriptive, and Expository. We curate and refine a total of 728 real-world prompts to ensure authenticity and meticulously categorize them into the \textit&#123;Open-Ended&#125; and \textit&#123;Constrained&#125; sets to capture diverse writing scenarios. To reliably evaluate generated essays, we develop a fine-grained, genre-specific scoring framework that hierarchically aggregates scores. We further validate our evaluation protocol through a comprehensive human agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their strengths and limitations across genres and instruction types. With \benchName, we aim to advance LLM-based Chinese essay evaluation and inspire future research on improving essay generation in educational settings.

### 摘要
中文写作及其评价在教育领域至关重要，然而大型语言模型（LLMs）在该领域的能力仍未得到充分探索。现有基准通常依赖于粗粒度的文本质量指标，很大程度上忽视了中文作文的结构与修辞复杂性，尤其是跨不同文类的表现。为填补这一空白，我们提出\benchName——一个专门针对四大文类（议论文、记叙文、说明文、描写文）构建的多文类中文写作基准。我们筛选并精修了总计728个真实场景下的写作提示，确保其真实性，并细致划分为\textit&#123;开放式&#125;与\textit&#123;约束式&#125;两组以涵盖多样化的写作情境。为可靠评估生成文本，我们开发了细粒度、分文类的分层聚合评分框架，并通过全面的人工一致性研究验证了评估方案的有效性。最终，我们对15个大型LLM进行了基准测试，系统分析了其在不同文类与指令类型中的优势与局限。通过\benchName，我们旨在推动基于LLM的中文作文评估发展，并为教育场景下文本生成能力的提升提供研究启示。

---

## [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)

### Abstract
arXiv:2506.02678v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.

### 摘要
大语言模型（LLMs）近期通过强化学习和扩展的思维链（CoT）技术取得了显著进展。然而，如何实现高效的语言推理——尤其是在生成长度极大输出时的推理过程——正日益受到研究界的关注。本研究提出了一种基于动态比例的训练框架，该框架无需复杂的数据标注或多模型插值。我们通过持续平衡模型的系统1与系统2数据权重，在保留模型推理能力的同时消除冗余的推理过程。我们在DeepSeek-R1-Distill-7B和DeepSeek-R1-Distill-14B模型上验证了该方法，并在不同难度级别的多样化基准测试中进行了评估。实验表明，我们的方法在保持推理准确性的同时，显著减少了近40%的输出标记数量。相关代码和数据即将公开。

---

## [Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models](https://arxiv.org/abs/2506.02615)

### Abstract
arXiv:2506.02615v1 Announce Type: cross 
Abstract: In this paper, we present a hierarchical question-answering (QA) approach for scene understanding in autonomous vehicles, balancing cost-efficiency with detailed visual interpretation. The method fine-tunes a compact vision-language model (VLM) on a custom dataset specific to the geographical area in which the vehicle operates to capture key driving-related visual elements. At the inference stage, the hierarchical QA strategy decomposes the scene understanding task into high-level and detailed sub-questions. Instead of generating lengthy descriptions, the VLM navigates a structured question tree, where answering high-level questions (e.g., "Is it possible for the ego vehicle to turn left at the intersection?") triggers more detailed sub-questions (e.g., "Is there a vehicle approaching the intersection from the opposite direction?"). To optimize inference time, questions are dynamically skipped based on previous answers, minimizing computational overhead. The extracted answers are then synthesized using handcrafted templates to ensure coherent, contextually accurate scene descriptions. We evaluate the proposed approach on the custom dataset using GPT reference-free scoring, demonstrating its competitiveness with state-of-the-art methods like GPT-4o in capturing key scene details while achieving significantly lower inference time. Moreover, qualitative results from real-time deployment highlight the proposed approach's capacity to capture key driving elements with minimal latency.

### 摘要
本文提出了一种用于自动驾驶车辆场景理解的分层问答(QA)方法，在成本效益与详细视觉解读之间实现平衡。该方法通过在车辆运行特定地理区域的自定义数据集上微调紧凑型视觉语言模型(VLM)，以捕捉与驾驶相关的关键视觉元素。在推理阶段，分层QA策略将场景理解任务分解为高层级和详细子问题。VLM不再生成冗长描述，而是导航结构化问题树——当高层级问题(如"本车是否可以在该路口左转？")得到解答后，将触发更详细的子问题(如"是否有车辆从对向接近路口？")。为优化推理时间，系统会根据先前答案动态跳过问题，最大限度减少计算开销。最终通过手工设计的模板合成提取的答案，确保生成连贯且上下文准确的场景描述。我们在自定义数据集上采用GPT无参考评分进行评估，结果表明该方法在捕捉关键场景细节方面与GPT-4o等最先进方法具有竞争力，同时显著降低推理时间。实时部署的定性结果进一步表明，该方法能够以极低延迟捕捉关键驾驶要素。

---

## [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)

### Abstract
arXiv:2506.02672v1 Announce Type: cross 
Abstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large language models (LLMs) on their learning capability and efficiency in challenging tasks, a critical, yet underexplored aspect of model potential. EvaLearn contains 648 challenging problems across six task types, grouped into 182 sequences, each sequence dedicated to one task type. Diverging from most existing benchmarks that evaluate models in parallel, EvaLearn requires models to solve problems sequentially, allowing them to leverage the experience gained from previous solutions. EvaLearn provides five comprehensive automated metrics to evaluate models and quantify their learning capability and efficiency. We extensively benchmark nine frontier models and observe varied performance profiles: some models, such as Claude-3.7-sonnet, start with moderate initial performance but exhibit strong learning ability, while some models struggle to benefit from experience and may even show negative transfer. Moreover, we investigate model performance under two learning settings and find that instance-level rubrics and teacher-model feedback further facilitate model learning. Importantly, we observe that current LLMs with stronger static abilities do not show a clear advantage in learning capability across all tasks, highlighting that EvaLearn evaluates a new dimension of model performance. We hope EvaLearn provides a novel evaluation perspective for assessing LLM potential and understanding the gap between models and human capabilities, promoting the development of deeper and more dynamic evaluation approaches. All datasets, the automatic evaluation framework, and the results studied in this paper are available at the GitHub repository.

### 摘要
我们推出EvaLearn这一开创性基准测试，旨在评估大语言模型（LLM）在挑战性任务中的学习能力与效率——这是模型潜力中至关重要却尚未充分探索的维度。该基准包含六大任务类型的648个挑战性问题，划分为182个任务序列，每个序列专注于单一任务类型。与多数现有并行评估模型的基准不同，EvaLearn要求模型按序解决问题，使其能够利用先前解决方案获得的经验。我们提供五项自动化评估指标，用以量化模型的学习能力与效率。通过对九个前沿模型的广泛测试，我们观察到差异化的表现特征：部分模型（如Claude-3.7-sonnet）初始表现中等但展现出强劲的学习能力，而另一些模型则难以从经验中获益甚至出现负迁移现象。此外，我们在两种学习设定下检验模型表现，发现实例级评分标准和教师模型反馈能进一步促进模型学习。值得注意的是，当前静态能力更强的LLM并非在所有任务中都显示出学习能力的明显优势，这表明EvaLearn评估的是模型性能的新维度。我们希望该基准能为评估LLM潜力及理解模型与人类能力差距提供全新视角，推动更深层次、更动态的评估方法发展。本文研究的所有数据集、自动评估框架及结果均公开于GitHub仓库。

---

## [Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions](https://arxiv.org/abs/2506.02742)

### Abstract
arXiv:2506.02742v1 Announce Type: cross 
Abstract: Existing expressive text-to-speech (TTS) systems primarily model a limited set of categorical emotions, whereas human conversations extend far beyond these predefined emotions, making it essential to explore more diverse emotional speech generation for more natural interactions. To bridge this gap, this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate unseen emotional speech via emotion-guided prompt learning. PUE is trained utilizing an LLM-TTS architecture to ensure emotional consistency between categorical emotion-relevant prompts and emotional speech, allowing the model to quantitatively capture different emotion weightings per utterance. During inference, mixed emotional speech can be generated by flexibly adjusting emotion proportions and leveraging LLM contextual knowledge, enabling the model to quantify different emotional styles. Our proposed PUE successfully facilitates expressive speech synthesis of unseen emotions in a zero-shot setting.

### 摘要
现有表现力文本转语音（TTS）系统主要建模有限数量的分类情感，而人类对话所涵盖的情感范围远超这些预定义类别，因此探索更具多样性的情感语音生成对实现自然交互至关重要。为弥补这一差距，本文提出一种新颖的提示-未见情感（PUE）方法，通过情感引导的提示学习生成未见情感语音。PUE采用LLM-TTS架构进行训练，确保分类情感相关提示与情感语音之间的情感一致性，使模型能够定量捕捉每段话语的不同情感权重。在推理阶段，通过灵活调整情感比例并利用LLM上下文知识，可生成混合情感语音，从而量化不同情感风格。我们提出的PUE方法成功实现了零样本设置下未见情感的表现力语音合成。

---

## [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)

### Abstract
arXiv:2506.02758v1 Announce Type: cross 
Abstract: Vocabulary use is a fundamental aspect of second language (L2) proficiency. To date, its assessment by automated systems has typically examined the context-independent, or part-of-speech (PoS) related use of words. This paper introduces a novel approach to enable fine-grained vocabulary evaluation exploiting the precise use of words within a sentence. The scheme combines large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP is a standard lexical resource that enables in-context vocabulary use to be linked with proficiency level. We evaluate the ability of LLMs to assign proficiency levels to individual words as they appear in L2 learner writing, addressing key challenges such as polysemy, contextual variation, and multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to exploit additional semantic information that yields improved performance. We also explore correlations between word-level proficiency and essay-level proficiency. Finally, the approach is applied to examine the consistency of the EVP proficiency levels. Results show that LLMs are well-suited for the task of vocabulary assessment.

### 摘要
词汇使用是第二语言（L2） proficiency 的核心要素。迄今为止，自动化系统的词汇评估通常关注与上下文无关或词性（PoS）相关的词汇使用。本文提出一种创新方法，通过分析单词在句子中的精确运用实现细粒度词汇评估。该方案结合大型语言模型（LLMs）与《英语词汇大纲》（EVP）——后者是一种能将语境化词汇使用与 proficiency 等级相关联的标准词汇资源。我们评估了LLMs对二语学习者写作中单词 proficiency 等级的判定能力，解决了多义性、语境变异和多词表达等关键挑战。通过将LLMs与基于词性的基线模型对比，发现LLMs能利用额外的语义信息提升性能。同时探究了词汇级 proficiency 与篇章级 proficiency 的关联性。最后应用该方法检验EVP proficiency 等级的一致性。结果表明，LLMs非常适用于词汇评估任务。

---

## [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)

### Abstract
arXiv:2506.02726v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) struggle with accuracy, domain-specific reasoning, and interpretability in vertical domains. Traditional preference alignment methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) often overlook the underlying knowledge sources and reasoning logic. This paper introduces RACE-Align (Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel framework designed to address these limitations. RACE-Align systematically constructs a binary preference dataset incorporating external knowledge support and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO algorithm. The core innovation lies in its preference data construction strategy: it integrates AI-driven retrieval for factual grounding, enhancing knowledgeability and accuracy, and emphasizes the optimization of domain-specific CoT, treating the reasoning process itself as a key preference dimension. A multi-stage, AI-driven refinement pipeline cost-effectively generates these preference pairs. Experimental validation in Traditional Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that RACE-Align significantly outperforms the original base model and a model fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed across multiple dimensions, including answer accuracy, information richness, application of TCM thinking patterns, logicality and depth of reasoning, and interpretability. These findings suggest RACE-Align offers an effective pathway to enhance LLMs' knowledge application, reasoning reliability, and process transparency in complex vertical domains.

### 摘要
大型语言模型（LLMs）在垂直领域中面临准确性、领域特定推理和可解释性方面的挑战。传统偏好对齐方法如基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO）往往忽视了潜在知识来源和推理逻辑。本文提出RACE-Align（检索增强与思维链强化对齐）框架，通过系统构建包含外部知识支持和显式思维链（CoT）推理的二元偏好数据集，并采用DPO算法对齐LLMs以解决上述局限。该框架的核心创新在于其偏好数据构建策略：整合AI驱动的检索机制实现事实锚定以增强知识性与准确性，同时将领域特定思维链优化作为关键偏好维度，强调推理过程本身的优化。通过多阶段AI驱动的精炼流程，该方法实现了高性价比的偏好对生成。以Qwen3-1.7B为基础模型在中医（TCM）领域的实验验证表明，RACE-Align显著优于原始基础模型及仅采用监督微调（SFT）的模型，在回答准确性、信息丰富度、中医思维模式应用、推理逻辑性与深度以及可解释性等多个维度均展现提升。这些发现证明RACE-Align为增强LLMs在复杂垂直领域中的知识应用、推理可靠性和过程透明度提供了有效路径。

---

## [Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2506.02718)

### Abstract
arXiv:2506.02718v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success across diverse natural language processing tasks, yet their deployment in real-world applications is hindered by fixed knowledge cutoffs and difficulties in generating controllable, accurate outputs in a single inference. Multi-agent systems (MAS) built from specialized LLM agents offer a promising solution, enabling dynamic collaboration and iterative reasoning. However, optimizing these systems remains a challenge, as conventional methods such as prompt engineering and supervised fine-tuning entail high engineering overhead and limited adaptability. Reinforcement learning (RL), particularly multi-agent reinforcement learning (MARL), provides a scalable framework by refining agent policies based on system-level feedback. Nevertheless, existing MARL algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on Critic networks, which can cause training instability and increase computational burden. To address these limitations and target the prototypical Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy updates by estimating relative reward advantages across heterogeneous groups of rollouts. MHGPO eliminates the need for Critic networks, enhancing stability and reducing computational overhead. Additionally, we introduce three group rollout sampling strategies that trade off between efficiency and effectiveness. Experiments on a multi-agent LLM-based search system demonstrate that MHGPO consistently outperforms MAPPO in both task performance and computational efficiency, without requiring warm-up, underscoring its potential for stable and scalable optimization of complex LLM-based MAS.

### 摘要
大型语言模型（LLMs）在多样化的自然语言处理任务中取得了显著成功，但其在实际应用中的部署仍受限于固定的知识截止点以及单次推理中难以生成可控、准确输出的问题。由专业化LLM智能体构建的多智能体系统（MAS）提供了一种颇具前景的解决方案，能够实现动态协作与迭代推理。然而，优化这些系统仍存在挑战，因为传统方法如提示工程和监督微调需要高昂的工程开销且适应性有限。强化学习（RL），特别是多智能体强化学习（MARL），通过基于系统级反馈优化智能体策略，提供了一个可扩展的框架。但现有MARL算法（如多智能体近端策略优化MAPPO）依赖Critic网络，可能导致训练不稳定并增加计算负担。针对这些局限性并以典型的多智能体搜索系统（MASS）为研究对象，我们提出多智能体异构群组策略优化（MHGPO）——一种无需Critic网络的新型算法，该算法通过估计异构 rollout 群组间的相对奖励优势来指导策略更新。MHGPO消除了对Critic网络的需求，从而提升稳定性并降低计算开销。此外，我们提出了三种在效率与效果间权衡的群组rollout抽样策略。基于多智能体LLM搜索系统的实验表明，MHGPO在任务性能和计算效率上均持续优于MAPPO，且无需预热阶段，这凸显了其在复杂LLM-based MAS稳定可扩展优化方面的潜力。

---

## [CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the Edge](https://arxiv.org/abs/2506.02847)

### Abstract
arXiv:2506.02847v1 Announce Type: cross 
Abstract: Deploying large language models (LLMs) on edge devices is crucial for delivering fast responses and ensuring data privacy. However, the limited storage, weight, and power of edge devices make it difficult to deploy LLM-powered applications. These devices must balance latency requirements with energy consumption and model accuracy. In this paper, we first quantify the challenges of deploying LLMs on off-the-shelf edge devices and then we present CLONE, an in-depth algorithm-hardware co-design at both the model- and system-level that intelligently integrates real-time, energy optimization while maintaining robust generality. In order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize in a 28nm scalable hardware accelerator system. We implement and extensively evaluate CLONE on two off-the-shelf edge platforms. Experiments show that CLONE effectively accelerates the inference process up to 11.92x, and saves energy up to 7.36x, while maintaining high-generation.

### 摘要
在边缘设备上部署大型语言模型（LLM）对于实现快速响应和保障数据隐私至关重要。然而，边缘设备有限的存储空间、重量和功耗使得LLM应用部署面临挑战。这些设备需要在延迟要求、能耗与模型精度之间取得平衡。本文首先量化了在商用边缘设备上部署LLM的难点，随后提出CLONE——一种在模型层和系统层进行深度算法-硬件协同设计的方案，该方案在保持强大通用性的同时，智能整合了实时能耗优化技术。为了在常开型和中间型边缘计算场景中最大化这些算法的协同效益，我们专门设计了一个28纳米可扩展硬件加速系统。我们在两个商用边缘平台上实现并全面评估了CLONE方案。实验表明，CLONE能有效将推理过程加速最高达11.92倍，节能最高达7.36倍，同时保持高质量生成能力。

---

## [Rethinking the effects of data contamination in Code Intelligence](https://arxiv.org/abs/2506.02791)

### Abstract
arXiv:2506.02791v1 Announce Type: cross 
Abstract: In recent years, code intelligence has gained increasing importance in the field of automated software engineering. Meanwhile, the widespread adoption of Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised concerns regarding data contamination and its potential impact on model performance evaluation. This paper presents a systematic empirical study to investigate the fine-grained data contamination on code intelligence tasks. Our study involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs, namely LLaMA and StarCoder, covering three major tasks: code translation, code generation, and code summarization. We categorize contamination scenarios into four types according to the code intelligence practice, namely input-only, output-only, unpaired, and paired contamination settings, and construct corresponding experimental and control groups for exploration.
  Experimental results show that, under the pre-training, fine-tuning, and inference paradigm adopted by PLMs, even deliberately injecting paired contamination does not lead to significant performance overestimation. But direct inference or small-scale fine-tuning uncovers the contamination effects. In contrast, LLMs with pre-training and inference paradigm are significantly affected by the paired contamination. Apart from the above, other contamination scenarios have no impact on both PLMs and LLMs. Our findings challenge the conventional belief that contamination inevitably leads to performance overestimation, providing new insights into the evaluation and deployment of code intelligence models.

### 摘要
近年来，代码智能在自动化软件工程领域的重要性日益凸显。与此同时，预训练语言模型（PLMs）和大语言模型（LLMs）的广泛应用引发了关于数据污染及其对模型性能评估潜在影响的担忧。本文通过系统化的实证研究，探究代码智能任务中的细粒度数据污染问题。我们选取了具有代表性的PLMs（RoBERTa和GPT-2）与LLMs（LLaMA和StarCoder）作为研究对象，涵盖代码翻译、代码生成和代码摘要三大任务。根据代码智能实践将污染场景划分为四类：仅输入污染、仅输出污染、非配对污染和配对污染，并构建相应实验组与对照组进行探究。

实验结果表明，在PLMs采用的预训练-微调-推理范式下，即使刻意注入配对污染也不会导致性能被显著高估。但直接推理或小规模微调会暴露污染效应。相比之下，采用预训练-推理范式的LLMs会显著受到配对污染的影响。除上述情况外，其他污染场景对PLMs和LLMs均无影响。我们的研究发现挑战了"污染必然导致性能高估"的传统认知，为代码智能模型的评估与部署提供了新见解。

---

## [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/abs/2506.02890)

### Abstract
arXiv:2506.02890v1 Announce Type: cross 
Abstract: Mixture of Experts (MoE) architectures have emerged as pivotal for scaling Large Language Models (LLMs) efficiently. Fine-grained MoE approaches - utilizing more numerous, smaller experts - have demonstrated potential in improving model convergence and quality. This work proposes a set of training recipes and provides a comprehensive empirical evaluation of fine-grained MoE, directly comparing its scaling properties against standard MoE configurations for models with up to 56B total (17B active) parameters. We investigate convergence speed, model performance on downstream benchmarks, and practical training considerations across various setups. Overall, at the largest scale we show that fine-grained MoE achieves better validation loss and higher accuracy across a set of downstream benchmarks. This study offers empirical grounding and practical insights for leveraging fine-grained MoE in the development of future large-scale models.

### 摘要
混合专家（MoE）架构已成为高效扩展大语言模型（LLM）的关键技术。细粒度MoE方法——采用更多数量、规模更小的专家——已展现出改善模型收敛性和质量的潜力。本研究提出一组训练方案，并对细粒度MoE进行了全面实证评估，在总参数量达560亿（活跃参数170亿）的模型上，直接比较其与标准MoE配置的扩展特性。我们探究了不同设置下的收敛速度、下游基准测试中的模型表现以及实际训练考量。总体而言，在最大规模实验中，细粒度MoE在验证损失和下游基准测试准确率上均表现更优。本研究为未来大规模模型中细粒度MoE的应用提供了实证依据和实践指导。

---

## [Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs](https://arxiv.org/abs/2506.02860)

### Abstract
arXiv:2506.02860v1 Announce Type: cross 
Abstract: Task planning under uncertainty is essential for home-service robots operating in the real world. Tasks involve ambiguous human instructions, hidden or unknown object locations, and open-vocabulary object types, leading to significant open-ended uncertainty and a boundlessly large planning space. To address these challenges, we propose Tru-POMDP, a planner that combines structured belief generation using Large Language Models (LLMs) with principled POMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH), which systematically queries an LLM to construct high-quality particle beliefs over possible world states and human goals. We further formulate an open-ended POMDP model that enables rigorous Bayesian belief tracking and efficient belief-space planning over these LLM-generated hypotheses. Experiments on complex object rearrangement tasks across diverse kitchen environments show that Tru-POMDP significantly outperforms state-of-the-art LLM-based and LLM-tree-search hybrid planners, achieving higher success rates with significantly better plans, stronger robustness to ambiguity and occlusion, and greater planning efficiency.

### 摘要
在不确定性下的任务规划对于在现实世界中运行的家用服务机器人至关重要。任务涉及模糊的人类指令、隐藏或未知的物体位置以及开放词汇的物体类型，导致显著的开放式不确定性和无限大的规划空间。为应对这些挑战，我们提出了Tru-POMDP规划器，该规划器结合了使用大型语言模型（LLMs）的结构化信念生成与原则性的POMDP规划。Tru-POMDP引入了一种层次化的假设树（TOH），该系统性地查询LLM以在可能的世界状态和人类目标上构建高质量的粒子信念。我们进一步制定了一个开放式POMDP模型，能够在这些LLM生成的假设上进行严格的贝叶斯信念跟踪和高效的信念空间规划。在多样化厨房环境中的复杂物体重排任务实验表明，Tru-POMDP显著优于最先进的基于LLM和LLM树搜索混合的规划器，以显著更好的规划实现了更高的成功率，对模糊性和遮挡具有更强的鲁棒性，并具备更高的规划效率。

---

## [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)

### Abstract
arXiv:2506.02878v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes. Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of Large Language Models on tasks requiring multi-step inference. This success has led to widespread claims of emergent reasoning capabilities in these models. In this paper, we present a theoretical counter-perspective: Chain-of-Thought (CoT) does not elicit genuine, abstract reasoning. Instead, we argue that Chain-of-Thought functions as a powerful structural constraint that guides Large Language Models to imitate the form of reasoning. By forcing the generation of intermediate steps, Chain-of-Thought leverages the model immense capacity for sequence prediction and pattern matching, effectively constraining its output to sequences that resemble coherent thought processes.

### 摘要
思维链（CoT）提示法已被证实能提升大语言模型在需要多步推理任务上的表现。这一成功使得学界普遍宣称这些模型具有涌现的推理能力。本文提出一个理论上的对立观点：思维链并不能引发真正抽象的推理行为。我们认为，思维链实质上是作为一种强大的结构性约束，引导大语言模型模仿推理的形式。通过强制生成中间步骤，思维链利用了模型在序列预测和模式匹配方面的强大能力，从而有效地将其输出约束为类似连贯思维过程的序列。思维链（CoT）提示法已被证实能提升大语言模型在需要多步推理任务上的表现。这一成功使得学界普遍宣称这些模型具有涌现的推理能力。本文提出一个理论上的对立观点：思维链并不能引发真正抽象的推理行为。我们认为，思维链实质上是作为一种强大的结构性约束，引导大语言模型模仿推理的形式。通过强制生成中间步骤，思维链利用了模型在序列预测和模式匹配方面的强大能力，从而有效地将其输出约束为类似连贯思维过程的序列。

---

## [ATAG: AI-Agent Application Threat Assessment with Attack Graphs](https://arxiv.org/abs/2506.02859)

### Abstract
arXiv:2506.02859v1 Announce Type: cross 
Abstract: Evaluating the security of multi-agent systems (MASs) powered by large language models (LLMs) is challenging, primarily because of the systems' complex internal dynamics and the evolving nature of LLM vulnerabilities. Traditional attack graph (AG) methods often lack the specific capabilities to model attacks on LLMs. This paper introduces AI-agent application Threat assessment with Attack Graphs (ATAG), a novel framework designed to systematically analyze the security risks associated with AI-agent applications. ATAG extends the MulVAL logic-based AG generation tool with custom facts and interaction rules to accurately represent AI-agent topologies, vulnerabilities, and attack scenarios. As part of this research, we also created the LLM vulnerability database (LVD) to initiate the process of standardizing LLM vulnerabilities documentation. To demonstrate ATAG's efficacy, we applied it to two multi-agent applications. Our case studies demonstrated the framework's ability to model and generate AGs for sophisticated, multi-step attack scenarios exploiting vulnerabilities such as prompt injection, excessive agency, sensitive information disclosure, and insecure output handling across interconnected agents. ATAG is an important step toward a robust methodology and toolset to help understand, visualize, and prioritize complex attack paths in multi-agent AI systems (MAASs). It facilitates proactive identification and mitigation of AI-agent threats in multi-agent applications.

### 摘要
评估由大型语言模型（LLM）驱动的多智能体系统（MAS）安全性具有挑战性，主要源于系统复杂的内部动态和LLM漏洞的持续演变特性。传统攻击图（AG）方法通常缺乏专门针对LLM攻击建模的能力。本文提出基于攻击图的AI智能体应用威胁评估框架（ATAG），该创新框架旨在系统分析AI智能体应用的安全风险。ATAG通过扩展MulVAL逻辑式攻击图生成工具，集成定制化事实与交互规则，精确表征AI智能体的拓扑结构、漏洞及攻击场景。作为研究组成部分，我们还构建了LLM漏洞数据库（LVD）以启动漏洞文档标准化进程。为验证ATAG有效性，我们在两个多智能体应用中实施验证。案例研究表明，该框架能够针对互联智能体间的提示注入、过度代理、敏感信息泄露及不安全输出处理等漏洞，构建复杂多阶段攻击场景的建模与攻击图生成。ATAG是建立健壮方法论与工具集的重要进展，有助于理解、可视化及优先处理多智能体AI系统（MAAS）中的复杂攻击路径，为多智能体应用中AI威胁的主动识别与缓解提供支持。

---

## [BNPO: Beta Normalization Policy Optimization](https://arxiv.org/abs/2506.02864)

### Abstract
arXiv:2506.02864v1 Announce Type: cross 
Abstract: Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that reinforcement learning with rule-based, binary-valued reward functions can significantly enhance the reasoning capabilities of large language models. These models primarily utilize REINFORCE-based policy optimization techniques, such as REINFORCE with baseline and group relative policy optimization (GRPO). However, a key limitation remains: current policy optimization methods either neglect reward normalization or employ static normalization strategies, which fail to adapt to the dynamic nature of policy updates during training. This may result in unstable gradient estimates and hinder training stability. To address this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel policy optimization method that adaptively normalizes rewards using a Beta distribution with dynamically updated parameters. BNPO aligns the normalization with the changing policy distribution, enabling more precise and lower-variance gradient estimation, which in turn promotes stable training dynamics. We provide theoretical analysis demonstrating BNPO's variance-reducing properties and show that it generalizes both REINFORCE and GRPO under binary-valued reward settings. Furthermore, we introduce an advantage decomposition mechanism to extend BNPO's applicability to more complex reward systems. Experimental results confirm that BNPO achieves state-of-the-art performance among policy optimization methods on reasoning tasks. The code is available at https://github.com/changyi7231/BNPO.

### 摘要
包括DeepSeek-R1和Kimi-k1.5在内的最新研究表明，基于规则的二值奖励函数强化学习能显著提升大语言模型的推理能力。这些模型主要采用基于REINFORCE的策略优化技术，如带基线的REINFORCE和组相对策略优化（GRPO）。然而当前方法存在关键局限：现有策略优化技术要么忽略奖励归一化，要么采用静态归一化策略，无法适应训练过程中策略更新的动态特性，这可能导致梯度估计不稳定并影响训练收敛性。为解决该问题，我们提出Beta归一化策略优化（BNPO），这是一种通过动态更新参数的Beta分布实现奖励自适应归一化的新型策略优化方法。BNPO使归一化过程与变化的策略分布保持同步，从而实现更精确、更低方差的梯度估计，进而提升训练稳定性。理论分析表明BNPO具有降低方差的特性，并证明其在二值奖励设定下可泛化REINFORCE和GRPO方法。此外，我们引入优势分解机制以扩展BNPO在复杂奖励系统中的适用性。实验结果表明，BNPO在推理任务上实现了策略优化方法中最先进的性能。代码已开源：https://github.com/changyi7231/BNPO。

---

## [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)

### Abstract
arXiv:2506.02911v1 Announce Type: cross 
Abstract: Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.

### 摘要
细胞类型注释是分析单细胞RNA测序数据异质性的关键任务。尽管近期的基础模型实现了该过程的自动化，但这些模型通常独立注释细胞，既未考虑批次水平的细胞背景，也未提供解释性推理。相比之下，人类专家常根据领域知识为不同细胞簇分配独特的细胞类型。为模拟这一工作流程，我们提出CellPuzzles任务，其目标是为批量细胞分配唯一的细胞类型。该基准涵盖多种组织、疾病和供体条件，要求通过批次水平的细胞背景推理来确保标签唯一性。我们发现现成的大型语言模型（LLMs）在CellPuzzles上表现欠佳，最佳基线模型（OpenAI的o1）仅达到19.0%的批次水平准确率。为此，我们提出Cell-o1——一个通过监督微调蒸馏推理轨迹训练、并采用批次水平奖励进行强化学习的70亿参数LLM。Cell-o1实现了最先进的性能，较o1提升超73%，且在不同情境下均表现出良好的泛化能力。对训练动态和推理行为的进一步分析，为理解批次水平注释性能和类专家推理的涌现机制提供了见解。代码和数据详见https://github.com/ncbi-nlp/cell-o1。

---

## [HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation](https://arxiv.org/abs/2506.02975)

### Abstract
arXiv:2506.02975v1 Announce Type: cross 
Abstract: With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strategy utilizing prior knowledge to extend capabilities. To address cross-modal compatibility challenges, we introduce feature pre-scaling and multimodal AdaLN techniques. Integrating the proposed technologies, we present the HaploOmni, a new single multimodal transformer. With limited training costs, HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks over advanced unified models. All codes will be made public at https://github.com/Tencent/HaploVLM.

### 摘要
随着语言模型的进步，统一多模态理解与生成取得了显著进展，模型架构也从分离组件发展为统一单模型框架。本文探索了一种高效训练范式，旨在构建适用于多模态理解与生成的单一Transformer模型。具体而言，我们提出利用先验知识进行能力扩展的多模态预热策略。针对跨模态兼容性挑战，我们引入了特征预缩放和多模态自适应层归一化技术。通过整合这些技术，我们提出了新型单模态Transformer模型HaploOmni。在有限训练成本下，HaploOmni在多个图像视频理解与生成基准测试中超越了先进统一模型的竞争性能。所有代码将在https://github.com/Tencent/HaploVLM公开。

---

## [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)

### Abstract
arXiv:2506.02959v1 Announce Type: cross 
Abstract: The misuse of large language models (LLMs) poses potential risks, motivating the development of machine-generated text (MGT) detection. Existing literature primarily concentrates on binary, document-level detection, thereby neglecting texts that are composed jointly by human and LLM contributions. Hence, this paper explores the possibility of fine-grained MGT detection under human-AI coauthoring. We suggest fine-grained detectors can pave pathways toward coauthored text detection with a numeric AI ratio. Specifically, we propose a dataset, HACo-Det, which produces human-AI coauthored texts via an automatic pipeline with word-level attribution labels. We retrofit seven prevailing document-level detectors to generalize them to word-level detection. Then we evaluate these detectors on HACo-Det on both word- and sentence-level detection tasks. Empirical results show that metric-based methods struggle to conduct fine-grained detection with a 0.462 average F1 score, while finetuned models show superior performance and better generalization across domains. However, we argue that fine-grained co-authored text detection is far from solved. We further analyze factors influencing performance, e.g., context window, and highlight the limitations of current methods, pointing to potential avenues for improvement.

### 摘要
大型语言模型（LLMs）的滥用存在潜在风险，这推动了机器生成文本（MGT）检测技术的发展。现有研究主要集中于文档级的二元检测，因而忽视了人类与LLM共同创作的文本。为此，本文探索人机协作场景下细粒度MGT检测的可能性。我们提出细粒度检测器可为具有数值化AI占比的协作文本检测开辟路径。具体而言，我们构建了HACo-Det数据集，该数据集通过自动化流程生成带有词级归属标签的人机协作文本。我们改造了七种主流文档级检测器，使其适用于词级检测任务，并在HACo-Det上对这些检测器进行了词级和句级检测评估。实验结果表明，基于度量的方法在细粒度检测中表现不佳（平均F1值为0.462），而微调模型则展现出优越性能及更强的跨领域泛化能力。然而，我们认为细粒度协作文本检测尚未得到根本解决。我们进一步分析了上下文窗口等影响性能的因素，指出当前方法的局限性，并提出了可能的改进方向。

---

## [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/abs/2506.03035)

### Abstract
arXiv:2506.03035v1 Announce Type: cross 
Abstract: Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.
  In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length.

### 摘要
理解用户查询是家庭助手、预订系统和推荐应用等诸多功能的基础。因此，开发准确的口语理解（SLU）方法对确保系统可靠性至关重要。当前最先进的SLU技术依赖于大量训练数据，但针对特定任务或语言的标注样本往往十分有限。与此同时，经过指令微调的大语言模型（LLM）在少量样本场景下，当获得适当提示时，能在未见任务中展现出卓越性能。本研究提出通过信息检索（IR）方法进行示例选择，构建增强型提示并应用于SLU任务。我们在多个SLU基准测试中评估了该方法的有效性，实验结果表明：在不增加提示长度的前提下，词汇级IR方法能显著提升模型性能。

---

## [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)

### Abstract
arXiv:2506.02951v1 Announce Type: cross 
Abstract: Large Language Model (LLM) based multi-agent systems have shown remarkable performance in various tasks, especially when enhanced through collaborative communication. However, current methods often rely on a fixed number of agents and static communication structures, limiting their ability to adapt to varying task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a novel task-adaptive multi-agent collaboration framework that jointly optimizes agent quantity (hard-pruning) and communication topology (soft-pruning). Specifically, our method employs a two-stage training strategy: firstly, independently training soft-pruning networks for different agent quantities to determine optimal agent-quantity-specific complete graphs and positional masks across specific tasks; and then jointly optimizing hard-pruning and soft-pruning within a maximum complete graph to dynamically configure the number of agents and their communication topologies per task. Extensive experiments demonstrate that our approach is: (1) High-performing, achieving state-of-the-art results across six benchmarks and consistently generalizes across multiple mainstream LLM architectures, with a increase in performance of $2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized communication topologies tailored to specific tasks, with an extremely high performance in all three task categories (general reasoning, mathematical reasoning, and code generation); (3) Token-economical, having fewer training steps and token consumption at the same time, with a decrease in token consumption of $90\%+$; and (4) Training-efficient, achieving high performance with very few training steps compared with other methods. The performance will surpass the existing baselines after about ten steps of training under six benchmarks.

### 摘要
基于大语言模型（LLM）的多智能体系统在各类任务中展现出卓越性能，尤其在通过协作通信增强时表现尤为突出。然而，现有方法通常依赖固定数量的智能体和静态通信结构，限制了其适应不同任务复杂度的能力。本文提出自适应图剪枝（AGP）框架，这是一种新型任务自适应多智能体协作方法，可同步优化智能体数量（硬剪枝）与通信拓扑结构（软剪枝）。具体而言，本方法采用两阶段训练策略：首先针对不同智能体数量独立训练软剪枝网络，以确定特定任务下最优的智能体数量专属完全图及位置掩码；随后在最大完全图中联合优化硬剪枝与软剪枝，动态配置每项任务的智能体数量及其通信拓扑。大量实验表明我们的方法具有以下特征：（1）高性能，在六个基准测试中取得最先进成果，且能稳定泛化至多种主流LLM架构，性能提升达2.58%∼9.84%；（2）任务自适应性，能针对特定任务动态构建优化的通信拓扑，在通用推理、数学推理和代码生成三类任务中均表现极佳；（3）令牌经济性，在减少90%以上令牌消耗的同时降低训练步数；（4）训练高效性，相比其他方法能以极少的训练步骤实现高性能，在六个基准测试中仅需约十步训练即可超越现有基线。

---

## [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/abs/2506.03051)

### Abstract
arXiv:2506.03051v1 Announce Type: cross 
Abstract: Factuality is a necessary precursor to useful educational tools. As adoption of Large Language Models (LLMs) in education continues of grow, ensuring correctness in all settings is paramount. Despite their strong English capabilities, LLM performance in other languages is largely untested. In this work, we evaluate the correctness of the Llama3.1 family of models in answering factual questions appropriate for middle and high school students. We demonstrate that LLMs not only provide extraneous and less truthful information, but also exacerbate existing biases against rare languages.

### 摘要
事实性是教育工具发挥效用的必要前提。随着大型语言模型(LLM)在教育领域的应用持续扩大，确保其在所有场景下的正确性至关重要。尽管这些模型在英语方面表现优异，但其在其他语言中的性能尚未得到充分验证。本研究评估了Llama3.1系列模型在回答适合初高中学生的事实性问题时的准确性。结果表明，大型语言模型不仅会提供冗余且真实性较低的信息，还会加剧对稀有语言已有的偏见。

---

## [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/abs/2506.03009)

### Abstract
arXiv:2506.03009v1 Announce Type: cross 
Abstract: The assessment of legal problems requires the consideration of a specific legal system and its levels of abstraction, from constitutional law to statutory law to case law. The extent to which Large Language Models (LLMs) internalize such legal systems is unknown. In this paper, we propose and investigate different approaches to condition LLMs at different levels of abstraction in legal systems. This paper examines different approaches to conditioning LLMs at multiple levels of abstraction in legal systems to detect potentially punishable hate speech. We focus on the task of classifying whether a specific social media posts falls under the criminal offense of incitement to hatred as prescribed by the German Criminal Code. The results show that there is still a significant performance gap between models and legal experts in the legal assessment of hate speech, regardless of the level of abstraction with which the models were conditioned. Our analysis revealed, that models conditioned on abstract legal knowledge lacked deep task understanding, often contradicting themselves and hallucinating answers, while models using concrete legal knowledge performed reasonably well in identifying relevant target groups, but struggled with classifying target conducts.

### 摘要
法律问题的评估需要考虑特定法律体系及其抽象层次，从宪法到成文法再到判例法。目前尚不清楚大型语言模型（LLMs）在何种程度上内化了此类法律体系。本文提出并研究了在法律体系不同抽象层次上对LLMs进行条件调节的不同方法，旨在检测可能构成刑事处罚的仇恨言论。我们重点关注社交媒体帖子是否构成德国刑法规定的煽动仇恨罪行的分类任务。结果表明，在仇恨言论的法律评估方面，无论模型采用何种抽象层次的条件调节，其性能与法律专家之间仍存在显著差距。分析发现，基于抽象法律知识调节的模型缺乏对任务的深入理解，经常自相矛盾并产生幻觉性答案；而基于具体法律知识调节的模型在识别相关目标群体方面表现尚可，但在目标行为分类方面仍存在困难。

---

## [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)

### Abstract
arXiv:2506.02987v1 Announce Type: cross 
Abstract: Background: Large language models (LLMs) have demonstrated substantial potential to support clinical practice. Other than Chat GPT4 and its predecessors, few LLMs, especially those of the leading and more powerful reasoning model class, have been subjected to medical specialty examination questions, including in the domain of primary care. This paper aimed to test the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) in primary care education, specifically in answering Member of the Royal College of General Practitioners (MRCGP) style examination questions.
  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest on 25 May 2025. Questions included textual information, laboratory results, and clinical images. Each model was prompted to answer as a GP in the UK and was provided with full question information. Each question was attempted once by each model. Responses were scored against correct answers provided by GP SelfTest.
  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was 99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the same questions was 73.0%.
  Discussion: All models performed remarkably well, and all substantially exceeded the average performance of GPs and GP registrars who had answered the same questions. o3 demonstrated the best performance, while the performances of the other leading models were comparable with each other and were not substantially lower than that of o3. These findings strengthen the case for LLMs, particularly reasoning models, to support the delivery of primary care, especially those that have been specifically trained on primary care clinical data.

### 摘要
背景：大型语言模型（LLMs）已展现出支持临床实践的巨大潜力。除Chat GPT4及其前代模型外，目前仅有少数LLMs（尤其是具备领先推理能力的模型类别）接受过包括初级保健领域在内的医学专科考试题目测试。本研究旨在评估截至2025年5月的四大领先LLMs（o3、Claude Opus 4、Grok3和Gemini 2.5 Pro）在初级保健教育中的表现，具体针对英国皇家全科医师学会（MRCGP）考试风格题目的作答能力。

方法：研究要求o3、Claude Opus 4、Grok3和Gemini 2.5 Pro于2025年5月25日作答从皇家全科医师学会GP自测题库中随机选取的100道选择题。题目包含文本信息、实验室结果和临床图像。各模型均被设定为英国全科医师角色，并获得完整题目信息。每个模型对每道题仅作答一次，答案根据GP自测题库提供的标准答案进行评分。

结果：o3、Claude Opus 4、Grok3和Gemini 2.5 Pro的总得分分别为99.0%、95.0%、95.0%和95.0%。相同题目的人类全科医师平均得分为73.0%。

讨论：所有模型均表现优异，且显著超越回答相同题目的全科医师及培训医师平均水平。o3展现出最佳性能，其他领先模型表现相近，与o3差距不大。这些发现进一步证实了LLMs（特别是推理模型）在支持初级保健服务方面的应用价值，尤其适用于那些经过初级保健临床数据专项训练的模型。

---

## [TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models](https://arxiv.org/abs/2506.03099)

### Abstract
arXiv:2506.03099v1 Announce Type: cross 
Abstract: In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/

### 摘要
本文提出TalkingMachines——一个将预训练视频生成模型转化为实时音频驱动角色动画的高效框架。该框架通过将音频大语言模型（LLM）与我们的视频生成基础模型相集成，实现了自然对话体验。我们的主要贡献包括：（1）将预训练的SOTA图像到视频DiT模型改造为180亿参数的音频驱动虚拟人生成模型；（2）通过从双向教师模型到稀疏因果自回归学生模型的不对称知识蒸馏，实现了无错误累积的无限视频流生成；（3）设计了高吞吐量、低延迟的推理管道，包含多项关键工程优化：a）将DiT与VAE解码器部署在分离设备上；b）利用CUDA流实现设备间通信与计算的高效重叠；c）消除冗余重计算以最大化帧生成吞吐量。演示视频请参见：https://aaxwaz.github.io/TalkingMachines/

---

## [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)

### Abstract
arXiv:2506.03100v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in recent years by aiding the LLM with external knowledge. However, its theoretical aspect has remained mostly unexplored. In this paper, we propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff. Our framework views the retrieved texts as query-dependent noisy in-context examples and recovers the classical in-context learning (ICL) and standard RAG as the limit cases. Our analysis suggests that an intrinsic ceiling on generalization error exists on RAG as opposed to the ICL. Furthermore, our framework is able to model retrieval both from the training data and from external corpora by introducing uniform and non-uniform RAG noise. In line with our theory, we show the sample efficiency of ICL and RAG empirically with experiments on common QA benchmarks, such as Natural Questions and TriviaQA.

### 摘要
检索增强生成（RAG）近年来通过为大型语言模型（LLM）提供外部知识支持，取得了许多实证成功。然而，其理论层面仍大多未被探索。本文提出了首个针对上下文线性回归中RAG的有限样本泛化界，并推导出精确的偏差-方差权衡。我们的框架将检索到的文本视为查询相关的噪声上下文示例，并将经典的上下文学习（ICL）和标准RAG恢复为极限情况。分析表明，与ICL不同，RAG的泛化误差存在内在上限。此外，通过引入均匀和非均匀RAG噪声，我们的框架能够建模从训练数据和外部语料库中的检索。与理论一致，我们在常见问答基准（如Natural Questions和TriviaQA）上的实验验证了ICL和RAG的样本效率。

---

## [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs](https://arxiv.org/abs/2506.03077)

### Abstract
arXiv:2506.03077v1 Announce Type: cross 
Abstract: Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.

### 摘要
在长序列数据上训练语言模型是提升模型处理复杂任务（如长链推理）能力的关键需求。然而随着序列长度增加，即便应用梯度检查点技术，反向传播（BP）过程中存储激活值的内存开销仍会变得极其庞大。为解决这一挑战，我们提出了一种内存高效且精确的反向传播方法StreamBP，该方法沿序列维度对链式法则进行分层线性分解，显著降低了激活值和逻辑值的内存占用。所提方法适用于SFT、GRPO、DPO等常见训练目标。从实现角度看，StreamBP通过利用语言模型的因果结构特性，实现了更低计算浮点操作数和更快反向传播速度。与梯度检查点相比，StreamBP将反向传播的最大序列长度扩展至2.8-5.5倍，同时保持相当甚至更短的反向传播时间。值得注意的是，StreamBP的序列长度扩展能力可直接转化为批量大小扩展以加速训练。我们进一步开发了通信高效的分布式StreamBP方案，有效支持多GPU训练并拓宽其适用性。本方法代码可轻松集成至任何Transformer模型的训练流程，项目地址：https://github.com/Ledzy/StreamBP。

---

## [PoLAR: Polar-Decomposed Low-Rank Adapter Representation](https://arxiv.org/abs/2506.03133)

### Abstract
arXiv:2506.03133v1 Announce Type: cross 
Abstract: We show that low-rank adaptation of large-scale models suffers from a low stable rank that is well below the linear algebraic rank of the subspace, degrading fine-tuning performance. To mitigate the underutilization of the allocated subspace, we propose PoLAR, a parameterization inspired by the polar decomposition that factorizes the low-rank update into two direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory shows that PoLAR yields an exponentially faster convergence rate on a canonical low-rank adaptation problem. Pairing the parameterization with Riemannian optimization leads to consistent gains on three different benchmarks testing general language understanding, commonsense reasoning, and mathematical problem solving with base model sizes ranging from 350M to 27B.

### 摘要
我们证明大规模模型的低秩适配存在稳定秩远低于子空间线性代数秩的问题，这会降低微调性能。为缓解已分配子空间的利用不足问题，我们提出PoLAR参数化方法——该方案受极分解启发，将低秩更新分解为两个约束于Stiefel流形的方向矩阵和一个无约束的尺度矩阵。理论分析表明，PoLAR在典型低秩适配问题上具有指数级更快的收敛速度。将该参数化方法与黎曼优化相结合，在测试通用语言理解、常识推理和数学问题解决的三个基准任务中均取得稳定提升，基础模型规模覆盖3.5亿至270亿参数范围。

---

## [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/abs/2506.03145)

### Abstract
arXiv:2506.03145v1 Announce Type: cross 
Abstract: Neuroscience research publications encompass a vast wealth of knowledge. Accurately retrieving existing information and discovering new insights from this extensive literature is essential for advancing the field. However, when knowledge is dispersed across multiple sources, current state-of-the-art retrieval methods often struggle to extract the necessary information. A knowledge graph (KG) can integrate and link knowledge from multiple sources, but existing methods for constructing KGs in neuroscience often rely on labeled data and require domain expertise. Acquiring large-scale, labeled data for a specialized area like neuroscience presents significant challenges. This work proposes novel methods for constructing KG from unlabeled large-scale neuroscience research corpus utilizing large language models (LLM), neuroscience ontology, and text embeddings. We analyze the semantic relevance of neuroscience text segments identified by LLM for building the knowledge graph. We also introduce an entity-augmented information retrieval algorithm to extract knowledge from the KG. Several experiments were conducted to evaluate the proposed approaches, and the results demonstrate that our methods significantly enhance knowledge discovery from the unlabeled neuroscience research corpus. It achieves an F1 score of 0.84 for entity extraction, and the knowledge obtained from the KG improves answers to over 54% of the questions.

### 摘要
神经科学研究文献蕴含着丰富的知识。从这一庞大文献库中准确检索现有信息并发现新见解，对推动该领域发展至关重要。然而当知识分散于多源数据时，当前最先进的检索方法往往难以提取所需信息。知识图谱（KG）虽能整合多源知识并建立关联，但现有神经科学领域KG构建方法通常依赖标注数据且需要领域专业知识。获取神经科学等专业领域的大规模标注数据面临重大挑战。本研究提出利用大语言模型（LLM）、神经科学本体论和文本嵌入技术，从未标注的大规模神经科学研究语料库构建KG的新方法。我们分析了LLM识别的神经科学文本片段的语义相关性以构建知识图谱，并提出了基于实体增强的信息检索算法从KG中提取知识。通过多项实验评估表明，我们的方法显著提升了从未标注神经科学文献中发现知识的能力：实体抽取F1值达0.84，且从KG获取的知识使超过54%问题的回答质量得到提升。

---

## [SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation](https://arxiv.org/abs/2506.03139)

### Abstract
arXiv:2506.03139v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at https://zju-real.github.io/SVGenius.

### 摘要
大语言模型（LLMs）与多模态大语言模型在SVG处理方面展现出显著潜力，但现有基准测试存在现实场景覆盖不足、复杂度层级缺失以及评估范式碎片化等问题。我们提出SVGenius——一个包含2,377项查询的综合性基准，涵盖理解、编辑与生成三个递进维度。该基准基于24个应用领域的真实数据构建，具有系统化的复杂度分层，通过8类任务和18项指标评估模型性能。我们对22个主流模型进行了全面测评，涵盖不同规模、架构、训练范式及开放程度。分析表明：尽管专有模型显著优于开源模型，但所有模型均随复杂度提升呈现系统性性能衰退，这揭示了现有方法的根本局限；值得注意的是，推理增强训练在突破这些局限方面比单纯规模扩展更有效，然而风格迁移仍是所有模型类型中最具挑战性的能力。SVGenius建立了首个系统化的SVG处理评估框架，为开发更强大的矢量图形模型及推进自动化平面设计应用提供了关键洞见。附录及补充材料（含全部数据与代码）详见https://zju-real.github.io/SVGenius。

---

## [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)

### Abstract
arXiv:2506.03106v1 Announce Type: cross 
Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.

### 摘要
基于数值反馈（如标量奖励）的强化学习（RL）最新进展显著提升了大型语言模型（LLM）的复杂推理能力。尽管取得这些成果，我们仍发现纯数值反馈的RL存在三个关键挑战：性能瓶颈、自我反思效果有限以及持续失败问题。通过实验证明，即使表现进入平台期，经过RL微调的模型仍能借助自然语言形式的批评反馈，在持续失败问题上生成正确改进方案。基于这一发现，我们提出Critique-GRPO——一种融合自然语言与数值反馈的在线RL框架，可实现高效策略优化。该框架使LLM能够同步学习初始响应与批评引导的改进方案，同时保持探索能力。基于Qwen2.5-7B-Base和Qwen3-8B-Base的广泛实验表明，在八项具有挑战性的数学、STEM及通用推理任务中，Critique-GRPO始终优于基于监督学习和RL的微调方法，平均pass@1分数分别提升约4.5%和5%。值得注意的是，该框架甚至超越了融合专家示范的强基线在线RL方法。进一步分析揭示了两项关于策略探索的重要发现：（1）更高熵值并不总能保证探索学习效率；（2）更长响应未必产生更有效的探索。

---

## [Causal Estimation of Tokenisation Bias](https://arxiv.org/abs/2506.03149)

### Abstract
arXiv:2506.03149v1 Announce Type: cross 
Abstract: Modern language models are typically trained over subword sequences, but ultimately define probabilities over character-strings. Ideally, the choice of the tokeniser -- which maps character-strings to subwords -- should not affect the probability assigned to the underlying character-string; in practice, it does. We define this mismatch as tokenisation bias. In this work, we quantify one particular type of tokenisation bias: the effect of including or not a subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the probability a trained model assigns to the corresponding characters (i.e., \textit&#123;``hello''&#125;). Estimating this effect is challenging because each model is trained with only one tokeniser. We address this by framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. Specifically, we exploit the fact that tokenisation algorithms rank subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an arbitrary cutoff point. As such, we can estimate a causal effect by comparing similar subwords around this cutoff. Experimentally, we find that tokenisation consistently affects models' outputs across scales, vocabularies, and tokenisers. Notably, a subword's presence in a small model's vocabulary may increase its characters' probability by up to 17 times, highlighting tokenisation as a key design choice in language modelling.

### 摘要
现代语言模型通常在子词序列上进行训练，但最终定义的是字符串的概率。理想情况下，分词器（将字符串映射为子词）的选择不应影响对底层字符串的概率分配；然而实践中却存在差异。我们将这种不匹配定义为分词偏差。本研究量化了一种特定类型的分词偏差：分词器词汇表中是否包含某个子词（如⟨hello⟩）对训练模型分配给对应字符串（即“hello”）概率的影响。由于每个模型仅使用单一分词器训练，估计该影响具有挑战性。我们通过将分词偏差框架化为因果效应，并采用断点回归设计进行估计来解决该问题。具体而言，我们利用分词算法对子词进行排序并将前K个加入词汇表的特性（其中K为任意截断点），通过比较截断点附近的相似子词来估计因果效应。实验发现，分词行为会持续影响不同规模、词汇表和分词器的模型输出。值得注意的是，子词出现在小模型词汇表中可能使其对应字符串概率提升高达17倍，这表明分词是语言建模中关键的设计选择。

---

## [Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges](https://arxiv.org/abs/2403.15587)

### Abstract
arXiv:2403.15587v2 Announce Type: replace 
Abstract: Social Media and Internet have the potential to be exploited as a source of opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a methodology able to infer opinions and decisions from plain texts, such as reviews published in social media platforms, by means of Sentiment Analysis. Currently, the emergence and potential of Large Language Models (LLMs) lead us to explore new scenarios of automatically understand written texts, also known as natural language processing. This paper analyzes the use of ChatGPT based on prompt design strategies to assist in CDM processes to extract opinions and make decisions. We integrate ChatGPT in CDM processes as a flexible tool that infer the opinions expressed in texts, providing numerical or linguistic evaluations where the decision making models are based on the prompt design strategies. We include a multi-criteria decision making scenario with a category ontology for criteria. We also consider ChatGPT as an end-to-end CDM model able to provide a general opinion and score on the alternatives. We conduct empirical experiments on real data extracted from TripAdvisor, the TripR-2020Large dataset. The analysis of results show a promising branch for developing quality decision making models using ChatGPT. Finally, we discuss the challenges of consistency, sensitivity and explainability associated to the use of LLMs in CDM processes, raising open questions for future studies.

### 摘要
社交媒体和互联网具有作为意见来源以丰富决策支持解决方案的潜力。群体决策（CDM）是一种能够通过情感分析从社交媒体平台发布的评论等纯文本中推断意见与决策的方法论。当前，大型语言模型（LLMs）的出现及其潜力促使我们探索自动理解书面文本（即自然语言处理）的新场景。本文分析了基于提示设计策略的ChatGPT在CDM过程中的应用，以辅助提取意见并形成决策。我们将ChatGPT整合至CDM流程中，作为灵活工具来推断文本所表达的观点，根据提示设计策略为决策模型提供数值化或语言化评估。研究包含一个带有标准类别本体的多准则决策场景，同时将ChatGPT视为能够对备选方案提供总体意见与评分的端到端CDM模型。基于从TripAdvisor平台提取的真实数据（TripR-2020Large数据集）进行了实证实验，结果分析表明利用ChatGPT开发高质量决策模型具有广阔前景。最后，我们讨论了LLMs在CDM过程中应用的稳定性、敏感性与可解释性挑战，提出了未来研究的开放性问题。

---

## [RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG Framework for IT Operations and Maintenance](https://arxiv.org/abs/2410.15805)

### Abstract
arXiv:2410.15805v2 Announce Type: replace 
Abstract: With the ever-increasing demands on Question Answering (QA) systems for IT operations and maintenance, an efficient and supervised fine-tunable framework is necessary to ensure the data security, private deployment and continuous upgrading. Although Large Language Models (LLMs) have notably improved the open-domain QA's performance, how to efficiently handle enterprise-exclusive corpora and build domain-specific QA systems are still less-studied for industrial applications. In this paper, we propose a general and comprehensive framework based on Retrieval Augmented Generation (RAG) and facilitate the whole business process of establishing QA systems for IT operations and maintenance. In accordance with the prevailing RAG method, our proposed framework, named with RAG4ITOps, composes of two major stages: (1) Models Fine-tuning \&amp; Data Vectorization, and (2) Online QA System Process. At the Stage 1, we leverage a contrastive learning method with two negative sampling strategies to fine-tune the embedding model, and design the instruction templates to fine-tune the LLM with a Retrieval Augmented Fine-Tuning method. At the Stage 2, an efficient process of QA system is built for serving. We collect enterprise-exclusive corpora from the domain of cloud computing, and the extensive experiments show that our method achieves superior results than counterparts on two kinds of QA tasks. Our experiment also provide a case for applying the RAG4ITOps to real-world enterprise-level applications.

### 摘要
随着IT运维领域对问答系统需求的日益增长，需要构建一个高效且支持监督微调的框架，以确保数据安全、私有化部署和持续升级能力。尽管大语言模型显著提升了开放域问答性能，但如何高效处理企业专有语料库并构建领域专用问答系统，在工业应用领域仍缺乏深入研究。本文提出一个基于检索增强生成（RAG）的通用综合框架，旨在支持IT运维问答系统全业务流程的构建。遵循主流RAG方法，我们提出的RAG4ITOps框架包含两个核心阶段：（1）模型微调与数据向量化；（2）在线问答系统流程。在第一阶段，我们采用对比学习方法结合两种负采样策略微调嵌入模型，并通过检索增强微调方法设计指令模板来微调大语言模型。第二阶段构建了高效的问答系统服务流程。通过收集云计算领域的企业专有语料库进行实验验证，结果表明我们的方法在两类问答任务上均优于对比方案。本实验也为RAG4ITOps框架在真实企业级应用中的实施提供了实践案例。

---

## [Grounded Persuasive Language Generation for Automated Marketing](https://arxiv.org/abs/2502.16810)

### Abstract
arXiv:2502.16810v2 Announce Type: replace 
Abstract: This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin while maintaining the same level of factual accuracy. Our findings suggest a promising agentic approach to automate large-scale targeted marketing while ensuring factuality of content generation.

### 摘要
本文提出了一种基于大语言模型（LLMs）的智能代理框架，旨在自动化生成具有说服力且基于事实的营销内容，并以房地产房源描述作为核心应用领域。该方法通过三个关键模块实现：（1）基础模块：模拟专家行为预测市场价值特征；（2）个性化模块：使生成内容符合用户偏好；（3）营销模块：确保事实准确性并纳入本地化特征。我们在房地产营销领域开展了系统性人因实验，以潜在购房者作为焦点小组。结果表明，本方法生成的营销描述在保持同等事实准确性的前提下，其受欢迎程度显著优于人类专家撰写的版本。研究发现表明，这种智能代理方法为实现大规模定向营销自动化提供了可行路径，同时保障了内容生成的事实性。

---

## [MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math Problem Mistake Finding by Prompt-Guided LLMs](https://arxiv.org/abs/2503.04291)

### Abstract
arXiv:2503.04291v2 Announce Type: replace 
Abstract: We propose a novel system, MathMistake Checker, designed to automate step-by-step mistake finding in mathematical problems with lengthy answers through a two-stage process. The system aims to simplify grading, increase efficiency, and enhance learning experiences from a pedagogical perspective. It integrates advanced technologies, including computer vision and the chain-of-thought capabilities of the latest large language models (LLMs). Our system supports open-ended grading without reference answers and promotes personalized learning by providing targeted feedback. We demonstrate its effectiveness across various types of math problems, such as calculation and word problems.

### 摘要
我们提出了一种名为MathMistake Checker的新型系统，旨在通过两阶段流程实现数学长答案题目中逐步错误的自动化检测。该系统从教学角度出发，致力于简化评分流程、提升效率并优化学习体验。其整合了包括计算机视觉和最新大语言模型（LLMs）思维链能力在内的先进技术。本系统支持无参考答案的开放式评分，并通过提供针对性反馈促进个性化学习。我们在计算题、应用题等各类数学题目上验证了该系统的有效性。

---

## [The Tug of War Within: Mitigating the Fairness-Privacy Conflicts in Large Language Models](https://arxiv.org/abs/2410.16672)

### Abstract
arXiv:2410.16672v2 Announce Type: replace 
Abstract: Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is critical. Interestingly, we discover a counter-intuitive trade-off phenomenon that enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT) methods significantly decreases its fairness awareness with thousands of samples. To address this issue, inspired by the information theory, we introduce a training-free method to \textbf&#123;S&#125;uppress the \textbf&#123;P&#125;rivacy and fa\textbf&#123;I&#125;rness coupled \textbf&#123;N&#125;eurons (\textbf&#123;SPIN&#125;), which theoretically and empirically decrease the mutual information between fairness and privacy awareness. Extensive experimental results demonstrate that SPIN eliminates the trade-off phenomenon and significantly improves LLMs' fairness and privacy awareness simultaneously without compromising general capabilities, \eg improving Qwen-2-7B-Instruct's fairness awareness by 12.2\% and privacy awareness by 14.0\%. More crucially, SPIN remains robust and effective with limited annotated data or even when only malicious fine-tuning data is available, whereas SFT methods may fail to perform properly in such scenarios. Furthermore, we show that SPIN could generalize to other potential trade-off dimensions. We hope this study provides valuable insights into concurrently addressing fairness and privacy concerns in LLMs and can be integrated into comprehensive frameworks to develop more ethical and responsible AI systems. Our code is available at https://github.com/ChnQ/SPIN.

### 摘要
确保大型语言模型（LLMs）对公平性和隐私的认知至关重要。有趣的是，我们发现了一个反直觉的权衡现象：通过监督微调（SFT）方法增强LLM的隐私认知会显著降低其公平性认知，这种现象在数千个样本中均存在。为解决这一问题，受信息论启发，我们提出了一种无需训练的方法——**抑制隐私与公平耦合神经元（SPIN）**，该方法从理论和实证上降低了公平性与隐私认知之间的互信息。大量实验结果表明，SPIN消除了这种权衡现象，在不损害通用能力的前提下（例如将Qwen-2-7B-Instruct的公平性认知提升12.2%，隐私认知提升14.0%），显著同步提升了LLMs的公平性和隐私认知。更重要的是，在标注数据有限或仅存在恶意微调数据的情况下，SPIN仍能保持稳健性和有效性，而SFT方法在此类场景中可能失效。此外，我们还证明了SPIN可推广至其他潜在的权衡维度。我们希望这项研究为同时解决LLMs中的公平性和隐私问题提供有价值的见解，并能被纳入综合框架以开发更符合伦理和负责任的AI系统。代码已开源：https://github.com/ChnQ/SPIN。

---

## [ThriftLLM: On Cost-Effective Selection of Large Language Models for Classification Queries](https://arxiv.org/abs/2501.04901)

### Abstract
arXiv:2501.04901v3 Announce Type: replace 
Abstract: In recent years, large language models (LLMs) have demonstrated remarkable capabilities in comprehending and generating natural language content, attracting widespread attention in both industry and academia. An increasing number of services offer LLMs for various tasks via APIs. Different LLMs demonstrate expertise in different domains of queries (e.g., text classification queries). Meanwhile, LLMs of different scales, complexities, and performance are priced diversely. Driven by this, several researchers are investigating strategies for selecting an ensemble of LLMs, aiming to decrease overall usage costs while enhancing performance. However, to the best of our knowledge, none of the existing works addresses the problem, how to find an LLM ensemble subject to a cost budget, which maximizes the ensemble performance with guarantees.
  In this paper, we formalize the performance of an ensemble of models (LLMs) using the notion of correctness probability, which we formally define. We develop an approach for aggregating responses from multiple LLMs to enhance ensemble performance. Building on this, we formulate the Optimal Ensemble Selection problem of selecting a set of LLMs subject to a cost budget that maximizes the overall correctness probability. We show that the correctness probability function is non-decreasing and non-submodular and provide evidence that the Optimal Ensemble Selection problem is likely to be NP-hard. By leveraging a submodular function that upper bounds correctness probability, we develop an algorithm called ThriftLLM and prove that it achieves an instance-dependent approximation guarantee with high probability. Our framework functions as a data processing system that selects appropriate LLM operators to deliver high-quality results under budget constraints.

### 摘要
近年来，大型语言模型（LLMs）在理解和生成自然语言内容方面展现出卓越能力，引起了工业界和学术界的广泛关注。越来越多的服务通过API提供各类任务的LLMs解决方案。不同LLMs在不同查询领域（如文本分类查询）表现出专业优势，同时不同规模、复杂度和性能的LLMs定价各异。在此驱动下，部分研究者开始探索LLMs组合选择策略，旨在降低总体使用成本的同时提升性能。然而据我们所知，现有研究均未解决以下核心问题：如何在成本预算约束下选择LLM组合，以保证获得性能最优化的解决方案。

本文通过正确定义的正确概率概念，形式化地描述了LLM组合的性能表现。我们开发了一种聚合多LLM响应的新方法以提升组合性能，在此基础上构建了"最优组合选择"问题模型——即在成本预算约束下选择能最大化整体正确概率的LLM集合。研究表明正确概率函数具有非递减性和非子模性，并证明最优组合选择问题很可能属于NP难问题。通过利用上界正确概率的子模函数，我们开发了名为ThriftLLM的算法，证明该算法能以高概率获得实例相关的近似保证。本框架作为数据处理系统运行，可在预算约束下选择最优LLM算子以提供高质量结果。

---

## [Assurance of AI Systems From a Dependability Perspective](https://arxiv.org/abs/2407.13948)

### Abstract
arXiv:2407.13948v3 Announce Type: replace 
Abstract: We outline the principles of classical assurance for computer-based systems that pose significant risks. We then consider application of these principles to systems that employ Artificial Intelligence (AI) and Machine Learning (ML).
  A key element in this "dependability" perspective is a requirement for thorough understanding of the behavior of critical components, and this is considered infeasible for AI and ML. Hence the dependability perspective aims to minimize trust in AI and ML elements by using "defense in depth" with a hierarchy of less complex systems, some of which may be highly assured conventionally engineered components, to "guard" them. This may be contrasted with the "trustworthy" perspective that seeks to apply assurance to the AI and ML elements themselves.
  In cyber-physical and many other systems, it is difficult to provide guards that do not depend on AI and ML to perceive their environment (e.g., vehicles sharing the road with a self-driving car), so both perspectives are needed and there is a continuum or spectrum between them. We focus on architectures toward the dependability end of the continuum and invite others to consider additional points along the spectrum.
  For guards that require perception using AI and ML, we examine ways to minimize the trust placed in these elements; they include diversity, defense in depth, explanations, and micro-ODDs. We also examine methods to enforce acceptable behavior, given a model of the world. These include classical cyber-physical calculations and envelopes, and normative rules based on overarching principles, constitutions, ethics, or reputation.
  We apply our perspective to autonomous systems, AI systems for specific functions, general-purpose AI such as Large Language Models (LLMs), and Artificial General Intelligence (AGI), and we propose current best practice and conclude with a fourfold agenda for research.

### 摘要
我们阐述了针对具有重大风险的计算机系统的经典保障原则，随后探讨了这些原则在人工智能（AI）与机器学习（ML）系统中的应用。这种"可靠性"视角的核心在于要求深入理解关键组件的行为特性，而这对AI和ML系统被认为难以实现。因此可靠性方法论通过构建多层次防御体系——由复杂度较低的子系统（部分可采用传统高保障工程组件）构成"防护层"来监管AI/ML组件，从而最大限度降低对其的依赖。这与试图直接对AI/ML组件实施保障的"可信性"方法论形成对比。

在信息物理系统及诸多应用场景中，完全脱离AI/ML的环境感知能力构建防护层存在固有困难（例如与自动驾驶车辆共享道路的其他车辆），因此需要两种方法论的协同应用，二者之间存在连续过渡谱系。本文聚焦于可靠性谱系端点的架构设计，同时呼吁学界探索谱系中的其他解决方案。

对于需要依赖AI/ML实现环境感知的防护层，我们研究了最小化其可信需求的技术路径，包括多样性策略、深度防御机制、可解释性框架及微观运行设计域（micro-ODD）方法。在给定世界模型的前提下，我们还探讨了行为合规性保障方法，涵盖传统信息物理计算与安全包络技术，以及基于核心原则、宪章框架、伦理规范或声誉机制的准则体系。

我们将该框架应用于自主系统、专用AI系统、大语言模型（LLM）等通用AI以及人工通用智能（AGI），提出当前最佳实践方案，最终形成包含四个维度的研究议程。

---

## [Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff](https://arxiv.org/abs/2502.20704)

### Abstract
arXiv:2502.20704v4 Announce Type: replace 
Abstract: Speculative Decoding (SD) enforces strict distributional equivalence to the target model when accepting candidate tokens. While it maintains the target model's generation quality, this strict equivalence limits the speedup achievable by SD and prevents users from trading deviations from the target distribution in exchange for further inference speed gains. To address these limitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding algorithm that generalizes SD by accepting candidate tokens based on the divergences between the target and draft model distributions. By allowing for controlled divergence from the target model, FSD enables users to flexibly trade generation quality for inference speed. Across several benchmarks, our method is able to achieve significant runtime improvements of over 5 tokens per second faster than SD at only an approximate 2% absolute reduction in benchmark accuracy. In many cases, FSD is even able to match SD benchmark accuracy at over 2 tokens per second faster, demonstrating that distributional equivalence is not necessary to maintain target model performance. Furthermore, FSD can be seamlessly integrated into existing SD extensions; we demonstrate this by applying FSD to EAGLE-2, greatly enhancing this existing extension's efficiency while allowing it to leverage FSD's tunable quality-speed trade-off.

### 摘要
推测解码（SD）在接受候选标记时严格保持与目标模型的分布等价性。虽然这种严格等价性维护了目标模型的生成质量，但它限制了SD可实现的加速效果，并阻止用户通过偏离目标分布来换取进一步的推理速度提升。为突破这些限制，我们提出模糊推测解码（FSD）——一种通过基于目标模型与草稿模型分布间差异来接受候选标记的解码算法。通过允许与目标模型的可控偏离，FSD使用户能够灵活地在生成质量与推理速度之间进行权衡。在多个基准测试中，本方法能以仅约2%的基准准确率绝对降幅，实现比SD每秒快5个标记以上的显著运行时提升。多数情况下，FSD甚至能在比SD快每秒2个标记以上的同时保持基准准确率相当，证明分布等价性并非维持目标模型性能的必要条件。此外，FSD可无缝集成至现有SD扩展框架；我们通过将其应用于EAGLE-2验证了这一点，在充分利用FSD可调质量-速度权衡优势的同时，显著提升了该现有扩展方案的效率。

---

## [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/abs/2404.16873)

### Abstract
arXiv:2404.16873v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well. In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds. AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs. We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.

### 摘要
大型语言模型（LLMs）易受越狱攻击影响，导致生成不当或有害内容。人工红队测试需耗费大量时间搜索对抗性提示，而自动生成对抗性提示通常会产生语义无意义的攻击，且扩展性较差。本文提出一种创新方法，利用另一个名为AdvPrompter的LLM在数秒内生成人类可读的对抗性提示。AdvPrompter通过交替优化算法训练，能够生成在不改变原指令含义的前提下掩盖输入指令的后缀，从而诱导目标LLM给出有害响应。在主流开源目标LLMs上的实验结果表明，该方法在AdvBench和HarmBench数据集上取得了极具竞争力的效果，并可迁移至闭源黑盒LLMs。研究还表明，利用AdvPrompter生成的对抗性后缀进行训练，是提升LLMs抗越狱攻击鲁棒性的有效策略。

---

## [LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought](https://arxiv.org/abs/2405.06705)

### Abstract
arXiv:2405.06705v3 Announce Type: replace-cross 
Abstract: Self-correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self-correction, recent research has proposed mistake detection as its initial step. However, current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies. To address this challenge, we introduce a unique prompting strategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes. PedCoT consists of pedagogical principles for prompts (PPP) design, two-stage interaction process (TIP) and grounded PedCoT prompts, all inspired by the educational theory of the Bloom Cognitive Model (BCM). We evaluate our approach on two public datasets featuring math problems of varying difficulty levels. The experiments demonstrate that our zero-shot prompting strategy significantly outperforms strong baselines. The proposed method can achieve the goal of reliable mathematical mistake identification and provide a foundation for automatic math answer grading. The results underscore the significance of educational theory, serving as domain knowledge, in guiding prompting strategy design for addressing challenging tasks with LLMs effectively.

### 摘要
自我校正正逐渐成为缓解大语言模型（LLM）幻觉问题的有效方法。为实现高效自我校正，近期研究提出将错误检测作为初始步骤。然而现有文献表明，使用简单提示策略时，LLM往往难以可靠识别推理错误（尤其是数学推理错误）。为此，我们提出一种创新提示策略——教学思维链（PedCoT），其设计灵感源自布鲁姆认知模型（BCM）教育理论，包含三大核心要素：教学原则提示设计（PPP）、两阶段交互流程（TIP）以及基于教学理论的PedCoT提示模板。我们在两个包含不同难度数学题的公开数据集上评估该方法，实验证明这种零样本提示策略显著优于现有基线方法。该方法能可靠实现数学错误识别目标，为自动数学答案评分奠定基础。研究结果凸显了教育理论作为领域知识在指导LLM提示策略设计、有效解决复杂任务方面的重要价值。

---

## [An Effective Approach to Embedding Source Code by Combining Large Language and Sentence Embedding Models](https://arxiv.org/abs/2409.14644)

### Abstract
arXiv:2409.14644v3 Announce Type: replace-cross 
Abstract: The advent of large language models (LLMs) has significantly advanced artificial intelligence (AI) in software engineering (SE), with source code embeddings playing a crucial role in tasks such as source code clone detection and source code clustering. However, existing methods for source code embedding, including those based on LLMs, often rely on costly supervised training or fine-tuning for domain adaptation. This paper proposes a novel approach to embedding source code by combining large language and sentence embedding models. This approach attempts to eliminate the need for task-specific training or fine-tuning and to effectively address the issue of erroneous information commonly found in LLM-generated outputs. To evaluate the performance of our proposed approach, we conducted a series of experiments on three datasets with different programming languages by considering various LLMs and sentence embedding models. The experimental results have demonstrated the effectiveness and superiority of our approach over the state-of-the-art unsupervised approaches, such as SourcererCC, Code2vec, InferCode, TransformCode, and LLM2Vec. Our findings highlight the potential of our approach to advance the field of SE by providing robust and efficient solutions for source code embedding tasks.

### 摘要
大型语言模型（LLM）的出现显著推动了人工智能（AI）在软件工程（SE）领域的进展，其中源代码嵌入在代码克隆检测与代码聚类等任务中发挥着关键作用。然而，现有的源代码嵌入方法（包括基于LLM的技术）通常需要依赖成本高昂的监督训练或领域适应的微调过程。本文提出一种结合大型语言模型与句子嵌入模型的新型源代码嵌入方法，该方法旨在消除任务特定训练或微调的需求，并有效解决LLM生成结果中普遍存在的错误信息问题。为评估所提方法的性能，我们通过选取不同编程语言的三个数据集，结合多种LLM和句子嵌入模型开展系列实验。实验结果表明，相较于SourcererCC、Code2vec、InferCode、TransformCode和LLM2Vec等最先进的无监督方法，本方法展现出显著的有效性与优越性。我们的研究成果凸显了该方法通过为源代码嵌入任务提供稳健高效的解决方案，进而推动软件工程领域发展的潜力。

---

## [How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not](https://arxiv.org/abs/2409.17044)

### Abstract
arXiv:2409.17044v3 Announce Type: replace-cross 
Abstract: The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In speech-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a Speech Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic Speech Recognition and Speech Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM.

### 摘要
大型语言模型（LLM）取得的卓越性能推动了利用其处理广泛任务与多模态输入的研究。在语音转文本（S2T）任务中，新兴解决方案通过适配器模块将语音基础模型（SFM）编码器的输出投影至LLM嵌入空间。然而，目前尚无研究系统评估下游任务性能对各组件（SFM、适配器、LLM）的依赖程度，也未验证适配器的最佳设计是否取决于所选SFM与LLM。为填补这一空白，我们在自动语音识别和语音翻译两大主流S2T任务上，评估了5种适配器模块、2种LLM（Mistral与Llama）及2种SFM（Whisper与SeamlessM4T）的组合效果。实验结果表明：SFM对下游性能起决定性作用，而适配器选择的影响相对有限且取决于特定SFM与LLM的组合。

---

## [Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization](https://arxiv.org/abs/2405.15861)

### Abstract
arXiv:2405.15861v5 Announce Type: replace-cross 
Abstract: Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL significantly challenge its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication-efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations. This paper proposes a novel dimension-free communication algorithm - DeComFL, which leverages the zeroth-order optimization techniques and reduces the communication cost from $\mathscr&#123;O&#125;(d)$ to $\mathscr&#123;O&#125;(1)$ by transmitting only a constant number of scalar values between clients and the server in each round, regardless of the dimension $d$ of the model parameters. Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions. With additional low effective rank assumption, we can further show the convergence rate is independent of the model dimension $d$ as well. Empirical evaluations, encompassing both classic deep learning training and large language model fine-tuning, demonstrate significant reductions in communication overhead. Notably, DeComFL achieves this by transmitting only around 1MB of data in total between the server and a client to fine-tune a model with billions of parameters. Our code is available at https://github.com/ZidongLiu/DeComFL.

### 摘要
联邦学习（FL）为分布式数据源间的协作式隐私保护机器学习提供了前景广阔的框架。然而，FL伴随的高昂通信成本严重制约了其效率。具体而言，在每轮通信中，通信成本随模型维度呈线性增长，这在大模型场景下构成了显著障碍。尽管已有多种高效通信策略，但固有的维度相关通信成本仍是当前FL实现的主要瓶颈。本文提出一种新型无维度通信算法DeComFL，该算法利用零阶优化技术，通过每轮仅在客户端与服务器间传输恒定数量的标量值（与模型参数维度d无关），将通信成本从$\mathscr&#123;O&#125;(d)$降至$\mathscr&#123;O&#125;(1)$。理论上，在非凸函数场景下，我们证明该算法达到了最先进的收敛速率，并在标准假设下展现出客户端数量与本地步数的线性加速特性。附加低有效秩假设后，我们进一步证明收敛速率与模型维度d无关。实证评估涵盖经典深度学习训练与大语言模型微调，结果表明通信开销显著降低。值得注意的是，DeComFL在微调具有数十亿参数模型时，仅需在服务器与客户端间传输总计约1MB数据。代码已开源：https://github.com/ZidongLiu/DeComFL。

---

## [CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming](https://arxiv.org/abs/2410.02677)

### Abstract
arXiv:2410.02677v2 Announce Type: replace-cross 
Abstract: Robust, diverse, and challenging cultural knowledge benchmarks are essential for measuring our progress towards making LMs that are helpful across diverse cultures. We introduce CulturalBench: a set of 1,696 human-written and human-verified questions to assess LMs' cultural knowledge, covering 45 global regions including underrepresented ones like Bangladesh, Zimbabwe, and Peru. Questions are each verified by five independent annotators and span 17 diverse topics ranging from food preferences to greeting etiquette. We construct CulturalBench using methods inspired by Human-AI Red-Teaming. Compared to human performance (92.4% accuracy), the hard version of CulturalBench is challenging even for the best-performing frontier LMs, ranging from 28.7% to 61.5% in accuracy. We find that LMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?), revealing a tendency to overfit to a single answer. Our results indicate that GPT-4o substantially outperform other models across cultures, besting local providers (e.g., Mistral on European culture and DeepSeek on Chinese culture). Across the board, models under-perform on questions related to North Africa, South America and Middle East.

### 摘要
构建稳健、多样且具有挑战性的文化知识基准对于衡量我们在开发跨文化通用语言模型方面的进展至关重要。本文提出CulturalBench：一个包含1,696道人工编写且经人工验证的问题集，用于评估语言模型的文化知识覆盖度，涵盖45个全球区域（包括孟加拉国、津巴布韦和秘鲁等代表性不足地区）。每道问题均通过五名独立标注者验证，涉及从饮食偏好到问候礼仪等17个多样化主题。该基准的构建方法受"人机对抗测试"启发。与人类表现（92.4%准确率）相比，CulturalBench困难版本即使对最先进的尖端语言模型也具有挑战性，其准确率介于28.7%至61.5%之间。研究发现，语言模型在处理具有多个正确答案的复杂问题时表现欠佳（例如"中国人通常使用什么餐具？"），显示出对单一答案的过度拟合倾向。实验结果表明，GPT-4o在不同文化背景下均显著优于其他模型，甚至超越地区性服务提供商（如Mistral在欧洲文化、DeepSeek在中国文化的表现）。所有模型在北非、南美和中东相关问题上均表现不佳。

---

## [In-context learning and Occam's razor](https://arxiv.org/abs/2410.14086)

### Abstract
arXiv:2410.14086v4 Announce Type: replace-cross 
Abstract: A central goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode.

### 摘要
机器学习的一个核心目标是泛化能力。尽管"没有免费午餐定理"指出，在没有额外假设的情况下我们无法获得泛化的理论保证，但在实践中我们观察到，能够解释训练数据的简单模型往往具有最佳泛化性能——这一原则被称为奥卡姆剃刀原理。尽管需要简单模型，当前大多数机器学习方法仅最小化训练误差，最多通过正则化或架构设计间接促进简单性。本文建立了奥卡姆剃刀原理与上下文学习（某些序列模型如Transformer展现出的新兴能力，能够在推理时从序列中的历史观察中学习）之间的联系。具体而言，我们证明了用于训练上下文学习者的下一词预测损失，与一种称为序贯预测编码的数据压缩技术直接等价；最小化该损失等同于联合最小化训练误差和从上下文中隐式学习到的模型复杂度。我们提出的理论及支持该理论的实证实验，不仅为上下文学习提供了规范性解释，还阐明了当前上下文学习方法的缺陷，并指出了改进方向。代码已发布于https://github.com/3rdCore/PrequentialCode。

---

## [Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://arxiv.org/abs/2410.11020)

### Abstract
arXiv:2410.11020v4 Announce Type: replace-cross 
Abstract: Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE. Motivated by the success of reinforcement learning in reasoning tasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs. We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels. PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models outperform GPT-4o by over 4\% on average across sentiment and natural language inference tasks, including gains of 7.3\% on the Mental Health dataset and 10.9\% on SIGA-nli. This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation.

### 摘要
参数规模低于140亿的指令微调大语言模型（LLMs）在自然语言理解（NLU）任务上表现持续欠佳，其在GLUE和SuperGLUE等基准测试中的成绩往往落后于BERT-base等小型模型。受强化学习在推理任务（如DeepSeek）中成功的启发，我们探索采用近端策略优化（PPO）作为提升LLMs自然语言理解能力的框架。我们将NLU任务构建为强化学习环境，将标记生成视为动作序列，并根据与真实标签的对齐程度优化奖励信号。PPO方法持续超越监督微调，在GLUE基准上平均提升6.3分，并分别以38.7分和26.1分的优势超过零样本和少样本提示方法。值得注意的是，在情感分析和自然语言推理任务中，经PPO调优的模型平均表现优于GPT-4o超过4%，其中在心理健康数据集上提升7.3%，在SIGA-nli数据集上提升10.9%。本研究通过将NLU任务重构为强化学习问题，证明了仅需通过简单的终端任务奖励而非大量数据标注即可实现模型适配的新路径。

---

## [A Hitchhiker's Guide to Scaling Law Estimation](https://arxiv.org/abs/2410.11840)

### Abstract
arXiv:2410.11840v2 Announce Type: replace-cross 
Abstract: Scaling laws predict the loss of a target machine learning model by extrapolating from easier-to-train models with fewer parameters or smaller training sets. This provides an efficient way for practitioners and researchers alike to compare pretraining decisions involving optimizers, datasets, and model architectures. Despite the widespread use of scaling laws to model the dynamics of language model training, there has been little work on understanding how to best estimate and interpret them. We collect (and release) a large-scale dataset containing losses and downstream evaluations for 485 previously published pretrained models. We use these to estimate more than 1000 scaling laws, then derive a set of best practices for estimating scaling laws in new model families. We find that fitting scaling laws to intermediate checkpoints of training runs (and not just their final losses) substantially improves accuracy, and that -- all else equal -- estimates of performance are generally most accurate when derived from other models of similar sizes. However, because there is a significant degree of variability across model seeds, training multiple small models is sometimes more useful than training a single large one. Moreover, while different model families differ scaling behavior, they are often similar enough that a target model's behavior can be predicted from a single model with the same architecture, along with scaling parameter estimates derived from other model families.

### 摘要
缩放定律通过外推参数较少或训练集较小的易训练模型，预测目标机器学习模型的损失。这为从业者和研究者提供了一种高效方法，用以比较涉及优化器、数据集和模型架构的预训练决策。尽管缩放定律被广泛用于建模语言模型训练动态，但关于如何最优估计和解释它们的研究却很少。我们收集（并发布）了一个大规模数据集，包含485个已发表预训练模型的损失和下游评估结果。利用这些数据，我们估计了1000多条缩放定律，进而推导出一套在新模型族中估计缩放定律的最佳实践。研究发现：对训练过程中的中间检查点（而不仅是最终损失）拟合缩放定律可显著提高准确性；在其他条件相同的情况下，来自相似规模模型的性能估计通常最准确。然而，由于模型种子间存在显著变异性，训练多个小模型有时比训练单个大模型更有价值。此外，尽管不同模型族的缩放行为存在差异，但它们往往足够相似，因此目标模型的行为可通过单一同架构模型预测，并结合从其他模型族推导的缩放参数估计来实现。

---

## [Self-Evolved Reward Learning for LLMs](https://arxiv.org/abs/2411.00418)

### Abstract
arXiv:2411.00418v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences, playing a pivotal role in the success of conversational models like GPT-4, ChatGPT, and Llama 2. A core challenge in employing RLHF lies in training a reliable reward model (RM), which relies on high-quality labels typically provided by human experts or advanced AI system. These methods can be costly and may introduce biases that affect the language model's responses. As language models improve, human input may become less effective in further enhancing their performance. In this paper, we propose Self-Evolved Reward Learning (SER), a novel approach where the RM generates additional training data to iteratively improve itself. We conducted extensive experiments on multiple datasets such as HH-RLHF and UltraFeedback, using models like Mistral and Llama 3, and compare SER against various baselines. Our results demonstrate that even with limited human-annotated data, learning from self-feedback can robustly enhance RM performance, thereby boosting the capabilities of large language models (LLMs). Resources of this paper can be found at https://aka.ms/ser

### 摘要
基于人类反馈的强化学习（RLHF）是将语言模型与人类偏好对齐的关键技术，对GPT-4、ChatGPT和Llama 2等对话模型的成功发挥了重要作用。应用RLHF的核心挑战在于训练可靠的奖励模型（RM），这通常需要依赖人类专家或先进AI系统提供的高质量标注数据。这些方法成本高昂且可能引入偏见，从而影响语言模型的响应表现。随着语言模型性能提升，人类反馈对其进一步优化的效果可能逐渐减弱。本文提出自演化奖励学习（SER）方法，通过奖励模型自主生成额外训练数据来实现迭代式自我改进。我们在HH-RLHF和UltraFeedback等多个数据集上使用Mistral和Llama 3等模型展开广泛实验，并将SER与多种基线方法进行对比。结果表明，即使在有限人工标注数据条件下，通过自我反馈学习仍能显著提升奖励模型性能，进而增强大语言模型（LLM）的能力。本文相关资源详见https://aka.ms/ser

---

## [EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation](https://arxiv.org/abs/2410.21271)

### Abstract
arXiv:2410.21271v4 Announce Type: replace-cross 
Abstract: While post-training compression techniques effectively reduce the memory footprint, latency, and power consumption of Large Language Models (LLMs), they often result in noticeable accuracy degradation and remain limited by hardware and kernel constraints that restrict supported compression formats ultimately reducing flexibility across a wide range of deployment scenarios. In this work, we propose EoRA, a novel fine-tuning-free method that augments compressed LLMs with low-rank matrices, allowing users to rapidly enhance task-specific performance and freely balance the trade-off between accuracy and computational overhead beyond the constraints of compression formats. EoRA consistently outperforms prior training-free low rank methods in recovering the accuracy of compressed LLMs, achieving notable accuracy improvements (e.g., $\mathbf&#123;10.84\%&#125;$ on ARC-Challenge, $\mathbf&#123;6.74\%&#125;$ on MathQA, and $\mathbf&#123;6.74\%&#125;$ on GSM8K) for LLaMA3-8B compressed to 3-bit. We also introduce an optimized CUDA kernel, accelerating inference by up to 1.4x and reducing memory overhead through quantizing EoRA. Overall, EoRA offers a prompt solution for improving the accuracy of compressed models under varying user requirements, enabling more efficient and flexible deployment of LLMs. Code is available at https://github.com/NVlabs/EoRA.

### 摘要
虽然训练后压缩技术能有效降低大语言模型(LLMs)的内存占用、延迟和功耗，但这些方法通常会导致明显的准确率下降，且受限于硬件和内核约束所支持的压缩格式，最终降低了在各种部署场景中的灵活性。本研究提出EoRA，一种无需微调的新方法，通过为压缩后的LLMs添加低秩矩阵，使用户能快速提升特定任务性能，并自由权衡压缩格式约束之外的准确率与计算开销。EoRA在恢复压缩LLMs准确率方面持续优于现有免训练低秩方法，对压缩至3比特的LLaMA3-8B模型实现了显著准确率提升(例如在ARC-Challenge上提升10.84%、MathQA上6.74%、GSM8K上6.74%)。我们还开发了优化的CUDA内核，通过对EoRA进行量化，将推理速度提升至1.4倍并降低内存开销。总体而言，EoRA为满足不同用户需求下提升压缩模型准确率提供了即时解决方案，实现了LLMs更高效灵活的部署。代码详见https://github.com/NVlabs/EoRA。

---

## [Hyperband-based Bayesian Optimization for Black-box Prompt Selection](https://arxiv.org/abs/2412.07820)

### Abstract
arXiv:2412.07820v2 Announce Type: replace-cross 
Abstract: Optimal prompt selection is crucial for maximizing large language model (LLM) performance on downstream tasks, especially in black-box settings where models are only accessible via APIs. Black-box prompt selection is challenging due to potentially large, combinatorial search spaces, absence of gradient information, and high evaluation cost of prompts on a validation set. We propose HbBoPs, a novel method that combines a structural-aware deep kernel Gaussian Process with Hyperband as a multi-fidelity scheduler to efficiently select prompts. HbBoPs uses embeddings of instructions and few-shot exemplars, treating them as modular components within prompts. This enhances the surrogate model's ability to predict which prompt to evaluate next in a sample-efficient manner. Hyperband improves query-efficiency by adaptively allocating resources across different fidelity levels, reducing the number of validation instances required for evaluating prompts. Extensive experiments across ten diverse benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art methods in both performance and efficiency.

### 摘要
最优提示选择对于最大化大型语言模型（LLM）在下游任务中的性能至关重要，尤其是在仅能通过API访问的黑盒设置下。黑盒提示选择面临三大挑战：可能庞大的组合搜索空间、梯度信息缺失，以及验证集上提示评估的高成本。我们提出HbBoPs方法，通过将结构感知深度核高斯过程与Hyperband多保真度调度器相结合，实现高效提示选择。该方法将指令和少量示例的嵌入向量作为提示中的模块化组件处理，从而增强代理模型以样本高效的方式预测下一个待评估提示的能力。Hyperband通过自适应分配不同保真度级别的资源，减少评估提示所需的验证实例数量，提升查询效率。在十个多样化基准测试和三种LLM上的大量实验表明，HbBoPs在性能和效率上均优于现有最先进方法。

---

## [Generative Emotion Cause Explanation in Multimodal Conversations](https://arxiv.org/abs/2411.02430)

### Abstract
arXiv:2411.02430v2 Announce Type: replace-cross 
Abstract: Multimodal conversation, a crucial form of human communication, carries rich emotional content, making the exploration of the causes of emotions within it a research endeavor of significant importance. However, existing research on the causes of emotions typically employs an utterance selection method within a single textual modality to locate causal utterances. This approach remains limited to coarse-grained assessments, lacks nuanced explanations of emotional causation, and demonstrates inadequate capability in identifying multimodal emotional triggers. Therefore, we introduce a task-\textbf&#123;Multimodal Emotion Cause Explanation in Conversation (MECEC)&#125;. This task aims to generate a summary based on the multimodal context of conversations, clearly and intuitively describing the reasons that trigger a given emotion. To adapt to this task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM combines video clips with detailed explanations of character emotions, helping to explore the causal factors behind emotional expression in multimodal conversations. A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos. By exploiting the contagion effect of facial emotions, FAME-Net effectively captures the emotional causes of individuals engaged in conversations. Our experimental results on the newly constructed dataset show that FAME-Net outperforms several excellent baselines. Code and dataset are available at https://github.com/3222345200/FAME-Net.

### 摘要
多模态对话作为人类交流的重要形式承载着丰富的情感内容，这使得探究其中情绪产生原因成为一项具有重要意义的研究课题。然而，现有关于情绪原因的研究通常采用单一文本模态下的语句选择方法来定位因果语句，这种方法仍局限于粗粒度的评估，缺乏对情绪因果关系的细致解释，且在识别多模态情绪触发因素方面表现不足。为此，我们提出了一项新任务——多模态对话情绪原因解释(MECEC)，该任务旨在根据对话的多模态上下文生成摘要，清晰直观地描述触发特定情绪的原因。为适配该任务，我们在MELD数据集基础上构建了新数据集ECEM，该数据集将视频片段与角色情绪的详细解释相结合，有助于探索多模态对话中情绪表达背后的因果因素。我们进一步提出创新方法FAME-Net，该方法利用大语言模型(LLMs)分析视觉数据，准确解读视频中面部表情传递的情绪信息。通过利用面部情绪的传染效应，FAME-Net能有效捕捉对话参与者的情绪成因。在新构建数据集上的实验结果表明，FAME-Net优于多个优秀基线模型。代码及数据集详见https://github.com/3222345200/FAME-Net。

---

## [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/abs/2412.17451)

### Abstract
arXiv:2412.17451v2 Announce Type: replace-cross 
Abstract: Self-evolving trainin--where models iteratively learn from their own outputs--has emerged as a key approach for complex reasoning tasks, addressing the scarcity of high-quality chain-of-thought data. However, its effectiveness in multimodal reasoning, a domain more intricate than text-only reasoning, remains underexplored, and the understanding of critical factors in this training paradigm remains limited. Furthermore, a central challenge for this training method is performance saturation, which impedes further improvements and scalability. Inspired by reinforcement learning (RL), in this paper, we reframe self-evolving training for multimodal reasoning through the lens of RL, identifying three pivotal factors: Training Method, Reward Model, and Prompt Variation. Through systematic analysis, we establish relatively optimal design principles that significantly enhance multimodal reasoning capabilities. Moreover, delving deeper into training dynamics, we uncover the roots of saturation and propose a new automatic balancing mechanism to mitigate this limitation. Building on these insights, we propose M-STAR (Multimodal Self-evolving Training for Reasoning), a framework that achieves consistent performance gains across models of varying sizes and diverse benchmarks. All resources are made publicly available at https://mstar-lmm.github.io.

### 摘要
自我进化训练——即模型通过迭代学习自身输出——已成为解决高质量思维链数据稀缺性的复杂推理任务关键方法。然而，这种训练在比纯文本推理更复杂的多模态推理领域的有效性仍待探索，且对该训练范式中关键因素的理解仍显不足。此外，该方法面临的核心挑战是性能饱和现象，这阻碍了进一步改进与扩展。受强化学习（RL）启发，本文通过RL视角重构多模态推理的自我进化训练，识别出三个关键因素：训练方法、奖励模型和提示变异。通过系统分析，我们建立了能显著增强多模态推理能力的相对最优设计原则。进一步深入训练动态机制后，我们揭示了饱和现象的根源，并提出新型自动平衡机制以缓解此限制。基于这些发现，我们提出M-STAR（多模态推理自我进化训练框架），该框架在不同规模模型和多样基准测试中均实现了持续性能提升。所有资源已公开于https://mstar-lmm.github.io。

---

## [SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications](https://arxiv.org/abs/2411.04975)

### Abstract
arXiv:2411.04975v2 Announce Type: replace-cross 
Abstract: Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \emph&#123;SuffixDecoding&#125;, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 5.3$\times$, outperforming state-of-the-art methods -- 2.8$\times$ faster than model-based approaches like EAGLE-2/3 and 1.9$\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced at https://github.com/snowflakedb/ArcticInference.

### 摘要
推测解码技术通过利用能够处理多样化用户任务的较小草稿模型，被广泛用于降低大语言模型（LLM）推理的延迟。然而，新兴的人工智能应用（如基于LLM的智能体）呈现出独特的工作负载特征：与多样化的独立请求不同，智能体框架通常会提交重复的推理请求，例如执行相似子任务的多智能体管道或迭代优化输出的自我精炼循环。这些工作负载会产生长且高度可预测的序列，而当前的推测解码方法并未有效利用这一特性。为填补这一空白，我们提出了一种新方法——后缀解码（SuffixDecoding），该方法利用高效的后缀树缓存来自提示和先前输出的长令牌序列。通过自适应地在接受概率高时推测更多令牌、在概率低时推测更少令牌，后缀解码有效利用了长序列推测的机会，同时在机会有限时节省计算资源。在智能体基准测试（包括SWE-Bench和Text-to-SQL）上的评估表明，后缀解码实现了高达5.3×的加速，优于现有最优方法——比基于模型的方法（如EAGLE-2/3）快2.8×，比无模型方法（如令牌回收）快1.9×。后缀解码已在https://github.com/snowflakedb/ArcticInference开源。

---

## [SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](https://arxiv.org/abs/2412.15289)

### Abstract
arXiv:2412.15289v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task such as a masked language model task or an element lookup by position task to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43.

### 摘要
大型语言模型（LLMs）虽已在多项任务中取得显著进展，但其安全对齐问题仍是主要隐患。探索越狱提示可暴露LLMs的脆弱性并为加固措施提供方向。现有方法主要通过设计复杂指令供模型遵循，或依赖多次迭代实现越狱，这些方式可能制约攻击效果与效率。本研究提出一种新型越狱范式——简单辅助任务联动（SATA），能有效规避LLMs安全防护并诱发有害响应。具体而言，SATA首先对恶意查询中的敏感关键词进行掩码处理，生成包含一个或多个[MASK]特殊标记的良性查询；随后采用掩码语言模型任务或按位置元素查找等简单辅助任务对掩码关键词语义进行编码；最终通过辅助任务与掩码查询的联动实现越狱。大量实验表明，SATA取得了最先进的性能表现，各项指标大幅超越基线方法。在AdvBench数据集上，当采用掩码语言模型（MLM）辅助任务时，SATA总体攻击成功率（ASR）达85%，危害评分（HS）为4.57；使用按位置元素查找（ELP）辅助任务时，总体ASR为76%，HS达4.43。

---

## [Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs](https://arxiv.org/abs/2410.16135)

### Abstract
arXiv:2410.16135v3 Announce Type: replace-cross 
Abstract: To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ($\leq 1.3$) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.

### 摘要
迄今为止，2:4稀疏模式仍是唯一可通过GPU稀疏张量核心加速的稀疏模式。然而实践中，2:4稀疏的实际加速比通常较低（≤1.3倍），且要求固定稀疏比例，这意味着其他比例（如4:8、8:16或超过50%的稀疏率）在GPU上无法获得加速。近期研究表明V:N:M稀疏模式有望解决2:4稀疏的这些局限性。但在精度方面，V:N:M稀疏对视觉Transformer和大型语言模型（LLM）等更广泛Transformer模型的影响尚未得到充分研究。此外，V:N:M稀疏相关的特定问题（如如何选择合适的V和M值）仍待解决。本研究系统探究了V:N:M稀疏在视觉模型和LLM中从预训练到下游任务的多任务应用，提出三个关键方法来提升V:N:M稀疏Transformer的适用性和精度：启发式V与M选择策略、V:N:M专用通道置换技术以及三阶段LoRA训练方法。实验结果表明，采用我们的方法后，DeiT-small模型在64:2:5稀疏率下实现无损精度，DeiT-base模型在64:2:8稀疏率下仍保持精度。此外，经64:2:5稀疏微调的LLama2-7B模型在下游任务中表现优于或持平无需训练的2:4稀疏方案。更重要的是，与2:4稀疏相比，V:N:M稀疏Transformer提供了更广泛的加速比-精度权衡空间。总体而言，我们的探索使得V:N:M稀疏能真正成为成本敏感推理场景中Transformer的高效加速解决方案。

---

## [HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases](https://arxiv.org/abs/2412.16311)

### Abstract
arXiv:2412.16311v2 Announce Type: replace-cross 
Abstract: Given a semi-structured knowledge base (SKB), where text documents are interconnected by relations, how can we effectively retrieve relevant information to answer user questions? Retrieval-Augmented Generation (RAG) retrieves documents to assist large language models (LLMs) in question answering; while Graph RAG (GRAG) uses structured knowledge bases as its knowledge source. However, many questions require both textual and relational information from SKB - referred to as "hybrid" questions - which complicates the retrieval process and underscores the need for a hybrid retrieval method that leverages both information. In this paper, through our empirical analysis, we identify key insights that show why existing methods may struggle with hybrid question answering (HQA) over SKB. Based on these insights, we propose HybGRAG for HQA consisting of a retriever bank and a critic module, with the following advantages: (1) Agentic, it automatically refines the output by incorporating feedback from the critic module, (2) Adaptive, it solves hybrid questions requiring both textual and relational information with the retriever bank, (3) Interpretable, it justifies decision making with intuitive refinement path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In experiments on the STaRK benchmark, HybGRAG achieves significant performance gains, with an average relative improvement in Hit@1 of 51%.

### 摘要
给定一个半结构化知识库（SKB），其中文本文档通过关系相互连接，我们如何有效检索相关信息以回答用户问题？检索增强生成（RAG）通过检索文档辅助大语言模型（LLM）进行问答；而图检索增强生成（GRAG）则以结构化知识库作为知识源。然而，许多问题需要同时利用SKB中的文本信息和关系信息——这类问题被称为"混合型"问题——这使得检索过程复杂化，并凸显了对融合两种信息的混合检索方法的需求。本文通过实证分析，揭示了现有方法在处理SKB混合问答（HQA）时可能存在的关键问题。基于这些发现，我们提出HybGRAG混合问答框架，该框架由检索器组和评判模块构成，具有以下优势：（1）自主性：通过评判模块反馈自动优化输出；（2）适应性：利用检索器组处理需要文本与关系信息的混合问题；（3）可解释性：通过直观的优化路径阐明决策过程；（4）高效性：在HQA基准测试中超越所有基线方法。在STaRK基准实验中，HybGRAG实现了显著的性能提升，Hit@1指标平均相对提升达51%。

---

## [Large Language Models to Diffusion Finetuning](https://arxiv.org/abs/2501.15781)

### Abstract
arXiv:2501.15781v2 Announce Type: replace-cross 
Abstract: We propose a new finetuning method to provide pre-trained large language models (LMs) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive ODE solvers. Our method is universally applicable to any foundation model pre-trained with a cross-entropy loss and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method is more effective and fully compatible with traditional finetuning approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks.

### 摘要
我们提出一种新的微调方法，通过扩散框架使预训练大型语言模型（LMs）具备动态调整测试阶段计算量的能力。研究表明，随着扩散步骤的增加，经我们微调的模型能够实现准确率的单调递增，从而直接提升下游任务性能。该方法通过整合强效引导技术，使模型能够专业解答特定领域问题；并利用自适应ODE求解器，自主确定解决给定问题所需的计算量。本方法普遍适用于所有采用交叉熵损失预训练的基础模型，且无需修改原始权重参数，完整保留其优异的单步生成能力。实验证明该方法不仅效果显著，还能与传统微调方式完全兼容，为统一自回归框架与扩散框架的优势开辟了全新的正交研究方向。

---

## [Not Every AI Problem is a Data Problem: We Should Be Intentional About Data Scaling](https://arxiv.org/abs/2501.13779)

### Abstract
arXiv:2501.13779v2 Announce Type: replace-cross 
Abstract: While Large Language Models require more and more data to train and scale, rather than looking for any data to acquire, we should consider what types of tasks are more likely to benefit from data scaling. We should be intentional in our data acquisition. We argue that the shape of the data itself, such as its compositional and structural patterns, informs which tasks to prioritize in data scaling, and shapes the development of the next generation of compute paradigms for tasks where data scaling is inefficient, or even insufficient.

### 摘要
尽管大型语言模型需要越来越多的数据进行训练和扩展，但我们不应盲目获取任何数据，而应考虑哪些类型的任务更可能从数据扩展中受益。我们应当有意识地选择数据获取策略。我们认为，数据本身的形态特征（如组合模式和结构规律）能够揭示哪些任务应优先进行数据扩展，并塑造下一代计算范式的发展方向——这些范式将专门针对数据扩展效率低下甚至效果有限的任务场景。

---

## [Logits are All We Need to Adapt Closed Models](https://arxiv.org/abs/2502.06806)

### Abstract
arXiv:2502.06806v3 Announce Type: replace-cross 
Abstract: Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to Plugin model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models.

### 摘要
许多商用大型语言模型（LLM）通常采用闭源形式，限制开发者只能通过提示调优来使内容生成与特定应用对齐。尽管这些模型目前未提供对词元对数概率的访问权限，我们认为若开放该权限，将能实现比提示工程更强大的适应技术。本文提出一种词元级概率重加权框架，在获得对数概率访问和少量任务特定数据的情况下，可有效引导黑盒LLM生成面向应用的内容。我们的方法从监督分类视角重构下一词元预测问题，证明将黑盒LLM与任务数据对齐可转化为标签噪声校正问题，由此衍生出Plugin模型——一种仅操作于对数概率的自回归概率重加权模型。我们通过理论论证表明仅重加权对数概率即可实现任务适应。基于多数据集、多LLM及多重加权模型的实验验证了方法的有效性，倡导闭源模型应更广泛开放词元对数概率访问权限。

---

## [Unveiling Privacy Risks in LLM Agent Memory](https://arxiv.org/abs/2502.13172)

### Abstract
arXiv:2502.13172v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent designer's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.

### 摘要
大型语言模型（LLM）代理在各类现实应用中日渐普及。它们通过将用户与代理的私有交互记录存储于记忆模块以增强决策能力，同时也为LLM代理引入了新的隐私风险。本研究系统探究了黑盒环境下LLM代理对我们提出的记忆提取攻击（MEXTRA）的脆弱性。为从记忆模块中提取私有信息，我们设计了一种高效攻击提示模板，并基于对LLM代理不同层级的认知提出自动化提示生成方法。在两个典型代理上的实验验证了MEXTRA的有效性。此外，我们从代理设计者和攻击者双重视角探究了影响记忆泄露的关键因素。研究结果揭示了在LLM代理设计与部署中实施有效记忆防护机制的迫切需求。

---

## [Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models](https://arxiv.org/abs/2502.11075)

### Abstract
arXiv:2502.11075v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: https://github.com/TreeAI-Lab/NumericBench.

### 摘要
大语言模型（LLMs）在自然语言处理任务中展现出卓越能力，如文本生成和语义理解。然而，其在数值推理任务（如基础算术、数值检索和数量比较）上的表现却出人意料地欠佳。这种差距源于模型依赖表层统计模式，而非将数字理解为连续量值。现有基准测试主要关注语言能力或结构化数学问题求解，忽视了现实场景所需的基础数值推理能力。为填补这一空白，我们提出NumericBench——一个评估六项基础数值能力的综合基准：数字识别、算术运算、上下文检索、比较、概括和逻辑推理。该基准包含从合成数字列表到爬取的真实数据等多类数据集，针对长上下文、噪声干扰和多步推理等挑战进行设计。通过对GPT-4、DeepSeek等前沿大语言模型的广泛实验，我们揭示了其在数值推理方面持续存在的缺陷，凸显了提升数值感知语言建模的迫切需求。基准测试已发布于：https://github.com/TreeAI-Lab/NumericBench。

---

## [Unnatural Languages Are Not Bugs but Features for LLMs](https://arxiv.org/abs/2503.01926)

### Abstract
arXiv:2503.01926v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving 49.71 win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis, we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words.

### 摘要
研究发现大型语言模型（LLMs）能够处理非人类可读的文本序列（如越狱提示），这一现象通常被视为对齐后LLMs的缺陷。本文通过系统性研究挑战了这一观点，证明非自然语言——对人类难以理解但LLMs仍能保持语义的字符串——蕴含模型可利用的潜在特征。值得注意的是，非自然语言具有的潜在特征可在推理过程中泛化至不同模型和任务。此外，基于非自然语言版本指令数据集微调的模型表现与自然语言训练的模型相当，在长度控制的AlpacaEval 2.0基准测试中，各类基础模型的平均胜率达到49.71%。通过综合分析，我们进一步揭示LLMs通过噪声过滤并从过滤词汇中推断上下文意义来处理非自然语言。

---

## [ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation](https://arxiv.org/abs/2502.09891)

### Abstract
arXiv:2502.09891v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has proven effective in integrating external knowledge into large language models (LLMs) for solving question-answer (QA) tasks. The state-of-the-art RAG approaches often use the graph data as the external data since they capture the rich semantic information and link relationships between entities. However, existing graph-based RAG approaches cannot accurately identify the relevant information from the graph and also consume large numbers of tokens in the online retrieval process. To address these issues, we introduce a novel graph-based RAG approach, called Attributed Community-based Hierarchical RAG (ArchRAG), by augmenting the question using attributed communities, and also introducing a novel LLM-based hierarchical clustering method. To retrieve the most relevant information from the graph for the question, we build a novel hierarchical index structure for the attributed communities and develop an effective online retrieval method. Experimental results demonstrate that ArchRAG outperforms existing methods in both accuracy and token cost. Moreover, ArchRAG has been successfully applied to domain knowledge QA in Huawei Cloud Computing.

### 摘要
检索增强生成（RAG）技术已被证实在将外部知识融入大语言模型（LLM）以解决问答（QA）任务方面具有显著效果。当前最先进的RAG方法通常采用图数据作为外部数据源，因其能捕捉丰富的语义信息及实体间的关联关系。然而，现有基于图的RAG方法既难以准确识别图中的相关信息，又会在在线检索过程中消耗大量token。为解决这些问题，我们提出了一种新型基于图的RAG方法——基于属性社区的层次化RAG（ArchRAG），该方法通过属性社区增强问题表述，并引入了一种基于LLM的新型层次聚类算法。为从图中检索与问题最相关的信息，我们构建了属性社区的新型层次索引结构，并开发了高效的在线检索方法。实验结果表明，ArchRAG在准确率和token消耗量上均优于现有方法。此外，该方法已成功应用于华为云计算领域的专业知识问答系统。

---

## [Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs](https://arxiv.org/abs/2502.11184)

### Abstract
arXiv:2502.11184v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.

### 摘要
多模态大语言模型（MLLMs）通过支持文本与图像交互，拓展了传统语言模型的能力。然而确保这些模型的安全性仍是重大挑战，尤其在准确识别多模态内容是否安全（我们称之为安全感知能力）方面。本文提出首个综合性多模态安全感知基准MMSafeAware，包含29种安全场景下的1500个精心构建的图像-提示对，用于评估MLLMs。该基准包含不安全子集和过度安全子集，分别评估模型识别危险内容的能力与避免过度敏感（可能损害实用性）的能力。基于MMSafeAware对九种主流MLLMs的评估表明，现有模型安全性不足且普遍过度敏感：例如GPT-4V将36.1%的危险输入误判为安全，同时将59.9%的良性输入误判为危险。我们进一步探索了三种提升安全感知的方法（基于提示的方法、视觉对比解码和以视觉为中心的微调推理），但发现均未达到理想效果。研究结果揭示了开发具备鲁棒安全感知能力的MLLMs所面临的深刻挑战，强调该领域需要进一步研究。所有代码与数据将公开以促进后续研究。

---

## [DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models](https://arxiv.org/abs/2503.04472)

### Abstract
arXiv:2503.04472v2 Announce Type: replace-cross 
Abstract: Recent advancements in slow thinking reasoning models have shown exceptional performance in complex reasoning tasks. However, these models often exhibit overthinking (generating redundant reasoning steps for simple problems), leading to excessive computational resource usage. While current mitigation strategies uniformly reduce reasoning tokens, they risk degrading performance on challenging tasks that require extended reasoning. This paper introduces Difficulty-Adaptive Slow Thinking (DAST), a novel framework that enables models to autonomously adjust the length of Chain-of-Thought (CoT) based on problem difficulty. We first propose a Token Length Budget (TLB) metric to quantify difficulty, then leverage budget-aware reward shaping and budget preference optimization to implement DAST. DAST penalizes overlong responses for simple tasks while incentivizing sufficient reasoning for complex problems. Experiments on diverse datasets and model scales demonstrate that DAST effectively mitigates overthinking (reducing token usage by over 30\% on average) while preserving reasoning accuracy on complex problems. Our codes and models are available at https://github.com/AnonymousUser0520/AnonymousRepo01.

### 摘要
近期慢思考推理模型的进展在复杂推理任务中展现出卓越性能。然而这些模型常存在过度思考现象（为简单问题生成冗余推理步骤），导致计算资源过度消耗。现有缓解策略虽能统一减少推理标记数量，却可能损害需要长链推理的困难任务性能。本文提出难度自适应的慢思考框架（DAST），使模型能根据问题难度自主调整思维链长度。我们首先提出标记长度预算（TLB）指标量化难度，进而通过预算感知的奖励塑形和预算偏好优化实现DAST。该框架会惩罚简单任务的冗长响应，同时激励复杂问题的充分推理。多数据集和模型规模的实验表明，DAST在保持复杂问题推理精度的同时，有效缓解过度思考现象（平均减少30%以上的标记使用）。代码和模型已开源：https://github.com/AnonymousUser0520/AnonymousRepo01。

---

## [OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses](https://arxiv.org/abs/2503.10927)

### Abstract
arXiv:2503.10927v3 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) have significantly advanced natural language processing, aligning them with human preferences remains an open challenge. Although current alignment methods rely primarily on explicit feedback, eye-tracking (ET) data offers insights into real-time cognitive processing during reading. In this paper, we present OASST-ETC, a novel eye-tracking corpus capturing reading patterns from 24 participants, while evaluating LLM-generated responses from the OASST1 dataset. Our analysis reveals distinct reading patterns between preferred and non-preferred responses, which we compare with synthetic eye-tracking data. Furthermore, we examine the correlation between human reading measures and attention patterns from various transformer-based models, discovering stronger correlations in preferred responses. This work introduces a unique resource for studying human cognitive processing in LLM evaluation and suggests promising directions for incorporating eye-tracking data into alignment methods. The dataset and analysis code are publicly available.

### 摘要
尽管大型语言模型（LLMs）在自然语言处理领域取得显著进展，如何使其与人类偏好保持一致仍是一个开放性问题。当前对齐方法主要依赖显式反馈，而眼动追踪（ET）数据为阅读过程中的实时认知处理提供了独特视角。本文提出OASST-ETC——一个新颖的眼动追踪语料库，记录了24名参与者在评估OASST1数据集中LLM生成回答时的阅读模式。分析揭示了偏好回答与非偏好回答之间的差异性阅读模式，并与合成眼动追踪数据进行了对比。此外，我们探究了人类阅读指标与多种基于Transformer模型注意力模式的相关性，发现偏好回答中两者相关性更强。本研究不仅为探索LLM评估中的人类认知处理提供了独特资源，同时为将眼动数据融入对齐方法指明了新方向。数据集与分析代码已公开。

---

## [A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos](https://arxiv.org/abs/2502.15806)

### Abstract
arXiv:2502.15806v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, Claude-Sonnet and Gemini-Thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking Claude-Sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.

### 摘要
大型推理模型（LRMs）凭借其卓越的逻辑推理能力已显著超越传统大语言模型（LLMs），但这些改进也带来了更高的安全风险。当遭受越狱攻击时，其生成更具针对性和组织性内容的能力可能导致更严重的危害。尽管有研究声称推理能力能使LRMs对现有LLM攻击具备更高安全性，但这些研究忽视了推理过程本身存在的固有缺陷。为填补这一空白，我们提出首个针对LRMs的越狱攻击方法，利用其高级推理能力衍生的独特漏洞。具体而言，我们引入了一种称为混沌机器的新型组件，通过多样化的一对一映射对攻击提示进行转换。该机器迭代生成的混沌映射被嵌入推理链中，既增强了攻击的变异性和复杂性，也促成了更强大的攻击效果。基于此，我们构建了Mousetrap框架，使攻击投射至类非线性低样本空间，并通过失配泛化实现增强。由于存在更多竞争目标，LRMs会逐渐保持不可预测的迭代推理惯性，最终落入我们的陷阱。Mousetrap攻击o1-mini、Claude-Sonnet和Gemini-Thinking的成功率在我们构建的有害数据集Trotter上分别高达96%、86%和98%。在AdvBench、StrongREJECT和HarmBench等基准测试中，攻击以安全性著称的Claude-Sonnet时，Mousetrap惊人地分别达到87.5%、86.58%和93.13%的成功率。注意：本文包含不当、冒犯性及有害内容。

---

## [Political Neutrality in AI Is Impossible- But Here Is How to Approximate It](https://arxiv.org/abs/2503.05728)

### Abstract
arXiv:2503.05728v2 Announce Type: replace-cross 
Abstract: AI systems often exhibit political bias, influencing users' opinions and decisions. While political neutrality-defined as the absence of bias-is often seen as an ideal solution for fairness and safety, this position paper argues that true political neutrality is neither feasible nor universally desirable due to its subjective nature and the biases inherent in AI training data, algorithms, and user interactions. However, inspired by Joseph Raz's philosophical insight that "neutrality [...] can be a matter of degree" (Raz, 1986), we argue that striving for some neutrality remains essential for promoting balanced AI interactions and mitigating user manipulation. Therefore, we use the term "approximation" of political neutrality to shift the focus from unattainable absolutes to achievable, practical proxies. We propose eight techniques for approximating neutrality across three levels of conceptualizing AI, examining their trade-offs and implementation strategies. In addition, we explore two concrete applications of these approximations to illustrate their practicality. Finally, we assess our framework on current large language models (LLMs) at the output level, providing a demonstration of how it can be evaluated. This work seeks to advance nuanced discussions of political neutrality in AI and promote the development of responsible, aligned language models.

### 摘要
人工智能系统常表现出政治偏见，影响用户的观点与决策。尽管政治中立性——即无偏见状态——常被视为公平与安全的理想解决方案，但本立场论文指出，由于AI训练数据、算法及用户交互中固有的主观性与偏见，真正的政治中立性既不可行，也非普遍可取。然而，受约瑟夫·拉兹哲学观点“中立性……可以是程度问题”（Raz, 1986）的启发，我们认为追求某种程度的中立性对促进平衡的AI交互和减少用户操纵仍至关重要。因此，我们采用“近似”政治中立性这一术语，将焦点从不可企及的绝对标准转向可实现的实践替代方案。我们提出八种技术，在AI概念化的三个层面实现中立性近似，并分析其权衡与实施策略。此外，我们通过两个具体应用案例展示这些近似的实用性。最后，我们在输出层面对现有大语言模型（LLMs）进行框架评估，演示其可验证性。本研究旨在推动关于AI政治中立性的精细化讨论，促进负责任、符合伦理的语言模型发展。

---

## [Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning](https://arxiv.org/abs/2503.13360)

### Abstract
arXiv:2503.13360v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4 points vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.

### 摘要
近年来，大型语言模型（LLMs）在推理能力方面取得显著进展，从思维链（CoT）提示逐步发展到类似OpenAI o1这类面向产品的先进解决方案。在重新实现该模型的过程中，我们发现在需要视觉输入的多模态任务（如几何问题）中，多模态大语言模型（MLLMs）难以持续关注视觉信息——具体表现为随着推理进程推进，模型对视觉信息的注意力逐渐衰减，导致输出过度依赖文本。为探究此现象，我们在长链推理过程中对图像输入进行消融实验：截断中间推理步骤后移除输入图像并重新完成推理流程。在MathVista测试集hard子集上仅观察到约2%的准确率下降，这表明模型的文本输出主导了后续推理过程。基于此发现，我们提出"伴随式视觉条件机制"（TVC），该策略通过将图像输入转移至关键推理阶段，并采用动态剪枝压缩冗余视觉标记，使模型能在整个推理过程中保持对视觉要素的关注。我们的方法在五项数学推理基准测试中平均达到最先进性能（较之前最优水平提升3.4分），证明了TVC对增强多模态推理系统的有效性。

---

## [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)

### Abstract
arXiv:2505.02862v2 Announce Type: replace-cross 
Abstract: Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies.

### 摘要
尽管大型语言模型（LLMs）表现卓越，但其仍易受越狱攻击影响，导致安全机制失效。现有研究多依赖暴力优化或人工设计，难以揭示真实场景中的潜在风险。为此，我们提出新型越狱攻击框架ICRT，其设计灵感源自人类认知的启发式与偏差。通过利用简洁效应，我们采用认知分解法降低恶意提示的复杂度；同时运用关联偏差重组提示，增强语义对齐以有效诱导有害输出。此外，我们提出基于排序的危害性评估指标，突破传统二元成败范式，采用Elo、HodgeRank和Rank Centrality等排序聚合方法，全面量化生成内容的危害程度。实验结果表明，本方法能持续突破主流LLMs的安全机制并生成高风险内容，为越狱攻击风险研究提供新视角，助力构建更强大的防御策略。

---

